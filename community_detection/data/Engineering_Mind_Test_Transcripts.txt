From a back-end perspective if we are ready to integrate. With the front end say sometime next week, then we'll be okay. So basically, you know the basic slack install flow right have a very simple kind of login. I mean installation process takes you to slack and authenticates and gives you the necessary permissions for installs, comes back right and then from the back end all the information we need is ready so that we can go and put the other pages in place right like all the other kind of corner cases. So if we do that then I think we'll be in good shape.


We need to do more regression tests on this admin flow before we deploy it into production, because one thing we need to do back fills and there are few migrations to be run properly. We haven't done the testing on staging on environment which is closer to our current production with the current database set up and everything. If possible, I suggest like his changes to go separately to the production because then will be free to do proper regression testing rather than waiting for our changes to be in. One question on new changes, it it the same API that got overridden, the channel list? Are there any schema migrations involved? 


What we are thinking to do now is, Parshwa tried using App auth JS. It requires using rendering server like localhost for electron, for a short time to handle the redirect and then that server shows up very basic white page. So what that close this browser and view your app something like that. So instead of that since we are like I'm working on custom URL so deep suggested that for sign in what we can do is directly take user to our website front end. And from there the user can sign in and from there we use the term we are able to redirect to. That token to the app. So that okay. Yeah, so we keep that two strategies one will be from if the sign is started from app and other will be sign in started from website. 


Any kind of error from ether like when ether gets the code from you and get negotiates. It can create proper error object that you can just use and determine rather than relying on error equal to query parameters. That is one more reason. I mean, but you can spread this to basically you can without using auth js. Just try it out. Okay, that works just do a POC if you verified then yeah, then you can just go ahead and raise another PR. Okay. So here we don't need any local storage and like how do you share the token back?




Today, I made changes to the config files for almost all services except director and Gateway output Handler. So those are the pending tasks along with the Lambda methods. So only three things are pending. And also what Karthik had said was right now creation of domain names under public hosted zone is manual tasks. So, I was trying to automate it I tried it. But the thing is teraform doesn't allow accessing public hosted zones from different accounts. So for some reason it said, hosted zone not found. So probably for now we have to do it manually and register new domain names under public hosted Zone manually and tested that.


Try those manual experiments because you are in the middle of it probably not a good idea to switch it. But if you can finish at least understand, you know, all the things that we talked about whether you can just manually deploy EC2, EBS have at least two or three node with one dgraph 0 kind of cluster and do some manual Peak, you know poke and Peak kind of things right then at least we'll know that okay, theoretically we can make it and then we can go and say which is the best continuation orchestration, you know approach we want to take because I think at least from initial production level and Karthik and Shawshank you guys can correct me our current approaches to take whatever we've done and just take it all the way to staging and production and then Implement a dgraph in a second pass.  Is that correct?. 


I will probably clean up the issues that I have posted in ml-ai. Basically right now, the current focus that we are doing is as Shivam already mentioned that he is going to work on channel mind scheme, right? And I think Shivam you should separate Nats from performance related changes. We should probably do that. Because what I am going to focus immediately today and tomorrow is going to be updating the API contracts for supporting key phrase extraction for both chapters and regular key words for each transcript and also to update the PIM scoring API so that they're all working and they all except context ID and context instance id so that they can go forward and use it. So I'm just completing the api contract and sharing it with everyone in the GitHub PR so they can just approve and just give thumbs up confirm once they're done so I can start implementation and have to also create an nats supported version it's basically publishing simply request reply pattern. The mode of communication doesn’t change, its synchronous. So. Once that is done here. I'll work with you want to just get that integrated on PIMs and Key-phrase serviced.


Second part is if it works. Well, you don't have to worry about okay because it's being the call is being loaded from meet or ether Bridge or whatever website.The cookie can be set for specific website itself. So you never have to worry about Chrome every time to deprecate anything for cross-domain cookie Creations. Cross domain cookie creation is considered bad practice, but not enforced by Google right now which Apple does so, you know, you will never know when they will deprecate it. Unless we keep proper track of it. 


There were few variables. Like for example all external DNS things are hard coded currently so we can't reuse etherlabs.io in different environments so that I have parameterised. And also there were a few bunch of manual tasks like creation of certificate, creation of hosted zone. I think you need to approve those things. I think there was one request yesterday. And also you need to add the name servers to the GoDaddy account. I am nothing down the steps, I will add it to the document and by the end of today I will run the scripts. Yesterday, when I ran it, there were a lot of errors, they are minimized now. Once this certificate issues are fixed, I will run it again. 


On highest priority we will first handle the UI changes. So we all are working on UI changes. We have split the task into smaller sections so that we can deploy them as soon as possible. After these UI changes are done. I'll start on the Auth part of this front end. So basically web and desktop so that like for authorization whenever it is required user is taken to the sign in page instead of directly to slack. After that, We'll work on menu part. So basically hamburger menu and header for only for like non-video and recording pages and then the trusted developer so that we will add the developer sign-in key in it. We should have timeline related tasks in staging by Monday. 


I start over with the customer service, basically it is a service that we use to keep track of all the customers that we have for Ether. Each customer is basically a unique work space installation at the moment unless there's a specific requirement to treat it differently. So for that there are two tables involved in the customer service, one is the customers table and the other is the admin table. So customers the customer service has at this point, three service methods basically something that can be called from within the either internal service, one is create which is kind of like a nats message that independent connectors can call to create a new customer and store information about the customer etc. The second is once the customer ends up creating a new entity it replies with a created message at which point we'll have to create an S3 bucket for that customer ID, and we have to provision a CDN bucket and we have to make it extra bucket private and Associate the CDN distribution ID with that S3 bucket and that will be part of the customer provision method. Right? So on created will call the customer provision method and once both of these tasks are completed, we emit customer.provisioned event which you know slack can listen to it.


Initially my thought was that we already have an issue where we see memory leak in Janus and over the period of time it kind of accumulates and crashes. So but this is not the the memory leak issue. It's a genuine segmentation fault happened when the websocket connection got disclosed with the client, but somehow was not able to get the core dump for that. Probably let me investigate more on that and see if we could find any clues or basically if we can reproduce that issue.That's one thing. Yeah, probably like if you're going to launch like just wanted to understand from the priority to from a perspective. Like if you want to launch with the support for Janus like shall we do some through testing and get the minimum things done to get the get the thing out or like we support only Zoom. 










So this one is generating it for an organization called Ether and in Ether you say this is the policy that there's only one type of policy you can specify as the default and that's what it is showing here. This does the single binary distribution policy applies to everything that I mentioned earlier, right the ffmpeg and Janus Gateway related projects. So which means for everything else I ran it against this we approve these kinds of licenses and we deny hard GPL licenses and SPL and AGPL Etc and we flag lgpl license saying that hey this is a warning or something. That's the kind of default license specifications that we set for all the repos except for the ones that I specifically mentioned earlier. So then when FOSSA runs the scans it creates some issues. Basically the way it does a scan is a pretty naive. It's a full text search in a way. So it goes to repos and goes through all the texts that it can find and tries to look for any kind of mention of GPL and LGPL or anywhere but doesn't really say whether it can never really say whether those license apply to that project or not. It's just that they were just mentioned casually somewhere so. So basically, this is for example with AV capture these are the concerns that it had. 





Over the past year, Apple has removed or restricted at least 11 of the 17 most downloaded screen-time and parental-control apps, according to an analysis by The New York Times and Sensor Tower, an app-data firm. Apple has also clamped down on a number of lesser-known apps. In some cases, Apple forced companies to remove features that allowed parents to control their children’s devices or that blocked children’s access to certain apps and adult content. In other cases, it simply pulled the apps from its App Store.


The screen-time app makers are the latest companies to suddenly find themselves both competing against Apple and at the mercy of the tech titan. By controlling the iPhone App Store, where companies find some of their most lucrative customers, Apple has unusual power over the fortunes of other corporations. Executives at the app makers believe they are being targeted because their apps could hurt Apple’s business. Apple’s tools, they add, aren’t as aggressive about limiting screen time and don’t provide as many options.


Although the accord was brokered by the United Nations and signed by President Barack Obama, it has never been ratified by the Senate. Experts in arms control note that the accord, even if ratified by the Senate, would not require the United States to alter any existing domestic laws or procedures governing how it sells conventional weapons overseas. Still, Mr. Trump said his decision to sign a letter asking the Senate to send the treaty back to the White House “is a big, big factor,” calling the accord a “badly misguided” arrangement.


Former Vice President Joe Biden is reporting he raised $6.3 million in the first day of his campaign, the most of any of the 2020 Democratic presidential candidates in the first 24 hours after their announcements. In a news release Friday, Biden’s campaign says he raised the money from nearly 97,000 individuals across all 50 states, including 65,000 who weren’t solicited by email. Advertising Biden edged former Texas Rep. Beto O’Rourke’s first-day total of $6.1 million and Vermont Sen. Bernie Sanders’ sum of slightly less than $6 million. Biden attended a fundraiser in Philadelphia on Thursday evening aimed at raising $500,000. Hosts said Friday raised substantially more.


China has invested heavily in infrastructure projects in PoK even before BRI was launched, much to India's discomfiture. India had served several protest letters, both in Beijing as well as Islamabad, about Chinese funded projects in PoK. China had reportedly posted troops in PoK, a move that was sharply criticised by India. Besides Chinese investments in PoK, Sino-Indian ties was adversely impacted by China offering stapled visas to residents to Jammu and Kashmir a few years back and hosting Hurriyat leaders. Beijing has been emphasising of late that India and Pakistan should address the Kashmir issue bilaterally. 


There has been an emperor in Japan for more than 15 centuries, making the Chrysanthemum Throne the world’s oldest continuous monarchy. On Tuesday, the emperor stepped down, yielding to his eldest son in the first abdication in 200 years. This is the family’s story. We know him as Akihito, the emperor of Japan, a gentle figure who championed peace in a nation devastated by war. But she called him Jimmy. It was the autumn of 1946, a year after the end of the Second World War, and he was a 12-year-old boy, the crown prince of a defeated land, sitting in an unheated classroom on the outskirts of Tokyo.


San Francisco is leading a campaign that could bring a wave of change. More and more counties are ending the practice of saddling prisoners with administrative fees, and many are also forgiving the debt of those who have been charged. In August 2018, the County of San Francisco forgave $32 million of those fees. Alameda County followed. Just this month, a California state bill — SB144, the Families Before Fees Act — was introduced by State Senators Holly Mitchell and Robert Hertzberg. During a news conference, Senator Mitchell said the legislation was necessary “to remove economic shackles on those who already paid their debt to society” because the shackling “makes reintegration in their communities, our communities, almost impossible.”


They must do this while also surviving one of the most dangerous neighborhoods in the country — South Central Los Angeles, which the film depicts as a place where violence is ubiquitous, police helicopters hang overhead, almost everyone owns a gun, and Crips prowl the streets day and night just looking for someone to kill. It’s as if the sweet, internal exploration of “Ferris Bueller’s Day Off” and all those big questions about Who am I becoming? were taking place in the middle of “The Hunger Games.”. It felt like South Central was the center of American culture then, with its overt racial tension, frightening gang wars and notorious police department. “Boyz” appeared at a time when that part of the United States was a captivating setting in any medium, and Mr. Singleton captured the vibe deftly, showing how nonviolent teenagers in the hood are victimized by both sadistic gangs and an insensitive police force.




John Cena in this case was not the American professional wrestler but a mangy, tawny shepherd dog who at that moment had his head stuck in the jaws of a rival in a blood-spattered fight, one of 13 held in a stadium on a single day late last month as part of Nowruz, or Persian New Year festivities. John Cena shook his head free and in a frenzy ripped into his foe, a dog named German, until the judge declared a winner and their handlers pulled them apart. Unlike dogfighting in most other countries, these matches were being held openly, with little fear of prosecution. In fact, a policeman named Ahmad Fawad was on duty, wielding a big stick in a not-always successful effort to keep the two-legged animals under control.




I used the official Grav guideline for a multisite setup and created the setup.php(for subdirectory) in Grav's root directory. But unfortunately, this only worked for sites which were at the level of my v1 folder. Sites which are located in folders on a deeper level, as my structure above requires, do not work. Therefore I tried customizing the setup.php, so that it generates the new path correctly. I thought that I matched the path to my folder structure accordingly, but sites are only displayed up to the level of subsitefolder1. So v1/subsitefolder1 works, as it shows the landing page but even the actual URL for the landing page v1/subsitefolder1/landingpage does not work anymore.


I want to use pypy to gain speed for a project I am doing and I need to use opencv and numpy but when trying to install opencv from pycharm interpreter settings I get the following error: A future version of pip will drop support for Python 2.7. ERROR: Could not find a version that satisfies the requirement opencv-python (from versions: none) ERROR: No matching distribution found for opencv-python


WebRTC apps need a service via which they can exchange network and media metadata, a process known as signaling. However, once signaling has taken place, video/audio/data is streamed directly between clients, avoiding the performance cost of streaming via an intermediary server. WebSocket on the other hand is designed for bi-directional communication between client and server. It is possible to stream audio and video over WebSocket (see here for example), but the technology and APIs are not inherently designed for efficient, robust streaming in the way that WebRTC is.


I have a nested form problem. I implemented the nested forms solution form the railscasts 196 & 197. It works if I have no validation errors. So, form renders perfectly when it is loaded, including the nested fields (in the fields_for part). But, the form has validations. When a validation fails, the controller does render :new. Then the form renders the linked model fields ok, but the nested fields are not anymore rendered. Is there a solution for this? My guess is your problem is that in your new action, you are doing the build, which is not in your edit action. When the validation fails, it will render your new action, but won't run your new action.


I'm building a multi-nested form in rails 3. I'm using the formtastic_cocoon gem, but I don't think that has much bearing on this issue. I've got users, users have tasks, tasks have steps. The nesting is users>tasks>steps. I can dynamically add and remove the task fields to the user, and the step fields from the tasks. However, when I submit the form, the user gets tasks, but the task>steps don't get saved to the database.


I just happened to have tried everything on the net. I got tired and just emptied all the files in the lib folder itself i.e the ones compiled using the links provided in the above answer. Finally I don't know why despite the downvotes you've got I tried your suggestion and it worked after a tremendous struggle I put up for a day behind all this.It didn't matter whether I changed the native library location in .bashrc or hadoop-env.sh. 


MapReduce is just a computing framework. HBase has nothing to do with it. That said, you can efficiently put or fetch data to/from HBase by writing MapReduce jobs. Alternatively you can write sequential programs using other HBase APIs, such as Java, to put or fetch the data. But we use Hadoop, HBase etc to deal with gigantic amounts of data, so that doesn't make much sense. Using normal sequential programs would be highly inefficient when your data is too huge. Coming back to the first part of your question, Hadoop is basically 2 things: a Distributed FileSystem (HDFS) + a Computation or Processing framework (MapReduce). Like all other FS, HDFS also provides us storage, but in a fault tolerant manner with high throughput and lower risk of data loss (because of the replication). But, being a FS, HDFS lacks random read and write access. This is where HBase comes into picture. It's a distributed, scalable, big data store, modelled after Google's BigTable. It stores data as key/value pairs.


Hadoop is basically 3 things, a FS (Hadoop Distributed File System), a computation framework (MapReduce) and a management bridge (Yet Another Resource Negotiator). HDFS allows you store huge amounts of data in a distributed (provides faster read/write access) and redundant (provides better availability) manner. And MapReduce allows you to process this huge data in a distributed and parallel manner. But MapReduce is not limited to just HDFS. Being a FS, HDFS lacks the random read/write capability. It is good for sequential data access. And this is where HBase comes into picture. It is a NoSQL database that runs on top your Hadoop cluster and provides you random real-time read/write access to your data.


I'm trying to concatenate two mp4 files using ffmpeg. I need this to be an automatic process hence why I chose ffmpeg. I'm converting the two files into .ts files and then concatenating them and then trying to encode that concated .ts file. The files are h264 and aac encoded and I'm hoping to keep the quality the same or as close to original as possible. Unfortunately I'm getting the following error message coming back from ffmpeg during encoding. This happens about half way through encoding which makes me think that you can't concat two .ts files together and have it work.
