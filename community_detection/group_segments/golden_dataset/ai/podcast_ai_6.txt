{"body":{
        "contextId": "01DBCS96Y867PTEH9Z5B6TAACM",
        "mindId": "01DADP74WFV607KNPCB6VVXGTG",
        "instanceId": "e43a5d5b-412e-4e39-81b1-fb164f73ee35",
        "segments": [
            {
                "id": "04e2080b-f2ab-4201-b4be-7bbd34173843",
                "originalText": "That you are familiar with the various terminologies. Let's dive deeper into the NLP approaches to analyze text Data. These approaches can be interrelated or they can be independently applied depending on the type of data to analyze. Basic text processing it's a way to analyze text and extract keywords that sum up the style or basic context of the text. For example, if the content is religious or fictional categorizing and tagging words. This approach is about finding lexical categories and automatically tagging each word with this word class. For example, tag a word using languages such as Chinese Spanish Etc. Words can also be tagged as adjectives verbs nouns and so on. Classify text with this approach you can identify particular features of language and use them to classify it for example classify the text as Sports politics or technology. Extract information this approach is about identifying the entities and relationships in a text to extract information in a structured way for example date time money and a direction can be used to establish relationships with other words available in the text. ",
                "confidence": 0.897243518,
                "startTime": "2020-01-31T07:56:19Z",
                "endTime": "2020-01-31T07:57:41Z",
                "duration": 82,
                "recordingId": "a3ddbc13-2a7c-4200-8768-c58b60b29596",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:57:42.798674063Z",
                "updatedAt": "2020-01-31T07:58:23.970116309Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "07d2bcc9-3ecc-4a36-81c6-96891f58a3ef",
                "originalText": "is to decode the input strip accents it removes accent present in the code. Tokenizer it overrides this string tokenizer method, but the default value is none. Stop words. It's a built-in stop words list that is used. Max threshold. It indicates the terms or words that appear more than a given threshold value and should therefore be ignored. Men threshold, it indicates the terms or words that appear less than a given threshold value and should therefore be ignored. ",
                "confidence": 0.8848072420000002,
                "startTime": "2020-01-31T08:11:40Z",
                "endTime": "2020-01-31T08:12:19Z",
                "duration": 39,
                "recordingId": "602055a6-632f-48aa-b64d-97476c40577d",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:12:19.679725081Z",
                "updatedAt": "2020-01-31T08:12:40.901156037Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "0ca53093-564d-47fa-8874-336d7c86ef9b",
                "originalText": "P terminology now that you have understood why NLP is so important in recent times. It's time to make yourself comfortable with the NLP terminologies one word boundaries. It determines where one word ends and the other begins to tokenization. It's a technique to split words phrases idioms Etc present in a document. Three stemming. It's a process to map word to their stem or root. It's very useful in finding synonyms and extensively used in search engines. ",
                "confidence": 0.88595465,
                "startTime": "2020-01-31T07:54:54Z",
                "endTime": "2020-01-31T07:55:32Z",
                "duration": 38,
                "recordingId": "3b118ca9-8a68-479b-9d99-b16b1ec86c9a",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:55:33.267637498Z",
                "updatedAt": "2020-01-31T07:55:54.465416472Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "0f850918-7639-4c1f-b058-d682b3d4c218",
                "originalText": "We'll use string class punctuation to remove all the punctuation present in each message as they are also less waited for text analysis, then we'll remove the stop words. ",
                "confidence": 0.86987597,
                "startTime": "2020-01-31T08:01:14Z",
                "endTime": "2020-01-31T08:01:27Z",
                "duration": 13,
                "recordingId": "36aa79dc-6a63-4121-a837-6b651c90a5e7",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:01:27.301649929Z",
                "updatedAt": "2020-01-31T08:01:48.113718317Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "11d761d8-f504-4219-844a-a39852210a5a",
                "originalText": "Let's create its head sequence to analyze will create a variable test sentence and passed the sentence. This is my first test ring. Wow. We are doing just fine. ",
                "confidence": 0.9052722,
                "startTime": "2020-01-31T08:00:57Z",
                "endTime": "2020-01-31T08:01:10Z",
                "duration": 13,
                "recordingId": "8e1df953-62ce-4be7-9e8f-d445e3f4abc4",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:01:10.477933938Z",
                "updatedAt": "2020-01-31T08:01:31.857367663Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "1603562c-c2fd-491c-9658-ab40f91a24e7",
                "originalText": "Let's talk about feature extraction another functionality of the scikit-learn approach. This is a technique to convert the content into numerical vectors to perform machine learning. The feature extraction technique is used mostly in machine learning while dealing with text or image data. There are two major types of feature extraction techniques. The text feature extraction it's used to extract features from text. Data examples include large data sets or documents image feature extraction. It's used to extract image features examples include patch extraction or connectivity graph of an image forming contiguous patches also known as hierarchical clustering bag of words. Now, let's learn about the bag of Words, which is one of the most common text feature extraction techniques. It's used to convert text Data into numerical feature vectors with a fixed size. Let's see how it works by considering a document where we need to extract text features the steps to follow our assign a fixed integer ID to each word by splitting them into several words also known as tokenizing then count the number of occurrences of each word or token finally store it as the value feature in Matrix format. The Matrix presentation on the screen shows how multiple documents and tokens are structured This screen shows have account vectorizer algorithm works in scikit-learn. The goal is to convert a collection of text documents to a matrix of token counts. There are several parameters, which affect the vectorization process. Let's take a look at the components of the signature input content. The input content can be a file name of the sequence of strings which needs to be vectorized. encoding the default encoding is utf-8 the ",
                "confidence": 0.8729885066666667,
                "startTime": "2020-01-31T08:09:40Z",
                "endTime": "2020-01-31T08:11:40Z",
                "duration": 120,
                "recordingId": "d978658f-6669-4cfa-9ecb-a168cf3dca4e",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T08:11:41.736402397Z",
                "updatedAt": "2020-01-31T08:12:23.038932Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "1854052f-b6d8-4ec1-af28-0a7497b716cb",
                "originalText": "That you see here helps create the pipeline for the entire text processing. Now, let's define parameters for grid search. ",
                "confidence": 0.8697141749999999,
                "startTime": "2020-01-31T08:23:47Z",
                "endTime": "2020-01-31T08:23:57Z",
                "duration": 10,
                "recordingId": "73dec864-3780-4c31-a3eb-e711dbe8eba6",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:23:57.955957078Z",
                "updatedAt": "2020-01-31T08:24:18.895324467Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "1950633e-41a3-49c3-940a-e5872c549563",
                "originalText": "We import pipeline from pipeline class. ",
                "confidence": 0.8165333,
                "startTime": "2020-01-31T08:23:15Z",
                "endTime": "2020-01-31T08:23:20Z",
                "duration": 5,
                "recordingId": "569986cd-300d-4ced-a7b0-5a6c988708ca",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:23:20.559672489Z",
                "updatedAt": "2020-01-31T08:23:41.442988426Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "281d85ad-92c3-48e8-8b06-43e8dc623374",
                "originalText": "Troll language processing or NLP is an automated way to understand analyze human languages and extract information from such data by applying machine learning algorithms. The data content can be text document image audio or video. Sometimes it's also referred to as a field of computer science or artificial intelligence to extract the Linguistics information from the underlying data in LP enables machines or computers to derive meaning from human or natural language input. The world is now connected globally due to the advancement of technology and devices. This has resulted in the high volume of Digital Data across the world and has led to a number of challenges in analyzing data including analyzing the tons of data that is generated in the form of text image audios and videos. Identifying approximately 6,500 languages and dialects followed across the globe applying quantitative analysis on huge collections of data. handling ambiguities while interpreting data and extracting information This is where natural language processing proves useful. One of the main goals of natural language processing is to understand various languages process them and extract information from them in NLP full automation can be easily achieved by modern software libraries modules and packages these software libraries and packages are aware of diverse language and culture and categorize data accordingly, which enables understanding Cinema it's better. With the help of these libraries we can also build analytical models and automate the natural language process with minimum or no human interventions. ",
                "confidence": 0.8748470414285716,
                "startTime": "2020-01-31T07:53:01Z",
                "endTime": "2020-01-31T07:54:54Z",
                "duration": 113,
                "recordingId": "08670af8-f608-4a2b-8994-cd6e2049f812",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:55:05.130923468Z",
                "updatedAt": "2020-01-31T07:55:46.784553478Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "358a204b-c3b1-48fc-aea7-b86254869041",
                "originalText": "Then import Stop Words, which we've installed earlier as part of the environment setup. ",
                "confidence": 0.87117475,
                "startTime": "2020-01-31T08:00:11Z",
                "endTime": "2020-01-31T08:00:18Z",
                "duration": 7,
                "recordingId": "bedd85a5-9afd-4623-becf-7252e9102aaf",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:00:18.527664728Z",
                "updatedAt": "2020-01-31T08:00:39.360896828Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "38a75d62-8e9e-4ea6-aea7-58460c83cd6b",
                "originalText": "An ism to perform an exhaustive search on the best parameters which affect the model however this approach has some constraints such as the whole grid search process is subjected to availability of the CPU cores of the system. This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory. ",
                "confidence": 0.9021531,
                "startTime": "2020-01-31T08:19:17Z",
                "endTime": "2020-01-31T08:19:44Z",
                "duration": 27,
                "recordingId": "8a672336-3d9a-4616-8a83-2068ab4e1d16",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:19:44.586282335Z",
                "updatedAt": "2020-01-31T08:20:05.631564532Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "3a861e7b-1736-4924-9827-63f8266e3ad7",
                "originalText": "Let's look at the scikit-learn approach and its features. It's a powerful library with a set of modules to process and analyze natural language data such as text and images and extract information using machine learning algorithms. These are some of the essential features of scikit-learn approach built-in modules. It has built-in modules to load the data sets content and categories will discuss the modules in detail in the upcoming screen. Feature extraction it's a way to extract information from data which can be text or images scikit-learn, 's built-in functions and methods help extract features and attributes from text to data and image data for the purpose of analysis. model training in model training we analyze the content based on particular categories and then train them according to a specific model model training can involve supervised or unsupervised models supervised models and supervised models. The goal is to generate the data and find the right answer in this model training the outcome of new observation and data set is predicted. Unsupervised models in this unsupervised models the response outcome or the label of the data is not known the main objective is to First understand the structure of the data and then identify the pattern in the data in this type of model training. We have to find the predictors that behave in the same fashion or have some familiarity. ",
                "confidence": 0.8567068179999999,
                "startTime": "2020-01-31T08:03:53Z",
                "endTime": "2020-01-31T08:05:30Z",
                "duration": 97,
                "recordingId": "c616c8b1-caf4-46b5-882c-6276890268ca",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:05:31.544590558Z",
                "updatedAt": "2020-01-31T08:05:52.710655195Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "3db7df99-6eb6-4181-aa00-84669e983017",
                "originalText": "Put all three documents together as a list. ",
                "confidence": 0.8606202,
                "startTime": "2020-01-31T08:13:23Z",
                "endTime": "2020-01-31T08:13:27Z",
                "duration": 4,
                "recordingId": "c4a2f033-ee44-405c-80ab-54f1217e7d7f",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:13:27.515037527Z",
                "updatedAt": "2020-01-31T08:13:49.097769994Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "3e7fd556-7076-463f-acda-677ef40bed02",
                "originalText": "This demo will view how to install an LP environment. ",
                "confidence": 0.85990626,
                "startTime": "2020-01-31T07:58:27Z",
                "endTime": "2020-01-31T07:58:32Z",
                "duration": 5,
                "recordingId": "8afd9120-8735-4c31-823a-4efd4892e2bc",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:58:33.023047541Z",
                "updatedAt": "2020-01-31T07:58:53.781864108Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "4ac0bcfc-09d6-4b83-96aa-fa755709e8fc",
                "originalText": "Let's split the sentence into words here. We can see that the list contains a lot of stop words. Let's remove the stop words will use stop words Corpus dot words function with English as an argument and basic python word and split function to clean each word. ",
                "confidence": 0.8942015,
                "startTime": "2020-01-31T08:02:11Z",
                "endTime": "2020-01-31T08:02:29Z",
                "duration": 18,
                "recordingId": "dca967a8-b048-43a5-bee0-de368e0e4366",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:02:29.825737284Z",
                "updatedAt": "2020-01-31T08:02:50.706297627Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "4b462d15-6f4d-414c-b760-7842ef6d1a11",
                "originalText": "Let's navigate through the other models and all packages ensure that stop words status is installed. ",
                "confidence": 0.8609464,
                "startTime": "2020-01-31T07:59:33Z",
                "endTime": "2020-01-31T07:59:40Z",
                "duration": 7,
                "recordingId": "433f9c01-f4b0-479c-8953-dab2807625d8",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:59:41.155281221Z",
                "updatedAt": "2020-01-31T08:00:02.008159519Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "516c0d72-3ac4-4dca-82db-afe84e8d0cae",
                "originalText": "Next import grid search from the sklearn grid search class to perform the grid search. ",
                "confidence": 0.8763118,
                "startTime": "2020-01-31T08:23:05Z",
                "endTime": "2020-01-31T08:23:12Z",
                "duration": 7,
                "recordingId": "1f32f731-bc3c-4823-b89e-321de6dd79c8",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:23:12.68409267Z",
                "updatedAt": "2020-01-31T08:23:33.560222307Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "520cf8c9-498d-4f4a-8a44-25aa796aabc0",
                "originalText": "Import SGD classifier from the linear model. This is to classify the message in the data set. ",
                "confidence": 0.91075355,
                "startTime": "2020-01-31T08:22:52Z",
                "endTime": "2020-01-31T08:23:01Z",
                "duration": 9,
                "recordingId": "55877478-cca6-40c6-acaf-00387c54f94e",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:23:02.18736393Z",
                "updatedAt": "2020-01-31T08:23:23.224425801Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "53949d4e-b7a0-400c-9749-6787eff62314",
                "originalText": "Demo, you learned how to install the NLP environment. In this demo, you'll learn how to perform sentence analysis. Let's start by importing the required Library class string from in ltk Corpus. ",
                "confidence": 0.8696459999999999,
                "startTime": "2020-01-31T07:59:54Z",
                "endTime": "2020-01-31T08:00:10Z",
                "duration": 16,
                "recordingId": "1c478144-3ee2-4d13-9b65-4f22983bbf99",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:00:10.8745175Z",
                "updatedAt": "2020-01-31T08:00:31.67180341Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "54736d9f-7cb6-4251-912a-632a480c81bb",
                "originalText": "Tf-idf it's a numerical value which represents how important the word is to a document or Corpus. Five semantic analytics. It's a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms. Six disambiguation. It's a technique to determine the meaning and a sense of words context versus intent 7 topic models. It's a type of statistical model for finding abstract topics which occur in a collection of documents. ",
                "confidence": 0.86721634,
                "startTime": "2020-01-31T07:55:33Z",
                "endTime": "2020-01-31T07:56:16Z",
                "duration": 43,
                "recordingId": "bf72ab96-5cb6-4366-8bee-98552845490a",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:56:16.620235103Z",
                "updatedAt": "2020-01-31T07:56:37.957711453Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "5494a3cb-1575-4bee-a553-062406bc6ea4",
                "originalText": "Next feature of scikit-learn is model training an important task in model training is to identify the right model for the given data set scikit-learn provides a range of models to choose from and also trains them by using the extracted features from the data set. The choice of model completely depends on the type of data set. The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features. Has and response in supervised learning we make use of all three tasks predict the outcome of new observations and data sets understand which predictors affect the response or outcome and generalize the data and find the right answer generalization is also known as making predictions models can be trained to classify the documents based on the features and response. For example, naive Bayes svm linear regression and KNN Neighbors. Unsupervised learning model in this type of learning the response or the outcome of the label of the data is not known there is no definite or right answer in this model. So here we try to train the model by following a few steps such as understanding the structure of the data first and identify the pattern in the data finding predictors that behave in the same fashion or have some familiarity exploring the data set and the goal is representation of the data. Rotation is also known as extracting structure unsupervised model can also be used to group documents by applying clustering algorithms. For example clustering text documents using k-means. ",
                "confidence": 0.896265665,
                "startTime": "2020-01-31T08:15:33Z",
                "endTime": "2020-01-31T08:17:17Z",
                "duration": 104,
                "recordingId": "b087e79c-b43d-4a9a-9f20-02672281c69c",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:17:18.65319845Z",
                "updatedAt": "2020-01-31T08:17:39.757675235Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "5a3bb0e1-3b96-4e7e-a4e9-45eee2c24343",
                "originalText": "Search consumes a lot of memory and in jobs equals negative one instructs the process to use all CPUs while processing. ",
                "confidence": 0.8267634,
                "startTime": "2020-01-31T08:24:34Z",
                "endTime": "2020-01-31T08:24:44Z",
                "duration": 10,
                "recordingId": "eab3648b-b74d-4f1b-8809-144c08270f6a",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:24:45.159917429Z",
                "updatedAt": "2020-01-31T08:25:06.001584119Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "5b02541f-d705-41ac-8a41-1d0236e66d01",
                "originalText": "Let's print the bag of words hand look at the details. You can see the properties of vectorized object. ",
                "confidence": 0.89131075,
                "startTime": "2020-01-31T08:14:10Z",
                "endTime": "2020-01-31T08:14:18Z",
                "duration": 8,
                "recordingId": "55585cf1-8829-4275-8a1d-b3397a57d19f",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:14:18.254782931Z",
                "updatedAt": "2020-01-31T08:14:39.671677589Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "5dc11789-193f-4ca4-a731-056e6f9e4174",
                "originalText": "Next comes the interesting part which is creating the bag of words for the list of documents. This is done by accessing the fit method and fitting the documents into the vectorized object. ",
                "confidence": 0.8872156,
                "startTime": "2020-01-31T08:13:35Z",
                "endTime": "2020-01-31T08:13:47Z",
                "duration": 12,
                "recordingId": "ad369c2b-2c4e-4107-ad3c-2f3c2afbc64c",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:13:47.891565095Z",
                "updatedAt": "2020-01-31T08:14:08.733697393Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "6289ede2-cc94-49c4-8a0b-7244eaced58c",
                "originalText": "The backslash T indicates that data in the spam data set is tab separated or Tab delimited and set function takes it as a parameter. Also message is a feature which refers to the text present in the data set response is the label or the category of the message. Let's view the first five records of the stamp collection. The head method is used to view the topmost records in a data set. ",
                "confidence": 0.9046583,
                "startTime": "2020-01-31T08:21:49Z",
                "endTime": "2020-01-31T08:22:17Z",
                "duration": 28,
                "recordingId": "fd995020-eeab-4da7-aa2d-f12fac2c2a98",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:22:18.13685637Z",
                "updatedAt": "2020-01-31T08:22:39.313728308Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "690e387d-0592-42ec-9046-5e6d8a02082b",
                "originalText": "Put displays sparse Matrix and it falls under scipy. In this demo, you learned how to use bag of words technique and transformed documents using the transform method. ",
                "confidence": 0.891178775,
                "startTime": "2020-01-31T08:15:20Z",
                "endTime": "2020-01-31T08:15:33Z",
                "duration": 13,
                "recordingId": "26a872b2-663f-44a3-ac5e-5a6383f0e17b",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:15:33.552029646Z",
                "updatedAt": "2020-01-31T08:15:54.400772247Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "6c4efd28-dbcc-4cb8-86a0-2e03aaa795c9",
                "originalText": "Document to we can add today is a very very very pleasant day and we can have some fun fun fun. And in document 3 we can say this was an amazing experience. ",
                "confidence": 0.798012285,
                "startTime": "2020-01-31T08:13:05Z",
                "endTime": "2020-01-31T08:13:20Z",
                "duration": 15,
                "recordingId": "24c47bd7-f809-4085-8c74-74934342113c",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:13:20.943798818Z",
                "updatedAt": "2020-01-31T08:13:41.959060908Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "70ef739b-df5c-4d1b-8805-e5613d7e9ce5",
                "originalText": "This we are done with sin. It's analysis and our sentences free of punctuation and stop words following are the major in alpide libraries used in Python in LT Kang. It's a python-based library for NLP and widely used in the industry to build programs to work with different human languages. Sty kid Lon. It's another powerful open source python package and module 4 in LP. It features various algorithms and is designed for operating with other python libraries like numpy and acai pie. Text blob this library is used for processing Text data and provides simple apis for diving into NLP. Spacing this is another Library which provides multiple useful views of textual meaning and linguistic structure. ",
                "confidence": 0.8252509425000001,
                "startTime": "2020-01-31T08:02:57Z",
                "endTime": "2020-01-31T08:03:51Z",
                "duration": 54,
                "recordingId": "770d6178-ed53-4d8a-8eeb-caf2a4d8b444",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:03:51.746062814Z",
                "updatedAt": "2020-01-31T08:04:13.207924703Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "740833c0-fc13-47d3-82db-c30d4bab26db",
                "originalText": "Let's create an object grid search and pass pipeline parameters jobs and verbose as arguments. ",
                "confidence": 0.8904945,
                "startTime": "2020-01-31T08:24:15Z",
                "endTime": "2020-01-31T08:24:24Z",
                "duration": 9,
                "recordingId": "52d28c0e-586d-4131-8d3f-ff6819b2a9ca",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:24:24.889790674Z",
                "updatedAt": "2020-01-31T08:24:47.22473735Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "7617f8c9-0778-429e-a591-157169b34028",
                "originalText": "We check the bag of words type for the documents. ",
                "confidence": 0.8636667,
                "startTime": "2020-01-31T08:15:12Z",
                "endTime": "2020-01-31T08:15:17Z",
                "duration": 5,
                "recordingId": "85ac14ec-9594-4196-b85d-ac53f7219331",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:15:17.660806702Z",
                "updatedAt": "2020-01-31T08:15:38.429809565Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "7999f002-6896-4e88-9c1c-96fa3d150b6a",
                "originalText": "Bayes is a basic technique for classification of text. It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore. It's very efficient. It's fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents. And language detection multinomial naive Bayes is used when multiple occurrences of the words matter multinomial distribution normally requires integer feature counts. Let's take a quick look at the psyche. It multi nominal in be model class. There are three major parameters in this class Alpha. It's also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior. It indicates whether to learn class prior probabilities. Not if false a uniform prior will be used but as you notice the default is true class prior. The default is none. But if specified the prior class is adjusted according to the data. The next feature of scikit-learn will be looking into is grid search. We use a document classifier to predict the category of a document to try predicting the outcome of a new document. We need to extract the features. A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters. It would be too tedious to run through each parameter and tweak it in such cases a grid search useful the whole data set can be divided into multiple grids like a chessboard and the search can be run on entire grids or a selected combination of Grids, so we use a grid search. ",
                "confidence": 0.886422778,
                "startTime": "2020-01-31T08:17:17Z",
                "endTime": "2020-01-31T08:19:17Z",
                "duration": 120,
                "recordingId": "ff10b0c0-8610-4172-b7cf-74e97eb22711",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:19:18.950968762Z",
                "updatedAt": "2020-01-31T08:20:00.249248139Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "7e4f326b-f583-4dd2-966c-0b383ec8629f",
                "originalText": "Let's check for repeated words in the vocabulary that is built with the get function and then print it. ",
                "confidence": 0.91157407,
                "startTime": "2020-01-31T08:14:51Z",
                "endTime": "2020-01-31T08:14:59Z",
                "duration": 8,
                "recordingId": "8db8ac9b-cdf4-4eb6-8eab-975b46ca5b95",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:14:59.264604287Z",
                "updatedAt": "2020-01-31T08:15:20.129422219Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "7e939963-8230-4982-92da-54c57d8a27e5",
                "originalText": "Don't type in Python in Anaconda prompt command line type in Import in ltk here to install the Corpus Collections and models. ",
                "confidence": 0.90183896,
                "startTime": "2020-01-31T07:58:57Z",
                "endTime": "2020-01-31T07:59:09Z",
                "duration": 12,
                "recordingId": "c71c11a8-c380-46be-94a4-adf68a7b2fba",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:59:09.894544858Z",
                "updatedAt": "2020-01-31T07:59:30.692684596Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "80f384af-2e77-461e-b861-593c93a060b8",
                "originalText": "Search a model can have multiple parameters and it's a powerful way to search parameters affecting the outcome for model training purposes. It's one of the most common ways to adjust the extracted features. ",
                "confidence": 0.90062016,
                "startTime": "2020-01-31T08:06:43Z",
                "endTime": "2020-01-31T08:06:57Z",
                "duration": 14,
                "recordingId": "3d613c86-23c8-4f95-b5d7-f9f6e2cfcad7",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:06:58.016764892Z",
                "updatedAt": "2020-01-31T08:07:19.509024599Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "81dc51ae-5ea4-439b-93a6-ad30b59c17ea",
                "originalText": "Open the Anaconda prompt, please ensure that your system is connected to the internet. Let's go ahead and enter conda install scikit-learn it will install the scikit-learn package enter conda install in ltk. It will install the in ltk package into your python environment. ",
                "confidence": 0.862641985,
                "startTime": "2020-01-31T07:58:33Z",
                "endTime": "2020-01-31T07:58:55Z",
                "duration": 22,
                "recordingId": "936b396d-f769-458b-8b15-ee7e112c0f77",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:58:55.959388748Z",
                "updatedAt": "2020-01-31T07:59:18.339136787Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "867aad19-34c8-4652-972f-63e59155e294",
                "originalText": "Demo will learn how to apply Pipeline and grid search. Let's start by importing the required Library pandas string print and time. ",
                "confidence": 0.8940718,
                "startTime": "2020-01-31T08:21:11Z",
                "endTime": "2020-01-31T08:21:22Z",
                "duration": 11,
                "recordingId": "6c46ab2c-10d7-4f6f-acf8-9efd658a97f1",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:21:22.801364139Z",
                "updatedAt": "2020-01-31T08:21:43.757837784Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "8dca9fee-5c8a-4714-80bd-e34aa2b246e2",
                "originalText": "Are interested in installing stop words Corpus select it and then click download button. It will install the stop words Corpus in your python environment. ",
                "confidence": 0.7790995,
                "startTime": "2020-01-31T07:59:11Z",
                "endTime": "2020-01-31T07:59:22Z",
                "duration": 11,
                "recordingId": "67d3f242-cdbe-4012-8baa-71f36766918f",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:59:22.801988093Z",
                "updatedAt": "2020-01-31T07:59:43.644541166Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "8f7b4256-2c12-41db-9683-3099c187d7ca",
                "originalText": "I can see the first number is the Tuple and the second number is the frequency of words the Tuple here indicates the document number and feature indices of each word, which belongs to the document feature indices are generated from the transform method. This is a feature extraction process and to extract the features. You have to refer to the indexes which is assigned to any particular feature. ",
                "confidence": 0.75649804,
                "startTime": "2020-01-31T08:14:21Z",
                "endTime": "2020-01-31T08:14:47Z",
                "duration": 26,
                "recordingId": "ea7054d8-f46c-4f33-bfcd-cbcf6a323bbd",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:14:47.96354393Z",
                "updatedAt": "2020-01-31T08:15:09.064619276Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "9a2e1a1c-863a-4f84-a3dd-847741a45fff",
                "originalText": "That you have gone through the scikit-learn approach. Let's learn about its built-in modules for loading the data set contents and categories. The diagram shows you how a data set is loaded using methods and how the object is structured in terms of the container folder and it's categories to load files. You have to use the main method scikit-learn datasets that load files. We can load txt files with categories as sub folder names. The folder names are used. And as supervised signal label names, it doesn't try to extract features into a numpy array or sci-fi sparse Matrix. In addition if load content parameter is false. It doesn't try to load the files in memory to use text files in a scikit-learn classification or clustering algorithm. You'll need to use the sklearn feature extraction text module to build a feature extraction Transformer. That suits your problem. A data load object helps load the contents of a data set shown here are the attributes of a data load object. Bunch it contains fields and can be accessed as dict keys or an object Target names. It's a list of requested categories. Data, it refers to an attribute in the memory where files are loaded. Let's see how a data set can be loaded using scikit-learn. There are several data sets that can be loaded. Let's load the digits data set to view the data using built-in methods perform. The following steps One Import the desired data set use the syntax from sklearn datasets import load digits. to load the data set by creating an object as we had discussed earlier a data load object is used to load contents 3 describe the data set using ",
                "confidence": 0.9006869771428571,
                "startTime": "2020-01-31T08:06:59Z",
                "endTime": "2020-01-31T08:08:59Z",
                "duration": 120,
                "recordingId": "32557a39-399c-4d6e-b9d1-29133df16d6e",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T08:09:01.026528382Z",
                "updatedAt": "2020-01-31T08:09:42.379877069Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "9a5f97b6-42be-4c0f-9f8a-17b40d97165b",
                "originalText": "Bes CR function using this function you can view all of the information which describes the data set. Next let's do the data set type use this syntax type digits data set as we can see the type of the data set is bunch to view the data of the digits data set apply the function data the output displays the actual data which is bound to the data set. Let's view the target with the function dot Target. The array is displayed here are the response data which is present in the data set. ",
                "confidence": 0.84456245,
                "startTime": "2020-01-31T08:08:59Z",
                "endTime": "2020-01-31T08:09:38Z",
                "duration": 39,
                "recordingId": "25fd83ed-6c5b-4624-8559-3decee46bdd9",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:09:38.52499722Z",
                "updatedAt": "2020-01-31T08:09:59.500083067Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "9d25f0a6-3e1c-425a-9171-df4c73b4381d",
                "originalText": "World of text analysis stop words usually have little lexical meaning some examples of stop words. Are I me myself we our hours you yours and so on. Now let's view the first 10 stop words present in the stop words Corpus of in ltk. Library. This is done by passing 0 to 10 index position in the parentheses. ",
                "confidence": 0.863330665,
                "startTime": "2020-01-31T08:00:24Z",
                "endTime": "2020-01-31T08:00:52Z",
                "duration": 28,
                "recordingId": "b7929e55-77ac-4e97-98f2-e406c8deb715",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:00:52.646165359Z",
                "updatedAt": "2020-01-31T08:01:13.617298291Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "a190dbad-c600-4c59-8b8d-1ef16cd7ce90",
                "originalText": "Import counter vectorizer to tokenize the document and convert the text into numeric values Also. Let's import tf-idf transformer for tf-idf transformation. ",
                "confidence": 0.9128385,
                "startTime": "2020-01-31T08:22:33Z",
                "endTime": "2020-01-31T08:22:46Z",
                "duration": 13,
                "recordingId": "670f3bd1-c669-4d78-910f-c24a65394b77",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:22:46.834485848Z",
                "updatedAt": "2020-01-31T08:23:07.798949932Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "a3a11c95-03db-4d95-8feb-e66aaa606553",
                "originalText": "Will create three random documents. Let's add some text to the documents document one can have Hi, how are you? ",
                "confidence": 0.8997306,
                "startTime": "2020-01-31T08:12:53Z",
                "endTime": "2020-01-31T08:13:03Z",
                "duration": 10,
                "recordingId": "33eed7c5-63be-4154-82e4-472c9b9bda68",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:13:03.36448899Z",
                "updatedAt": "2020-01-31T08:13:24.154625028Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "aa217665-d5e0-487d-9259-540258ba91f8",
                "originalText": "I'm building. This is a technique in scikit-learn approach to streamline the NLP process in two stages. Let's take a look at the various stages of pipeline learning one vectorization. This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization. The documents are split into words or tokens and each document is assigned a number. To transformation and transformation the approaches to extract features around the word of Interest also in this stage you find the occurrence of each word in a document. Three model training and application. This is required for accurate predictions a model is divided into training and test data sets to optimize the overall process. For example in this stage different algorithms are used to classify text documents and find the sentiment analysis. Performance optimization in this stage. We trained the models to optimize the overall process. ",
                "confidence": 0.8687019325,
                "startTime": "2020-01-31T08:05:31Z",
                "endTime": "2020-01-31T08:06:42Z",
                "duration": 71,
                "recordingId": "d9c26438-8ce6-40b4-9e5b-cd09ea22e84d",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:06:43.313840813Z",
                "updatedAt": "2020-01-31T08:07:04.380886381Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "ae811bf9-4271-4986-8bf6-6a63982c4616",
                "originalText": "Import scikit-learn specific libraries for text processing and creating a pipeline and performing The Grid search. ",
                "confidence": 0.8796914,
                "startTime": "2020-01-31T08:22:22Z",
                "endTime": "2020-01-31T08:22:30Z",
                "duration": 8,
                "recordingId": "a9497665-518e-4a58-86dc-cbd5c1b775c4",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:22:31.015259095Z",
                "updatedAt": "2020-01-31T08:22:51.957626207Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "c434f4e4-632a-4187-97b4-f03ec380a445",
                "originalText": "This demo you'll learn how to process documents using bag of words. Let's start by importing the required Library count vectorizer class. ",
                "confidence": 0.85109261,
                "startTime": "2020-01-31T08:12:21Z",
                "endTime": "2020-01-31T08:12:32Z",
                "duration": 11,
                "recordingId": "9b5ec862-3b60-4d4b-ba27-179be3e7400e",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:12:32.569316194Z",
                "updatedAt": "2020-01-31T08:12:53.52750175Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "c5a346ba-86c0-4c6c-8fcc-29a627310e3f",
                "originalText": "Wise sentence structure in this approach capture formal grammar to describe the structure of a set of sentences. For example find a well formed or ill-formed sentence structure. Build feature based structure through this approach. We get an insight into grammatical categories of the text document. For example, the text features of text based on speech tag or some grammar rules. Analyze the meaning perform quantitative analysis of the given set of data to extract the information for example find entities in the text and try to establish a relationship between them. ",
                "confidence": 0.8403200666666666,
                "startTime": "2020-01-31T07:57:42Z",
                "endTime": "2020-01-31T07:58:25Z",
                "duration": 43,
                "recordingId": "f99335bf-ea11-4b36-afa7-f4e0694b8de7",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:58:25.703538775Z",
                "updatedAt": "2020-01-31T07:58:46.693943372Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "c920660d-4b47-42cb-891a-8685f93b1d67",
                "originalText": "Far we have learned that there are three major steps for analyzing text Data one vectorizer. This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization. The documents are split into words or tokens and each document is assigned a number. Two Transformer during transformation you find the occurrence of each word in a document. This process is primarily used for feature extraction. Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance. For example different algorithms are used to classify text documents and find the sentiment analysis. This is where scikit-learn becomes more productive by creating the pipeline for the data set. A pipeline is essentially a combination of these three steps. So using scikit-learn pipeline can be created and we can train the model using a single command. We just need to put them all together in a single class and create a pipeline to operate on a given data set. The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different. printers ",
                "confidence": 0.822097744,
                "startTime": "2020-01-31T08:19:47Z",
                "endTime": "2020-01-31T08:21:08Z",
                "duration": 81,
                "recordingId": "f351afd0-f992-4422-a828-a1991623552f",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T08:21:09.70600554Z",
                "updatedAt": "2020-01-31T08:21:30.99868156Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "c97f4d0c-97b0-42ee-a8db-e8b7ddeef4e9",
                "originalText": "Print the parameters and run the grid search on the data set for both the message and response after fitting them into the grid search object using the fit method. ",
                "confidence": 0.8962215,
                "startTime": "2020-01-31T08:24:50Z",
                "endTime": "2020-01-31T08:25:01Z",
                "duration": 11,
                "recordingId": "9f942b05-f8d2-4d43-8ffd-c39424966734",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:25:02.056259095Z",
                "updatedAt": "2020-01-31T08:25:23.117321742Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "e0489437-09c9-4e51-a22a-b0ecc71c9bd3",
                "originalText": "You can close the window and go back to jupyter notebook environment. ",
                "confidence": 0.8828274,
                "startTime": "2020-01-31T07:59:48Z",
                "endTime": "2020-01-31T07:59:54Z",
                "duration": 6,
                "recordingId": "ae0ebba2-cb5a-49f8-b6b4-8adae158c816",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:59:54.44156261Z",
                "updatedAt": "2020-01-31T08:00:15.164492434Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "e05cb905-601b-471d-ba02-512a4f07c074",
                "originalText": "Search is a CPU intensive process and it takes a little time since it does an exhaustive search on the entire data set on the passed parameters. ",
                "confidence": 0.8387349,
                "startTime": "2020-01-31T08:25:32Z",
                "endTime": "2020-01-31T08:25:43Z",
                "duration": 11,
                "recordingId": "3e02c088-c38d-461b-8787-09b6d8ee1321",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:25:43.840078914Z",
                "updatedAt": "2020-01-31T08:26:04.600930488Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "e18d7fa5-63d5-4f33-9a54-fe6c318ccfb9",
                "originalText": "Create a pipeline by instantiating the pipeline class and passing vectorize transformed and model classifier has argument. ",
                "confidence": 0.8489707,
                "startTime": "2020-01-31T08:23:24Z",
                "endTime": "2020-01-31T08:23:33Z",
                "duration": 9,
                "recordingId": "6d753746-552f-4e6b-a397-7966c618c587",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:23:34.025171217Z",
                "updatedAt": "2020-01-31T08:23:54.953948435Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "e75412de-1eff-43b9-a73e-1cdf925a5a17",
                "originalText": "Class helps tokenize the document it converts the text into vectors by assigning numeric values to each word. Let's create an object vectorizer by instantiating the count vectorizer class. ",
                "confidence": 0.9128384,
                "startTime": "2020-01-31T08:12:37Z",
                "endTime": "2020-01-31T08:12:50Z",
                "duration": 13,
                "recordingId": "f0c64d39-ee57-43d7-afc9-a94cd08f503c",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:12:51.026087758Z",
                "updatedAt": "2020-01-31T08:13:11.755566253Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "e76c9b1e-00ba-4839-ba68-f032793423de",
                "originalText": "We'll use basic python syntax to display the characters which don't have punctuation as you can see we make use of basic string class and its punctuation function. ",
                "confidence": 0.802947,
                "startTime": "2020-01-31T08:01:43Z",
                "endTime": "2020-01-31T08:01:56Z",
                "duration": 13,
                "recordingId": "394c39d9-5f3b-480d-900f-2a82bb1afb7e",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:01:56.323746958Z",
                "updatedAt": "2020-01-31T08:02:17.314233197Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "e871a7f7-e161-46f0-9308-0e2277d2f16a",
                "originalText": "Bond to accessing the transform method and transform the list of documents. ",
                "confidence": 0.76635784,
                "startTime": "2020-01-31T08:13:57Z",
                "endTime": "2020-01-31T08:14:03Z",
                "duration": 6,
                "recordingId": "cfcdf4a8-54ab-4dbc-8d7c-5c1cf60031bb",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:14:03.590592273Z",
                "updatedAt": "2020-01-31T08:14:24.455649742Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "f020de9f-3646-469b-add2-69d7a918e155",
                "originalText": "The whole sentence with no punctuation. We just completed the first step of the Senate's analysis process. ",
                "confidence": 0.8837481,
                "startTime": "2020-01-31T08:02:01Z",
                "endTime": "2020-01-31T08:02:09Z",
                "duration": 8,
                "recordingId": "41952cc4-76df-4757-9683-99708bd59348",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:02:09.943674588Z",
                "updatedAt": "2020-01-31T08:02:30.852232227Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "f2310317-8450-438e-ad3b-16cada2808e6",
                "originalText": "We're going to use the tf-idf parameter to weigh the terms present in the message. ",
                "confidence": 0.75282305,
                "startTime": "2020-01-31T08:23:59Z",
                "endTime": "2020-01-31T08:24:05Z",
                "duration": 6,
                "recordingId": "fddd5185-1284-4f5b-927b-19b940752791",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:24:06.242705932Z",
                "updatedAt": "2020-01-31T08:24:27.029188395Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "f95338af-3e80-472e-a164-494372b849cf",
                "originalText": "We'll get the Spam data collection with the help of the pandas Library. ",
                "confidence": 0.8663651,
                "startTime": "2020-01-31T08:21:38Z",
                "endTime": "2020-01-31T08:21:45Z",
                "duration": 7,
                "recordingId": "4c06b076-5918-4241-93a3-92091ad4c8ad",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "37624e0f-5264-4b19-b51d-3c3733ad0746",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:21:45.453117725Z",
                "updatedAt": "2020-01-31T08:22:06.316973024Z",
                "deletedAt": null,
                "deleted": false
            }
        ]
    }}