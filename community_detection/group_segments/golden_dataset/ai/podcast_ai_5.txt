{"body":{
        "contextId": "01DBCS96Y867PTEH9Z5B6TAACM",
        "mindId": "01DADP74WFV607KNPCB6VVXGTG",
        "instanceId": "fef93ef8-777d-4787-b00f-c31d22b49bc3",
        "segments": [
            {
                "id": "0055bc75-2657-4694-81d2-0e4564df65ce",
                "originalText": "If you happen to be gone on a given night, let's say yesterday. You were out. You don't know what was for dinner yesterday. You can still predict what's going to be for dinner tonight by thinking back two days ago think what was for dinner then so what would be predicted for you last night? And then you can use that prediction in turn to make a prediction for tonight. So we make use of not only our actual information from yesterday. But also what our prediction was yesterday. So at this point it's helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if I want to describe the weather to you for a given day, I could say Say the high 76 degrees Fahrenheit the lows 43 wins 13 miles an hour. There's going to be a quarter inch of rain and the relative humidity is 83 percent. ",
                "confidence": 0.8553284066666667,
                "startTime": "2020-01-31T07:53:23Z",
                "endTime": "2020-01-31T07:54:21Z",
                "duration": 58,
                "recordingId": "f0763f2a-343f-4dde-8a26-95a7fc8283ae",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:54:22.270918667Z",
                "updatedAt": "2020-01-31T07:55:03.463988271Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "0b0dc278-ae3e-4774-8f76-aca2c6c4fbbf",
                "originalText": "And by a similar method anytime we come across the word saw or a period we know that a name has to come after that. So it will learn to vote very strongly for a name Jane Doug or spot. ",
                "confidence": 0.9043908,
                "startTime": "2020-01-31T07:58:40Z",
                "endTime": "2020-01-31T07:58:57Z",
                "duration": 17,
                "recordingId": "1a598901-78c1-4bf2-aa12-8adbcee4a0b4",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:58:57.390171549Z",
                "updatedAt": "2020-01-31T07:59:18.446750035Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "1368a07c-94f3-478d-a444-3027d5ffabb0",
                "originalText": "Applications of machine learning have gotten a lot of traction in the last few years. There's a couple of big categories that have had wins one is identifying pictures the equivalent of finding cats on the internet and any problem that can be made to look like that and the other is sequence to sequence deflation is can be speech to text or one language to another most of the former are done with convolutional neural networks. Most of the latter are done with recurrent neural networks particularly long short term memory to give an example of how long short-term memory works. We will consider the question of what's For Dinner. Let's say for a minute that you are a very lucky apartment dweller and you have a flatmate who loves to cook dinner every night. He cooks one of three things Sushi waffles or pizza and you would like to be able to predict what you're going to have on a Of a night so you can plan the rest of your days eating accordingly in order to predict what you're going to have for dinner. You set up a neural network the inputs to this neural network are a bunch of items like the day of the week the month of the year whether or not your flatmate was in a late meeting variables that might reasonably affect what you're going to have for dinner. Now if you're new to neural networks, I highly recommend you take a minute and stopped to watch the how neural networks work tutorial. There's a link down in the comment section. If you'd rather not do that right now and you're still not familiar with neural networks, you can think of them as a voting process and so in the neural network that you set up there's a complicated voting process and all of the inputs like day of the week and month of the year go into it and then you try train it on your history of what you've had for dinner. ",
                "confidence": 0.82545971,
                "startTime": "2020-01-31T07:50:13Z",
                "endTime": "2020-01-31T07:52:13Z",
                "duration": 120,
                "recordingId": "b88d2abc-66d6-4d91-8303-0fa02edf0a12",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:52:14.660304531Z",
                "updatedAt": "2020-01-31T07:52:56.173377737Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "1ce9a6a5-048d-4df5-9308-e477ec8e25a1",
                "originalText": "We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector. And the neural network is just connections between every element in each of those input vectors to every element in the output connector. ",
                "confidence": 0.89154242,
                "startTime": "2020-01-31T07:55:35Z",
                "endTime": "2020-01-31T07:56:06Z",
                "duration": 31,
                "recordingId": "ba591097-4c05-49a2-9de4-ce297cc62fb1",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:56:07.315121831Z",
                "updatedAt": "2020-01-31T07:56:28.648671582Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "1ea711a9-4901-45ea-b868-dcce8ca4b37a",
                "originalText": "Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in. This is an intention mechanism it lets things that aren't immediately relevant be set aside. So if they don't Cloud the predictions in memory going forward It has its own neural network and its own Logistics squashing function and its own gating activity right here. Now long short term memory has a lot of pieces a lot of bits that were together and it's a little much to wrap your head around it all at once. So what we'll do is take a very simple example and step through it just to illustrate how a couple of these pieces work. It's admittedly an overly simplistic example, and feel free to poke holes at it later. When you get to that point then you know, you're ready to move on to the next level of material. So we are now in the process of writing our children's book. And for the purposes of demonstration, we will assume that this LS TM has been trained on our children's books examples that we want to mimic and all of the appropriate votes and weights in those neural networks have been learned now will show it in action. So so far our story so far is Jane saucepot period Doug so Doug is the most recent word that's occurred in our story and also not surprisingly for this time step. The name's Doug Jane and spot where all predicted as viable options. This makes sense. We had just wrapped up a sentence with a period the new sentence can start with any name. So these are all great predictions. So we have our New information which is the word Doug we have our recent prediction. ",
                "confidence": 0.8692102383333333,
                "startTime": "2020-01-31T08:07:33Z",
                "endTime": "2020-01-31T08:09:33Z",
                "duration": 120,
                "recordingId": "f0992446-fb58-45c2-a9f9-da947820f0c4",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:09:34.546554399Z",
                "updatedAt": "2020-01-31T08:10:16.130697926Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "1eaffeca-e82f-4e22-b471-37fbd27318c1",
                "originalText": "When we introduced all of these together what we get, we still have the combination of our previous predictions in our new information those vectors get past and we make predictions based on them. Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network. And some of them here's a gate right here. Some of them are forgotten. Some of them are remembered the ones that are remembered are added back into the prediction. So now we have not just prediction for a predictions plus the memories that we've accumulated and that we haven't chosen to forget. Now there's an entirely separate neural network here that learns when to forget what based on what we're seeing right now. What do we want to remember? What do we want to forget? ",
                "confidence": 0.8075596099999999,
                "startTime": "2020-01-31T08:04:58Z",
                "endTime": "2020-01-31T08:06:01Z",
                "duration": 63,
                "recordingId": "507810ec-9508-4419-965d-054322495886",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:06:03.053208595Z",
                "updatedAt": "2020-01-31T08:06:44.401422852Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "3f6d0dea-c156-48f4-95ad-fe274d7c1490",
                "originalText": "After training this neural network and teaching it what to do. We would expect to see certain patterns. For instance anytime a name comes up Jane Doug or spot. We would expect that to vote heavily for the word saw or for a period because those are the two words in our dictionary that can follow a name. ",
                "confidence": 0.8379092,
                "startTime": "2020-01-31T07:58:04Z",
                "endTime": "2020-01-31T07:58:29Z",
                "duration": 25,
                "recordingId": "f62a4fdf-ed2d-4b7e-a4cc-03d3bc9dde1d",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:58:29.52766967Z",
                "updatedAt": "2020-01-31T07:58:50.990957088Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "459c46c7-b5e7-4ae4-becd-26da1f772d9a",
                "originalText": "The word saw and not Doug that we were maintaining internally get passed to a forgetting gate. Now the forgetting gate says hey my last word that came that occurred was the word saw based on my past experience. Then I freaking forget about you know, I know that it occurred I can forget that it happened. But I want to keep any predictions having to do with names. So it forgets saw holds onto the vote for not Doug. And now at this element element by element addition. We have a positive vote for dog an Get a vote for Doug and so they cancel each other out. So now we just have votes for Jane and spot. Those get passed forward our selection gate. It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step. Then we get predictions of only Jane and spot not Doug this avoids the Dog Saw Doug period type of error and the other errors that we saw. What this shows is that long short-term memory can look back two three many time steps and use that information to make good predictions about what's going to happen next now to be fair to vanilla recurrent neural networks. They can actually look back several time steps as well, but not very many. LST M can look back many times steps and has shown that successfully. ",
                "confidence": 0.7720908083333334,
                "startTime": "2020-01-31T08:11:51Z",
                "endTime": "2020-01-31T08:13:37Z",
                "duration": 106,
                "recordingId": "ed494e50-f81b-4c86-aa7e-abd144a21e88",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:13:38.572208274Z",
                "updatedAt": "2020-01-31T08:14:19.809696331Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "59f1e10c-7d02-40b7-a916-2a5563bc4be3",
                "originalText": "It looks like is that they find the higher-level idea and translate it from one mode of expression to another just using the bits and pieces that we just walked through. Another thing that they do. Well is translating speech to text. Speech is just some signals that vary in time. It takes them and uses that been to predict what text what word is being spoken and it can use the history the recent history of words to make a better guess for what's going to come next. L spms are a great fit for any information that's embedded in time audio video on my favorite application of all forces robotics robotics is nothing more than an agent taking in information from a set of sensors and then based on that information making a decision and carrying out an action. It's inherently sequential and actions taken now can influence what is sent and what should be done many times steps down the line. If you're curious what LS ems look like in math. This is it this is lifted straight from the Wikipedia page. I won't step through it, but it's encouraging that something that looks so complex expressed mathematically actually makes it fairly straightforward. Picture and story and if you'd like to dig into it more I encourage you to go to the Wikipedia page. Also, there are a collection of really good tutorials and discussions other ways of explaining lsdm so that you may find helpful as well. I'd also strongly encourage you to visit Andre carpet these blog posts showing examples of what L stm's can do in text. ",
                "confidence": 0.82239972,
                "startTime": "2020-01-31T08:14:11Z",
                "endTime": "2020-01-31T08:16:11Z",
                "duration": 120,
                "recordingId": "c4c3d35d-232d-4daa-a1ce-6c66634b923a",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:16:13.539815955Z",
                "updatedAt": "2020-01-31T08:16:55.288508785Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "748469f5-ce1e-45d5-b075-1a0d38faccc1",
                "originalText": "Cross in a circle is element by element addition. The way it works is you start with two vectors of equal size and you go down each one. You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector. So 3 plus 6 equals 9, then you go to the next element 4 plus 7 equals 11. And so your output Vector is the same size of each of your input vectors. Just a list of numbers same length, but it's the some element by element of the two. And very closely related to this. You've probably guessed the X in the circle is element by element multiplication. It's just like Edition except instead of adding you multiply for instance. Three times six gives you a first element of 18 4 times 7 gives you 28 again. The output Vector is the same size of each of the input vectors. ",
                "confidence": 0.8668750475,
                "startTime": "2020-01-31T08:02:06Z",
                "endTime": "2020-01-31T08:03:12Z",
                "duration": 66,
                "recordingId": "057c5d67-a636-489c-99dc-60a672b58fcf",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:03:13.181680137Z",
                "updatedAt": "2020-01-31T08:03:54.326026613Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "75a5f128-252c-44c8-a63e-29d15defef8e",
                "originalText": "In order to explain how this works. I'll have to describe a few new symbols. We've introduced here one is another squashing function this one with a flat bottom. One is an X in a circle and one is a cross in a circle. ",
                "confidence": 0.8303819800000001,
                "startTime": "2020-01-31T08:01:49Z",
                "endTime": "2020-01-31T08:02:05Z",
                "duration": 16,
                "recordingId": "d5876ebb-e7d1-488a-a0b7-6f226a472e57",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:02:05.97587324Z",
                "updatedAt": "2020-01-31T08:02:27.043359592Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "94002779-6ea2-4c0b-a75b-538b2a1cda37",
                "originalText": "You've probably noticed though when we are combining our predictions with our memories. We may not necessarily want to release all of those memories out as new predictions each time. So we want a little filter to keep our memories inside and let our predictions get out. And that's we add another gate for that to do selection. It has its own neural network. So its own voting process so that our new information and our previous predictions. Oceans can be used to vote on what all the gates should be. What should be kept internal and what should be released as a prediction. We've also introduced another squashing function here since we do an addition here. It's possible that things could become greater than 1 or smaller than -1. So we just squash it to be careful to make sure it never gets out of control. And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step. We select just a few to release as the prediction for that moment. Each of these things when to forget and when to let things out of our memory are learned by their own neural networks. ",
                "confidence": 0.8627889025,
                "startTime": "2020-01-31T08:06:09Z",
                "endTime": "2020-01-31T08:07:32Z",
                "duration": 83,
                "recordingId": "0d193f88-7bb9-4956-8aef-3cf069f2c6a5",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:07:33.509747541Z",
                "updatedAt": "2020-01-31T08:07:54.741729595Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "a1772ba2-b967-4483-b518-6fe281080e37",
                "originalText": "It's really useful in some surprisingly practical applications if I have text in one language, and I want to translate it to text to another language L stm's work very well, even though translation is not a word to word process. It's a phrase to phrase or even in some cases a sentence to sentence process LS TMS are able to represent those grammar structures that are specific to each language and ",
                "confidence": 0.8941315000000001,
                "startTime": "2020-01-31T08:13:38Z",
                "endTime": "2020-01-31T08:14:11Z",
                "duration": 33,
                "recordingId": "80dfe36b-00fe-42d7-8942-40cc35949453",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T08:14:12.666940729Z",
                "updatedAt": "2020-01-31T08:14:33.977073441Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "a8e01ed7-f7df-4351-971d-2265654b4b34",
                "originalText": "Dictionary is small just the words Doug Jane spot saw and a period. And the task of the neural network is to put these together in the right order to make a good children's book. So to do this, we replace our food vectors with our dictionary vectors here again, it's just a list of numbers representing each of the words. So for instance, if Doug was the most recent word that I saw my new information Vector would be all zeros except for a one and the dog position. And we similarly can represent our predictions and our predictions from yesterday. ",
                "confidence": 0.8510824166666667,
                "startTime": "2020-01-31T07:57:21Z",
                "endTime": "2020-01-31T07:58:04Z",
                "duration": 43,
                "recordingId": "4d846143-8f74-40a4-99f7-db9082e210ed",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:58:04.869003731Z",
                "updatedAt": "2020-01-31T07:58:25.94228005Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "b9d07405-49d7-48e9-a8bf-284e87ecb9c2",
                "originalText": "The vector is the reason that it's useful is vectors list of numbers are computers native language. If you want to get something into a format that it's natural for a computer to compute to do operations on to do statistical machine learning list of numbers are the way to go everything gets reduced to a list of numbers before it goes through an algorithm. We can also have Vector for statements like, it's Tuesday in order to encode this kind of information what we do is we make a List of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true. Right. Now. This format is called one hot and coding and it's very common to see a long Vector of zeros with just one element being one. It seems inefficient, but for a computer, this is a lot easier way to ingest that information. ",
                "confidence": 0.85817145,
                "startTime": "2020-01-31T07:54:21Z",
                "endTime": "2020-01-31T07:55:21Z",
                "duration": 60,
                "recordingId": "35e3e766-5192-4a01-92d8-6d85719b8cae",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:55:21.658983737Z",
                "updatedAt": "2020-01-31T07:55:42.956510577Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "bfaa9950-9d51-4b55-80b7-fe9d3f573352",
                "originalText": "In this form and this formulation, we have a recurrent neural network for Simplicity. I'll take the vectors in the weights and collapse them down to that little symbol with the dots in the arrows the dots in the lines connecting them and there's one more symbol we haven't talked about yet. This is a squashing function and it just helps the network to behave. How it works is you take all of your votes coming out and you subject them to this squashing function. For instance. If something received a total vote of .5, you draw a vertical line up where it crosses the function you draw a horizontal line over to the y axis and there is your squash diversion out for small numbers. The squashed version is pretty close to the original version. But as your number gets larger, the number that comes out is closer and closer. R21 and similarly if you put in a big negative number, then what you'll get out will be very close to minus 1. No matter what you put in what comes out is between minus 1 and 1. So this is really helpful. When you have a loop like this where the same values get processed again and again day after day it is possible. You can imagine if in the course of that processing say something got voted for twice. We got x 2 in that case it would get twice as big every time and very soon blow up to be astronomical. By ensuring that it's always less than 1 but more than minus 1 you can multiply it as many times as you want. You can go through that Loop and it won't explode in a feedback loop. This is an example of negative feedback or attenuating feedback. So you may have noticed our neural network in its current state is subject to some mistakes. We could get a sentence for instance of the form Doug saw Doug. ",
                "confidence": 0.8696557062500001,
                "startTime": "2020-01-31T07:58:58Z",
                "endTime": "2020-01-31T08:00:58Z",
                "duration": 120,
                "recordingId": "824f41f6-5e40-4be8-ba3c-ab016c89cf4a",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:00:59.904118629Z",
                "updatedAt": "2020-01-31T08:02:02.037328489Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "c53586d5-0e6a-4799-9c8b-9aba5cbb11bf",
                "originalText": "Complete our picture we can show how the prediction for today will get recycled the dotted line there means hold onto it for a day and then reuse it tomorrow and it becomes our yesterday's predictions tomorrow. Now we can see how if we were lacking some information. Let's say we were out of town for two weeks. We can still make a good guess about what's going to be for dinner tonight. We just ignore the new information part and we can unwrap or unwind this Vector in time until we do have some information to base it on and then just play it forward. And when it's unwrapped it looks like this and we can go back as far as we need to and see what was for dinner and then just trace it forward and play out our menu over the last two weeks until we find out what's for dinner tonight. So this is a nice simple example that showed recurrent neural networks now to show how they don't meet all of our needs. We're going to write a children's book. It'll have sentences of the format Doug saw Jane period Jane sauce. Hot period spot saw Doug period and so on. ",
                "confidence": 0.83091834,
                "startTime": "2020-01-31T07:56:07Z",
                "endTime": "2020-01-31T07:57:20Z",
                "duration": 73,
                "recordingId": "c4b690c5-d509-496e-9f39-2f383d090cf5",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:57:21.32899199Z",
                "updatedAt": "2020-01-31T07:57:44.240393383Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece",
                "originalText": "We can make a one hot Vector for our prediction for dinner tonight. We set everything equal to 0 except for the dinner item that we predict. So in this case will be predicting sushi. ",
                "confidence": 0.9066643,
                "startTime": "2020-01-31T07:55:21Z",
                "endTime": "2020-01-31T07:55:35Z",
                "duration": 14,
                "recordingId": "66316ceb-fdb4-4e6a-ad27-eb1e961e6e39",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:55:35.298182533Z",
                "updatedAt": "2020-01-31T07:55:56.358710266Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "d32264d8-9064-44d7-b36c-1eff8b5144f5",
                "originalText": "And you learn how to predict what's going to be for dinner tonight? The trouble is that your network doesn't work very well despite carefully choosing your inputs and training it thoroughly. You still can't get much better than chance predictions on dinner as is often the case with complicated machine learning problems. It's useful to take a step back and just look at the data and when you do that, you notice a pattern your flatmate makes pizza then Sushi then waffles then pizza again in a Michael it doesn't depend on the day of the week or anything else. It's in a regular cycle. So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday. So if we know if we had pizza for dinner yesterday, it'll be sushi tonight. Sushi yesterday waffles tonight and Waffles yesterday pizza tonight. It becomes a very simple voting process and and it's right all the time because your flatmate is incredibly consistent. ",
                "confidence": 0.8636140939999999,
                "startTime": "2020-01-31T07:52:13Z",
                "endTime": "2020-01-31T07:53:22Z",
                "duration": 69,
                "recordingId": "89d53b98-d0e5-4edd-badd-49477ba37314",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:53:22.976473992Z",
                "updatedAt": "2020-01-31T07:53:44.504718618Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "dafc63db-af9c-4928-ae8b-7babeb97b33c",
                "originalText": "The first one of these make some predictions given that the word Doug just occurred. This is learned that the word saw is a great guess to make for next word, but it's also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence. So it makes a positive prediction for Saul and a negative. Addiction for Doug it says I do not expect to see dog in the near future. So that's why I dog is in Black. So this example is so simple. We don't need to focus on attention or ignoring. So we'll skip over it for now and this prediction of Saw not Doug is passed forward and again for the purposes of Simplicity. Let's say there's no memory at the moment. So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out. So the fact that there's a vote for not Doug gets blocked here and the word saw get sent out as the prediction for the next time step. So we take a step forward in time. Now. The word saw is our most recent word and our most recent prediction. They get passed forward to all of these neural networks, and we get a new set of predictions because the word saw just occurred. We now predict that the words Doug Jane or spot might come next. I will pass over ignoring and attention and this example and we'll take those predictions forward. Now. The other thing that happened is our previous set of possibilities. ",
                "confidence": 0.841654516,
                "startTime": "2020-01-31T08:09:51Z",
                "endTime": "2020-01-31T08:11:51Z",
                "duration": 120,
                "recordingId": "85024bd7-55d1-43c3-a325-7953e752ada4",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:11:52.379111529Z",
                "updatedAt": "2020-01-31T08:12:34.059108311Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "e1a7877d-d189-4854-a707-efc6f5c017f4",
                "originalText": "Element wise multiplication lets you do something pretty cool. You imagine that you have a signal and it's like a bunch of pipes and they have a certain amount of water trying to flow down them. And this case we'll just assign the number two that 0.8. It's like a signal. Now on each of those pipes, we have a faucet and we can open it all the way close it all the way or keep it somewhere in the middle to either let that signal come through or block it. So in this case an open gate an open faucet would be a one and a closed faucet would be a zero and the way this works with element wise multiplication. We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times 0 equals 0 that signal the original signal was effectively blocked. And then with the gating value of 0.5 the single was passed through but it's smaller. It's attenuated. So gating lets us control what passes through and what gets blocked which is really useful. Now in order to do gating it's nice to have a value that you know is always between 0 and 1 so we introduce another squashing function. This will represent with a circle with a flat bottom and this is It's called The logistic function. It's very similar to the other squashing function the hyperbolic tangent except that it just goes between 0 and 1 instead of minus 1 and 1. ",
                "confidence": 0.8574025128571429,
                "startTime": "2020-01-31T08:03:13Z",
                "endTime": "2020-01-31T08:04:57Z",
                "duration": 104,
                "recordingId": "ef8c543c-58f4-458a-8dcb-2eda706ecb3b",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:04:57.859017239Z",
                "updatedAt": "2020-01-31T08:05:39.073575794Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "e483de91-6b4d-4058-ac7d-f157349c1794",
                "originalText": "You can see this is powerful this let us hold onto things for as long as we want. ",
                "confidence": 0.8431589,
                "startTime": "2020-01-31T08:06:01Z",
                "endTime": "2020-01-31T08:06:08Z",
                "duration": 7,
                "recordingId": "b8f7751a-0925-4224-8f70-4d37bc31e3c8",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:06:09.132093193Z",
                "updatedAt": "2020-01-31T08:06:29.979373386Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "f05c0675-3618-4081-bbf3-c46f76f3e736",
                "originalText": "early, if we had predicted a name on the previous time step, we would expect those to vote also for the word saw or for a period ",
                "confidence": 0.7169895,
                "startTime": "2020-01-31T07:58:29Z",
                "endTime": "2020-01-31T07:58:39Z",
                "duration": 10,
                "recordingId": "78707771-81b0-4f42-88fa-68907d594b5c",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T07:58:40.311086881Z",
                "updatedAt": "2020-01-31T07:59:01.302648142Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "f4510718-0d67-47aa-941c-26df17007ed5",
                "originalText": "Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection. ",
                "confidence": 0.8169449,
                "startTime": "2020-01-31T08:09:33Z",
                "endTime": "2020-01-31T08:09:50Z",
                "duration": 17,
                "recordingId": "07545ca4-645b-48de-aaac-3c8fd83ace21",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:09:51.048881963Z",
                "updatedAt": "2020-01-31T08:10:11.841769718Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "f7fab60f-c02e-4756-a4d5-eeb0b878c084",
                "originalText": "Period because Doug strongly votes for the word saw which in turn strongly votes for uh name any name which could be Doug similarly we could get something like Doug saw Jane saw spot saw Doug. Because each of our predictions only looks back one time Step It has very short term memory. Then it doesn't use the information from further back and it's subject to these types of mistakes. In order to overcome this we take our recurrent neural network, and we expand it and we've add some more pieces to it. The critical part that we add to the middle here is memory. We want to be able to remember what happened many times steps ago. ",
                "confidence": 0.86537014,
                "startTime": "2020-01-31T08:00:58Z",
                "endTime": "2020-01-31T08:01:49Z",
                "duration": 51,
                "recordingId": "52c95adf-5201-48a5-937a-9189b22dc26c",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "89d07e4b-f755-443f-ad79-5b4774a3a05d",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:01:49.867824493Z",
                "updatedAt": "2020-01-31T08:02:10.91221806Z",
                "deletedAt": null,
                "deleted": false
            }
        ]
    }}