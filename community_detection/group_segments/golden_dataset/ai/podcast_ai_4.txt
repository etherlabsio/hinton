{"body":{
        "contextId": "01DBCS96Y867PTEH9Z5B6TAACM",
        "mindId": "01DADP74WFV607KNPCB6VVXGTG",
        "instanceId": "a4b80cd2-2969-48f5-9289-765e3240c414",
        "segments": [
            {
                "id": "0396879a-033e-4dea-a186-bddb5887378e",
                "originalText": "Sometimes our agent might get lucky. Sometimes it's going to randomly select a whole sequence of actions that actually lead to scoring a goal. And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward. We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial. And so what policy gradients are going to do is that That for every episode where we got a positive reward. We're going to use the normal gradients to increase the probability of those actions in the future. But whenever we got a negative reward, we're going to apply the same gradient. We're going to multiply it with -1 and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future. And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely. So in a sense our agent is learning how to play the game of pong. Now. I know this was a very quick introduction to reinforcement learning. So if you want to read up a bit and spend a little bit more timing thinking about the details, I really recommend to read and red carpet these blog post punk from pixels. It does a phenomenal job at explaining all the details. All right, so we can use policy gradients that train a neural network to play the game of pong. That's amazing. Right? Well, yes it is. But as always there are a few very significant downsides to using these methods. Let's go back to pong one more time. So imagine that your agent has been training for a while and it's actually doing a pretty decent job at playing the game of pong. It's bouncing the ball back and forth. But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty. So the problem with policy gradients is that our policy gradient is going to ",
                "confidence": 0.8694515224999999,
                "startTime": "2020-01-31T07:59:21Z",
                "endTime": "2020-01-31T08:01:21Z",
                "duration": 120,
                "recordingId": "2cb16eb7-0f23-4143-9b4c-aa741015b650",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "101410e8-60d1-4fd8-b267-1e693a9200bf",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T08:01:22.707822808Z",
                "updatedAt": "2020-01-31T08:02:04.222350954Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "0a51398a-64b1-45f6-8ed2-1ad78aec5e4a",
                "originalText": "That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future. But remember that actually the most part of that episode we were doing really well. So we don't really want to decrease the likelihood of those actions and in reinforcement learning. This is called the credit assignment problem. It's where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem isn't Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action. We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it's only the actions right before it hits the ball that are truly important everything else. Once the ball is flying off. It doesn't really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior. And I've made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely. So a famous example is the game Montezuma's Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level. And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated. It's never going to get there with random actions. And so your policy gradient is never going to see a single positive reward. So it has no idea what to do and the same case applies to robotic control where for example you would like to train in a row. ",
                "confidence": 0.8428389275000001,
                "startTime": "2020-01-31T08:01:21Z",
                "endTime": "2020-01-31T08:03:21Z",
                "duration": 120,
                "recordingId": "3dca8654-ec15-410d-8cad-9f15f8d04b47",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "101410e8-60d1-4fd8-b267-1e693a9200bf",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:03:22.766225544Z",
                "updatedAt": "2020-01-31T08:04:04.088235196Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db",
                "originalText": "From The Amazing results and vintage Atari games deep mines Victory with alphago stunning breakthroughs in robotic arm manipulation and even beating professional players at what do you want to build up? The field reinforcement learning has literally exploded in recent years every city impressive breakthrough on the imagenet classification challenge in 2012. The successes of supervised deep learning have continued to pile up and people from many different backgrounds have started using deep neural Nets to solve a wide range of new tie. Tasks including how to learn intelligent behavior in complex Dynamic environments. So in this episode, I will give a general introduction into the field of reinforcement learning as well as an overview of the most challenging problems that we're facing today. If you're looking for a solid introduction into the field of deep reinforcement learning then this episode is exactly what you're looking for. My name is Xander and welcome to Archive insights. So nips 2017 Peter at Beale gave me very inspiring demo in front of a large audience of some of the brightest Minds in Ai and machine learning. So you showed this video where a robot is cleaning a living room bringing somebody a bottle of beer and basically doing a whole range of mundane tasks that robots in Sci-Fi movies can do without question and then End of the video Peter revealed that the robots actions were actually entirely remote controlled by a human operator and the takeaway from this demo. I think is a very important one it basically says that the robots we've been building for decades now are physically perfectly capable of doing a wide range of useful tasks, but the problem is that we can't embed them with the needed intelligence to do those things. So basically creating useful state-of-the-art robotics is a software Challenge and not a hardware. Problem. So it turns out that having a robot learn how to do something very simple like picking up. ",
                "confidence": 0.8265709299999999,
                "startTime": "2020-01-31T07:53:21Z",
                "endTime": "2020-01-31T07:55:21Z",
                "duration": 120,
                "recordingId": "a2369d46-0a3b-4e6d-870c-b6eaeb25e8c2",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "101410e8-60d1-4fd8-b267-1e693a9200bf",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:55:22.676380315Z",
                "updatedAt": "2020-01-31T07:56:04.049826991Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "14d5dec1-c18a-4389-8d76-4335306d578e",
                "originalText": "Farm to pick up an object and stack it on to something else. Well, the typical robot has about seven joints that it can move. So it's a relatively High action space and if you only give it a positive reward when it's actually successfully stacked a block well by doing random exploration. It's never going to get to see any of that reward and I think it's important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation. Whereas in reinforcement learning setting you're having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult. Even for state-of-the-art keep learning. Ice cold Michelle Pfeiffer that white gold this one for them hood girls them good girls straight and so the traditional approach to solve this issue of sparse rewards has been the use of reward shaping. So reward shipping is the process of manually designing a reward function that needs to guide your policy to some desired Behavior. So in the case of Montezuma's Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior. And while this obviously makes it easier for your policy to converge to the desired Behavior. There are some significant downsides to reward shaping. So firstly we were taping is a custom process that needs to be redone for every new environment. You want to train a policy. So if you're looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games. That's just not scalable the second problem. Is that reward shaping suffers from what we call the alignment problem. So it turns out that reward shaping is actually surprisingly difficult in a lot of cases when you ",
                "confidence": 0.870261968,
                "startTime": "2020-01-31T08:03:21Z",
                "endTime": "2020-01-31T08:05:21Z",
                "duration": 120,
                "recordingId": "e41f0d51-ae2c-4775-996b-dfce25869922",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "101410e8-60d1-4fd8-b267-1e693a9200bf",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T08:05:22.818747598Z",
                "updatedAt": "2020-01-31T08:06:04.164506156Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "635d06e9-b7bb-4d6a-9720-5a9eb205e62a",
                "originalText": "We'll have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we don't actually know the target label so we don't know in any situation whether we should have gone up or down because we don't have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network. Now one of the simplest ways to train a policy network is a method called. Elysee gradients. So the approaching policy gradients is that you start out with a completely random Network you feed that Network frame from the game engine. It produces a random output action, you know, either up or down you send that action back to the game engine in the game engine produces the next frame and this is how the loop continues and the network in this case. It could be a fully connected network, but you can obviously apply convolutions there as well. And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you're not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself. The only feedback that we're going to give it is the scoreboard in the game. So whenever our agent manages to score a goal it will see where reward of plus 1 and if the opponent's Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible. So in order to train or policy Network, the first thing we're going to do is collect a bunch of experience. So we're just going to run a whole bunch of those game frames through your network select random actions feed them back into the engine and just create a whole bunch of random pong games. And now obviously since our agent hasn't learned anything useful yet. It's going to lose most of those games, but the thing is that ",
                "confidence": 0.8257551375,
                "startTime": "2020-01-31T07:57:21Z",
                "endTime": "2020-01-31T07:59:21Z",
                "duration": 120,
                "recordingId": "fcd75549-dc3e-4fc9-97dd-7267585c0be0",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "101410e8-60d1-4fd8-b267-1e693a9200bf",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T07:59:22.547583003Z",
                "updatedAt": "2020-01-31T08:00:23.902036431Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "98d73e6d-9061-460e-84fd-2137177746f0",
                "originalText": "Support me on patreon for which I would just like to say. Thank you very much. I mean it really means a big deal to me. I'm doing these videos completely in my spare time and knowing that there's people out there that appreciate this content really feels great. So thank you very much. Thanks for watching. Don't forget to subscribe and I'd love to see you again in the next episode of archive insights. ",
                "confidence": 0.8844334,
                "startTime": "2020-01-31T08:09:21Z",
                "endTime": "2020-01-31T08:09:46Z",
                "duration": 25,
                "recordingId": "56dd25b7-55f2-43ad-b08d-d444d17b7edd",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "101410e8-60d1-4fd8-b267-1e693a9200bf",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:09:47.298838657Z",
                "updatedAt": "2020-01-31T08:10:09.365592683Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "d425094a-088e-4acc-a972-6560fc8dac2b",
                "originalText": ".2. At last from Boston Dynamics because everybody has seen the video where he does a backflip, but the reality is that if you think about what's what Boston Dynamics is actually doing well, it's very likely that there's not a lot of deep learning going on there. If you look at their previous papers in their research track record. Well, they're doing a lot of very Advanced robotics. Don't get me wrong, but there's not a lot of self driven Behavior. There's not a lot of intelligent decision making going on in those robots. So don't get me wrong. Boston Dynamics is a very impressive robotics. Company, but the media images they've created might be a little bit confusing to a lot of people that don't know what's going on behind the seats. But nonetheless if you look at the progress of research that is going on. I think we should not be negligible of the potential risks that these Technologies can bring so I think it's very good at a lot more people are getting involved in the whole AI Safety Research because this is going to become very fundamental threats, like autonomous weapons and mass surveillance are to be taken very seriously. And so the only hope we have is that International law is going to be somewhat able to keep up with the rapid progress. We see in technology, but on the other hand, I also feel like the media is focusing way too much on the negative side of these Technologies simply because people fear what they don't understand and well fear sells more advertisement than Utopias. So I personally believe that most if not all technological progress is beneficial in the long run as long as we can make sure that there are no monopolies that can maintain or enforce their power with the Views of AI well anyway enough politics for one video. So this video is an introduction into deep reinforcement learning and an overview of the most challenging problems that we're facing in the field in the next video. I will dive into some of the most recent approaches that try to tackle these problems as sample efficiency and these parts reward setting so specifically I will cover a few technical papers dealing with approaches like auxiliary reward settings intrinsic curiosity hindsight experience replay and so on. I've also seen that a few people have ",
                "confidence": 0.80432378,
                "startTime": "2020-01-31T08:07:21Z",
                "endTime": "2020-01-31T08:09:21Z",
                "duration": 120,
                "recordingId": "71681c66-cc33-4778-8ecb-e62d7a34efee",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "101410e8-60d1-4fd8-b267-1e693a9200bf",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T08:09:22.844543979Z",
                "updatedAt": "2020-01-31T08:10:04.780164875Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "dba1aa05-b75d-4a7e-a2ff-40daf4b580e0",
                "originalText": "Shape your reward function your agent will find some very surprising way to make sure that it's getting a lot of reward but not doing at all what you want to do. And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there's a lot of funny cases where reward shaping goes terribly wrong. So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping. I mean, look at this shaped reward function for a robotic control task. I don't even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition. You don't want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation. So the situation that we're in right now is that we know that it's really hard to train in a sparse reward setting but at the same time it's also very tricky to shape a reward function and we don't always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today? And there's a lot of very hard engineering going on behind the scenes. So I think that one of the biggest challenges in navigating our digital landscape is Discerning truth from fiction in this ocean of Click bait that is powered by the advertisement industry. And I think the atlas robot from Boston Dynamics is a very clear example of what I mean. So I think if you go out on the streets and you asked a thousand people with the most advanced robots today are but they would probably ",
                "confidence": 0.8927802425,
                "startTime": "2020-01-31T08:05:21Z",
                "endTime": "2020-01-31T08:07:21Z",
                "duration": 120,
                "recordingId": "cd5c3595-3000-4970-a7a2-079f65c4f4d6",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "101410e8-60d1-4fd8-b267-1e693a9200bf",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T08:07:22.962897343Z",
                "updatedAt": "2020-01-31T08:08:04.637074861Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "ebdb1aa6-0290-4a65-8a95-43c05626d908",
                "originalText": "Beer can be a very challenging task. And so in this video, I want to introduce you guys to the whole subfield in machine learning that's called reinforcement learning which I think is one of the most promising directions to actually get to very intelligent robotic Behavior. So in the most common machine learning applications people use what we call supervised learning and this means that you give an input to your neural network model, but you know the output that your model should produce and therefore you can compute gradients using something like the back propagation. Algorithm to train that Network to produce your outputs. So imagine you want to train a neural network to play the game of pong. What you would do in a supervised setting is you would have a good human gamer play the game of pong for a couple of hours and you would create a data set where you log all of the frames that that human is seeing on the screen as well as the actions that he takes in response to those frames. So whether he's pushing the up Arrow or down arrow and we can then feed those input frames through a very simple neural network that at the output Produce two simple actions. It's either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation. We can actually train that neural network to replicate the actions of the human gamer, but there are two significant downside to this approach. So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong. Then that human gamer. For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we can't use supervised learning. So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning. So the framework in reinforcement learning is actually surprisingly similar to the normal frame work in supervised learning so ",
                "confidence": 0.8635529175000001,
                "startTime": "2020-01-31T07:55:21Z",
                "endTime": "2020-01-31T07:57:21Z",
                "duration": 120,
                "recordingId": "938f1fb6-1970-40f8-89e4-deb99306cd29",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "101410e8-60d1-4fd8-b267-1e693a9200bf",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T07:57:22.786025401Z",
                "updatedAt": "2020-01-31T07:58:04.107330415Z",
                "deletedAt": null,
                "deleted": false
            }
        ]
    }}