{"body":{
        "contextId": "01DBCS96Y867PTEH9Z5B6TAACM",
        "mindId": "01DADP74WFV607KNPCB6VVXGTG",
        "instanceId": "3e11851a-aaab-4d4c-8576-63e4455e19fc",
        "segments": [
            {
                "id": "02a59eb5-dff7-4c20-ae59-e6b823403a95",
                "originalText": "hierarchy of clusters ",
                "confidence": 0.82747346,
                "startTime": "2020-01-31T07:52:17Z",
                "endTime": "2020-01-31T07:52:20Z",
                "duration": 3,
                "recordingId": "17b920af-a2ec-4a8a-96c1-bd3db7c0e73d",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "8c688b21-bac1-4c35-aa44-b88c2192814f",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T07:52:20.886126479Z",
                "updatedAt": "2020-01-31T07:52:41.824535305Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "17f84449-d736-4e5d-a8a2-ccd6bf258e48",
                "originalText": "Are two types of hierarchical clustering agglomerative or bottom-up and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones. Then we continue with 3 4 5 and so on until each observation. Is it separate cluster? However, in order to find the best split we Must explore all possibilities at each step. Therefore faster methods have been developed such as K means. With K means we can simulate this divisive technique when it comes to agglomerative clustering. The approach is bottom up. We start from different dog and cat breeds cluster them into dogs and cats respectively and then we continue pairing up Species until we reach the animal cluster. ",
                "confidence": 0.80973131,
                "startTime": "2020-01-31T07:52:20Z",
                "endTime": "2020-01-31T07:53:16Z",
                "duration": 56,
                "recordingId": "ce39ab7e-c85a-4118-b99b-54a941b76b99",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "8c688b21-bac1-4c35-aa44-b88c2192814f",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:53:17.277275422Z",
                "updatedAt": "2020-01-31T07:53:38.422170696Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "7c954dac-81dd-40a1-b705-7aafe7181f76",
                "originalText": "Two lines that merge are those of Germany and France according to the dendrogram these two countries of the closest in terms of the features considered at this point. There are five clusters Germany and France are 1 and each country has its own cluster from this point on going up Germany, and France will be considered one cluster. Now here's where it becomes interesting. The next two lines that emerge are those of the Germany and France cluster and the UK at this point. There are four clusters Germany France and the UK are one and the rest are single observation clusters at the next stage of the hierarchy Canada and the u.s. Joined forces. The next step is to unite the Germany France UK cluster with the Canada US 1 Australia is still alone. Finally all countries become one big cluster representing the whole sample. Okay, cool. What other information can we get from the dendrogram? Well, the bigger the distance between two lengths the bigger the difference in terms of the chosen features as you can see Germany France and the UK merged into one cluster very quickly. This shows that they are very similar in terms of longitude and latitude moreover Germany and France are closer than Germany and UK or France and UK the USA and Canada came together not long after. However, it took half of the dendrogram to join these five countries together. This indicates the Europe cluster and the North America cluster are not so alike. Finally the distance needed for Australia to join. The other five countries was the other half of the dendrogram meaning it is extremely different from them. To sum up the distance between the links show similarity or better dissimilarity between features. ",
                "confidence": 0.8586502428571426,
                "startTime": "2020-01-31T07:54:42Z",
                "endTime": "2020-01-31T07:56:42Z",
                "duration": 120,
                "recordingId": "5b94577e-ab17-4aa0-8c28-7355fce60e13",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "8c688b21-bac1-4c35-aa44-b88c2192814f",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:56:43.641230845Z",
                "updatedAt": "2020-01-31T07:57:25.056319508Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4",
                "originalText": "All right. Next on our list is the choice of number of clusters. If we draw a straight line piercing these two links. We will be left with two clusters, right Australia in one and all the rest in the other instead. If we Pierce them here we will get three Clusters North America, Europe and Australia. The general rule is when you draw a straight line. You should count. The number of links that have been broken. In this case. We have broken Three Links. So we will be left with three clusters because the links were coming out of those three clusters. Should we break the links here? There will be four clusters and so on great. Finally, how should we decide where to draw the line? Well, there is no specific rule, but after solving several problems, you kind of develop an intuition when the distance between two stages is too big. It is probably a good idea to stop there for our case. I would draw the line at three clusters and remain with North America Europe and Australia. Okay, when most people get acquainted with dendrograms, they like them a lot and I presume that is the case. With you too. Let's see some pros and cons the biggest Pro is that hierarchical clustering shows all the possible linkages between clusters. This helps us understand the data much much better moreover. We don't need to preset the number of clusters. We just observe the dendrogram and take a decision. Another Pro is that there are many different methods to perform hierarchical clustering the most famous of which is the ward method different data behaves in different ways. So it is a nice option to be able to choose the method that works better for you k-means is a one size fits it all method so you don't have that luxury. How about a con the biggest con which is also one of the reasons why hierarchical clustering is far from a ",
                "confidence": 0.8879559316666668,
                "startTime": "2020-01-31T07:56:42Z",
                "endTime": "2020-01-31T07:58:42Z",
                "duration": 120,
                "recordingId": "1dbbd6ff-0e80-4e70-abd8-00188f85feed",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "8c688b21-bac1-4c35-aa44-b88c2192814f",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T07:58:43.476203378Z",
                "updatedAt": "2020-01-31T07:59:24.863900735Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4",
                "originalText": "Bomber Ativan device of clustering should reach similar results, but agglomerative is much easier to solve mathematically. This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering. We start with each case being its own cluster. There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n- 1 cluster solution Then we repeat this procedure until all observations are in a single cluster. The end result looks like this Animal Kingdom representation. The name for this type of graph is a dendrogram a line starts from each observation. Then the two closest clusters are combined then another 2 and so on until we are left with a single cluster note that all cluster Solutions are nested inside the dendrogram. Alright, let's explore a dendrogram and see how it works. Here is the dendrogram created on our country cluster data. Okay so each Line starts from a cluster you can see the names of the countries at the beginning of those lines. This is the show that at the start each country is a separate cluster. ",
                "confidence": 0.8319235800000001,
                "startTime": "2020-01-31T07:53:17Z",
                "endTime": "2020-01-31T07:54:42Z",
                "duration": 85,
                "recordingId": "68aeba95-26ad-41aa-8e10-2365cf8500d5",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "8c688b21-bac1-4c35-aa44-b88c2192814f",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:54:43.103160712Z",
                "updatedAt": "2020-01-31T07:55:04.303118138Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "f9fccf2c-d252-4988-90f9-115e0dd5acf9",
                "originalText": "Hi and welcome, we at three six five data science specializing in data science trainings. We post videos weekly so you can Master indispensable skills for free. All right, let's get started. We will have a short lecture about clustering of clustering originally cluster analysis was developed by anthropologists aiming to explain more engine of human beings later. It was adopted by psychology intelligence and other areas. Nowadays, there are two broad types of clustering flat and hierarchical k-means is a flat method in the sense that there is no hierarchy but rather we choose the number of clusters and the magic happens. The other type is hierarchical and that's what we are going to discuss in this lecture historically hierarchical clustering was developed first. So it makes sense to get acquainted with it an example of clustering with hierarchy is taxonomy of the animal kingdom. For instance. There is the general term animal subclusters are fish mammals and birds for instance. There are birds which can fly and those that can't we can continue in this way until we reach dogs and cats even then we can divide dogs and cats into different breeds moreover. Some breeds have sub breeds. ",
                "confidence": 0.846387365,
                "startTime": "2020-01-31T07:50:50Z",
                "endTime": "2020-01-31T07:52:16Z",
                "duration": 86,
                "recordingId": "0d7e0cf1-b84e-4392-86e2-025e9f688197",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "8c688b21-bac1-4c35-aa44-b88c2192814f",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T07:52:17.711019169Z",
                "updatedAt": "2020-01-31T07:52:39.003691794Z",
                "deletedAt": null,
                "deleted": false
            }
        ]
    }}