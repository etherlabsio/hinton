{"body":{
        "contextId": "01DBCS96Y867PTEH9Z5B6TAACM",
        "mindId": "01DADP74WFV607KNPCB6VVXGTG",
        "instanceId": "ad1a6f4a-bd9e-41f5-8cad-29cb1086a68e",
        "segments": [
            {
                "id": "39cb0782-85d4-4e1b-ad99-ca933adf62b5",
                "originalText": "Appears with the word joke in the third sentence. So ideally the word embedding would figure out then that laughing laughter related since they're both related to joke. So that's kind of where a word embedding gets its Knowledge from it learns things via context. It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we're after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say 64 numbers and those numbers would describe the word and forward embedding to be good. We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they're very related words. Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger. I want the vector for Ferrari to be far away from The vector for hamburger and of course all these distances are relative, but you can kind of see what I mean that we want the closeness of these vectors to resemble the closeness of the words that they represent and in addition to this kind of idea of closeness. We might also want there to be even more structure. For example, if I do the vector from Man - the vector for woman and to subtract vectors, we just subtract each number from the corresponding number. Of the other Vector. So if I take the vector from man and I subtract the vector for woman, I want that to somehow represent the difference between male and female and then if I add that Vector to the vector for for Queen, I want it to give me out something very very close to the vector for King. So I you know, I want these vectors to be related and I want the differences between vectors to also carry some meaning and you know, I might add other constraints but the idea is I just want to encode as ",
                "confidence": 0.8376907849999999,
                "startTime": "2020-01-31T07:57:53Z",
                "endTime": "2020-01-31T07:59:53Z",
                "duration": 120,
                "recordingId": "57a9172c-0cd5-473f-92a9-3fbc9cb269ad",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "f160159d-9e30-4b7f-9246-9f104b96977a",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T07:59:54.48499013Z",
                "updatedAt": "2020-01-31T08:00:35.857388592Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "3b23e820-dc5a-41ef-a6d8-9fa398f84c13",
                "originalText": "Just add one to that entry in this Matrix. Now different methods will use this Matrix in different ways. But pretty much all of the methods utilized some amount of linear algebra. So I'm going to be just talking about matrices and Matrix multiplications dot products things like that. And if you don't know linear algebra You're Not Really Gonna Get Much from this but that's why I kind of left it at the end of the video because some people will get something out of this and I think it's somewhat interesting. So probably the simplest approach To generating word embeddings with the co-occurrence Matrix is to just decompose The co-occurrence Matrix into the product of two much smaller matrices. So I've drawn out the picture here. You can basically see that I get this massive Square Matrix, which is our co-occurrence matrix by multiplying a tall skinny Matrix and a short wide Matrix, you know, you think about how many entries are in the big Matrix? There's a hundred thousand squared. That's a lot more. That is stored on the right side of this equation, which is you know, two relatively small matrices multiplied together. So by decomposing this big co-occurrence Matrix into these smaller ones were clearly compressing some information and in doing so hopefully we have to extract a lot of meaning from The Matrix so that we can do that compression and that should allow us to at least generate decent embeddings. Once we have this Matrix decomposition, which I haven't described exactly how we might find this yet, but you could imagine there's Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that. But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big co-occurrence Matrix each row and each column corresponds to a word. So if I go into this tall skinny Matrix, and I grab the, you know entry, you know, the entry for a certain word that is going to give me a vector which is pretty small and ",
                "confidence": 0.8288694475,
                "startTime": "2020-01-31T08:03:53Z",
                "endTime": "2020-01-31T08:05:53Z",
                "duration": 120,
                "recordingId": "b09520c3-a8e8-47ce-82e5-c7e7b85564df",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "f160159d-9e30-4b7f-9246-9f104b96977a",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T08:05:55.046620462Z",
                "updatedAt": "2020-01-31T08:06:56.455310984Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "49d74149-4a6c-4d9c-83dd-11b8fd5142ec",
                "originalText": "It's 64 components and I can call that a word embedding for that for that word. And of course I didn't have to pick it from the tall. Skinny Matrix. I could have picked it from the short wide Matrix or I could even just decide to average these two vectors and use that as the embedding overall and there's actually a good reason to assume that these vectors would represent a decent amount of meaning. The reason is that an entry in the big co-occurrence. Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix. So basically a given co-occurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to co-occur. So I've gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings. So now I'll tell you a little bit about the particular method that I use to generate the word embeddings at the beginning of this video the method I used is known as glove which is short for Global vectors and it is a kind of co-occurrence decomposition method now, it's a little unique in that it decomposes the log the logarithm of the co-occurrence Matrix instead of the actual co-occurrence Matrix. And it also is weighted uses a model where certain entries in The co-occurrence Matrix are more important than others and you use gradient descent to learn the embedding. So it's similar to training a neural network and it has really good results and it's extremely fast. So I really like glove I had a lot more fun implementing glove than I did implementing word Avec and I will certainly have a link to that paper in the description that describes glove. ",
                "confidence": 0.8953475624999999,
                "startTime": "2020-01-31T08:05:53Z",
                "endTime": "2020-01-31T08:07:53Z",
                "duration": 120,
                "recordingId": "befbbbc7-d4e3-4cad-b717-b31eaa484324",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "f160159d-9e30-4b7f-9246-9f104b96977a",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:07:54.381998954Z",
                "updatedAt": "2020-01-31T08:08:35.519013407Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "7162f0aa-7409-4369-93b8-bf0b0a6c6bab",
                "originalText": "Is to put a color right? I painted The Bench red. I painted the bench green something like that and already we can see that, you know, if a word can show up in this context it's likely to be a color but unfortunately that's not always true. You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there's another way that context can help us and that's if two words happen to always appear in the same context at once. So here are three different sentences that will help us understand this idea. And the first sentence I actually have two examples Donald and Trump are likely to appear together because ones the first name of a person and one's the last name of that same person. So those words are closely related. We also have United States which is kind of just one logical. Broken up into two smaller words, so United and states are likely to appear together and then in the second and third example, we have a joke and laugh are kind of related words you laugh at a joke. So they're also likely to appear in the same context. Now, there's one subtle thing that I'd like to point out in this example, which is that laughed and laughed our might be different words, like laughs is the present tense and laughed as the past tense and likewise, you know, we could think about joke. Versus jokes like one is plural one is singular. These are different forms of the same word. And ideally since we knew nothing about English going in our algorithm. Our word embedding is going to have to learn the different forms of the same word or related. It has to learn that laughed is somehow related to laugh and what you can see is, you know, these examples give you an idea of how the model might be able to do that because you can see laughed appears with the word joke in the second sentence and ",
                "confidence": 0.8414907675000001,
                "startTime": "2020-01-31T07:55:53Z",
                "endTime": "2020-01-31T07:57:53Z",
                "duration": 120,
                "recordingId": "4ad60754-8704-4606-9d28-a65040bf1f5c",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "f160159d-9e30-4b7f-9246-9f104b96977a",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T07:57:54.367963096Z",
                "updatedAt": "2020-01-31T07:58:55.742100473Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "d3b26266-0fcd-4c48-9d10-b07f3caba0f0",
                "originalText": "Today I'm going to be talking about word embeddings, which I think are one of the coolest things you can do with machine learning right now and to explain why I think that I'm just going to jump right in and give you an example of something you can do with word weddings. So a few months ago, I set up a program that just downloads tweets a people write tweets on Twitter and saves them to a file and after running this program from there to I had collected a massive file with over five gigabytes of tweets after I can press is it so I got a massive amount of data and it's just raw data that people typed on the internet after getting all this data. I fed it directly into a word embedding algorithm which was able to figure out tons of relationships between words just from the raw things that people happen to type on the internet. So for example, if I put in a color it can tell me a bunch of other colors it never actually knew the idea of color going in it didn't really know anything and piece together that all these words are related. You can see I can put in other things like a kind of And I get other kinds of food out and I'll actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words. So you're probably wondering how this actually worked because it's kind of baffling all I gave it was things that people typed on the internet. I didn't give it a dictionary. I didn't give it a list of synonyms. I mean I chose English, but I could have chosen any language and it still would have been just as successful. So it's pretty impressive that it was able to do this just from raw text. And in this video, I really want to explain why this works and how it works and just I think it's super cool. So I want to share it with you. So pretty much every word embedding algorithm uses the idea of context and to show you what I mean. I'm just going to give you a really simple example. So here is an example of a sentence where there's a word missing. So it says I painted the bench blank and were expected to fill in the blank. So the obvious thing ",
                "confidence": 0.8508386950000001,
                "startTime": "2020-01-31T07:53:53Z",
                "endTime": "2020-01-31T07:55:53Z",
                "duration": 120,
                "recordingId": "8f69076b-9846-4f43-866c-96f98a96259e",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "f160159d-9e30-4b7f-9246-9f104b96977a",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T07:55:54.483470176Z",
                "updatedAt": "2020-01-31T07:56:55.925507977Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "e1b6ba78-88cf-441e-833c-60ba230190a0",
                "originalText": "Blair with 64 neurons and then using more neural network magic. I'll produce back out a vector maybe with a hundred thousand components in each neuron and that output Vector corresponds to a word as well. And I want every neuron to be set that is in the context and I want every neuron that wasn't in the context not to be set. So why does this work? Why do we expect that the middle layer when the word gets turned into a small Vector? Why do we expect that small Vector to actually be meeting? Awful. Well, the answer is that that small Vector is all the network has to figure out the context. So, you know, it goes right from that small Vector to the context. So if two words have very similar contexts, it's really helpful for the neural network. If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense. So essentially this model is just forcing the middle layer of the neural network. To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that's what's easiest for the network to do. So that's kind of just a really General overview of how weird to deck works and there's a lot more to it. So I'll have a link to the actual original word defect paper and the description if you want to read more about it. So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this. Thing called The co-occurrence Matrix. So here's a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing. For example with Twitter data, we might just Loop through all the tweets go through all the words and all those tweets and for every time two words occur in the same. ",
                "confidence": 0.8895777924999999,
                "startTime": "2020-01-31T08:01:53Z",
                "endTime": "2020-01-31T08:03:53Z",
                "duration": 120,
                "recordingId": "89a0a921-dd20-43d7-885f-c95963d302ec",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "f160159d-9e30-4b7f-9246-9f104b96977a",
                "isEndOfSentence": true,
                "createdAt": "2020-01-31T08:03:54.300772735Z",
                "updatedAt": "2020-01-31T08:04:35.665568535Z",
                "deletedAt": null,
                "deleted": false
            },
            {
                "id": "f3cc5613-f30d-4bce-948a-1a1d991c0c3c",
                "originalText": "Meaning as I can into the vectors. So how are we actually going to do that? How are we going to you know solve for vectors for all of the words that ever appear on Twitter? You know, how are we going to produce the set of vectors that work? So the first approach I'm going to be talking about is known as word Tyvek and it's probably the most famous kind of word of matting because it was the first word embedding to get the kind of impressive results that word embeddings state-of-the-art word embeddings get today. So essentially we're and devack is just a really simple neural network. And if you've already seen my other videos on neural networks, you might actually already be able to implement or defect but I'm just going to try to describe it here on a high level to give you an idea of how it works. So here's a really simple picture of what a word to Veeck neural network. Looks like you essentially feed in a word and it produces in the middle. It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail. I'm actually going to give an example of something we might ask a word Tyvek neural network to do. What I've done is I've picked out a random tweet from my Corpus and I picked out a random word from within that tweet. In this case. It was yellow and I'm going to feed as input the word yellow to the word Tyvek neural network, and I'm going to try to get it as output to give me all the other words that were in the Tweet. So the word Avec neural network in this case is just trying to picked context words from a word. So how exactly is it that I feed in the word yellow and I get out all these contacts words. You know, how do I represent that for the neural network? Well, basically the network has a different input neuron for each different word. So I take the neuron for whatever word I want to feed in and I'll set that neuron to one and I'll set all the other neurons to 0 then the neural network will just use regular neural network stuff to produce a small Vector. Basically, that's just a ",
                "confidence": 0.7964470849999999,
                "startTime": "2020-01-31T07:59:53Z",
                "endTime": "2020-01-31T08:01:53Z",
                "duration": 120,
                "recordingId": "4aa2a291-a4a9-4a31-b260-5f669b1a03fa",
                "spokenBy": "716067a60a1a4034abc49a12ecafb39b",
                "languageCode": "en-US",
                "transcriber": "google_speech_api",
                "status": "completed",
                "transcriptId": "f160159d-9e30-4b7f-9246-9f104b96977a",
                "isEndOfSentence": false,
                "createdAt": "2020-01-31T08:01:54.256366018Z",
                "updatedAt": "2020-01-31T08:02:35.800730451Z",
                "deletedAt": null,
                "deleted": false
            }
        ]
    }}