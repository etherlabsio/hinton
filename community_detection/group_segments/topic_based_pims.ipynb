{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T10:00:32.508599Z",
     "start_time": "2019-10-14T10:00:32.471648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T10:00:32.908196Z",
     "start_time": "2019-10-14T10:00:32.654982Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T07:40:18.128408Z",
     "start_time": "2019-10-15T07:40:17.658160Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('topic_testing/validation_set.txt', 'rb') as f:\n",
    "    request = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T07:40:23.191387Z",
     "start_time": "2019-10-15T07:40:19.752138Z"
    }
   },
   "outputs": [],
   "source": [
    "from main import handler\n",
    "\n",
    "res = handler(request, None)\n",
    "group = json.loads(res['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T10:35:32.826188Z",
     "start_time": "2019-10-03T10:35:18.213Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## visualization\n",
    "import iso8601\n",
    "from datetime import datetime\n",
    "# meeting start time.\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n",
    "\n",
    "#m_time = formatTime(\"2019-09-19T06:05:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-22T09:37:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-16T09:53:21Z\", True)\n",
    "m_time = formatTime(\"2019-09-30T10:08:00Z\", True)\n",
    "for i in group['group'].keys():\n",
    "    print (\"\\n\\n\\nPIMs \", i)\n",
    "    print (\"\\n\\nDiscussion:\\n\\n \")\n",
    "    for seg in group['group'][i]:\n",
    "        print (\"Minutes from the start of the meeting: \", formatTime(seg['startTime'], True) - m_time , seg['id'],\"\\n\")\n",
    "        print (seg['originalText'],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:10:47.227956Z",
     "start_time": "2019-10-08T13:10:14.001Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "import json\n",
    "aws_config = Config(\n",
    "        connect_timeout=60,\n",
    "        read_timeout=300,\n",
    "        retries={\"max_attempts\": 0},\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "lambda_client = client(\"lambda\", config=aws_config)\n",
    "\n",
    "def get_pims_score(req):\n",
    "\n",
    "    #if req_data is None:\n",
    "    #    lambda_payload = {\"body\": input_list}\n",
    "    #    print (json.dumps(lambda_payload))\n",
    "    #else:\n",
    "    #    lambda_payload = {\"body\": {\"request\": req_data, \"text_input\": input_list}}\n",
    "        \n",
    "    try:\n",
    "        #logger.info(\"Invoking lambda function\")\n",
    "        invoke_response = lambda_client.invoke(\n",
    "            FunctionName=\"pim\",\n",
    "            InvocationType=\"RequestResponse\",\n",
    "            Payload=json.dumps(req),\n",
    "        )\n",
    "        lambda_output = (\n",
    "            invoke_response[\"Payload\"].read().decode(\"utf8\")\n",
    "        )\n",
    "        response = json.loads(lambda_output)\n",
    "        status_code = response[\"statusCode\"]\n",
    "        response_body = response[\"body\"]\n",
    "\n",
    "        if status_code == 200:\n",
    "            result = json.loads(response_body)['d2vResult'][0]['distance']\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T13:10:47.229189Z",
     "start_time": "2019-10-08T13:10:42.510Z"
    }
   },
   "outputs": [],
   "source": [
    "pim_result = {}\n",
    "pim_request = {\"contextId\": request[\"body\"][\"contextId\"], \"mindId\": \"01daaqyn9gbebc92aywnxedp0c\"}\n",
    "for seg in request['body']['segments']:\n",
    "    pim_request[\"segments\"] = [seg]\n",
    "    # get_pims_score({\"body\":pim_request})\n",
    "    pim_result[seg[\"recordingId\"]] =  get_pims_score({\"body\":pim_request})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T16:28:32.051879Z",
     "start_time": "2019-09-26T16:28:14.172Z"
    }
   },
   "outputs": [],
   "source": [
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T16:28:32.053215Z",
     "start_time": "2019-09-26T16:28:20.726Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_pim = {\n",
    "}\n",
    "group_result = {}\n",
    "for keys in group['group'].keys():\n",
    "    for seg in group['group'][keys]:\n",
    "        group_result[seg['recordingId']] = keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T16:28:32.054521Z",
     "start_time": "2019-09-26T16:28:23.077Z"
    }
   },
   "outputs": [],
   "source": [
    "ranked_pims = sorted([(k,v) for (k,v) in pim_result.items()], key= lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T16:28:32.055869Z",
     "start_time": "2019-09-26T16:28:25.308Z"
    }
   },
   "outputs": [],
   "source": [
    "used_topics = []\n",
    "group_no = None\n",
    "index = 0\n",
    "for (rec_id, distance) in ranked_pims:\n",
    "    if rec_id in group_result.keys():\n",
    "        group_no = group_result[rec_id]\n",
    "        \n",
    "        if group_no not in used_topics:\n",
    "            topic_pim[index] = group_no\n",
    "            used_topics.append(group_no)\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:24:15.350052Z",
     "start_time": "2019-09-24T12:24:15.315045Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_pim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:27:53.496456Z",
     "start_time": "2019-09-24T12:27:53.452298Z"
    }
   },
   "outputs": [],
   "source": [
    "final_output = []\n",
    "final_output = list(map(lambda x: group['group'][x] , topic_pim.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:29:57.846996Z",
     "start_time": "2019-09-24T12:29:57.814595Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:30:55.676310Z",
     "start_time": "2019-09-24T12:30:55.644859Z"
    }
   },
   "outputs": [],
   "source": [
    "users = []\n",
    "for result in final_output:\n",
    "    temp_users = []\n",
    "    for seg in result:\n",
    "        if seg['spokenBy'] not in temp_users:\n",
    "            temp_users.append(seg['spokenBy'])\n",
    "    users.append(temp_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:30:57.563763Z",
     "start_time": "2019-09-24T12:30:57.531893Z"
    }
   },
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"level\": \"warning\", \"filename\": \"tokenization_openai.py\", \"lineno\": 96, \"module\": \"tokenization_openai\", \"ts\": \"2019-11-05T13:09:05.368077Z\", \"msg\": \"ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\"}\n",
      "{\"level\": \"info\", \"filename\": \"tokenization_utils.py\", \"lineno\": 416, \"module\": \"tokenization_utils\", \"ts\": \"2019-11-05T13:09:05.394911Z\", \"msg\": \"Adding _start_ to the vocabulary\"}\n",
      "{\"level\": \"info\", \"filename\": \"tokenization_utils.py\", \"lineno\": 416, \"module\": \"tokenization_utils\", \"ts\": \"2019-11-05T13:09:05.395430Z\", \"msg\": \"Adding _delimiter_ to the vocabulary\"}\n",
      "{\"level\": \"info\", \"filename\": \"tokenization_utils.py\", \"lineno\": 416, \"module\": \"tokenization_utils\", \"ts\": \"2019-11-05T13:09:05.395815Z\", \"msg\": \"Adding _classify_ to the vocabulary\"}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/arjun/BERT_Similarity_experiments/code/\")\n",
    "import gpt_feat_utils\n",
    "\n",
    "gpt_model = gpt_feat_utils.GPT_SimInference(\"/home/arjun/gpt_experiments/models/model_lm+sim_ep3/\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.099517345428467"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"indoors Let's see. The other alternate approaches install ether meet as a zoom integration as mm along to along with slack, correct? And these are the assumptions a with Irma Terrazas.com level information ether meet is able to listen for the books on recording available across here come the meat is able to infer from the recording available what the meeting was and who the participants on the call and who the owner for the call was extract the user information basically their email address. And hopefully Enterprise. Oh, Matt, people will have their own email addresses properly. They cannot Pollard cannot create a call without it the signing into Zoom. So you'll always get the email address with that email address. We'll figure out what the corresponding slack user ID is for that email address and pick that slack user ID and send them a DM saying that they are summaries ready if you like asked me to share with specific Channel, please. Select the channel name from the drop-down will select it because okay or did you select it and this gets submitted? \"\n",
    "text2 = \"One possibility. Ah, if the meeting is recorded, we will wait for the recording. There are two possibilities one is join the live call if there were basic a works are correct. We know that we have to listen for the recording for this meeting because we got invited and we will work get a you know will have to register as a Zoom app wait for the call back.\"\n",
    "gpt_model.get_sim_score(text1, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.302844047546387"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"hello\"\n",
    "text2 = \"hello\"\n",
    "gpt_model.get_sim_score(text1, text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing topic level pims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read json Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:08:19.066028Z",
     "start_time": "2019-10-16T12:08:15.539283Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "import json\n",
    "\n",
    "with open('topic_testing/sync_eng_11_05(2).txt', 'rb') as f:\n",
    "    request = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment:  Yeah, okay. So on the okay. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Once again everyone else. No, please. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yeah, a couple of things right on the on the community based teams. We had seen some somewhat off of the track pins yesterday. So Shri was looking into that and he said there is a small bug in the recent deployment that he did from the error perspective, but from the logic it's illogical book, so he's going to fix that today and And that s*** - or at least improve improve the way we are seeing the community based films. I mean it wouldn't be so off but still parallely. We are also discussing on how to how to take more control of the communities instead of relying on this, you know sensitive instead of making making it more or less sensitive. Actually So currently it's very sensitive to any any Community formation and other aspects so at a high level like a posted in the engineering Channel We are looking at ways to take take more control of the graph formation. And they and also the body called the Dead the relationship formation the graph Community formation. So it needs some experimentation. We are we are discussing on that and also parallel. We are what you call experimenting so we should have when we will keep you posted on this. But as of this is this what it is we have we have to address some of the The problems with the community formation for the wherein you know hierarchies are not really a currently hierarchies are not in our control. So it's like a blanket hierarchy for Direction. So we have to take control of that. That means we should be fuzzy about it. We shouldn't be doing like a blanket formation so which which is the problem that we have. I mean currently solving a tap from a more technical standpoint. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Let's say we even do this. Is there a like a test case of something that we can build to validate the performance or say consistently. This is based on this these test cases. This is better than the what we have right now. Yes. If you give you a set of tests we take the same meeting transcripts again, and it is funded through a processor that compares different help. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yeah, we have the test set. Actually we can we can we will continue to increase the number of you know test calls our test a transcripts that we have and that is also driven by the algorithm or the first is that we're developing currently what all the tested all the tests meetings that we have our segment Centre, right? So then we started to gather this, you know, this body called community-centric tests that we have. Going to build so there is also parallel going on. So any validation would happen on that. W? \n",
      "\n",
      "\n",
      "\n",
      "Segment:  And yeah on the action items, I just spoke to pressure on top Christian. So he said he has got something to update on his side one studies in if we if we enable the if we enable it for ether engineering Channel, I think it will be easy for us to take feedback and then and then improve on top of it and I unlock one for the on the quality. Whereas I think we fairly have good. The trans at least whatever it detects would be highly confident and can be reliable. So I'm a little aggressive there just to be sure that we are getting good sleep. And so I think we can enable that for The Ether engineering Channel. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  So the regression that Shri fix like the bug is it already in production? \n",
      "\n",
      "\n",
      "\n",
      "Segment:  I'm not yet. So that's a logical work. That's not there in production yet. So he still testing their right so we actually made it. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  I don't know whether it looks locally on. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  You fix the bug. I think at least as of now, I don't I don't think that's in production yet. Yeah. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  So maybe once he fixes that eyeshadow from the meeting ID for which the this thing we have if you want to fix it, maybe let him run it against the same system and see what food is. Yeah. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  He did is he said he said there's a lot of noise. You must be you never alone. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yeah, so there's a lot of noise in the segments actually. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  But the bim's did look, okay, like if you look at the films as opposed to the chapters, yeah, there's not necessarily bad. It's actually pretty decent terms if this is a detective, yeah. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  We'll find out but that's also we have seen we have seen few noises segments. That's why God or whether you are in an open. Mike. Well, you're on the actual headphones are on the open window the own societies that a call and iOS and you just get the \n",
      "\n",
      "\n",
      "\n",
      "Segment:  On the table and started speaking Yeah, it was environment and it doesn't open. Yeah, so that could have been \n",
      "\n",
      "\n",
      "\n",
      "Segment:  So because of that there is actually an overlap on the segments. So that's the reason that's excellent cook. Yeah, there is no there is a there is no breakage between two kind of two segments. So they just got merged into a single one. So so be \n",
      "\n",
      "\n",
      "\n",
      "Segment:  That's because you speaking continuously we were not pausing. Actually we were just it's to is already well informed discussion like in our mind and we just discussing to and fro and it would be similar to alakazoom recording where it's just play back and there's no miss a stronger silence points between signals. Yep. Think would be the example you see ya. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Until sighs first, right? So whether there is a way to improve the quality of key phrases, I think we shouldn't see any difference between the pins and communities from the key phrase quality perspective if I think because the input to them is different and then the inherently the communities were back. You got the back key phrases. I think it's the yeah. So otherwise if the community algorithm is good at the key phrases would be at least as good as the pin pin ches. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Correct. So you're saying because of the regression and communities it trickle down to the effects of it. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yeah, because eventually that's what we see, right? Yeah, if the community is good and I like like they have enough content. I think whatever we see for the pins would reflect their so let's go God. Yeah. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Okay, maybe one sure he's done. Let him if he's ready to push to production the fix letting just announce it and then yeah. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yeah, it's not that that's what yeah. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  One, okay one says yeah, that's one says here maybe. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  He is waiting is what you do. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  I think I can online. Can you hear me? \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yeah, so yeah, pretty much those were the things those are the things that I'm working on. So like converting. So removing this network X in the middle and having everything I need for meeting level and topics markers everything and and yeah, so so I'm going to continue working on that and so I will create a new issue where I will put the specs for the rick wanted watches and related meetings. How's it going? What is the approach and what it requires as input? And the other thing is I'll make the changes for the community-based summary. So we had this question that you're going to get you that list of groups and with analyzed take all right. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  That is one position the one where we don't perform population of graph if you provider. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  if you provide like a \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Little feature flag thing with popular graph is equal to false. So that was in place. But now I think in the request before you also get different modified text. I just want to keep as the original. So these were like hard-coded in the current key phrase except he faces function so I need okay. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Okay, let me start over. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Remove using textile in squared. To put like a and List It altered text. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Okay. Okay. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  this suggests that redundant key phrases \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yeah, I saw that for some reason this duplication post-processing thing did not catch a right way of loading lower case in all the people's. Okay. That's why it goes like Okay. So we've got a two redundant funds even ignored. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Plurals there is in cases and so that's surprising. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  So in before post-processing like in the original list that it got it had two more slack Channel slack and slack Channel keywords. So where there was like sea and Earth has capitals and small and everything. So Google those got filtered but these who still came up. So so something like some fun videos that I'm checking on me like an ideal way to remove. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  I think that's it from the era 18, right? Yeah. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Patient you want to go ahead. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  So our interest you to be comments in my beard so I can be can merge this change and then I started looking on this service implementation of this. So basically, I mean since there was no Stables, you're not maintaining any tables for actions. No, it's taking some time. So by end of day today, I'll end up dating curtain on my status. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Anything else you think I can make them do you want to go ahead? \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yogurt for somebody new the new community-based summary, right? So I have mode all the analysis code from transcription service. It's a new service analysis service and even the schema thing. So the changes are there like it still yet to be tested or that it was. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  So now the logical summary services like aggregator as you expand the right so it calls the analysis are with to start analyzed based on the analyzer. So slick if it is a segment strategy will go with volt annulment analyzer. Otherwise, we'll go with their Community segment analyzer the the that's all like and also the summary we also moved to a different schema all those push the code now Cody's first time ever. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  No doubt, my great you never mind. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  My it is also done. Yeah instead of you do so. Those things are done. Like maybe you can take a look at the code ones. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Okay sure. If should I start reviewing the code again, or do you know? \n",
      "\n",
      "\n",
      "\n",
      "Segment:  I just have to ask for really quick. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Once you are done just send a request a show. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Right. Now what I have done is the key phrase since it doesn't have the popular graph flag red line has just commented it out because I'm not sure whether it works or not if I sent a text. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Sign stating so once he is done, I will just we can do a new PR or whatever if it if we are delayed with the pr we can address that issue. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  This just check which is shank. I don't think it would break necessarily break anything it. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Then we believe might break so. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Perfect. Okay, I think we can move to parsa for the box seat update. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  So like I have completed the basic integration plus the participant list and have completed active speaker part, but need to test it out. So currently I'm working on the recorder and so like I think I should be able to complete it in couple of hours and Then once it's done, I'll deploy it on stage in two. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Okay, but should you need any assistance from the front thinking of testing or evaluating certain part pieces of the yeah. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  So like deep is here. He is helping me out. So mostly like maybe like we'll do it together and this chain 1 E both will help with the testing. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Okay good. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Why did you want to go ahead with the oh by the way past my have made the changes for box Eden staging in jail. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yeah, it is working. But just now I created a call it was showing that throwing some error that looks like wait. Let me just get that error. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Okay, let me check. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Looks like there is a problem. Please try later. Let me just try again. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yeah, okay. So I'm taking the consumer key and all that from the meeting info now sure. Yeah. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Analysis would have been filled because of the schema. It was not me. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Now it is done there. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  There's one more error. I think I'll check. It's on. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Why did you want to go ahead Dixie? \n",
      "\n",
      "\n",
      "\n",
      "Segment:  stop stop \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Looking now like some basic dancing. It's \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Finally you can do some good. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Queen \n",
      "\n",
      "\n",
      "\n",
      "Segment:  anything how's everything? \n",
      "\n",
      "\n",
      "\n",
      "Segment:  At the end the call by any chance. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  What are you talking about? \n",
      "\n",
      "\n",
      "\n",
      "Segment:  we've been under which iOS \n",
      "\n",
      "\n",
      "\n",
      "Segment:  yeah, because \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Hello. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  I just staggered a bit or for adding that bug snack logs the issues. I was with me yesterday. What recent call was Capcom was kept on loading right spinning. So once the build is done, we'll just test it again and then a sinking I haven't yet started on the localization task. Just thinking maybe I can put refactor on hold for some time and then either start localization or take a pig roasting GitHub issues regarding the time taken from splash screen to the mesh will pay, you know, \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Let's scrub through the GitHub issues for any bugs. Definitely. I think the biggest issue we are facing is with operate like being able to use the app at all. Like I'm constantly seeing the red network error going back and forth to the canteen. So I think that should be like a higher priority. So \n",
      "\n",
      "\n",
      "\n",
      "Segment:  People can go to the shoes. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  On picking up from the terrorist list the dear. Yeah the now the safe open. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Unbox it with the spatula today is helping testing in order. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Let me know if you need anything on the platform side that I second. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Should we talk about Zoom one tree? \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Anytime your places where we need to call the apis and we need to figure out a way what they do what Zoom Force when they when somebody installs an apple with zoom account. So you need to get the account ID and last term if I remember there are few things missing. There was no way to get contact ID another thing. So even if I will check if they have change in there because they're not giving the parent account ID. They were given some sub account. Eid are U second to have multiple sub accounts, so we need to understand a little bit about \n",
      "\n",
      "\n",
      "\n",
      "Segment:  So I will focus on getting showed it and sort of how the zoom option works and other things we need to look at to the tokens also because the tokens are expiring though considering the cooperate. So let me see what I can find. I'll just add all the information to document whatever I find a husband. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Let's do all the investigations and maybe record in a GitHub issue. I just create an effect on base everything. That's I shared that. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Do you think that? \n",
      "\n",
      "\n",
      "\n",
      "Segment:  It was like a low-hanging fruit that because you were already working on it. You just get that thing. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  My birthday Carrie how they are we how we are not sure if you are going to be use the web SDK or not. So I'm just want to find out all the details and then we can proceed we can because we have clear understanding how to disable Decay passwords. Just need to see even even with the believability cave might need to associate one workspace to 1 inch and when Zoom account so still need to do this, this is something that we are in we have to do account to slack, you know some Association. Okay. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  The best Decay is something like with the weather latest Rebels because we need to rework on our client because just by upgrading the JS library is not working. So we need to spend a little bit more time on that. So I once Parsha finishes on walks it we can you can you can directly find every after like once this is free. I can also look into upgrading that JavaScript. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Let's create an epic and paste all the discussions. We had one. So we'll just keep track of all the investigations. We do a lot of things, okay. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  That's it. We should also I think a for resubmission of The Ether of the production. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yeah, so sorry said like let's find out on this room thing. Once we are clear and we are we are clear about if you need more permissions with respect to Z more any other fruit implementations will just park it and if you need anything any new changes will Loot and they submit later because we are finding new things as we are implementing new things, right, so We're just gonna put this way. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  A lot of losing out on the data because now through God we cannot even capture history as its history. So at least we should start getting data as my concern you can in a soil to check but that's why I mean more main concern actually with this one because we realize that we are not able to use the body butter going to capture Channel History right anymore. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Yeah, but at the very least. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Be sure to save for a resubmission at least the same start getting messages not lose data the Roadies. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Sorry. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  Okay, let's let's see what size it is. \n",
      "\n",
      "\n",
      "\n",
      "Segment:  but \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seg_list = []\n",
    "for seg in request[\"body\"][\"segments\"]:\n",
    "    seg_list.append((seg[\"originalText\"], seg[\"startTime\"]))\n",
    "seg_list = sorted(seg_list, key=lambda kv:kv[1], reverse=False)\n",
    "\n",
    "for seg in seg_list:\n",
    "    print (\"Segment: \", seg[0])\n",
    "    print (\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Groups for the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:09:56.535906Z",
     "start_time": "2019-10-16T12:08:19.069721Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /tmp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /tmp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 72, \"module\": \"scorer\", \"batches count\": 1, \"number of sentences\": 111, \"ts\": \"2019-11-06T14:15:38.982188Z\", \"msg\": \"computing in batches\"}\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 77, \"module\": \"scorer\", \"iteration count:\": 0, \"ts\": \"2019-11-06T14:15:38.985269Z\", \"msg\": \"getting feature vector from mind service\"}\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 79, \"module\": \"scorer\", \"iteration count\": 0, \"ts\": \"2019-11-06T14:16:28.297907Z\", \"msg\": \"Request Sent\"}\n",
      "recieved Feature Vector\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 103, \"module\": \"scorer\", \"ts\": \"2019-11-06T14:16:29.984468Z\", \"msg\": \"Response Recieved\"}\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 212, \"module\": \"grouper_segments\", \"nodes: \": 111, \"edges: \": 6105, \"ts\": \"2019-11-06T14:16:30.237038Z\", \"msg\": \"Normalising the Graph\"}\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 221, \"module\": \"grouper_segments\", \"nodes: \": 111, \"edges: \": 6215, \"ts\": \"2019-11-06T14:16:30.254567Z\", \"msg\": \"Completed Normalization\"}\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 223, \"module\": \"grouper_segments\", \"nodes: \": 111, \"edges: \": 6104, \"ts\": \"2019-11-06T14:16:30.255002Z\", \"msg\": \"Completed Normalization and after removing diagonal values\"}\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 242, \"module\": \"grouper_segments\", \"outlier threshold is : \": 0.005106647596218299, \"ts\": \"2019-11-06T14:16:30.263367Z\", \"msg\": \"Outlier Score\"}\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 624, \"module\": \"grouper_segments\", \"edges before prunning\": 6104, \"edges after prunning\": 6104, \"modularity\": 0.060061180134656625, \"ts\": \"2019-11-06T14:16:30.397061Z\", \"msg\": \"Meeting Graph results\"}\n",
      "cluster =========>\n",
      "Let is say we even do this.\n",
      "Is there a like a test case of something that we can build to validate the performance or say consistently.\n",
      "This is based on this these test cases.\n",
      "This is better than the what we have right now.\n",
      "If you give you a set of tests we take the same meeting transcripts again, and it is funded through a processor that compares different help.\n",
      "We had seen some somewhat off of the track pins yesterday.\n",
      "We are also discussing on how to how to take more control of the communities instead of relying on this, you know sensitive instead of making making it more or less sensitive.\n",
      "Actually So currently it is very sensitive to any any Community formation and other aspects so at a high level like a posted in the engineering Channel We are looking at ways to take take more control of the graph formation.\n",
      "And they and also the body called the Dead the relationship formation the graph Community formation.\n",
      "We are we are discussing on that and also parallel.\n",
      "We are what you call experimenting so we should have when we will keep you posted on this.\n",
      "But as of this is this what it is we have we have to address some of the The problems with the community formation for the wherein you know hierarchies are not really a currently hierarchies are not in our control.\n",
      "So it is like a blanket hierarchy for Direction.\n",
      "So we have to take control of that.\n",
      "That means we should be fuzzy about it.\n",
      "We should not be doing like a blanket formation so which which is the problem that we have.\n",
      "I mean currently solving a tap from a more technical standpoint.\n",
      "Actually we can we can we will continue to increase the number of you know test calls our test a transcripts that we have and that is also driven by the algorithm or the first is that we are developing currently what all the tested all the tests meetings that we have our segment Centre, right?\n",
      "So then we started to gather this, you know, this body called communitycentric tests that we have.\n",
      "Going to build so there is also parallel going on.\n",
      "So any validation would happen on that.\n",
      "Whereas I think we fairly have good.\n",
      "The trans at least whatever it detects would be highly confident and can be reliable.\n",
      "That is not there in production yet.\n",
      "Well find out but that is also we have seen we have seen few noises segments.\n",
      "So because of that there is actually an overlap on the segments.\n",
      "So that is the reason that is excellent cook.\n",
      "Yeah, there is no there is a there is no breakage between two kind of two segments.\n",
      "That is because you speaking continuously we were not pausing.\n",
      "Think would be the example you see ya.\n",
      "So otherwise if the community algorithm is good at the key phrases would be at least as good as the pin pin ches.\n",
      "What is the approach and what it requires as input?\n",
      "So we had this question that you are going to get you that list of groups and with analyzed take all right.\n",
      "That is why it goes like Okay.\n",
      "So we have got a two redundant funds even ignored.\n",
      "So basically, I mean since there was no Stables, you are not maintaining any tables for actions.\n",
      "So the changes are there like it still yet to be tested or that it was.\n",
      "Let is scrub through the GitHub issues for any bugs.\n",
      "I think the biggest issue we are facing is with operate like being able to use the app at all.\n",
      "Let is do all the investigations and maybe record in a GitHub issue.\n",
      "So we need to spend a little bit more time on that.\n",
      "Let is create an epic and paste all the discussions.\n",
      "So we will just keep track of all the investigations.\n",
      "We do a lot of things, okay.\n",
      "cluster =========>\n",
      "Yeah, a couple of things right on the on the community based teams.\n",
      "So Shri was looking into that and he said there is a small bug in the recent deployment that he did from the error perspective, but from the logic it is illogical book, so he is going to fix that today and And that s  or at least improve improve the way we are seeing the community based films.\n",
      "I mean it would not be so off but still parallely.\n",
      "And yeah on the action items, I just spoke to pressure on top Christian.\n",
      "So I am a little aggressive there just to be sure that we are getting good sleep.\n",
      "So he still testing their right so we actually made it.\n",
      "But the bim is did look, okay, like if you look at the films as opposed to the chapters, yeah, there is not necessarily bad.\n",
      "It is actually pretty decent terms if this is a detective, yeah.\n",
      "That is why God or whether you are in an open.\n",
      "So they just got merged into a single one.\n",
      "Yeah, because eventually that is what we see, right?\n",
      "Yeah, if the community is good and I like like they have enough content.\n",
      "Okay, maybe one sure he is done.\n",
      "Yeah, so yeah, pretty much those were the things those are the things that I am working on.\n",
      "So removing this network X in the middle and having everything I need for meeting level and topics markers everything and and yeah, so so I am going to continue working on that and so I will create a new issue where I will put the specs for the rick wanted watches and related meetings.\n",
      "And the other thing is I will make the changes for the communitybased summary.\n",
      "I just want to keep as the original.\n",
      "Yeah, I saw that for some reason this duplication postprocessing thing did not catch a right way of loading lower case in all the people is.\n",
      "So where there was like sea and Earth has capitals and small and everything.\n",
      "So so something like some fun videos that I am checking on me like an ideal way to remove.\n",
      "So by end of day today, I will end up dating curtain on my status.\n",
      "So like I have completed the basic integration plus the participant list and have completed active speaker part, but need to test it out.\n",
      "So currently I am working on the recorder and so like I think I should be able to complete it in couple of hours and Then once it is done, I will deploy it on stage in two.\n",
      "I just staggered a bit or for adding that bug snack logs the issues.\n",
      "Like I am constantly seeing the red network error going back and forth to the canteen.\n",
      "So I think that should be like a higher priority.\n",
      "There was no way to get contact ID another thing.\n",
      "So let me see what I can find.\n",
      "I will just add all the information to document whatever I find a husband.\n",
      "I just create an effect on base everything.\n",
      "cluster =========>\n",
      "So he said he has got something to update on his side one studies in if we if we enable the if we enable it for ether engineering Channel, I think it will be easy for us to take feedback and then and then improve on top of it and I unlock one for the on the quality.\n",
      "And so I think we can enable that for The Ether engineering Channel.\n",
      "Well, you are on the actual headphones are on the open window the own societies that a call and iOS and you just get the\n",
      "Actually we were just it is to is already well informed discussion like in our mind and we just discussing to and fro and it would be similar to alakazoom recording where it is just play back and there is no miss a stronger silence points between signals.\n",
      "So whether there is a way to improve the quality of key phrases, I think we should not see any difference between the pins and communities from the key phrase quality perspective if I think because the input to them is different and then the inherently the communities were back.\n",
      "I think whatever we see for the pins would reflect their so let us go God.\n",
      "Let him if he is ready to push to production the fix letting just announce it and then yeah.\n",
      "Little feature flag thing with popular graph is equal to false.\n",
      "But now I think in the request before you also get different modified text.\n",
      "So these were like hardcoded in the current key phrase except he faces function so I need okay.\n",
      "So in before postprocessing like in the original list that it got it had two more slack Channel slack and slack Channel keywords.\n",
      "So Google those got filtered but these who still came up.\n",
      "So our interest you to be comments in my beard so I can be can merge this change and then I started looking on this service implementation of this.\n",
      "Yogurt for somebody new the new communitybased summary, right?\n",
      "So I have mode all the analysis code from transcription service.\n",
      "It is a new service analysis service and even the schema thing.\n",
      "So now the logical summary services like aggregator as you expand the right so it calls the analysis are with to start analyzed based on the analyzer.\n",
      "So slick if it is a segment strategy will go with volt annulment analyzer.\n",
      "Otherwise, we will go with their Community segment analyzer the the that is all like and also the summary we also moved to a different schema all those push the code now Cody is first time ever.\n",
      "What recent call was Capcom was kept on loading right spinning.\n",
      "So once the build is done, we will just test it again and then a sinking I have not yet started on the localization task.\n",
      "Just thinking maybe I can put refactor on hold for some time and then either start localization or take a pig roasting GitHub issues regarding the time taken from splash screen to the mesh will pay, you know,\n",
      "Anytime your places where we need to call the apis and we need to figure out a way what they do what Zoom Force when they when somebody installs an apple with zoom account.\n",
      "So you need to get the account ID and last term if I remember there are few things missing.\n",
      "So even if I will check if they have change in there because they are not giving the parent account ID.\n",
      "Eid are U second to have multiple sub accounts, so we need to understand a little bit about\n",
      "So I will focus on getting showed it and sort of how the zoom option works and other things we need to look at to the tokens also because the tokens are expiring though considering the cooperate.\n",
      "My birthday Carrie how they are we how we are not sure if you are going to be use the web SDK or not.\n",
      "So I am just want to find out all the details and then we can proceed we can because we have clear understanding how to disable Decay passwords.\n",
      "Just need to see even even with the believability cave might need to associate one workspace to XnumberX inch and when Zoom account so still need to do this, this is something that we are in we have to do account to slack, you know some Association.\n",
      "The best Decay is something like with the weather latest Rebels because we need to rework on our client because just by upgrading the JS library is not working.\n",
      "So I once Parsha finishes on walks it we can you can you can directly find every after like once this is free.\n",
      "I can also look into upgrading that JavaScript.\n",
      "Yeah, so sorry said like let us find out on this room thing.\n",
      "Once we are clear and we are we are clear about if you need more permissions with respect to Z more any other fruit implementations will just park it and if you need anything any new changes will Loot and they submit later because we are finding new things as we are implementing new things, right, so Were just gonna put this way.\n",
      "A lot of losing out on the data because now through God we cannot even capture history as its history.\n",
      "So at least we should start getting data as my concern you can in a soil to check but that is why I mean more main concern actually with this one because we realize that we are not able to use the body butter going to capture Channel History right anymore.\n",
      "cluster =========>\n",
      "Let is say we even do this.\n",
      "Is there a like a test case of something that we can build to validate the performance or say consistently.\n",
      "This is based on this these test cases.\n",
      "This is better than the what we have right now.\n",
      "If you give you a set of tests we take the same meeting transcripts again, and it is funded through a processor that compares different help.\n",
      "We had seen some somewhat off of the track pins yesterday.\n",
      "We are also discussing on how to how to take more control of the communities instead of relying on this, you know sensitive instead of making making it more or less sensitive.\n",
      "Actually So currently it is very sensitive to any any Community formation and other aspects so at a high level like a posted in the engineering Channel We are looking at ways to take take more control of the graph formation.\n",
      "And they and also the body called the Dead the relationship formation the graph Community formation.\n",
      "We are we are discussing on that and also parallel.\n",
      "We are what you call experimenting so we should have when we will keep you posted on this.\n",
      "But as of this is this what it is we have we have to address some of the The problems with the community formation for the wherein you know hierarchies are not really a currently hierarchies are not in our control.\n",
      "So it is like a blanket hierarchy for Direction.\n",
      "So we have to take control of that.\n",
      "That means we should be fuzzy about it.\n",
      "We should not be doing like a blanket formation so which which is the problem that we have.\n",
      "I mean currently solving a tap from a more technical standpoint.\n",
      "Actually we can we can we will continue to increase the number of you know test calls our test a transcripts that we have and that is also driven by the algorithm or the first is that we are developing currently what all the tested all the tests meetings that we have our segment Centre, right?\n",
      "So then we started to gather this, you know, this body called communitycentric tests that we have.\n",
      "Going to build so there is also parallel going on.\n",
      "So any validation would happen on that.\n",
      "So because of that there is actually an overlap on the segments.\n",
      "So that is the reason that is excellent cook.\n",
      "Yeah, there is no there is a there is no breakage between two kind of two segments.\n",
      "That is because you speaking continuously we were not pausing.\n",
      "Think would be the example you see ya.\n",
      "That is why it goes like Okay.\n",
      "So we have got a two redundant funds even ignored.\n",
      "Let is create an epic and paste all the discussions.\n",
      "So we will just keep track of all the investigations.\n",
      "We do a lot of things, okay.\n",
      "cluster =========>\n",
      "So he still testing their right so we actually made it.\n",
      "But the bim is did look, okay, like if you look at the films as opposed to the chapters, yeah, there is not necessarily bad.\n",
      "It is actually pretty decent terms if this is a detective, yeah.\n",
      "Yeah, because eventually that is what we see, right?\n",
      "Yeah, if the community is good and I like like they have enough content.\n",
      "Yeah, so yeah, pretty much those were the things those are the things that I am working on.\n",
      "So removing this network X in the middle and having everything I need for meeting level and topics markers everything and and yeah, so so I am going to continue working on that and so I will create a new issue where I will put the specs for the rick wanted watches and related meetings.\n",
      "And the other thing is I will make the changes for the communitybased summary.\n",
      "So like I have completed the basic integration plus the participant list and have completed active speaker part, but need to test it out.\n",
      "So currently I am working on the recorder and so like I think I should be able to complete it in couple of hours and Then once it is done, I will deploy it on stage in two.\n",
      "Like I am constantly seeing the red network error going back and forth to the canteen.\n",
      "So I think that should be like a higher priority.\n",
      "So let me see what I can find.\n",
      "I will just add all the information to document whatever I find a husband.\n",
      "I just create an effect on base everything.\n",
      "cluster =========>\n",
      "So he said he has got something to update on his side one studies in if we if we enable the if we enable it for ether engineering Channel, I think it will be easy for us to take feedback and then and then improve on top of it and I unlock one for the on the quality.\n",
      "And so I think we can enable that for The Ether engineering Channel.\n",
      "Well, you are on the actual headphones are on the open window the own societies that a call and iOS and you just get the\n",
      "So whether there is a way to improve the quality of key phrases, I think we should not see any difference between the pins and communities from the key phrase quality perspective if I think because the input to them is different and then the inherently the communities were back.\n",
      "Let him if he is ready to push to production the fix letting just announce it and then yeah.\n",
      "Little feature flag thing with popular graph is equal to false.\n",
      "But now I think in the request before you also get different modified text.\n",
      "So these were like hardcoded in the current key phrase except he faces function so I need okay.\n",
      "So in before postprocessing like in the original list that it got it had two more slack Channel slack and slack Channel keywords.\n",
      "So Google those got filtered but these who still came up.\n",
      "So our interest you to be comments in my beard so I can be can merge this change and then I started looking on this service implementation of this.\n",
      "Yogurt for somebody new the new communitybased summary, right?\n",
      "So I have mode all the analysis code from transcription service.\n",
      "It is a new service analysis service and even the schema thing.\n",
      "So now the logical summary services like aggregator as you expand the right so it calls the analysis are with to start analyzed based on the analyzer.\n",
      "So slick if it is a segment strategy will go with volt annulment analyzer.\n",
      "Otherwise, we will go with their Community segment analyzer the the that is all like and also the summary we also moved to a different schema all those push the code now Cody is first time ever.\n",
      "What recent call was Capcom was kept on loading right spinning.\n",
      "So once the build is done, we will just test it again and then a sinking I have not yet started on the localization task.\n",
      "Just thinking maybe I can put refactor on hold for some time and then either start localization or take a pig roasting GitHub issues regarding the time taken from splash screen to the mesh will pay, you know,\n",
      "Anytime your places where we need to call the apis and we need to figure out a way what they do what Zoom Force when they when somebody installs an apple with zoom account.\n",
      "So you need to get the account ID and last term if I remember there are few things missing.\n",
      "So even if I will check if they have change in there because they are not giving the parent account ID.\n",
      "Eid are U second to have multiple sub accounts, so we need to understand a little bit about\n",
      "My birthday Carrie how they are we how we are not sure if you are going to be use the web SDK or not.\n",
      "So I am just want to find out all the details and then we can proceed we can because we have clear understanding how to disable Decay passwords.\n",
      "Just need to see even even with the believability cave might need to associate one workspace to XnumberX inch and when Zoom account so still need to do this, this is something that we are in we have to do account to slack, you know some Association.\n",
      "The best Decay is something like with the weather latest Rebels because we need to rework on our client because just by upgrading the JS library is not working.\n",
      "So I once Parsha finishes on walks it we can you can you can directly find every after like once this is free.\n",
      "I can also look into upgrading that JavaScript.\n",
      "Yeah, so sorry said like let us find out on this room thing.\n",
      "Once we are clear and we are we are clear about if you need more permissions with respect to Z more any other fruit implementations will just park it and if you need anything any new changes will Loot and they submit later because we are finding new things as we are implementing new things, right, so Were just gonna put this way.\n",
      "A lot of losing out on the data because now through God we cannot even capture history as its history.\n",
      "So at least we should start getting data as my concern you can in a soil to check but that is why I mean more main concern actually with this one because we realize that we are not able to use the body butter going to capture Channel History right anymore.\n",
      "Checking Timerange --------------\n",
      "--------------\n",
      "So it is like a blanket hierarchy for Direction.\n",
      "We had seen some somewhat off of the track pins yesterday.\n",
      "That means we should be fuzzy about it.\n",
      "We are also discussing on how to how to take more control of the communities instead of relying on this, you know sensitive instead of making making it more or less sensitive.\n",
      "We are we are discussing on that and also parallel.\n",
      "We are what you call experimenting so we should have when we will keep you posted on this.\n",
      "Actually So currently it is very sensitive to any any Community formation and other aspects so at a high level like a posted in the engineering Channel We are looking at ways to take take more control of the graph formation.\n",
      "We should not be doing like a blanket formation so which which is the problem that we have.\n",
      "And they and also the body called the Dead the relationship formation the graph Community formation.\n",
      "I mean currently solving a tap from a more technical standpoint.\n",
      "So we have to take control of that.\n",
      "But as of this is this what it is we have we have to address some of the The problems with the community formation for the wherein you know hierarchies are not really a currently hierarchies are not in our control.\n",
      "If you give you a set of tests we take the same meeting transcripts again, and it is funded through a processor that compares different help.\n",
      "This is based on this these test cases.\n",
      "This is better than the what we have right now.\n",
      "Is there a like a test case of something that we can build to validate the performance or say consistently.\n",
      "Let is say we even do this.\n",
      "Going to build so there is also parallel going on.\n",
      "Actually we can we can we will continue to increase the number of you know test calls our test a transcripts that we have and that is also driven by the algorithm or the first is that we are developing currently what all the tested all the tests meetings that we have our segment Centre, right?\n",
      "So any validation would happen on that.\n",
      "So then we started to gather this, you know, this body called communitycentric tests that we have.\n",
      "So because of that there is actually an overlap on the segments.\n",
      "So that is the reason that is excellent cook.\n",
      "Yeah, there is no there is a there is no breakage between two kind of two segments.\n",
      "That is because you speaking continuously we were not pausing.\n",
      "Think would be the example you see ya.\n",
      "So we have got a two redundant funds even ignored.\n",
      "That is why it goes like Okay.\n",
      "Let is create an epic and paste all the discussions.\n",
      "We do a lot of things, okay.\n",
      "So we will just keep track of all the investigations.\n",
      "--------------\n",
      "So he still testing their right so we actually made it.\n",
      "It is actually pretty decent terms if this is a detective, yeah.\n",
      "But the bim is did look, okay, like if you look at the films as opposed to the chapters, yeah, there is not necessarily bad.\n",
      "Yeah, because eventually that is what we see, right?\n",
      "Yeah, if the community is good and I like like they have enough content.\n",
      "And the other thing is I will make the changes for the communitybased summary.\n",
      "Yeah, so yeah, pretty much those were the things those are the things that I am working on.\n",
      "So removing this network X in the middle and having everything I need for meeting level and topics markers everything and and yeah, so so I am going to continue working on that and so I will create a new issue where I will put the specs for the rick wanted watches and related meetings.\n",
      "So currently I am working on the recorder and so like I think I should be able to complete it in couple of hours and Then once it is done, I will deploy it on stage in two.\n",
      "So like I have completed the basic integration plus the participant list and have completed active speaker part, but need to test it out.\n",
      "So I think that should be like a higher priority.\n",
      "Like I am constantly seeing the red network error going back and forth to the canteen.\n",
      "So let me see what I can find.\n",
      "I will just add all the information to document whatever I find a husband.\n",
      "I just create an effect on base everything.\n",
      "--------------\n",
      "So he said he has got something to update on his side one studies in if we if we enable the if we enable it for ether engineering Channel, I think it will be easy for us to take feedback and then and then improve on top of it and I unlock one for the on the quality.\n",
      "And so I think we can enable that for The Ether engineering Channel.\n",
      "Well, you are on the actual headphones are on the open window the own societies that a call and iOS and you just get the\n",
      "So whether there is a way to improve the quality of key phrases, I think we should not see any difference between the pins and communities from the key phrase quality perspective if I think because the input to them is different and then the inherently the communities were back.\n",
      "Let him if he is ready to push to production the fix letting just announce it and then yeah.\n",
      "So these were like hardcoded in the current key phrase except he faces function so I need okay.\n",
      "Little feature flag thing with popular graph is equal to false.\n",
      "But now I think in the request before you also get different modified text.\n",
      "So in before postprocessing like in the original list that it got it had two more slack Channel slack and slack Channel keywords.\n",
      "So Google those got filtered but these who still came up.\n",
      "So our interest you to be comments in my beard so I can be can merge this change and then I started looking on this service implementation of this.\n",
      "It is a new service analysis service and even the schema thing.\n",
      "So I have mode all the analysis code from transcription service.\n",
      "Yogurt for somebody new the new communitybased summary, right?\n",
      "Otherwise, we will go with their Community segment analyzer the the that is all like and also the summary we also moved to a different schema all those push the code now Cody is first time ever.\n",
      "So slick if it is a segment strategy will go with volt annulment analyzer.\n",
      "So now the logical summary services like aggregator as you expand the right so it calls the analysis are with to start analyzed based on the analyzer.\n",
      "Just thinking maybe I can put refactor on hold for some time and then either start localization or take a pig roasting GitHub issues regarding the time taken from splash screen to the mesh will pay, you know,\n",
      "What recent call was Capcom was kept on loading right spinning.\n",
      "So once the build is done, we will just test it again and then a sinking I have not yet started on the localization task.\n",
      "So even if I will check if they have change in there because they are not giving the parent account ID.\n",
      "Anytime your places where we need to call the apis and we need to figure out a way what they do what Zoom Force when they when somebody installs an apple with zoom account.\n",
      "Eid are U second to have multiple sub accounts, so we need to understand a little bit about\n",
      "So you need to get the account ID and last term if I remember there are few things missing.\n",
      "My birthday Carrie how they are we how we are not sure if you are going to be use the web SDK or not.\n",
      "So I am just want to find out all the details and then we can proceed we can because we have clear understanding how to disable Decay passwords.\n",
      "Just need to see even even with the believability cave might need to associate one workspace to XnumberX inch and when Zoom account so still need to do this, this is something that we are in we have to do account to slack, you know some Association.\n",
      "I can also look into upgrading that JavaScript.\n",
      "So I once Parsha finishes on walks it we can you can you can directly find every after like once this is free.\n",
      "The best Decay is something like with the weather latest Rebels because we need to rework on our client because just by upgrading the JS library is not working.\n",
      "Once we are clear and we are we are clear about if you need more permissions with respect to Z more any other fruit implementations will just park it and if you need anything any new changes will Loot and they submit later because we are finding new things as we are implementing new things, right, so Were just gonna put this way.\n",
      "Yeah, so sorry said like let us find out on this room thing.\n",
      "A lot of losing out on the data because now through God we cannot even capture history as its history.\n",
      "So at least we should start getting data as my concern you can in a soil to check but that is why I mean more main concern actually with this one because we realize that we are not able to use the body butter going to capture Channel History right anymore.\n",
      "<---------------->\n",
      "order difference: 1\n",
      "Relevant sentence:  But as of this is this what it is we have we have to address some of the The problems with the community formation for the wherein you know hierarchies are not really a currently hierarchies are not in our control.    =====    If you give you a set of tests we take the same meeting transcripts again, and it is funded through a processor that compares different help.\n",
      "order difference: 1\n",
      "Relevant sentence:  Let is say we even do this.    =====    Going to build so there is also parallel going on.\n",
      "Not Relevant sentence:  So then we started to gather this, you know, this body called communitycentric tests that we have.    !=    So because of that there is actually an overlap on the segments.\n",
      "order difference: 1\n",
      "Relevant sentence:  Yeah, there is no there is a there is no breakage between two kind of two segments.    =====    That is because you speaking continuously we were not pausing.\n",
      "Not Relevant sentence:  Think would be the example you see ya.    !=    So we have got a two redundant funds even ignored.\n",
      "Not Relevant sentence:  That is why it goes like Okay.    !=    Let is create an epic and paste all the discussions.\n",
      "order difference: 1\n",
      "Relevant sentence:  So he still testing their right so we actually made it.    =====    It is actually pretty decent terms if this is a detective, yeah.\n",
      "Not Relevant sentence:  But the bim is did look, okay, like if you look at the films as opposed to the chapters, yeah, there is not necessarily bad.    !=    Yeah, because eventually that is what we see, right?\n",
      "Not Relevant sentence:  Yeah, if the community is good and I like like they have enough content.    !=    And the other thing is I will make the changes for the communitybased summary.\n",
      "Not Relevant sentence:  So removing this network X in the middle and having everything I need for meeting level and topics markers everything and and yeah, so so I am going to continue working on that and so I will create a new issue where I will put the specs for the rick wanted watches and related meetings.    !=    So currently I am working on the recorder and so like I think I should be able to complete it in couple of hours and Then once it is done, I will deploy it on stage in two.\n",
      "Not Relevant sentence:  So like I have completed the basic integration plus the participant list and have completed active speaker part, but need to test it out.    !=    So I think that should be like a higher priority.\n",
      "Not Relevant sentence:  Like I am constantly seeing the red network error going back and forth to the canteen.    !=    So let me see what I can find.\n",
      "order difference: 1\n",
      "Relevant sentence:  I will just add all the information to document whatever I find a husband.    =====    I just create an effect on base everything.\n",
      "Not Relevant sentence:  And so I think we can enable that for The Ether engineering Channel.    !=    Well, you are on the actual headphones are on the open window the own societies that a call and iOS and you just get the\n",
      "Not Relevant sentence:  Well, you are on the actual headphones are on the open window the own societies that a call and iOS and you just get the    !=    So whether there is a way to improve the quality of key phrases, I think we should not see any difference between the pins and communities from the key phrase quality perspective if I think because the input to them is different and then the inherently the communities were back.\n",
      "Not Relevant sentence:  So whether there is a way to improve the quality of key phrases, I think we should not see any difference between the pins and communities from the key phrase quality perspective if I think because the input to them is different and then the inherently the communities were back.    !=    Let him if he is ready to push to production the fix letting just announce it and then yeah.\n",
      "Not Relevant sentence:  Let him if he is ready to push to production the fix letting just announce it and then yeah.    !=    So these were like hardcoded in the current key phrase except he faces function so I need okay.\n",
      "Not Relevant sentence:  But now I think in the request before you also get different modified text.    !=    So in before postprocessing like in the original list that it got it had two more slack Channel slack and slack Channel keywords.\n",
      "order difference: 1\n",
      "Relevant sentence:  So Google those got filtered but these who still came up.    =====    So our interest you to be comments in my beard so I can be can merge this change and then I started looking on this service implementation of this.\n",
      "order difference: 1\n",
      "Relevant sentence:  So our interest you to be comments in my beard so I can be can merge this change and then I started looking on this service implementation of this.    =====    It is a new service analysis service and even the schema thing.\n",
      "order difference: 1\n",
      "Relevant sentence:  Yogurt for somebody new the new communitybased summary, right?    =====    Otherwise, we will go with their Community segment analyzer the the that is all like and also the summary we also moved to a different schema all those push the code now Cody is first time ever.\n",
      "Not Relevant sentence:  So now the logical summary services like aggregator as you expand the right so it calls the analysis are with to start analyzed based on the analyzer.    !=    Just thinking maybe I can put refactor on hold for some time and then either start localization or take a pig roasting GitHub issues regarding the time taken from splash screen to the mesh will pay, you know,\n",
      "Not Relevant sentence:  So once the build is done, we will just test it again and then a sinking I have not yet started on the localization task.    !=    So even if I will check if they have change in there because they are not giving the parent account ID.\n",
      "Not Relevant sentence:  So you need to get the account ID and last term if I remember there are few things missing.    !=    My birthday Carrie how they are we how we are not sure if you are going to be use the web SDK or not.\n",
      "order difference: 1\n",
      "Relevant sentence:  Just need to see even even with the believability cave might need to associate one workspace to XnumberX inch and when Zoom account so still need to do this, this is something that we are in we have to do account to slack, you know some Association.    =====    I can also look into upgrading that JavaScript.\n",
      "Not Relevant sentence:  The best Decay is something like with the weather latest Rebels because we need to rework on our client because just by upgrading the JS library is not working.    !=    Once we are clear and we are we are clear about if you need more permissions with respect to Z more any other fruit implementations will just park it and if you need anything any new changes will Loot and they submit later because we are finding new things as we are implementing new things, right, so Were just gonna put this way.\n",
      "order difference: 1\n",
      "Relevant sentence:  Yeah, so sorry said like let us find out on this room thing.    =====    A lot of losing out on the data because now through God we cannot even capture history as its history.\n",
      "2 26\n",
      "8 26\n",
      "14 26\n",
      "5 26\n",
      "10 26\n",
      "12 26\n",
      "19 26\n",
      "21 26\n",
      "24 26\n",
      "3 26\n",
      "6 26\n",
      "9 26\n",
      "11 26\n",
      "13 26\n",
      "18 26\n",
      "20 26\n",
      "22 26\n",
      "26 26\n",
      "appending extra segment based on order:  {'id': '3488311a426940fd924c7df70ed64cb5', 'originalText': \"Let's create an epic and paste all the discussions. We had one. So we'll just keep track of all the investigations. We do a lot of things, okay. \", 'confidence': 0.89713645, 'startTime': '2019-11-05T06:57:16Z', 'endTime': '2019-11-05T06:57:27Z', 'duration': 11, 'recordingId': '6d1fa5551ae6425c99442e116ffb4f5d', 'spokenBy': '62b6ae1d7f834b0bb2055f7c72bc3368', 'languageCode': 'en-IN', 'transcriber': 'google_speech_api', 'status': 'completed', 'transcriptId': '008e5dac-2438-4d55-aff6-3764a88db97f', 'createdAt': '2019-11-05T06:57:27.829044982Z', 'updatedAt': '2019-11-05T06:57:48.80692033Z', 'deletedAt': None, 'deleted': False} 17\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 716, \"module\": \"grouper_segments\", \"PIMs\": {\"0\": {\"segment0\": [[\"Yeah, a couple of things right on the on the community based teams. We had seen some somewhat off of the track pins yesterday. So Shri was looking into that and he said there is a small bug in the recent deployment that he did from the error perspective, but from the logic it's illogical book, so he's going to fix that today and And that s*** - or at least improve improve the way we are seeing the community based films. I mean it wouldn't be so off but still parallely. We are also discussing on how to how to take more control of the communities instead of relying on this, you know sensitive instead of making making it more or less sensitive. Actually So currently it's very sensitive to any any Community formation and other aspects so at a high level like a posted in the engineering Channel We are looking at ways to take take more control of the graph formation. And they and also the body called the Dead the relationship formation the graph Community formation. So it needs some experimentation. We are we are discussing on that and also parallel. We are what you call experimenting so we should have when we will keep you posted on this. But as of this is this what it is we have we have to address some of the The problems with the community formation for the wherein you know hierarchies are not really a currently hierarchies are not in our control. So it's like a blanket hierarchy for Direction. So we have to take control of that. That means we should be fuzzy about it. We shouldn't be doing like a blanket formation so which which is the problem that we have. I mean currently solving a tap from a more technical standpoint. \"], \"2019-11-05T06:33:54Z\", \"b1e8787a9a1f4859ac11cbb6a8124fd9\", \"516dce6af97741929a1da5e52d0d4a16\"], \"segment1\": [[\"Let's say we even do this. Is there a like a test case of something that we can build to validate the performance or say consistently. This is based on this these test cases. This is better than the what we have right now. Yes. If you give you a set of tests we take the same meeting transcripts again, and it is funded through a processor that compares different help. \"], \"2019-11-05T06:35:50Z\", \"62b6ae1d7f834b0bb2055f7c72bc3368\", \"f621ac9d6aba42159cb4a49132967749\"], \"segment2\": [[\"Yeah, we have the test set. Actually we can we can we will continue to increase the number of you know test calls our test a transcripts that we have and that is also driven by the algorithm or the first is that we're developing currently what all the tested all the tests meetings that we have our segment Centre, right? So then we started to gather this, you know, this body called community-centric tests that we have. Going to build so there is also parallel going on. So any validation would happen on that. W? \"], \"2019-11-05T06:36:22Z\", \"b1e8787a9a1f4859ac11cbb6a8124fd9\", \"e4331d0b261b4239a82fba773bf0301c\"]}, \"1\": {\"segment0\": [[\"So I will focus on getting showed it and sort of how the zoom option works and other things we need to look at to the tokens also because the tokens are expiring though considering the cooperate. So let me see what I can find. I'll just add all the information to document whatever I find a husband. \"], \"2019-11-05T06:55:16Z\", \"1a21542584494fcaba957d768b595b80\", \"353c7be408ed49cfa9bb90a35c8d6ca1\"], \"segment1\": [[\"Let's do all the investigations and maybe record in a GitHub issue. I just create an effect on base everything. That's I shared that. \"], \"2019-11-05T06:55:41Z\", \"62b6ae1d7f834b0bb2055f7c72bc3368\", \"eca9a1e4c11f42a1a121533618f24a71\"]}, \"2\": {\"segment0\": [[\"I just staggered a bit or for adding that bug snack logs the issues. I was with me yesterday. What recent call was Capcom was kept on loading right spinning. So once the build is done, we'll just test it again and then a sinking I haven't yet started on the localization task. Just thinking maybe I can put refactor on hold for some time and then either start localization or take a pig roasting GitHub issues regarding the time taken from splash screen to the mesh will pay, you know, \"], \"2019-11-05T06:51:53Z\", \"c66797a92e6d46ad9573926e57f7dac3\", \"e04864365d1949b0a9f3ebedcefd59e9\"]}, \"3\": {\"segment0\": [[\"And yeah on the action items, I just spoke to pressure on top Christian. So he said he has got something to update on his side one studies in if we if we enable the if we enable it for ether engineering Channel, I think it will be easy for us to take feedback and then and then improve on top of it and I unlock one for the on the quality. Whereas I think we fairly have good. The trans at least whatever it detects would be highly confident and can be reliable. So I'm a little aggressive there just to be sure that we are getting good sleep. And so I think we can enable that for The Ether engineering Channel. \"], \"2019-11-05T06:37:00Z\", \"b1e8787a9a1f4859ac11cbb6a8124fd9\", \"a7d82816e0d24b9dbe4f43645f5f0384\"]}, \"4\": {\"segment0\": [[\"Yeah, so yeah, pretty much those were the things those are the things that I'm working on. So like converting. So removing this network X in the middle and having everything I need for meeting level and topics markers everything and and yeah, so so I'm going to continue working on that and so I will create a new issue where I will put the specs for the rick wanted watches and related meetings. How's it going? What is the approach and what it requires as input? And the other thing is I'll make the changes for the community-based summary. So we had this question that you're going to get you that list of groups and with analyzed take all right. \"], \"2019-11-05T06:42:13Z\", \"7e7ccbba232d411aa95ad3f244a35f40\", \"2bf145322e4d421484c5be3d34c45a58\"]}, \"5\": {\"segment0\": [[\"So because of that there is actually an overlap on the segments. So that's the reason that's excellent cook. Yeah, there is no there is a there is no breakage between two kind of two segments. So they just got merged into a single one. So so be \"], \"2019-11-05T06:39:20Z\", \"b1e8787a9a1f4859ac11cbb6a8124fd9\", \"d7529d570c774cb6b87adef4784f9e26\"], \"segment1\": [[\"That's because you speaking continuously we were not pausing. Actually we were just it's to is already well informed discussion like in our mind and we just discussing to and fro and it would be similar to alakazoom recording where it's just play back and there's no miss a stronger silence points between signals. Yep. Think would be the example you see ya. \"], \"2019-11-05T06:39:36Z\", \"62b6ae1d7f834b0bb2055f7c72bc3368\", \"f8d20183a3e34389865f1df3f2003b98\"]}, \"6\": {\"segment0\": [[\"My birthday Carrie how they are we how we are not sure if you are going to be use the web SDK or not. So I'm just want to find out all the details and then we can proceed we can because we have clear understanding how to disable Decay passwords. Just need to see even even with the believability cave might need to associate one workspace to 1 inch and when Zoom account so still need to do this, this is something that we are in we have to do account to slack, you know some Association. Okay. \"], \"2019-11-05T06:56:01Z\", \"1a21542584494fcaba957d768b595b80\", \"6a565c02d9304106ab038ad6c21e686c\"], \"segment1\": [[\"The best Decay is something like with the weather latest Rebels because we need to rework on our client because just by upgrading the JS library is not working. So we need to spend a little bit more time on that. So I once Parsha finishes on walks it we can you can you can directly find every after like once this is free. I can also look into upgrading that JavaScript. \"], \"2019-11-05T06:56:44Z\", \"1a21542584494fcaba957d768b595b80\", \"2e2647dded0c4765b33cbe192410e4a5\"], \"segment2\": [\"Let's create an epic and paste all the discussions. We had one. So we'll just keep track of all the investigations. We do a lot of things, okay. \", \"62b6ae1d7f834b0bb2055f7c72bc3368\", \"2019-11-05T06:57:16Z\", \"3488311a426940fd924c7df70ed64cb5\"]}, \"7\": {\"segment0\": [[\"So in before post-processing like in the original list that it got it had two more slack Channel slack and slack Channel keywords. So where there was like sea and Earth has capitals and small and everything. So Google those got filtered but these who still came up. So so something like some fun videos that I'm checking on me like an ideal way to remove. \"], \"2019-11-05T06:44:42Z\", \"7e7ccbba232d411aa95ad3f244a35f40\", \"03bdb0a4e7ae4e3985582c5bd2dd4dca\"], \"segment1\": [[\"So our interest you to be comments in my beard so I can be can merge this change and then I started looking on this service implementation of this. So basically, I mean since there was no Stables, you're not maintaining any tables for actions. No, it's taking some time. So by end of day today, I'll end up dating curtain on my status. \"], \"2019-11-05T06:45:34Z\", \"75bdf310110b4b8fab88b16fafce920e\", \"212eb9ca06d04dec9f8928e34bd595d3\"], \"segment2\": [[\"Yogurt for somebody new the new community-based summary, right? So I have mode all the analysis code from transcription service. It's a new service analysis service and even the schema thing. So the changes are there like it still yet to be tested or that it was. \"], \"2019-11-05T06:46:19Z\", \"84fbaa66a2474ea29ae053f3a2e519d6\", \"c41fd0e424e349f2b52789f4d4c73117\"], \"segment3\": [[\"So now the logical summary services like aggregator as you expand the right so it calls the analysis are with to start analyzed based on the analyzer. So slick if it is a segment strategy will go with volt annulment analyzer. Otherwise, we'll go with their Community segment analyzer the the that's all like and also the summary we also moved to a different schema all those push the code now Cody's first time ever. \"], \"2019-11-05T06:46:42Z\", \"84fbaa66a2474ea29ae053f3a2e519d6\", \"7b400835c1eb473391b033c4b0ec9755\"]}, \"8\": {\"segment0\": [[\"I'm not yet. So that's a logical work. That's not there in production yet. So he still testing their right so we actually made it. \"], \"2019-11-05T06:37:52Z\", \"b1e8787a9a1f4859ac11cbb6a8124fd9\", \"3b5bba2275c34b6997853aa80499f939\"], \"segment1\": [[\"But the bim's did look, okay, like if you look at the films as opposed to the chapters, yeah, there's not necessarily bad. It's actually pretty decent terms if this is a detective, yeah. \"], \"2019-11-05T06:38:40Z\", \"62b6ae1d7f834b0bb2055f7c72bc3368\", \"3f528d8706c342adadc1177e4fa8f2c4\"]}, \"9\": {\"segment0\": [[\"Yeah, so sorry said like let's find out on this room thing. Once we are clear and we are we are clear about if you need more permissions with respect to Z more any other fruit implementations will just park it and if you need anything any new changes will Loot and they submit later because we are finding new things as we are implementing new things, right, so We're just gonna put this way. \"], \"2019-11-05T06:57:38Z\", \"1a21542584494fcaba957d768b595b80\", \"e708f000baeb40c0be30454cd6edb3c5\"], \"segment1\": [[\"A lot of losing out on the data because now through God we cannot even capture history as its history. So at least we should start getting data as my concern you can in a soil to check but that's why I mean more main concern actually with this one because we realize that we are not able to use the body butter going to capture Channel History right anymore. \"], \"2019-11-05T06:58:11Z\", \"62b6ae1d7f834b0bb2055f7c72bc3368\", \"9b60758a5c6945ef821942903414bf11\"]}}, \"ts\": \"2019-11-06T14:16:30.415910Z\", \"msg\": \"Final PIMs\"}\n"
     ]
    }
   ],
   "source": [
    "from main import handler\n",
    "\n",
    "res = handler(request, None)\n",
    "group = json.loads(res['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for g in group['group'].keys():\n",
    "    if len(group['group'][g])>1:\n",
    "        print (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Id:  0\n",
      "Venkata Dikshit, Karthik Muralidharan Discussed \n",
      "\n",
      " Text:  Yeah, a couple of things right on the on the community based teams. We had seen some somewhat off of the track pins yesterday. So Shri was looking into that and he said there is a small bug in the recent deployment that he did from the error perspective, but from the logic it's illogical book, so he's going to fix that today and And that s*** - or at least improve improve the way we are seeing the community based films. I mean it wouldn't be so off but still parallely. We are also discussing on how to how to take more control of the communities instead of relying on this, you know sensitive instead of making making it more or less sensitive. Actually So currently it's very sensitive to any any Community formation and other aspects so at a high level like a posted in the engineering Channel We are looking at ways to take take more control of the graph formation. And they and also the body called the Dead the relationship formation the graph Community formation. So it needs some experimentation. We are we are discussing on that and also parallel. We are what you call experimenting so we should have when we will keep you posted on this. But as of this is this what it is we have we have to address some of the The problems with the community formation for the wherein you know hierarchies are not really a currently hierarchies are not in our control. So it's like a blanket hierarchy for Direction. So we have to take control of that. That means we should be fuzzy about it. We shouldn't be doing like a blanket formation so which which is the problem that we have. I mean currently solving a tap from a more technical standpoint.  Let's say we even do this. Is there a like a test case of something that we can build to validate the performance or say consistently. This is based on this these test cases. This is better than the what we have right now. Yes. If you give you a set of tests we take the same meeting transcripts again, and it is funded through a processor that compares different help.  Yeah, we have the test set. Actually we can we can we will continue to increase the number of you know test calls our test a transcripts that we have and that is also driven by the algorithm or the first is that we're developing currently what all the tested all the tests meetings that we have our segment Centre, right? So then we started to gather this, you know, this body called community-centric tests that we have. Going to build so there is also parallel going on. So any validation would happen on that. W?  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  1\n",
      "Parshwa Nemi Jain, Nisha Yadav Discussed \n",
      "\n",
      " Text:  So like I have completed the basic integration plus the participant list and have completed active speaker part, but need to test it out. So currently I'm working on the recorder and so like I think I should be able to complete it in couple of hours and Then once it's done, I'll deploy it on stage in two.  I just staggered a bit or for adding that bug snack logs the issues. I was with me yesterday. What recent call was Capcom was kept on loading right spinning. So once the build is done, we'll just test it again and then a sinking I haven't yet started on the localization task. Just thinking maybe I can put refactor on hold for some time and then either start localization or take a pig roasting GitHub issues regarding the time taken from splash screen to the mesh will pay, you know,  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  2\n",
      "Mithun Discussed \n",
      "\n",
      " Text:  Yogurt for somebody new the new community-based summary, right? So I have mode all the analysis code from transcription service. It's a new service analysis service and even the schema thing. So the changes are there like it still yet to be tested or that it was.  So now the logical summary services like aggregator as you expand the right so it calls the analysis are with to start analyzed based on the analyzer. So slick if it is a segment strategy will go with volt annulment analyzer. Otherwise, we'll go with their Community segment analyzer the the that's all like and also the summary we also moved to a different schema all those push the code now Cody's first time ever.  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  3\n",
      "Venkata Dikshit, Karthik Muralidharan Discussed \n",
      "\n",
      " Text:  So because of that there is actually an overlap on the segments. So that's the reason that's excellent cook. Yeah, there is no there is a there is no breakage between two kind of two segments. So they just got merged into a single one. So so be  That's because you speaking continuously we were not pausing. Actually we were just it's to is already well informed discussion like in our mind and we just discussing to and fro and it would be similar to alakazoom recording where it's just play back and there's no miss a stronger silence points between signals. Yep. Think would be the example you see ya.  Until sighs first, right? So whether there is a way to improve the quality of key phrases, I think we shouldn't see any difference between the pins and communities from the key phrase quality perspective if I think because the input to them is different and then the inherently the communities were back. You got the back key phrases. I think it's the yeah. So otherwise if the community algorithm is good at the key phrases would be at least as good as the pin pin ches.  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  4\n",
      "Vamshi Krishna, Karthik Muralidharan Discussed \n",
      "\n",
      " Text:  Anytime your places where we need to call the apis and we need to figure out a way what they do what Zoom Force when they when somebody installs an apple with zoom account. So you need to get the account ID and last term if I remember there are few things missing. There was no way to get contact ID another thing. So even if I will check if they have change in there because they're not giving the parent account ID. They were given some sub account. Eid are U second to have multiple sub accounts, so we need to understand a little bit about  So I will focus on getting showed it and sort of how the zoom option works and other things we need to look at to the tokens also because the tokens are expiring though considering the cooperate. So let me see what I can find. I'll just add all the information to document whatever I find a husband.  Let's do all the investigations and maybe record in a GitHub issue. I just create an effect on base everything. That's I shared that.  My birthday Carrie how they are we how we are not sure if you are going to be use the web SDK or not. So I'm just want to find out all the details and then we can proceed we can because we have clear understanding how to disable Decay passwords. Just need to see even even with the believability cave might need to associate one workspace to 1 inch and when Zoom account so still need to do this, this is something that we are in we have to do account to slack, you know some Association. Okay.  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  5\n",
      "Vamshi Krishna, Karthik Muralidharan Discussed \n",
      "\n",
      " Text:  The best Decay is something like with the weather latest Rebels because we need to rework on our client because just by upgrading the JS library is not working. So we need to spend a little bit more time on that. So I once Parsha finishes on walks it we can you can you can directly find every after like once this is free. I can also look into upgrading that JavaScript.  Let's create an epic and paste all the discussions. We had one. So we'll just keep track of all the investigations. We do a lot of things, okay.  Yeah, so sorry said like let's find out on this room thing. Once we are clear and we are we are clear about if you need more permissions with respect to Z more any other fruit implementations will just park it and if you need anything any new changes will Loot and they submit later because we are finding new things as we are implementing new things, right, so We're just gonna put this way.  A lot of losing out on the data because now through God we cannot even capture history as its history. So at least we should start getting data as my concern you can in a soil to check but that's why I mean more main concern actually with this one because we realize that we are not able to use the body butter going to capture Channel History right anymore.  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  6\n",
      "Venkata Dikshit, Karthik Muralidharan, Shashank Discussed \n",
      "\n",
      " Text:  Yeah, because eventually that's what we see, right? Yeah, if the community is good and I like like they have enough content. I think whatever we see for the pins would reflect their so let's go God. Yeah.  Okay, maybe one sure he's done. Let him if he's ready to push to production the fix letting just announce it and then yeah.  Yeah, so yeah, pretty much those were the things those are the things that I'm working on. So like converting. So removing this network X in the middle and having everything I need for meeting level and topics markers everything and and yeah, so so I'm going to continue working on that and so I will create a new issue where I will put the specs for the rick wanted watches and related meetings. How's it going? What is the approach and what it requires as input? And the other thing is I'll make the changes for the community-based summary. So we had this question that you're going to get you that list of groups and with analyzed take all right.  Little feature flag thing with popular graph is equal to false. So that was in place. But now I think in the request before you also get different modified text. I just want to keep as the original. So these were like hard-coded in the current key phrase except he faces function so I need okay.  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  7\n",
      "Venkata Dikshit, Karthik Muralidharan Discussed \n",
      "\n",
      " Text:  And yeah on the action items, I just spoke to pressure on top Christian. So he said he has got something to update on his side one studies in if we if we enable the if we enable it for ether engineering Channel, I think it will be easy for us to take feedback and then and then improve on top of it and I unlock one for the on the quality. Whereas I think we fairly have good. The trans at least whatever it detects would be highly confident and can be reliable. So I'm a little aggressive there just to be sure that we are getting good sleep. And so I think we can enable that for The Ether engineering Channel.  I'm not yet. So that's a logical work. That's not there in production yet. So he still testing their right so we actually made it.  But the bim's did look, okay, like if you look at the films as opposed to the chapters, yeah, there's not necessarily bad. It's actually pretty decent terms if this is a detective, yeah.  We'll find out but that's also we have seen we have seen few noises segments. That's why God or whether you are in an open. Mike. Well, you're on the actual headphones are on the open window the own societies that a call and iOS and you just get the  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  8\n",
      "Shashank, Trishanth Diwate Discussed \n",
      "\n",
      " Text:  So in before post-processing like in the original list that it got it had two more slack Channel slack and slack Channel keywords. So where there was like sea and Earth has capitals and small and everything. So Google those got filtered but these who still came up. So so something like some fun videos that I'm checking on me like an ideal way to remove.  So our interest you to be comments in my beard so I can be can merge this change and then I started looking on this service implementation of this. So basically, I mean since there was no Stables, you're not maintaining any tables for actions. No, it's taking some time. So by end of day today, I'll end up dating curtain on my status.  \n",
      "\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_id_map = {}\n",
    "user_id_map = {\"3e1a008f734448b0ad9190778449af81\":\"Cullen\",\"b4a57b25de68446cac990f856d3fe4d5\":\"Cullen\",\"716067a60a1a4034abc49a12ecafb39b\":\"Cullen\",\"2f506a3d9e814de69d46a1fbf949fdc9\":\"Cullen\",\"8d6db5f7d9b74c54ba38fe710ffcaf3f\":\"Krishna Sai\", \"c66797a92e6d46ad9573926e57f7dac3\":\"Nisha Yadav\",\"31a3ba4761854ad9a041ddf1c4c6a1dc\":\"Reagan Rewop\",\"84fbaa66a2474ea29ae053f3a2e519d6\":\"Mithun\",\"75bdf310110b4b8fab88b16fafce920e\":\"Trishanth Diwate\",\"b1e8787a9a1f4859ac11cbb6a8124fd9\": \"Venkata Dikshit\", \"fb52cb663aec4795aee38ccfd904d315\":\"Reagan Rewop\", \"81a3e15469374fceba1cf972faa209b2\":\"Arjun Kini\", \"ecfeeb757f0a4d47af1ebd513929264a\":\"Shubham\", \"62b6ae1d7f834b0bb2055f7c72bc3368\":\"Karthik Muralidharan\", \"1a21542584494fcaba957d768b595b80\":\"Vamshi Krishna\", \"7e7ccbba232d411aa95ad3f244a35f40\":\"Shashank\", \"65bb83952fb54409a4bb59bb707f1375\":\"Vani\", \"0bbbfe84c66145af8d0ffcd5258bba38\":\"Parshwa Nemi Jain\"}\n",
    "\n",
    "for groupid in group['group'].keys():\n",
    "    user_list =[]\n",
    "    seg_list = []\n",
    "    keyphrase = []\n",
    "    for segi in group['group'][groupid]:\n",
    "        if segi['spokenBy'] not in user_list:\n",
    "            user_list.append(segi['spokenBy'])\n",
    "        seg_list.append(segi['originalText'])\n",
    "        #keyphrase.append(get_desc(segi['originalText']))\n",
    "    #print (\"User\", end=\" \")\n",
    "    #if len(seg_list) == 1 :\n",
    "    #    continue\n",
    "    print (\"Group Id: \", groupid)\n",
    "    print (*[user_id_map[user] for user in user_list], sep=\", \", end=\" \")\n",
    "    print (\"Discussed \\n\\n Text: \", *seg_list, \"\\n\\n  \")\n",
    "    #print (\"Keyphrases: \", end=\"\")\n",
    "    #print (*get_desc(\" \".join(sent for sent in seg_list))[1][:5], sep=\", \")\n",
    "    print ( \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for g in group['group'].keys():\n",
    "    if len(group['group'][g])>1:\n",
    "        print (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Id:  0\n",
      "Venkata Dikshit, Karthik Muralidharan Discussed \n",
      "\n",
      " Text:  Yeah, a couple of things right on the on the community based teams. We had seen some somewhat off of the track pins yesterday. So Shri was looking into that and he said there is a small bug in the recent deployment that he did from the error perspective, but from the logic it's illogical book, so he's going to fix that today and And that s*** - or at least improve improve the way we are seeing the community based films. I mean it wouldn't be so off but still parallely. We are also discussing on how to how to take more control of the communities instead of relying on this, you know sensitive instead of making making it more or less sensitive. Actually So currently it's very sensitive to any any Community formation and other aspects so at a high level like a posted in the engineering Channel We are looking at ways to take take more control of the graph formation. And they and also the body called the Dead the relationship formation the graph Community formation. So it needs some experimentation. We are we are discussing on that and also parallel. We are what you call experimenting so we should have when we will keep you posted on this. But as of this is this what it is we have we have to address some of the The problems with the community formation for the wherein you know hierarchies are not really a currently hierarchies are not in our control. So it's like a blanket hierarchy for Direction. So we have to take control of that. That means we should be fuzzy about it. We shouldn't be doing like a blanket formation so which which is the problem that we have. I mean currently solving a tap from a more technical standpoint.  Let's say we even do this. Is there a like a test case of something that we can build to validate the performance or say consistently. This is based on this these test cases. This is better than the what we have right now. Yes. If you give you a set of tests we take the same meeting transcripts again, and it is funded through a processor that compares different help.  Yeah, we have the test set. Actually we can we can we will continue to increase the number of you know test calls our test a transcripts that we have and that is also driven by the algorithm or the first is that we're developing currently what all the tested all the tests meetings that we have our segment Centre, right? So then we started to gather this, you know, this body called community-centric tests that we have. Going to build so there is also parallel going on. So any validation would happen on that. W?  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  1\n",
      "Vamshi Krishna, Karthik Muralidharan Discussed \n",
      "\n",
      " Text:  So I will focus on getting showed it and sort of how the zoom option works and other things we need to look at to the tokens also because the tokens are expiring though considering the cooperate. So let me see what I can find. I'll just add all the information to document whatever I find a husband.  Let's do all the investigations and maybe record in a GitHub issue. I just create an effect on base everything. That's I shared that.  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  5\n",
      "Venkata Dikshit, Karthik Muralidharan Discussed \n",
      "\n",
      " Text:  So because of that there is actually an overlap on the segments. So that's the reason that's excellent cook. Yeah, there is no there is a there is no breakage between two kind of two segments. So they just got merged into a single one. So so be  That's because you speaking continuously we were not pausing. Actually we were just it's to is already well informed discussion like in our mind and we just discussing to and fro and it would be similar to alakazoom recording where it's just play back and there's no miss a stronger silence points between signals. Yep. Think would be the example you see ya.  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  6\n",
      "Vamshi Krishna, Karthik Muralidharan Discussed \n",
      "\n",
      " Text:  My birthday Carrie how they are we how we are not sure if you are going to be use the web SDK or not. So I'm just want to find out all the details and then we can proceed we can because we have clear understanding how to disable Decay passwords. Just need to see even even with the believability cave might need to associate one workspace to 1 inch and when Zoom account so still need to do this, this is something that we are in we have to do account to slack, you know some Association. Okay.  The best Decay is something like with the weather latest Rebels because we need to rework on our client because just by upgrading the JS library is not working. So we need to spend a little bit more time on that. So I once Parsha finishes on walks it we can you can you can directly find every after like once this is free. I can also look into upgrading that JavaScript.  Let's create an epic and paste all the discussions. We had one. So we'll just keep track of all the investigations. We do a lot of things, okay.  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  7\n",
      "Shashank, Trishanth Diwate, Mithun Discussed \n",
      "\n",
      " Text:  So in before post-processing like in the original list that it got it had two more slack Channel slack and slack Channel keywords. So where there was like sea and Earth has capitals and small and everything. So Google those got filtered but these who still came up. So so something like some fun videos that I'm checking on me like an ideal way to remove.  So our interest you to be comments in my beard so I can be can merge this change and then I started looking on this service implementation of this. So basically, I mean since there was no Stables, you're not maintaining any tables for actions. No, it's taking some time. So by end of day today, I'll end up dating curtain on my status.  Yogurt for somebody new the new community-based summary, right? So I have mode all the analysis code from transcription service. It's a new service analysis service and even the schema thing. So the changes are there like it still yet to be tested or that it was.  So now the logical summary services like aggregator as you expand the right so it calls the analysis are with to start analyzed based on the analyzer. So slick if it is a segment strategy will go with volt annulment analyzer. Otherwise, we'll go with their Community segment analyzer the the that's all like and also the summary we also moved to a different schema all those push the code now Cody's first time ever.  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  8\n",
      "Venkata Dikshit, Karthik Muralidharan Discussed \n",
      "\n",
      " Text:  I'm not yet. So that's a logical work. That's not there in production yet. So he still testing their right so we actually made it.  But the bim's did look, okay, like if you look at the films as opposed to the chapters, yeah, there's not necessarily bad. It's actually pretty decent terms if this is a detective, yeah.  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "Group Id:  9\n",
      "Vamshi Krishna, Karthik Muralidharan Discussed \n",
      "\n",
      " Text:  Yeah, so sorry said like let's find out on this room thing. Once we are clear and we are we are clear about if you need more permissions with respect to Z more any other fruit implementations will just park it and if you need anything any new changes will Loot and they submit later because we are finding new things as we are implementing new things, right, so We're just gonna put this way.  A lot of losing out on the data because now through God we cannot even capture history as its history. So at least we should start getting data as my concern you can in a soil to check but that's why I mean more main concern actually with this one because we realize that we are not able to use the body butter going to capture Channel History right anymore.  \n",
      "\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_id_map = {}\n",
    "user_id_map = {\"3e1a008f734448b0ad9190778449af81\":\"Cullen\",\"b4a57b25de68446cac990f856d3fe4d5\":\"Cullen\",\"716067a60a1a4034abc49a12ecafb39b\":\"Cullen\",\"2f506a3d9e814de69d46a1fbf949fdc9\":\"Cullen\",\"8d6db5f7d9b74c54ba38fe710ffcaf3f\":\"Krishna Sai\", \"c66797a92e6d46ad9573926e57f7dac3\":\"Nisha Yadav\",\"31a3ba4761854ad9a041ddf1c4c6a1dc\":\"Reagan Rewop\",\"84fbaa66a2474ea29ae053f3a2e519d6\":\"Mithun\",\"75bdf310110b4b8fab88b16fafce920e\":\"Trishanth Diwate\",\"b1e8787a9a1f4859ac11cbb6a8124fd9\": \"Venkata Dikshit\", \"fb52cb663aec4795aee38ccfd904d315\":\"Reagan Rewop\", \"81a3e15469374fceba1cf972faa209b2\":\"Arjun Kini\", \"ecfeeb757f0a4d47af1ebd513929264a\":\"Shubham\", \"62b6ae1d7f834b0bb2055f7c72bc3368\":\"Karthik Muralidharan\", \"1a21542584494fcaba957d768b595b80\":\"Vamshi Krishna\", \"7e7ccbba232d411aa95ad3f244a35f40\":\"Shashank\", \"65bb83952fb54409a4bb59bb707f1375\":\"Vani\", \"0bbbfe84c66145af8d0ffcd5258bba38\":\"Parshwa Nemi Jain\"}\n",
    "\n",
    "for groupid in group['group'].keys():\n",
    "    user_list =[]\n",
    "    seg_list = []\n",
    "    keyphrase = []\n",
    "    for segi in group['group'][groupid]:\n",
    "        if segi['spokenBy'] not in user_list:\n",
    "            user_list.append(segi['spokenBy'])\n",
    "        seg_list.append(segi['originalText'])\n",
    "        #keyphrase.append(get_desc(segi['originalText']))\n",
    "    #print (\"User\", end=\" \")\n",
    "    if len(seg_list) == 1 :\n",
    "        continue\n",
    "    print (\"Group Id: \", groupid)\n",
    "    print (*[user_id_map[user] for user in user_list], sep=\", \", end=\" \")\n",
    "    print (\"Discussed \\n\\n Text: \", *seg_list, \"\\n\\n  \")\n",
    "    #print (\"Keyphrases: \", end=\"\")\n",
    "    #print (*get_desc(\" \".join(sent for sent in seg_list))[1][:5], sep=\", \")\n",
    "    print ( \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:10:08.092248Z",
     "start_time": "2019-10-16T12:09:56.539286Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_id_map = {}\n",
    "user_id_map = {\"716067a60a1a4034abc49a12ecafb39b\":\"Ether\",\"2f506a3d9e814de69d46a1fbf949fdc9\":\"ether\",\"8d6db5f7d9b74c54ba38fe710ffcaf3f\":\"Krishna Sai\", \"c66797a92e6d46ad9573926e57f7dac3\":\"Nisha Yadav\",\"31a3ba4761854ad9a041ddf1c4c6a1dc\":\"Reagan Rewop\",\"84fbaa66a2474ea29ae053f3a2e519d6\":\"Mithun\",\"75bdf310110b4b8fab88b16fafce920e\":\"Trishanth Diwate\",\"b1e8787a9a1f4859ac11cbb6a8124fd9\": \"Venkata Dikshit\", \"fb52cb663aec4795aee38ccfd904d315\":\"Reagan Rewop\", \"81a3e15469374fceba1cf972faa209b2\":\"Arjun Kini\", \"ecfeeb757f0a4d47af1ebd513929264a\":\"Shubham\", \"62b6ae1d7f834b0bb2055f7c72bc3368\":\"Karthik Muralidharan\", \"1a21542584494fcaba957d768b595b80\":\"Vamshi Krishna\", \"7e7ccbba232d411aa95ad3f244a35f40\":\"Shashank\", \"65bb83952fb54409a4bb59bb707f1375\":\"Vani\", \"0bbbfe84c66145af8d0ffcd5258bba38\":\"Parshwa Nemi Jain\"}\n",
    "\n",
    "for groupid in group['group'].keys():\n",
    "    user_list =[]\n",
    "    seg_list = []\n",
    "    keyphrase = []\n",
    "    for segi in group['group'][groupid]:\n",
    "        if segi['spokenBy'] not in user_list:\n",
    "            user_list.append(segi['spokenBy'])\n",
    "        seg_list.append(segi['originalText'])\n",
    "        #keyphrase.append(get_desc(segi['originalText']))\n",
    "    #print (\"User\", end=\" \")\n",
    "    print (\"Group Id: \", groupid)\n",
    "    print (*[user_id_map[user] for user in user_list], sep=\", \", end=\" \")\n",
    "    print (\"Discussed \\n\\n Text: \", *seg_list, \"\\n\\n  \")\n",
    "    #print (\"Keyphrases: \", end=\"\")\n",
    "    #print (*get_desc(\" \".join(sent for sent in seg_list))[1][:5], sep=\", \")\n",
    "    print ( \"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim_result = {}\n",
    "pim_response = {}\n",
    "pim_request = {\"contextId\": request[\"body\"][\"contextId\"], \"mindId\": \"01DAAYHEKY5F4E02QVRJPTFTXV\"}\n",
    "pim_response = {\"contextId\": request[\"body\"][\"contextId\"], \"mindId\": \"01DAAYHEKY5F4E02QVRJPTFTXV\", \"segments\": []}\n",
    "temp = \"\"\n",
    "temp_users = []\n",
    "for groupid in group['group'].keys():\n",
    "    temp = \"\"\n",
    "    temp_users = []\n",
    "    for segi in group['group'][groupid]:\n",
    "        temp+=segi[\"originalText\"]\n",
    "        temp_users.append(segi[\"spokenBy\"])\n",
    "    pim_response[\"segments\"].append({\"id\":\"abc\",\"originalText\":temp,\"spokenBy\":temp_users})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_pims_score({\"body\":pim_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pim = json.loads(result)['d2vResult']\n",
    "pim = sorted(pim, key=lambda kv:kv[\"distance\"], reverse=False)\n",
    "for seg in pim:\n",
    "    print ( \" , \".join(list(set(user_id_map[i] for i in seg[\"speaker\"]))), \" discussed: \\n\", seg[\"text\"], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get PIMs for the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:32:07.101125Z",
     "start_time": "2019-10-16T09:32:07.014546Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "import json\n",
    "aws_config = Config(\n",
    "        connect_timeout=60,\n",
    "        read_timeout=300,\n",
    "        retries={\"max_attempts\": 0},\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "lambda_client = client(\"lambda\", config=aws_config)\n",
    "\n",
    "def get_pims_score(req):\n",
    "\n",
    "    #if req_data is None:\n",
    "    #    lambda_payload = {\"body\": input_list}\n",
    "    #    print (json.dumps(lambda_payload))\n",
    "    #else:\n",
    "    #    lambda_payload = {\"body\": {\"request\": req_data, \"text_input\": input_list}}\n",
    "        \n",
    "    try:\n",
    "        #logger.info(\"Invoking lambda function\")\n",
    "        invoke_response = lambda_client.invoke(\n",
    "            FunctionName=\"pim\",\n",
    "            InvocationType=\"RequestResponse\",\n",
    "            Payload=json.dumps(req),\n",
    "        )\n",
    "        lambda_output = (\n",
    "            invoke_response[\"Payload\"].read().decode(\"utf8\")\n",
    "        )\n",
    "        response = json.loads(lambda_output)\n",
    "        status_code = response[\"statusCode\"]\n",
    "        response_body = response[\"body\"]\n",
    "\n",
    "        #if status_code == 200:\n",
    "        #    result = json.loads(response_body)['d2vResult'][0]['distance']\n",
    "        return response_body\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:32:55.418939Z",
     "start_time": "2019-10-16T09:32:07.762456Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pim_result = {}\n",
    "pim_response = {}\n",
    "pim_request = {\"contextId\": request[\"body\"][\"contextId\"], \"mindId\": \"01DAAYHEKY5F4E02QVRJPTFTXV\"}\n",
    "pim_response = {\"contextId\": request[\"body\"][\"contextId\"], \"mindId\": \"01DAAYHEKY5F4E02QVRJPTFTXV\", \"segments\": []}\n",
    "temp = {}\n",
    "for seg in request['body']['segments']:\n",
    "    pim_request[\"segments\"] = [seg]\n",
    "    # get_pims_score({\"body\":pim_request})\n",
    "    pim_result[seg[\"recordingId\"]] =  get_pims_score({\"body\":pim_request})\n",
    "    temp = seg\n",
    "    temp[\"distance\"] = pim_result[seg[\"recordingId\"]]\n",
    "    pim_response[\"segments\"].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:33:47.292227Z",
     "start_time": "2019-10-16T09:33:47.205140Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for seg in pim_response[\"segments\"]:\n",
    "    result.append( (seg[\"originalText\"], seg[\"distance\"], seg[\"recordingId\"]))\n",
    "result = sorted(result, key=lambda kv:kv[1])\n",
    "for (text, score, segid) in result:\n",
    "    print (text , \" =====> \", score, segid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T07:16:12.948463Z",
     "start_time": "2019-10-16T07:16:12.892361Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract topic level pims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:33:49.539609Z",
     "start_time": "2019-10-16T09:33:49.494149Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import extract_topic_pims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:33:52.577313Z",
     "start_time": "2019-10-16T09:33:52.520939Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from extract_topic_pims.main import handler\n",
    "\n",
    "res = handler({\"body\":{\"groups\": group[\"group\"], \"pims\": pim_response}}, None)\n",
    "final_pims = json.loads(res)[\"body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:33:56.517504Z",
     "start_time": "2019-10-16T09:33:56.462922Z"
    }
   },
   "outputs": [],
   "source": [
    "user_id_map = {}\n",
    "user_id_map = {\"2f506a3d9e814de69d46a1fbf949fdc9\":\"Shubham\",\"2cd90f0674f348cc922acd6b8782ba0f\":\"Shubham\",\"8d6db5f7d9b74c54ba38fe710ffcaf3f\":\"Krishna Sai\",\"31a3ba4761854ad9a041ddf1c4c6a1dc\":\"Reagan Rewop\",\"b4a57b25de68446cac990f856d3fe4d5\":\"Deep Moradia\",\"84fbaa66a2474ea29ae053f3a2e519d6\":\"Mithun\",\"75bdf310110b4b8fab88b16fafce920e\":\"Trishanth Diwate\",\"b1e8787a9a1f4859ac11cbb6a8124fd9\": \"Venkata Dikshit\", \"fb52cb663aec4795aee38ccfd904d315\":\"Reagan Rewop\", \"81a3e15469374fceba1cf972faa209b2\":\"Arjun Kini\", \"ecfeeb757f0a4d47af1ebd513929264a\":\"Shubham\", \"62b6ae1d7f834b0bb2055f7c72bc3368\":\"Karthik Muralidharan\", \"1a21542584494fcaba957d768b595b80\":\"Vamshi Krishna\", \"7e7ccbba232d411aa95ad3f244a35f40\":\"Shashank\", \"65bb83952fb54409a4bb59bb707f1375\":\"Vani\", \"0bbbfe84c66145af8d0ffcd5258bba38\":\"Parshwa Nemi Jain\"}\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../ai-engine/pkg/\")\n",
    "\n",
    "from graphrank.core import GraphRank\n",
    "from graphrank.utils import GraphUtils, TextPreprocess\n",
    "\n",
    "gr = GraphRank()\n",
    "tp = TextPreprocess()\n",
    "gu = GraphUtils()\n",
    "\n",
    "def get_desc(sentence):\n",
    "    original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(sentence, filter_by_pos=True, stop_words=False)\n",
    "    word_graph = gr.build_word_graph(graph_obj=None, input_pos_text=pos_tuple, window=4, preserve_common_words=False)\n",
    "    normal_keyphrase = gr.get_keyphrases(word_graph, pos_tuple, post_process=True)\n",
    "    desc_keyphrase = gr.get_keyphrases(word_graph, pos_tuple, descriptive=True, post_process_descriptive=True)\n",
    "    desc_keyphrase = sorted(desc_keyphrase, key=lambda kv:kv[1], reverse=True)\n",
    "    normal_kp = [phrase for phrase, score in normal_keyphrase]\n",
    "    desc_kp = [phrase for phrase, score in desc_keyphrase]\n",
    "    \n",
    "    return normal_kp, desc_kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:34:06.058534Z",
     "start_time": "2019-10-16T09:34:05.255435Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for groupid in final_pims:\n",
    "    user_list =[]\n",
    "    seg_list = []\n",
    "    keyphrase = []\n",
    "    for segi in groupid:\n",
    "        if segi['spokenBy'] not in user_list:\n",
    "            user_list.append(segi['spokenBy'])\n",
    "        seg_list.append(segi['originalText'])\n",
    "        #keyphrase.append(get_desc(segi['originalText']))\n",
    "    #print (\"User\", end=\" \")\n",
    "    print (*[user_id_map[user] for user in user_list], sep=\", \", end=\" \")\n",
    "    print (\"Discussed \\n\\n Text: \", *seg_list, \"\\n\\n  \")\n",
    "    print (\"Keyphrases: \", end=\"\")\n",
    "    print (*get_desc(\" \".join(sent for sent in seg_list))[1][:5], sep=\", \")\n",
    "    print ( \"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:34:01.892976Z",
     "start_time": "2019-10-16T09:34:01.837042Z"
    }
   },
   "outputs": [],
   "source": [
    "user_id_map = {}\n",
    "user_id_map = {\"8d6db5f7d9b74c54ba38fe710ffcaf3f\":\"Krishna Sai\", \"c66797a92e6d46ad9573926e57f7dac3\":\"Nisha Yadav\",\"31a3ba4761854ad9a041ddf1c4c6a1dc\":\"Reagan Rewop\",\"84fbaa66a2474ea29ae053f3a2e519d6\":\"Mithun\",\"75bdf310110b4b8fab88b16fafce920e\":\"Trishanth Diwate\",\"b1e8787a9a1f4859ac11cbb6a8124fd9\": \"Venkata Dikshit\", \"fb52cb663aec4795aee38ccfd904d315\":\"Reagan Rewop\", \"81a3e15469374fceba1cf972faa209b2\":\"Arjun Kini\", \"ecfeeb757f0a4d47af1ebd513929264a\":\"Shubham\", \"62b6ae1d7f834b0bb2055f7c72bc3368\":\"Karthik Muralidharan\", \"1a21542584494fcaba957d768b595b80\":\"Vamshi Krishna\", \"7e7ccbba232d411aa95ad3f244a35f40\":\"Shashank\", \"65bb83952fb54409a4bb59bb707f1375\":\"Vani\", \"0bbbfe84c66145af8d0ffcd5258bba38\":\"Parshwa Nemi Jain\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing hierarchy community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:02:27.110487Z",
     "start_time": "2019-09-30T15:02:27.050494Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('topic_testing/podcast_28.txt', 'rb') as f:\n",
    "    request = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:04:15.380539Z",
     "start_time": "2019-09-30T15:02:27.410077Z"
    }
   },
   "outputs": [],
   "source": [
    "from main import handler\n",
    "\n",
    "res = handler(request, None)\n",
    "group = json.loads(res['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:05:44.525847Z",
     "start_time": "2019-09-30T15:05:44.435681Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## visualization\n",
    "import iso8601\n",
    "from datetime import datetime\n",
    "# meeting start time.\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n",
    "\n",
    "#m_time = formatTime(\"2019-09-19T06:05:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-22T09:37:00Z\", True)\n",
    "m_time = formatTime(\"2019-09-28T05:34:21Z\", True)\n",
    "#m_time = formatTime(\"2019-09-30T10:28:00Z\", True)\n",
    "for i in group['group'].keys():\n",
    "    print (\"\\n\\n\\nPIMs \", i)\n",
    "    print (\"\\n\\nDiscussion:\\n\\n \")\n",
    "    for seg in group['group'][i]:\n",
    "        print (\"Minutes from the start of the meeting: \", formatTime(seg['startTime'], True) - m_time , seg['id'],\"\\n\")\n",
    "        print (seg['originalText'],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:16:21.322195Z",
     "start_time": "2019-09-30T15:16:21.261435Z"
    }
   },
   "outputs": [],
   "source": [
    "groups = []\n",
    "for i in group['group'].keys():\n",
    "    if len(group['group'][i])==1:\n",
    "        continue\n",
    "    else:\n",
    "        temp = []\n",
    "        for seg in group['group'][i]:\n",
    "            temp.append(seg['originalText'])\n",
    "        groups.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:17:35.047968Z",
     "start_time": "2019-09-30T15:16:21.757535Z"
    }
   },
   "outputs": [],
   "source": [
    "group_result = {}\n",
    "#group_response = {}\n",
    "group_request = {\"contextId\": request[\"body\"][\"contextId\"], \"mindId\": \"01daaqyn9gbebc92aywnxedp0c\", \"instanceId\": \"xyz\"}\n",
    "#group_response = {\"contextId\": request[\"body\"][\"contextId\"], \"mindId\": \"01DADP74WFV607KNPCB6VVXGTG\", \"segments\": []}\n",
    "temp = {}\n",
    "group_itr = None\n",
    "for segments_id in group['group'].keys():\n",
    "    if len(group['group'][segments_id]) > 2:\n",
    "        group_request['segments'] = group['group'][segments_id]\n",
    "        res = handler({\"body\":group_request}, None)\n",
    "        group_itr = json.loads(res['body'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T15:21:36.916596Z",
     "start_time": "2019-09-30T15:21:36.514422Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## visualization\n",
    "import iso8601\n",
    "from datetime import datetime\n",
    "# meeting start time.\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n",
    "\n",
    "#m_time = formatTime(\"2019-09-19T06:05:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-22T09:37:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-16T09:53:21Z\", True)\n",
    "m_time = formatTime(\"2019-09-30T10:28:00Z\", True)\n",
    "m_time = formatTime(\"2019-09-28T05:34:21Z\", True)\n",
    "for i in group_itr['group'].keys():\n",
    "    print (\"\\n\\n\\nPIMs \", i)\n",
    "    print (\"\\n\\nDiscussion:\\n\\n \")\n",
    "    for seg in group_itr['group'][i]:\n",
    "        print (\"Minutes from the start of the meeting: \", formatTime(seg['startTime'], True) - m_time , seg['id'],\"\\n\")\n",
    "        print (seg['originalText'],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:29:39.535024Z",
     "start_time": "2019-09-30T14:29:07.063968Z"
    }
   },
   "outputs": [],
   "source": [
    "group_result = {}\n",
    "#group_response = {}\n",
    "group_request = {\"contextId\": request[\"body\"][\"contextId\"], \"mindId\": \"01DADP74WFV607KNPCB6VVXGTG\", \"instanceId\": \"xyz\"}\n",
    "#group_response = {\"contextId\": request[\"body\"][\"contextId\"], \"mindId\": \"01DADP74WFV607KNPCB6VVXGTG\", \"segments\": []}\n",
    "temp = {}\n",
    "group_itr_2 = None\n",
    "for segments_id in group_itr['group'].keys():\n",
    "    if len(group_itr['group'][segments_id]) > 2:\n",
    "        group_request['segments'] = group_itr['group'][segments_id]\n",
    "        res = handler({\"body\":group_request}, None)\n",
    "        group_itr_2 = json.loads(res['body'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:30:41.736210Z",
     "start_time": "2019-09-30T14:30:41.595388Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## visualization\n",
    "import iso8601\n",
    "from datetime import datetime\n",
    "# meeting start time.\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n",
    "\n",
    "#m_time = formatTime(\"2019-09-19T06:05:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-22T09:37:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-16T09:53:21Z\", True)\n",
    "m_time = formatTime(\"2019-09-30T10:28:00Z\", True)\n",
    "for i in group_itr_2['group'].keys():\n",
    "    print (\"\\n\\n\\nPIMs \", i)\n",
    "    print (\"\\n\\nDiscussion:\\n\\n \")\n",
    "    for seg in group_itr_2['group'][i]:\n",
    "        print (\"Minutes from the start of the meeting: \", formatTime(seg['startTime'], True) - m_time , seg['id'],\"\\n\")\n",
    "        print (seg['originalText'],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:12:14.592267Z",
     "start_time": "2019-09-30T14:12:14.446839Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## visualization\n",
    "import iso8601\n",
    "from datetime import datetime\n",
    "# meeting start time.\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n",
    "\n",
    "#m_time = formatTime(\"2019-09-19T06:05:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-22T09:37:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-16T09:53:21Z\", True)\n",
    "m_time = formatTime(\"2019-09-30T10:08:00Z\", True)\n",
    "for i in group['group'].keys():\n",
    "    print (\"\\n\\n\\nPIMs \", i)\n",
    "    print (\"\\n\\nDiscussion:\\n\\n \")\n",
    "    for seg in group['group'][i]:\n",
    "        print (\"Minutes from the start of the meeting: \", formatTime(seg['startTime'], True) - m_time , seg['id'],\"\\n\")\n",
    "        print (seg['originalText'],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../ai-engine/pkg/\")\n",
    "import math\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from boto3 import client as boto3_client\n",
    "import json\n",
    "import logging\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "config = Config(connect_timeout=240, read_timeout=240, retries={'max_attempts': 0} )\n",
    "lambda_client = boto3_client('lambda', config=config,     aws_access_key_id=\"AKIA5SUS6MWO4MP7KDEJ\",\n",
    "    aws_secret_access_key=\"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\"\n",
    ")\n",
    "\n",
    "def get_output(input_sent, req_data=None):\n",
    "    #aws_config = Config(\n",
    "    #    connect_timeout=60,\n",
    "    ##    read_timeout=300,\n",
    "    #    retries={\"max_attempts\": 0},\n",
    "    #    region_name=\"us-east-1\",\n",
    "    #)\n",
    "    #lambda_client = boto3_client(\"lambda\", config=aws_config)\n",
    "    if req_data is None:\n",
    "        lambda_payload = input_sent\n",
    "    #logger.info(\"Invoking lambda function\")\n",
    "    invoke_response = lambda_client.invoke(\n",
    "        FunctionName=\"arn:aws:lambda:us-east-1:933389821341:function:group-segments\",\n",
    "        InvocationType=\"RequestResponse\",\n",
    "        Payload=lambda_payload\n",
    "    )\n",
    "    print (\"response recieved\", invoke_response)\n",
    "    lambda_output = (\n",
    "        invoke_response[\"Payload\"].read().decode(\"utf8\").replace(\"'\", '\"')\n",
    "    )\n",
    "    response = json.loads(lambda_output)\n",
    "    status_code = response[\"statusCode\"]\n",
    "    response_body = response[\"body\"]\n",
    "\n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topic_testing/sync_eng_21_10.txt\",\"rb\") as f:\n",
    "    request = json.load(f)\n",
    "response = get_output(json.dumps(request))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = response\n",
    "user_id_map = {}\n",
    "user_id_map = {\"716067a60a1a4034abc49a12ecafb39b\":\"Ether\",\"2f506a3d9e814de69d46a1fbf949fdc9\":\"ether\",\"8d6db5f7d9b74c54ba38fe710ffcaf3f\":\"Krishna Sai\", \"c66797a92e6d46ad9573926e57f7dac3\":\"Nisha Yadav\",\"31a3ba4761854ad9a041ddf1c4c6a1dc\":\"Reagan Rewop\",\"84fbaa66a2474ea29ae053f3a2e519d6\":\"Mithun\",\"75bdf310110b4b8fab88b16fafce920e\":\"Trishanth Diwate\",\"b1e8787a9a1f4859ac11cbb6a8124fd9\": \"Venkata Dikshit\", \"fb52cb663aec4795aee38ccfd904d315\":\"Reagan Rewop\", \"81a3e15469374fceba1cf972faa209b2\":\"Arjun Kini\", \"ecfeeb757f0a4d47af1ebd513929264a\":\"Shubham\", \"62b6ae1d7f834b0bb2055f7c72bc3368\":\"Karthik Muralidharan\", \"1a21542584494fcaba957d768b595b80\":\"Vamshi Krishna\", \"7e7ccbba232d411aa95ad3f244a35f40\":\"Shashank\", \"65bb83952fb54409a4bb59bb707f1375\":\"Vani\", \"0bbbfe84c66145af8d0ffcd5258bba38\":\"Parshwa Nemi Jain\"}\n",
    "\n",
    "for groupid in group['group'].keys():\n",
    "    user_list =[]\n",
    "    seg_list = []\n",
    "    keyphrase = []\n",
    "    for segi in group['group'][groupid]:\n",
    "        if segi['spokenBy'] not in user_list:\n",
    "            user_list.append(segi['spokenBy'])\n",
    "        seg_list.append(segi['originalText'])\n",
    "        #keyphrase.append(get_desc(segi['originalText']))\n",
    "    #print (\"User\", end=\" \")\n",
    "    print (\"Group Id: \", groupid)\n",
    "    print (*[user_id_map[user] for user in user_list], sep=\", \", end=\" \")\n",
    "    print (\"Discussed \\n\\n Text: \", *seg_list, \"\\n\\n  \")\n",
    "    #print (\"Keyphrases: \", end=\"\")\n",
    "    #print (*get_desc(\" \".join(sent for sent in seg_list))[1][:5], sep=\", \")\n",
    "    print ( \"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sri_gpt",
   "language": "python3",
   "name": "sri_gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "376px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
