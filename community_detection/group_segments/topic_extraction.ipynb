{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# (<VB.>* (JJR)*)( (<NN>+ <NN>+)|((<JJ>|<NN>) <NN>)| ((<JJ>|<NN>)+|((<JJ>|<NN>)* (<NN> <NN.>)? (<JJ>|<NN>)*) <NN.>))\n",
    "# (<VBG><.*>{0,1}?)?(<CD>?<JJ.*>*<NN.*>+(<IN>|<POS>))?<VB[ND]>?<JJ.?>*<NN.?>+((<IN><DT>?<JJ.*>?<NN.*>){0,1}?|(<[^,]>{0,1}<VBG>(<.+>{0,1}<JJ.*>?)?<NN.*>))?\n",
    "import nltk\n",
    "import string\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "def st_get_candidate_phrases(text, pos_search_pattern_list=[r\"\"\"base: {(<VBG><.*>{0,1}?)?(<CD>?<JJ.*>*<NN.*>+(<IN>|<POS>))?<VB[ND]>?<JJ.?>*<NN.?>+((<IN><DT>?<JJ.*>?<NN.*>){0,1}?|(<[^,]>{0,1}<VBG>(<.+>{0,1}<JJ.*>?)?<NN.*>))?}\"\"\"]\n",
    "):\n",
    "        punct = set(string.punctuation)\n",
    "        all_chunks = []\n",
    "\n",
    "        for pattern in pos_search_pattern_list:\n",
    "            all_chunks+=st_getregexChunks(text,pattern)\n",
    "\n",
    "        candidates_tokens = [' '.join(word for word, pos, \n",
    "                    chunk in group).lower() \n",
    "                    for key, group in itertools.groupby(all_chunks, \n",
    "                    lambda_unpack(lambda word, pos, chunk: chunk != 'O')) if key]\n",
    "\n",
    "        candidate_phrases = [cand for cand in candidates_tokens if cand not in stop_words and not all(char in punct for char in cand)]\n",
    "        #print (candidate_phrases)\n",
    "        return candidate_phrases\n",
    "    \n",
    "def st_getregexChunks(text,grammar):\n",
    "\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
    "                                                    for tagged_sent in tagged_sents))\n",
    "    #print(grammar)\n",
    "    #print(all_chunks)\n",
    "    #print()\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "def lambda_unpack(f):\n",
    "    return lambda args: f(*args)\n",
    "\n",
    "def get_filtered_pos(filtered, pos_list=['NN', 'JJ']):\n",
    "    filtered_list_temp = []\n",
    "    filtered_list = []\n",
    "    flag = False\n",
    "    flag_JJ = False\n",
    "    for word, pos in filtered:\n",
    "        if pos == 'NN' or pos == 'JJ':\n",
    "            flag=True\n",
    "            if pos == 'JJ':\n",
    "                flag_JJ = True\n",
    "            else:\n",
    "                flag_JJ = False\n",
    "            filtered_list_temp.append((word, pos))\n",
    "            continue\n",
    "        if flag:\n",
    "            if 'NN' in list(map(lambda x: x[1], filtered_list_temp)):\n",
    "                if not flag_JJ:\n",
    "                    filtered_list.append(list(map(lambda x:x[0], filtered_list_temp)))\n",
    "                else:\n",
    "                    filtered_list.append(list(map(lambda x:x[0], filtered_list_temp))[:-1])\n",
    "                    #print (filtered_list_temp)\n",
    "                    #print (filtered_list[-1])\n",
    "                    flag_JJ = False\n",
    "            filtered_list_temp = []\n",
    "            flag=False\n",
    "            \n",
    "    return filtered_list\n",
    "\n",
    "def prune_edge(graph):\n",
    "        c_weight = 0\n",
    "        import statistics\n",
    "        max_connection = {}\n",
    "        max_score = {}\n",
    "        outlier_score = {}\n",
    "        for node in graph.nodes():\n",
    "            closest_connection_n = sorted(dict(graph[node]).items(), key=lambda kv:kv[1][\"weight\"], reverse=True)\n",
    "            weights_n = list(map(lambda kv: (kv[1][\"weight\"]).tolist(), closest_connection_n))\n",
    "            q3 = np.percentile(weights_n, 75)\n",
    "            iqr = np.subtract(*np.percentile(weights_n, [75, 25]))\n",
    "            outlier_score[node] = {}\n",
    "            outlier_score[node][\"outlier\"] = q3 + 1 * iqr\n",
    "            outlier_score[node][\"iqr\"] = iqr\n",
    "            outlier_score[node][\"q3\"] = q3\n",
    "            outlier_score[node][\"weights_n\"] = closest_connection_n\n",
    "            outlier_score[node][\"avg+pstd\"] = statistics.mean(weights_n)+statistics.pstdev(weights_n)\n",
    "\n",
    "\n",
    "        graph_data = deepcopy(graph.edges.data())\n",
    "        for nodea, nodeb, weight in graph_data:\n",
    "            if weight[\"weight\"] >= outlier_score[nodea][\"q3\"] :\n",
    "                pass\n",
    "            else:\n",
    "                graph.remove_edge(nodea, nodeb)\n",
    "        return graph\n",
    "    \n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import List\n",
    "from extra_preprocess import preprocess_text\n",
    "\n",
    "class ClusterFeatures(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: ndarray,\n",
    "        algorithm: str = 'kmeans',\n",
    "        pca_k: int = None,\n",
    "        random_state: int = 12345\n",
    "    ):\n",
    "\n",
    "        if pca_k:\n",
    "            self.features = PCA(n_components=pca_k).fit_transform(features)\n",
    "        else:\n",
    "            self.features = features\n",
    "\n",
    "        self.algorithm = algorithm\n",
    "        self.pca_k = pca_k\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __get_model(self, k: int):\n",
    "        if self.algorithm == 'gmm':\n",
    "            return GaussianMixture(n_components=k, random_state=self.random_state)\n",
    "        return KMeans(n_clusters=k, random_state=self.random_state)\n",
    "\n",
    "    def __get_centroids(self, model):\n",
    "        if self.algorithm == 'gmm':\n",
    "            return model.means_\n",
    "        return model.cluster_centers_\n",
    "\n",
    "    def __find_closest_args(self, centroids: np.ndarray):\n",
    "        centroid_min = 1e10\n",
    "        cur_arg = -1\n",
    "        args = {}\n",
    "        used_idx = []\n",
    "\n",
    "        for j, centroid in enumerate(centroids):\n",
    "\n",
    "            for i, feature in enumerate(self.features):\n",
    "                #value = np.linalg.norm(feature - centroid)\n",
    "                value = cosine(feature, centroid)\n",
    "                if value < centroid_min and i not in used_idx:\n",
    "                    cur_arg = i\n",
    "                    centroid_min = value\n",
    "\n",
    "            used_idx.append(cur_arg)\n",
    "            args[j] = cur_arg\n",
    "            centroid_min = 1e10\n",
    "            cur_arg = -1\n",
    "\n",
    "        return args\n",
    "\n",
    "    def cluster(self, ratio: float = 0.1) -> List[int]:\n",
    "        k = 1 if ratio * len(self.features) < 1 else int(len(self.features) * ratio)\n",
    "        model = self.__get_model(k).fit(self.features)\n",
    "        centroids = self.__get_centroids(model)\n",
    "        cluster_args = self.__find_closest_args(centroids)\n",
    "        sorted_values = sorted(cluster_args.values())\n",
    "        return sorted_values\n",
    "\n",
    "    def __call__(self, ratio: float = 0.1) -> List[int]:\n",
    "        return self.cluster(ratio)\n",
    "    \n",
    "def summarize(text, ratio=0.3):\n",
    "    sent_list = preprocess_text(text)\n",
    "    summarized_text = None\n",
    "    if len(sent_list)!=0:\n",
    "        sent_list_fv = [gpt_model.get_text_feats(sent) for sent in sent_list]\n",
    "        cf = ClusterFeatures(np.asarray(sent_list_fv))\n",
    "        res = cf.cluster(ratio)\n",
    "        summarized_text = [sent_list[s] for s in res]\n",
    "    return summarized_text\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    embed = hub.Module(\"/home/arjun/gpt_experiments/models/tfhub_models/\")\n",
    "    encoder = embed(text_input)\n",
    "    init_op = tf.group(\n",
    "        [tf.global_variables_initializer(), tf.tables_initializer()]\n",
    "    )\n",
    "g.finalize()\n",
    "# Create session and initialize.\n",
    "session = tf.Session(graph=g)\n",
    "session.run(init_op)\n",
    "def get_embedding_vector(input_list):\n",
    "    embeddings = session.run(\n",
    "        encoder, feed_dict={text_input: input_list}\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = \"staging\"\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from boto3 import client as boto3_client\n",
    "import json\n",
    "import logging\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "config = Config(connect_timeout=2400, read_timeout=2400, retries={'max_attempts': 0} )\n",
    "if env==\"staging\":    \n",
    "    lambda_client = boto3_client('lambda', config=config,     aws_access_key_id=\"AKIA5SUS6MWO4MP7KDEJ\",\n",
    "       aws_secret_access_key=\"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\"\n",
    "    )\n",
    "elif env==\"production\":\n",
    "    lambda_client = boto3_client('lambda', config=config,     aws_access_key_id=\"AKIAJEW775PAAWKXWLDQ\",\n",
    "        aws_secret_access_key=\"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\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Please, assign 'env' variable to proper environment (staging or production)\")\n",
    "def get_embeddings(lambda_payload):\n",
    "    invoke_response = lambda_client.invoke(\n",
    "        #FunctionName=\"segment-analyser\",\n",
    "        FunctionName=\"arn:aws:lambda:us-east-1:933389821341:function:segment-analyser\",\n",
    "        #FunctionName=\"arn:aws:lambda:us-east-1:817390009919:function:segment-analyser\",\n",
    "        InvocationType=\"RequestResponse\",\n",
    "        Payload=json.dumps(lambda_payload)\n",
    "    )\n",
    "    #lambda_output = (\n",
    "    #    invoke_response[\"Payload\"].read().decode(\"utf8\").replace(\"'\", '\"')\n",
    "    #)\n",
    "    lambda_output = (\n",
    "        invoke_response[\"Payload\"].read().decode(\"utf8\")\n",
    "    )\n",
    "    if isinstance(lambda_output, str):\n",
    "        response = lambda_output\n",
    "    else:\n",
    "        response = json.loads(lambda_output)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute Feature vectors using lambda*\n",
    "request = json.load(open('topic_testing/cullen_test.json','rb'))\n",
    "if isinstance(request, str):\n",
    "    request = json.loads(request)[\"body\"]\n",
    "else:\n",
    "    request = request[\"body\"]\n",
    "request = {\"body\":{\"detail\": request}}\n",
    "request[\"body\"][\"type\"] = \"segment_analyzer.extract_features\"\n",
    "\n",
    "result  = json.loads(get_embeddings(request))\n",
    "if not json.loads(result[\"body\"])[\"analyzedSegment\"]:\n",
    "    raise Exception(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute groups using lambda*\n",
    "request = json.load(open('topic_testing/cullen_test.json','rb'))\n",
    "if isinstance(request, str):\n",
    "    request = json.loads(request)[\"body\"]\n",
    "else:\n",
    "    request = request[\"body\"]\n",
    "request = {\"body\":{\"detail\": request}}\n",
    "request[\"body\"][\"type\"] = \"segment_analyzer.compute_groups\"\n",
    "\n",
    "group_lambda  = json.loads(json.loads(get_embeddings(request))[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {\n",
    "        \"0\": {\n",
    "            \"segment0\": [\n",
    "                [\n",
    "                    \"Integration or CI is a development practice where developers a integrate code and a shared repository frequently preferably several times a day each integration can then be verified but automated build an automated test well automated testing is not strictly part of Ci. It is typically implied one of the key benefits of integrating regularly is that you can detect errors quickly and locate them more easily as each change introduced and typically small pointing the specific change that introduced defect can be done quickly in recent year has become a best practice for software development is guided by set of key protocols among them are revision control build automation and automated testing. Additionally additional continuous deployment and continuous about delivery have developed as best practices for keeping your application deplorable at any point or even pushing your main code base automatically into production whenever new changes it brought into it. This allows your team to move fast while keeping high quality standards that can be checked automatically.\"\n",
    "                ],\n",
    "                \"2019-12-15T10:30:00Z\",\n",
    "                \"716067a60a1a4034abc49a12ecafb39b\",\n",
    "                \"6193e407-cd54-4aaf-8507-e9af08cba083\"\n",
    "            ],\n",
    "            \"segment1\": [\n",
    "                [\n",
    "                    \"The technical goal of CI is to establish a consistent and automated way to build, package, and test applications. With consistency in the integration process in place, teams are more likely to commit code changes more frequently, which leads to better collaboration and software quality.\"\n",
    "                ],\n",
    "                \"2019-12-15T10:40:41Z\",\n",
    "                \"716067a60a1a4034abc49a12ecafb39b\",\n",
    "                \"6193e407-cd54-4aaf-8507-e9af08cba084\"\n",
    "            ]\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"segment0\": [\n",
    "                [\n",
    "                    \"Choose attachment to share a file from your device images that are shared appears thumbnails in the thread while other doc*ment types are linked for download Amazon Chime supports the and Emojis across all chat types Amazon supports common emoji codes that follow the format Colin word collin for instance, Colin and fire colon turns into a flame emoji amazon time also auto completes most space for instance eight close parentheses results on a smiley face with sungla*ses to prevent complete add forward slash code to the beginning of the text century for instance forward slash code space eight close parentheses does not result in the face with sungla*ses slash code also changes the text to a based and does not auto complete urls other common features across Amazon time chat types include the ability to adjust font size search and persistent chat threads across all your devices not just the ones you're are logged into read a message on one device and it is marked as red on all your others. Now let's explore the Amazon Chime interface and the recent message is section your latest one on one and group message threads are listed these are threads that have been active in the last seven days Amazon time displays a max of twenty five chat threads in order of recent activity. You can close a recent message by hovering over the message and choosing the red X favorites allow you to organize chat threads select a one on one or group message thread from recent messages and choose the star icon to add them to favorites drag and drop your favorites to rearrange them to remove a thread from favorites choose the store again, a threat can either be in favourites or recent messages, but not in both Amazon Chime also displays user availability a red dot means that the person is busy on an Amazon call or currently in a meeting.\"\n",
    "                ],\n",
    "                \"2019-12-15T09:23:02Z\",\n",
    "                \"716067a60a1a4034abc49a12ecafb39b\",\n",
    "                \"526b1e81-543d-49d9-85df-8d428f1ca6ee\"\n",
    "            ],\n",
    "            \"segment1\": [\n",
    "                [\n",
    "                    \"A green means that the person is available and on a desktop device and orange dot means that the person is desktop idle. They have the Amazon time map open but are currently away from their computer if a green icon appears next to a person's name. Then they have a mobile device enabled and you may be able to reach them well they may not have the Amazon time app open you can send them an app one on one or group message Amazon time sends them a push notification to get their attention. If a person status is a gray circle. They have not been logged in for more than six days. If you see a gray lock next to a person they have their privacy settings set to not share their availability with you to take control of your availability use the list next to your name you can manually set whether you are available or busy.\"\n",
    "                ],\n",
    "                \"2019-12-15T09:25:02Z\",\n",
    "                \"716067a60a1a4034abc49a12ecafb39b\",\n",
    "                \"96d9628a-2fe5-4ec3-9ed3-8d215a7c24dc\"\n",
    "            ]\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"segment0\": [\n",
    "                [\n",
    "                    \"See your colleagues name under my contacts choose ad contact and enter an email address amazon time sends an invite to your colleague if the email address is not already a*sociated with an Amazon chime account to start a group message choose group message and add users to the two field amazon chime supports up to fifty users on a group message all one on one in group messages are sent to your desktop app if it is the most active device. If you're are away or on mobile amazon time sends push notifications to your mobile app. You see a status sent when the Amazon servers receive your message read receipts are displayed when your message has been read.\"\n",
    "                ],\n",
    "                \"2019-12-15T09:26:06Z\",\n",
    "                \"716067a60a1a4034abc49a12ecafb39b\",\n",
    "                \"e9b9572f-2537-4bf2-b32d-8d97b7b85f3c\"\n",
    "            ]\n",
    "        },\n",
    "        \"3\": {\n",
    "            \"segment0\": [\n",
    "                [\n",
    "                    \"Amazon time is your all in one solution for collaboration. This video shows you how to collaborate with Amazon time chat there are three chat types one on one messages group messages and chat rooms all of these chat types can be accessed on the less navigation area on Amazon time desktop apps and in the messages and rooms tab on mobile apps use one on one message to chat directly with a colleague use group messages to communicate the same thing to multiple people.\"\n",
    "                ],\n",
    "                \"2019-12-15T09:21:34Z\",\n",
    "                \"716067a60a1a4034abc49a12ecafb39b\",\n",
    "                \"0b8b56e1-1170-4306-b89a-e577bac52c27\"\n",
    "            ]\n",
    "        },\n",
    "        \"4\": {\n",
    "            \"segment0\": [\n",
    "                [\n",
    "                    \"Chat to hold conversations with your colleagues in a less direct manner chatrooms rooms are like discussion boards for topic areas or teams some customers use chat rooms for their teams to collaborate on projects others use chat during explanations and invite necessary members to participate in the discussion team members transitioning on from the next time zone can catch up on what happened why they were away chat rooms are always private meaning a member must be added to the chat to view messages and participate. There are common features across all Amazon Chime chat types Amazon Chime support sharing attachments up to fifty MB. So you can share doc*ments presentations and photos as you talk about them drag and drop your file into the chat field type a message and press return to send.\"\n",
    "                ],\n",
    "                \"2019-12-15T09:22:08Z\",\n",
    "                \"716067a60a1a4034abc49a12ecafb39b\",\n",
    "                \"d86647cd-8204-4b1a-82c6-5087c065dd9e\"\n",
    "            ]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Integration or CI is a development practice where developers a integrate code and a shared repository frequently preferably several times a day each integration can then be verified but automated build an automated test well automated testing is not strictly part of Ci. It is typically implied one of the key benefits of integrating regularly is that you can detect errors quickly and locate them more easily as each change introduced and typically small pointing the specific change that introduced defect can be done quickly in recent year has become a best practice for software development is guided by set of key protocols among them are revision control build automation and automated testing. Additionally additional continuous deployment and continuous about delivery have developed as best practices for keeping your application deplorable at any point or even pushing your main code base automatically into production whenever new changes it brought into it. This allows your team to move fast while keeping high quality standards that can be checked automatically.', 'The technical goal of CI is to establish a consistent and automated way to build, package, and test applications. With consistency in the integration process in place, teams are more likely to commit code changes more frequently, which leads to better collaboration and software quality.']\n",
      "[\"Choose attachment to share a file from your device images that are shared appears thumbnails in the thread while other doc*ment types are linked for download Amazon Chime supports the and Emojis across all chat types Amazon supports common emoji codes that follow the format Colin word collin for instance, Colin and fire colon turns into a flame emoji amazon time also auto completes most space for instance eight close parentheses results on a smiley face with sungla*ses to prevent complete add forward slash code to the beginning of the text century for instance forward slash code space eight close parentheses does not result in the face with sungla*ses slash code also changes the text to a based and does not auto complete urls other common features across Amazon time chat types include the ability to adjust font size search and persistent chat threads across all your devices not just the ones you're are logged into read a message on one device and it is marked as red on all your others. Now let's explore the Amazon Chime interface and the recent message is section your latest one on one and group message threads are listed these are threads that have been active in the last seven days Amazon time displays a max of twenty five chat threads in order of recent activity. You can close a recent message by hovering over the message and choosing the red X favorites allow you to organize chat threads select a one on one or group message thread from recent messages and choose the star icon to add them to favorites drag and drop your favorites to rearrange them to remove a thread from favorites choose the store again, a threat can either be in favourites or recent messages, but not in both Amazon Chime also displays user availability a red dot means that the person is busy on an Amazon call or currently in a meeting.\", \"A green means that the person is available and on a desktop device and orange dot means that the person is desktop idle. They have the Amazon time map open but are currently away from their computer if a green icon appears next to a person's name. Then they have a mobile device enabled and you may be able to reach them well they may not have the Amazon time app open you can send them an app one on one or group message Amazon time sends them a push notification to get their attention. If a person status is a gray circle. They have not been logged in for more than six days. If you see a gray lock next to a person they have their privacy settings set to not share their availability with you to take control of your availability use the list next to your name you can manually set whether you are available or busy.\"]\n",
      "[\"See your colleagues name under my contacts choose ad contact and enter an email address amazon time sends an invite to your colleague if the email address is not already a*sociated with an Amazon chime account to start a group message choose group message and add users to the two field amazon chime supports up to fifty users on a group message all one on one in group messages are sent to your desktop app if it is the most active device. If you're are away or on mobile amazon time sends push notifications to your mobile app. You see a status sent when the Amazon servers receive your message read receipts are displayed when your message has been read.\"]\n",
      "['Amazon time is your all in one solution for collaboration. This video shows you how to collaborate with Amazon time chat there are three chat types one on one messages group messages and chat rooms all of these chat types can be accessed on the less navigation area on Amazon time desktop apps and in the messages and rooms tab on mobile apps use one on one message to chat directly with a colleague use group messages to communicate the same thing to multiple people.']\n",
      "['Chat to hold conversations with your colleagues in a less direct manner chatrooms rooms are like discussion boards for topic areas or teams some customers use chat rooms for their teams to collaborate on projects others use chat during explanations and invite necessary members to participate in the discussion team members transitioning on from the next time zone can catch up on what happened why they were away chat rooms are always private meaning a member must be added to the chat to view messages and participate. There are common features across all Amazon Chime chat types Amazon Chime support sharing attachments up to fifty MB. So you can share doc*ments presentations and photos as you talk about them drag and drop your file into the chat field type a message and press return to send.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_list = []\n",
    "for groupid in groups.keys():\n",
    "    print ([groups[groupid][segkey][0][0] for segkey in groups[groupid].keys()])\n",
    "    text_list.append(\" \".join([groups[groupid][segkey][0][0] for segkey in groups[groupid].keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Group Id:  0\n",
      "Cullen Discussed \n",
      "\n",
      " Text: \n",
      "LS is the software that would be installed on your personal computer or on a server. Then you would use it to manage one or more databases. The database has your actual data and the rules about the data while the dbms is the program that surrounds a managers you actual data and enforces the rules you specified on your data the rules for example could be the type of data like integer or string or the relationship between the database does broad search searching functionality a database can update records in bulk even Millions. Zoom or more records this would be useful for example, if you wanted to add new columns or apply data patch of some sort if the database is relational, which most databases are they can cross-reference records in different tables. This means that you can create relationships between tables database can perform complex aggregate calculations across multiple tables. For example, you could list expenses across multiple retail outlets, including all possible subtotals and then a final total data. Base can enforce consistency and data Integrity which means that it can avoid duplication and ensure data accuracy through its design and a series of constraints. \n",
      "\n",
      "\n",
      "\n",
      "User Group Id:  1\n",
      "Cullen Discussed \n",
      "\n",
      " Text: \n",
      "Our is a tool designed to make it easier to create deploy and run applications by using containers containers allow a developer to package up an application with all the parts that need such as libraries and other dependencies and ship it out to this one package by doing. So thanks to the container. The developer can rest assured the application or run on any other Linux machine regardless of any customized settings that machine might have the could differ from the machine used to writing a testing the code. \n",
      "\n",
      "\n",
      "\n",
      "User Group Id:  2\n",
      "Cullen Discussed \n",
      "\n",
      " Text: \n",
      "Africa is an open source stream processing software platform developing linked by LinkedIn it donated to the Apache software Foundation written in Scala and Java the project aims to provide a unified High throughput latency platform for handling real-time data feeds its storage layer is essentially a massively scalable pubsub message queue designed as a distributed transaction law making it highly valuable for Enterprise infrastructures to process training data more information on Big Data building blocks. You can find this almost an interactive environment The most popular tools are spark and cafta. They're definitely worth exploring. Preferably understanding how they work from the inside. Jake reps co-author. It's gascon 2013 published a Monumental work on the Law, whatever software engineer should know about real-time data as unifying extraction core ideas from this book by the way was used for the creation of the patchy Gaskill. \n",
      "\n",
      "\n",
      "\n",
      "User Group Id:  3\n",
      "Cullen Discussed \n",
      "\n",
      " Text: \n",
      "Web services or AWS provides on-demand Computing resources and services in the cloud with pay as you go go pricing. For example, you can run a server on AWS that you can log on to configure secure and run just as you would a server the sitting in front of you the whole concept Builds on top of the cloud computing principal the provides it infrastructure and them Services over the Internet because on our uses Hadoop and open source framework to manage and process data using this. That reduced engine to distribute processing using gold cluster. \n",
      "\n",
      "\n",
      "\n",
      "User Group Id:  4\n",
      "Cullen Discussed \n",
      "\n",
      " Text: \n",
      "Is it is an open source tool that allows you to take advantage of on-premises hybrid or public Cloud infrastructure giving you the freedom to move workloads where ever you want it offers security networking and storage services and can manage more than one cluster at a time kubernetes makes more efficient use of hard work allowing you to maximize your resources and save money, but here's where things get tricky use a container orchestration tool like kubernetes. You describe the configuration of your application and yeah mol file this configuration. action file is where you tell kubernetes how to do things like gather container images how to establish networking between containers how to mount storage volumes in order to store logs for that container containers of deployed onto hosts usually replicated groups and when it's time to deploy a new container into a cluster kubernetes schedules the deployment and looks from the most appropriate host to place the container based on predefined constraints of your choosing like CPU or memory availability basically once the container Is running on the host communities manages its life cycle according to the specifications. You laid out and the containers Docker file, which means the Cooper Nettie's is automating all of these tasks for you, but it does so based on the configuration you set up as the developer and while you may be a crack shot engineer chances are you don't know exactly how much traffic you're going to get within the first month of the point where all your application will behave that's why I specially for this first couple of months monitoring. Your kubernetes cluster is super important. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group = group_lambda\n",
    "user_id_map = {}\n",
    "user_id_map = {\"d78b6120-3951-4b66-8e5d-5a8f30b9b2a9\": \"Cullen\",\"2c94451217a049129a166a3408da807c\":\"Sai\",\"3f01f2032f584b178fafde6b437058ae\":\"Venkat\",\"8fff81b5b2f14aa5ad67405f3e8127f3\":\"Sai\",\"60d2ea6bed8c48269c8c024202a4148d\":\"Shubham\",\"70caa6269d8e4869a45f7ea91ade3472\":\"Ether\",\"3e1a008f734448b0ad9190778449af81\":\"Cullen\",\"b4a57b25de68446cac990f856d3fe4d5\":\"Deep\",\"716067a60a1a4034abc49a12ecafb39b\":\"Cullen\",\"2f506a3d9e814de69d46a1fbf949fdc9\":\"Cullen\",\"8d6db5f7d9b74c54ba38fe710ffcaf3f\":\"Krishna Sai\", \"c66797a92e6d46ad9573926e57f7dac3\":\"Nisha Yadav\",\"31a3ba4761854ad9a041ddf1c4c6a1dc\":\"Reagan Rewop\",\"84fbaa66a2474ea29ae053f3a2e519d6\":\"Mithun\",\"75bdf310110b4b8fab88b16fafce920e\":\"Trishanth Diwate\",\"b1e8787a9a1f4859ac11cbb6a8124fd9\": \"Venkata Dikshit\", \"fb52cb663aec4795aee38ccfd904d315\":\"Reagan Rewop\", \"81a3e15469374fceba1cf972faa209b2\":\"Arjun Kini\", \"ecfeeb757f0a4d47af1ebd513929264a\":\"Shubham\", \"62b6ae1d7f834b0bb2055f7c72bc3368\":\"Karthik Muralidharan\", \"1a21542584494fcaba957d768b595b80\":\"Vamshi Krishna\", \"7e7ccbba232d411aa95ad3f244a35f40\":\"Shashank\", \"65bb83952fb54409a4bb59bb707f1375\":\"Vani\", \"0bbbfe84c66145af8d0ffcd5258bba38\":\"Parshwa Nemi Jain\"}\n",
    "text_list = []\n",
    "for groupid in group['group'].keys():\n",
    "    user_list =[]\n",
    "    seg_list = []\n",
    "    keyphrase = []\n",
    "    for segi in group['group'][groupid]:\n",
    "        if segi['spokenBy'] not in user_list:\n",
    "            user_list.append(segi['spokenBy'])\n",
    "        seg_list.append(segi['originalText'])\n",
    "        #keyphrase.append(get_desc(segi['originalText']))\n",
    "    print (\"User\", end=\" \")\n",
    "#     if len(seg_list) == 1 :\n",
    "#         continue\n",
    "    print (\"Group Id: \", groupid)\n",
    "    print (*[user_id_map[user] for user in user_list], sep=\", \", end=\" \")\n",
    "    print (\"Discussed \\n\\n Text: \")\n",
    "    print ( *seg_list, sep=\"\\n\\n\", end=\"\\n\\n\")\n",
    "    #print (\"Keyphrases: \", end=\"\")\n",
    "    #print (*get_desc(\" \".join(sent for sent in seg_list))[1][:5], sep=\", \")\n",
    "    print ( \"\\n\")\n",
    "    text_list.append(\" \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../ai-engine_temp/pkg/\")\n",
    "from text_preprocessing import preprocess as tp\n",
    "sys.path.append(\"/home/arjun/BERT_Similarity_experiments/code/\")\n",
    "from gpt_feat_utils import GPT_Inference\n",
    "#\n",
    "# #gpt_model = gpt_feat_utils.GPT_SimInference(\"/home/arjun/gpt_experiments/models/model_lm+sim_ep3/\", device=\"cuda\")\n",
    "# #gpt_model = gpt_feat_utils.GPT_SimInference(\"/home/arjun/gpt_experiments/models/model_lm+nsp_sim_ep3/\", device=\"cuda\")\n",
    "#gpt_model = GPT_Inference(\"/home/arjun/gpt_experiments/engg_models/se+ether_2+1s_ep5_#2/\", device=\"cpu\")\n",
    "#gpt_model = GPT_Inference(\"/home/ether/hdd/ether/gpt_domain_minds/marketing/epoch3/\", device=\"cpu\")\n",
    "gpt_model = GPT_Inference(\"/home/shubham/projects/domain_minds_v2_gpt/se/model/epoch3/\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\" \".join(summarize(text, ratio=0.3)) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import pagerank\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import cosine\n",
    "result = {\n",
    "}\n",
    "for text in text_list:\n",
    "    candidate_topics = st_get_candidate_phrases(text)\n",
    "    kg = nx.Graph()\n",
    "    kg.add_nodes_from(candidate_topics)\n",
    "    kp_fv = {}\n",
    "    #kp_fv_raw = [gpt_model.get_text_feats(k+\".\") for k in candidate_topics]\n",
    "    kp_fv_raw = [get_embedding_vector([k+\".\"])[0] for k in candidate_topics]\n",
    "    for index, kp in enumerate(candidate_topics):\n",
    "        kp_fv[kp] = kp_fv_raw[index]\n",
    "    for index1, nodea in enumerate(kg.nodes()):\n",
    "        for index2, nodeb in enumerate(kg.nodes()):\n",
    "            if index2 >= index1:\n",
    "                kg.add_edge(nodea, nodeb, weight = 1 - cosine(kp_fv[nodea], kp_fv[nodeb]))\n",
    "    kg = deepcopy(prune_edge(kg))\n",
    "    pg = pagerank(kg, weight=\"weight\")\n",
    "    pg_sorted = (sorted(pg.items(), key=lambda kv:kv[1], reverse=True))[:10]\n",
    "    #text_fv = gpt_model.get_text_feats(text)\n",
    "    text_fv = get_embedding_vector([text])[0]\n",
    "    ranked = [(kp, 1-cosine(text_fv, kp_fv[kp])) for kp in [i[0] for i in pg_sorted]]\n",
    "    pg_sorted = (sorted(ranked, key=lambda kv:kv[1], reverse=True))[:5]\n",
    "    result[text] = pg_sorted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Text: \n",
      "\n",
      " LS is the software that would be installed on your personal computer or on a server. Then you would use it to manage one or more databases. The database has your actual data and the rules about the data while the dbms is the program that surrounds a managers you actual data and enforces the rules you specified on your data the rules for example could be the type of data like integer or string or the relationship between the database does broad search searching functionality a database can update records in bulk even Millions. Zoom or more records this would be useful for example, if you wanted to add new columns or apply data patch of some sort if the database is relational, which most databases are they can cross-reference records in different tables. This means that you can create relationships between tables database can perform complex aggregate calculations across multiple tables. For example, you could list expenses across multiple retail outlets, including all possible subtotals and then a final total data. Base can enforce consistency and data Integrity which means that it can avoid duplication and ensure data accuracy through its design and a series of constraints. \n",
      "\n",
      "\n",
      " Topic:  ['most databases', 'records in different tables', 'relationships between tables', 'type of data', 'complex aggregate calculations across multiple tables'] \n",
      "\n",
      "\n",
      "Group Text: \n",
      "\n",
      " Our is a tool designed to make it easier to create deploy and run applications by using containers containers allow a developer to package up an application with all the parts that need such as libraries and other dependencies and ship it out to this one package by doing. So thanks to the container. The developer can rest assured the application or run on any other Linux machine regardless of any customized settings that machine might have the could differ from the machine used to writing a testing the code. \n",
      "\n",
      "\n",
      " Topic:  ['container', 'using containers containers', 'machine', 'package', 'other dependencies'] \n",
      "\n",
      "\n",
      "Group Text: \n",
      "\n",
      " Africa is an open source stream processing software platform developing linked by LinkedIn it donated to the Apache software Foundation written in Scala and Java the project aims to provide a unified High throughput latency platform for handling real-time data feeds its storage layer is essentially a massively scalable pubsub message queue designed as a distributed transaction law making it highly valuable for Enterprise infrastructures to process training data more information on Big Data building blocks. You can find this almost an interactive environment The most popular tools are spark and cafta. They're definitely worth exploring. Preferably understanding how they work from the inside. Jake reps co-author. It's gascon 2013 published a Monumental work on the Law, whatever software engineer should know about real-time data as unifying extraction core ideas from this book by the way was used for the creation of the patchy Gaskill. \n",
      "\n",
      "\n",
      " Topic:  ['scala', 'real-time data as unifying extraction core ideas', 'cafta', 'distributed transaction law', 'scalable pubsub message queue'] \n",
      "\n",
      "\n",
      "Group Text: \n",
      "\n",
      " Web services or AWS provides on-demand Computing resources and services in the cloud with pay as you go go pricing. For example, you can run a server on AWS that you can log on to configure secure and run just as you would a server the sitting in front of you the whole concept Builds on top of the cloud computing principal the provides it infrastructure and them Services over the Internet because on our uses Hadoop and open source framework to manage and process data using this. That reduced engine to distribute processing using gold cluster. \n",
      "\n",
      "\n",
      " Topic:  ['cloud', 'server on aws', 'cloud with pay', 'web services', 'aws'] \n",
      "\n",
      "\n",
      "Group Text: \n",
      "\n",
      " Is it is an open source tool that allows you to take advantage of on-premises hybrid or public Cloud infrastructure giving you the freedom to move workloads where ever you want it offers security networking and storage services and can manage more than one cluster at a time kubernetes makes more efficient use of hard work allowing you to maximize your resources and save money, but here's where things get tricky use a container orchestration tool like kubernetes. You describe the configuration of your application and yeah mol file this configuration. action file is where you tell kubernetes how to do things like gather container images how to establish networking between containers how to mount storage volumes in order to store logs for that container containers of deployed onto hosts usually replicated groups and when it's time to deploy a new container into a cluster kubernetes schedules the deployment and looks from the most appropriate host to place the container based on predefined constraints of your choosing like CPU or memory availability basically once the container Is running on the host communities manages its life cycle according to the specifications. You laid out and the containers Docker file, which means the Cooper Nettie's is automating all of these tasks for you, but it does so based on the configuration you set up as the developer and while you may be a crack shot engineer chances are you don't know exactly how much traffic you're going to get within the first month of the point where all your application will behave that's why I specially for this first couple of months monitoring. Your kubernetes cluster is super important. \n",
      "\n",
      "\n",
      " Topic:  ['container orchestration tool like kubernetes', 'container containers', 'configuration', 'deployment', 'container'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text, topic in result.items():\n",
    "    print (\"Group Text: \\n\\n\", text)\n",
    "    print (\"\\n\\n Topic: \", [t[0] for t in topic], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Text: \n",
      "\n",
      " LS is the software that would be installed on your personal computer or on a server. Then you would use it to manage one or more databases. The database has your actual data and the rules about the data while the dbms is the program that surrounds a managers you actual data and enforces the rules you specified on your data the rules for example could be the type of data like integer or string or the relationship between the database does broad search searching functionality a database can update records in bulk even Millions. Zoom or more records this would be useful for example, if you wanted to add new columns or apply data patch of some sort if the database is relational, which most databases are they can cross-reference records in different tables. This means that you can create relationships between tables database can perform complex aggregate calculations across multiple tables. For example, you could list expenses across multiple retail outlets, including all possible subtotals and then a final total data. Base can enforce consistency and data Integrity which means that it can avoid duplication and ensure data accuracy through its design and a series of constraints. \n",
      "\n",
      "\n",
      " Topic:  ['dbms', 'most databases', 'data integrity', 'type of data', 'database'] \n",
      "\n",
      "\n",
      "Group Text: \n",
      "\n",
      " Our is a tool designed to make it easier to create deploy and run applications by using containers containers allow a developer to package up an application with all the parts that need such as libraries and other dependencies and ship it out to this one package by doing. So thanks to the container. The developer can rest assured the application or run on any other Linux machine regardless of any customized settings that machine might have the could differ from the machine used to writing a testing the code. \n",
      "\n",
      "\n",
      " Topic:  ['other dependencies', 'using containers containers', 'developer', 'container', 'machine'] \n",
      "\n",
      "\n",
      "Group Text: \n",
      "\n",
      " Africa is an open source stream processing software platform developing linked by LinkedIn it donated to the Apache software Foundation written in Scala and Java the project aims to provide a unified High throughput latency platform for handling real-time data feeds its storage layer is essentially a massively scalable pubsub message queue designed as a distributed transaction law making it highly valuable for Enterprise infrastructures to process training data more information on Big Data building blocks. You can find this almost an interactive environment The most popular tools are spark and cafta. They're definitely worth exploring. Preferably understanding how they work from the inside. Jake reps co-author. It's gascon 2013 published a Monumental work on the Law, whatever software engineer should know about real-time data as unifying extraction core ideas from this book by the way was used for the creation of the patchy Gaskill. \n",
      "\n",
      "\n",
      " Topic:  ['apache software foundation', 'real-time data as unifying extraction core ideas', 'handling real-time data', 'enterprise infrastructures', 'scalable pubsub message queue'] \n",
      "\n",
      "\n",
      "Group Text: \n",
      "\n",
      " Web services or AWS provides on-demand Computing resources and services in the cloud with pay as you go go pricing. For example, you can run a server on AWS that you can log on to configure secure and run just as you would a server the sitting in front of you the whole concept Builds on top of the cloud computing principal the provides it infrastructure and them Services over the Internet because on our uses Hadoop and open source framework to manage and process data using this. That reduced engine to distribute processing using gold cluster. \n",
      "\n",
      "\n",
      " Topic:  ['on-demand computing resources', 'server on aws', 'web services', 'aws', 'infrastructure'] \n",
      "\n",
      "\n",
      "Group Text: \n",
      "\n",
      " Is it is an open source tool that allows you to take advantage of on-premises hybrid or public Cloud infrastructure giving you the freedom to move workloads where ever you want it offers security networking and storage services and can manage more than one cluster at a time kubernetes makes more efficient use of hard work allowing you to maximize your resources and save money, but here's where things get tricky use a container orchestration tool like kubernetes. You describe the configuration of your application and yeah mol file this configuration. action file is where you tell kubernetes how to do things like gather container images how to establish networking between containers how to mount storage volumes in order to store logs for that container containers of deployed onto hosts usually replicated groups and when it's time to deploy a new container into a cluster kubernetes schedules the deployment and looks from the most appropriate host to place the container based on predefined constraints of your choosing like CPU or memory availability basically once the container Is running on the host communities manages its life cycle according to the specifications. You laid out and the containers Docker file, which means the Cooper Nettie's is automating all of these tasks for you, but it does so based on the configuration you set up as the developer and while you may be a crack shot engineer chances are you don't know exactly how much traffic you're going to get within the first month of the point where all your application will behave that's why I specially for this first couple of months monitoring. Your kubernetes cluster is super important. \n",
      "\n",
      "\n",
      " Topic:  ['kubernetes cluster', 'networking between containers', 'storage services', 'workloads', 'cluster'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text, topic in result.items():\n",
    "    print (\"Group Text: \\n\\n\", text)\n",
    "    print (\"\\n\\n Topic: \", [t[0] for t in topic], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('type', 'NN'), ('of', 'IN'), ('data', 'NNS')]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.get_pos(\"type of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['family',\n",
       " 'none',\n",
       " 'trouble',\n",
       " 'way',\n",
       " 'translation',\n",
       " 'pace',\n",
       " 'subfield',\n",
       " 'machine learning',\n",
       " 'algorithms',\n",
       " 'structure',\n",
       " 'brain',\n",
       " 'set',\n",
       " 'network',\n",
       " 'circle',\n",
       " 'core',\n",
       " 'network',\n",
       " 'input layer',\n",
       " 'input',\n",
       " 'output layer',\n",
       " 'output',\n",
       " 'network',\n",
       " 'image',\n",
       " 'circle',\n",
       " 'image',\n",
       " 'pixel',\n",
       " 'input',\n",
       " 'neuron',\n",
       " 'layer',\n",
       " 'layer',\n",
       " 'layer',\n",
       " 'value',\n",
       " 'wait',\n",
       " 'sum',\n",
       " 'input',\n",
       " 'layer',\n",
       " 'value',\n",
       " 'bias',\n",
       " 'value',\n",
       " 'function',\n",
       " 'activation function',\n",
       " 'result',\n",
       " 'activation function',\n",
       " 'neuron',\n",
       " 'neuron',\n",
       " 'layer',\n",
       " 'data',\n",
       " 'network',\n",
       " 'propagation',\n",
       " 'output',\n",
       " 'neuron',\n",
       " 'value',\n",
       " 'output',\n",
       " 'probability',\n",
       " 'example',\n",
       " 'square',\n",
       " 'probability',\n",
       " 'output',\n",
       " 'network',\n",
       " 'course',\n",
       " 'look',\n",
       " 'network',\n",
       " 'prediction',\n",
       " 'top',\n",
       " 'note',\n",
       " 'network',\n",
       " 'training process',\n",
       " 'input',\n",
       " 'output',\n",
       " 'output',\n",
       " 'output',\n",
       " 'error',\n",
       " 'prediction',\n",
       " 'magnitude',\n",
       " 'error',\n",
       " 'sign suggestive',\n",
       " 'indication',\n",
       " 'direction',\n",
       " 'magnitude',\n",
       " 'change',\n",
       " 'error',\n",
       " 'information',\n",
       " 'backpropagation',\n",
       " 'information',\n",
       " 'cycle',\n",
       " 'propagation',\n",
       " 'propagation',\n",
       " 'process',\n",
       " 'network',\n",
       " 'training process',\n",
       " 'training process',\n",
       " 'time',\n",
       " 'trade',\n",
       " 'scope',\n",
       " 'crime',\n",
       " 'recognition',\n",
       " 'age',\n",
       " 'person',\n",
       " 'base',\n",
       " 'background',\n",
       " 'face',\n",
       " 'forecasting',\n",
       " 'possibility',\n",
       " 'rain fall',\n",
       " 'rise',\n",
       " 'stock',\n",
       " 'accuracy music composition',\n",
       " 'music',\n",
       " 'question',\n",
       " 'activation',\n",
       " 'fungus',\n",
       " 'error',\n",
       " 'layer',\n",
       " 'network',\n",
       " 'propagation take place',\n",
       " 'training process',\n",
       " 'network',\n",
       " 'data processing',\n",
       " 'comment section']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_get_candidate_phrases(\"Summer my family and I visited Russia, even though none of us could read Russian. We did not have any trouble and figuring our way out all things to Google's real-time translation of Russian boards into English. This is just one of the several applications of neural networks neural networks from the pace of people earning a subfield a machine learning where the algorithms are inspired by the structure of the human brain neural networks take in data train themselves to recognize the patterns in the data and then predict the outputs where a new set of similar data. Let's understand how this is done. Let's construct a neural network that differentiates between a square circle and triangle neural networks are made up of layers of neurons. These neurons are the core processing units of the network first. We have the input layer which receives the input the output layer predicts our final output in between exists a hidden layers, which perform most of the As required by our Network, here's an image of a circle. This image is composed of 28 by 28 x cells which make up for 784 pixels. Each pixel is fed as input to each neuron of the first layer neurons have one layer are connected to neurons of the next layer through channels. Each of these channels is assigned a numerical value known as wait the inputs are multiplied to the corresponding weights and their sum is sent. as input to the neurons in the hidden layer, each of these neurons is associated with a numerical value called the bias, which is then added to the inputs of this value is then passed through a threshold function called the activation function the result of the activation function determines if the particular neuron will get activated or not and activate a neuron transmits data do the neurons of the next layer over the channels in  The data is propagated through the network. This is called forward propagation in the output layer the neuron with the highest value fires in determines the output the values are basically a probability. For example, here are neurons associated with square at the highest probability. Hence. That's the output predicted by the neural network. Of course just by a look at it. We know our neural network has made a wrong prediction, but top of the The Network's figured this out note that our network is yet to be trained during this training process along with the input. Our Network also has the output fed to it. The predicted output is compared against the actual output to realize the error in prediction the magnitude of the error indicates how wrong we are in the sign suggestive of our predicted values are higher or lower than expected the arrows here given indication of the direction and magnitude of change too. Use the error this information is then transferred backward through our Network. This is known as backpropagation now based on this information. The weights are adjusted this cycle of forward propagation and back propagation is iteratively performed with multiple inputs this process continues until our weights are assigned such that the network can predict the shapes correctly in most of the cases this brings our training process to an and you might wonder how long this training process takes honestly neural networks may take hours or even months to train but time is a reasonable trade off when compared to its scope. Let us look at some of the crime applications of neural networks facial recognition cameras on smartphones. These days can estimate the age of the person based on their facial features. This is neural networks that play first differentiating the base from the background and then correlating.  Lines and spots on your face to a possible H forecasting neural networks are trained to understand the patterns and detect the possibility of rain fall or rise in stock prices with high accuracy music composition neural networks, and even learn patterns and music constrain itself enough to compose a fresh too. So here's a question for you, which of the following statements does not hold true a activation functions are threshold fungus. The error is calculated at each layer of the neural network see both forward and back propagation take place during the training process of a neural network. The most of the data processing is carried out in the hidden layers leave your answers in the comment section below three of you standing cheap. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sri_gpt",
   "language": "python3",
   "name": "sri_gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
