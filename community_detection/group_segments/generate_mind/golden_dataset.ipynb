{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T07:21:08.436297Z",
     "start_time": "2020-01-17T07:21:08.409059Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T07:21:27.095457Z",
     "start_time": "2020-01-17T07:21:11.779473Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ray__/ssd/BERT/\")\n",
    "sys.path.append(\"/home/ray__/CS/org/etherlabs/ai-engine/pkg/\")\n",
    "sys.path.append(\"../\")\n",
    "from gpt_feat_utils import GPT_Inference\n",
    "\n",
    "#gpt_model = GPT_Inference(\"/home/ray__/ssd/BERT/models/se/epoch3/\", device=\"cpu\")\n",
    "gpt_model = GPT_Inference(\"/home/ray__/ssd/BERT/models/ether/\", device=\"cpu\")\n",
    "\n",
    "# with open('../topic_testing/cullen_test.json','rb') as f:\n",
    "#     request = json.load(f)\n",
    "#     if isinstance(request, str):\n",
    "#         request = json.loads(request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T07:21:32.589097Z",
     "start_time": "2020-01-17T07:21:27.236678Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "# S.E\n",
    "#ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/se/se_entity_feats_v3.pkl\",\"rb\"))\n",
    "#sent_dict = pickle.load(open(\"/home/ray__/ssd/minds/se/se_sent_dict_v3.pkl\", \"rb\"))\n",
    "#com_map = pickle.load(open(\"/home/ray__/ssd/minds/se/tests/com_map.pkl\", \"rb\"))\n",
    "# ent_graph = nx.read_gpickle(\"/home/ray__/ssd/minds/se/se_ent_graph_wcosine_pruned_v3.gpkl\")\n",
    "\n",
    "# Ether\n",
    "ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/ether/ether_entity_feats_grp.pkl\",\"rb\"))\n",
    "sent_dict = pickle.load(open(\"/home/ray__/ssd/minds/ether/ether_sent_dict_v1.pkl\", \"rb\"))\n",
    "com_map = pickle.load(open(\"/home/ray__/ssd/minds/ether/com_map_refined.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "# # A.Ient\n",
    "# ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/ai/ai_entity.pkl\", \"rb\"))\n",
    "# com_map = pickle.load(open(\"/home/ray__/ssd/minds/ai/com_map_ai.pkl\", \"rb\"))\n",
    "# ent_graph = nx.read_gpickle(\"/home/ray__/ssd/minds/ai/ai_pruned_entity_wfv.gpkl\")\n",
    "\n",
    "\n",
    "common_entities = ent_fv_full.keys() & com_map.keys()\n",
    "ent_fv = {}\n",
    "for ent in common_entities:\n",
    "    ent_fv[ent] = ent_fv_full[ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T07:21:47.056454Z",
     "start_time": "2020-01-17T07:21:38.820281Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /tmp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /tmp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /tmp/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import text_preprocessing.preprocess as tp\n",
    "from extra_preprocess import preprocess_text\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def generate_gs(set_list):\n",
    "    com_freq = {}\n",
    "    for file in set_list:\n",
    "        req = json.load(open(file, \"r\"))\n",
    "        if isinstance(req, str):\n",
    "            request = json.loads(req)[\"body\"]\n",
    "        else:\n",
    "            request = req[\"body\"]\n",
    "            \n",
    "        request[\"segments\"] = sorted(request['segments'], key=lambda kv:kv['startTime'])\n",
    "        for index, seg in enumerate(request[\"segments\"]):\n",
    "            request[\"segments\"][index][\"originalText\"] = \" \".join(preprocess_text(seg[\"originalText\"]))\n",
    "        segments_map = {}\n",
    "        for index, seg in enumerate(request[\"segments\"]):\n",
    "            if seg[\"originalText\"] != \"\":\n",
    "                segments_map[seg['id']] = seg\n",
    "                # if len(seg[\"originalText\"].split(\". \"))==1 and len(seg[\"originalText\"].split(\" \"))<=6 :\n",
    "                #continue\n",
    "                segments_map[seg['id']][\"order\"] = index\n",
    "        text = list(map(lambda seg: (seg[\"originalText\"], seg[\"id\"]), [segment for segment in request['segments'] if segment[\"originalText\"]!=\"\"]))\n",
    "        seg_list = [sent for sent, id  in text]\n",
    "        segid_list = [id for sent, id in text]\n",
    "        sent_list = list(map(lambda seg, segid:([sent + \". \" for sent in seg.split(\". \")],segid), seg_list, segid_list))\n",
    "        sent_list = [(sent, segid) for seg, segid in sent_list for sent in seg]\n",
    "        \n",
    "        segments_fv = {}\n",
    "        for segments in segments_map.values():\n",
    "            mod_sent = preprocess_text(segments[\"originalText\"])\n",
    "            if mod_sent:\n",
    "                sent_fv = list(map(lambda kv: gpt_model.get_text_feats(kv), mod_sent))\n",
    "                segments_fv[segments[\"id\"]] = np.mean(sent_fv, axis=0)\n",
    "        \n",
    "        \n",
    "        ent_score = {}\n",
    "        for segid in segments_fv.keys():\n",
    "            ent_score[segid] = []\n",
    "            for ent in ent_fv.keys():\n",
    "                ent_score[segid].append( (ent, 1 - cosine(segments_fv[segid], ent_fv[ent])))\n",
    "                \n",
    "        keys_in_ent_score = ent_score.keys()\n",
    "        ent_score_sorted = {}\n",
    "        for segid in keys_in_ent_score:\n",
    "            sorted_score = sorted(ent_score[segid], key=lambda x: x[1], reverse=True)\n",
    "            ent_score_sorted[segid] = [i for i,j in sorted_score][:10]\n",
    "            for ent in [i for i,j in sorted_score][:10]:\n",
    "                if com_map[ent] in com_freq.keys():\n",
    "                    com_freq[com_map[ent]] += 1\n",
    "                else:\n",
    "                    com_freq[com_map[ent]] = 1\n",
    "            print (\"\\n\\n\\n\", segments_map[segid][\"originalText\"], \"\\n\\n Entities picked: \", [i for i,j in sorted_score][:10], \"\\n\\n community mapped: \",  [com_map[i] for i in [i for i,j in sorted_score][:10]])\n",
    "        \n",
    "    return com_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T07:22:39.159873Z",
     "start_time": "2020-01-17T07:21:47.059170Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " I just done with some pocs one thing which I wanted to figure it out like why it was happening. I frame and why it is not in the with the web and if I hosted our entire lives, it is a mate on the iframe whether I could able to hear the sounds or not. \n",
      "\n",
      " Entities picked:  ['Internet', 'Ios', 'Gypsy', 'Janus', 'Web', 'Dixie', 'AI', 'Web Sdk', 'Sdk', 'App'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 5, 5, 48, 48, 48, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " The first one like when I hosted it on iframe, I could able to get the audio without any distortion. But one more effort we had was the mixing and then I was doing the mixing yesterday. I was booking on the mixing part in that like we do not have to do much work by because it seemed it always gives the updated strength initially only once we have to do it because since I was Computing again, and again, it was giving me Cool. So right now like I have a I can get the audio output which like after all of our conversation. And also I get the transcript the problem now I am facing is if I have like attachment my microphone and if I speak I am able to get the transcripts and also it I am able to get this trains as well as that so I was getting the transcript, but if I remove Move the geofoam and spake. I am able to hear but I am not able to get the stream. So it is something related to device our ready which we have to give for the output. Although I am going like I am facing that issue comes out right now. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Ios', 'Web', 'Dixie', 'Internet', 'App', 'Sdk', 'Websocket', 'AI'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 5, 48, 5, 5, 48, 38, 48]\n",
      "\n",
      "\n",
      "\n",
      " Dont run away when you remove the like earphones, it will not be like for dekada it. But so what happens like when you remove the Air Force it like you are able to hear it in on the speakers, but you are not getting that. \n",
      "\n",
      " Entities picked:  ['Uuids', 'Uuid', 'Janus', 'Etc', 'Json', 'Controller', 'Name', 'Gypsy', 'Get', 'D'] \n",
      "\n",
      " community mapped:  [42, 42, 5, 42, 42, 41, 2, 5, 42, 94]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, I am not able to I am not able to get the Streamside self. I the the one which we compute and we get the wave form, right? \n",
      "\n",
      " Entities picked:  ['Dixie', 'AI', 'Janus', 'Pim', 'Internet', 'Gypsy', 'Sdk', 'Names', 'Ios', 'Web'] \n",
      "\n",
      " community mapped:  [48, 48, 5, 25, 5, 5, 48, 48, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " It is select the device to be earphones and then start or you selected some other input device then move their phones and then started. \n",
      "\n",
      " Entities picked:  ['Device', 'App', 'Janus', 'Slack', 'Internet', 'Name', 'Screen', 'Ios', 'Apa', 'Gypsy'] \n",
      "\n",
      " community mapped:  [3, 5, 5, 8, 5, 2, 174, 5, 2, 5]\n",
      "\n",
      "\n",
      "\n",
      " I mean, they do not have to listen for the renegotiation on soft because the streams which helps it gives in the initial node will be automatically updated. Like if we hear the blocks which they send the like if we transform the stream to blob and if you hear the jobs we get the entire conversation. \n",
      "\n",
      " Entities picked:  ['Janus', 'Dixie', 'Channel Minds', 'Names', 'Workspace', 'Websockets', 'Id', 'Apa', 'Gypsy', 'Context'] \n",
      "\n",
      " community mapped:  [5, 48, 41, 48, 48, 38, 2, 2, 5, 41]\n",
      "\n",
      "\n",
      "\n",
      " Aztecs added we need to compute but then if some tracks are removed or if someone mute or unmute we do not have to compute again. \n",
      "\n",
      " Entities picked:  ['Ebs', 'S3', 'Lambdas', 'Janus', 'Lambda', 'Ec2', 'Apa', 'Cdn', 'Aws', 'Internet'] \n",
      "\n",
      " community mapped:  [76, 2, 2, 5, 2, 76, 2, 2, 2, 5]\n",
      "\n",
      "\n",
      "\n",
      " Dont try and deploy it on staging to recorder and you can test like how it is coming actually in you have not deployed on recorder yet, right? \n",
      "\n",
      " Entities picked:  ['Dixie', 'AI', 'App Center', 'Sdk', 'Janus', 'Internet', 'Ios', 'Gypsy', 'Web', 'Web Sdk'] \n",
      "\n",
      " community mapped:  [48, 48, 3, 48, 5, 5, 5, 5, 5, 48]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, so like in this case cell like it is like earphone giving the issues so we can test it out whether it is working properly. \n",
      "\n",
      " Entities picked:  ['Pim', 'Dixie', 'Ios', 'Janus', 'Gypsy', 'Internet', 'Xcode', 'Chrome', 'AI', 'Graham'] \n",
      "\n",
      " community mapped:  [25, 48, 5, 5, 5, 5, 3, 3, 48, 25]\n",
      "\n",
      "\n",
      "\n",
      " I am I am seeing like Channel not not found something like after change of the meeting lie. So I need I need to see if we come back from like give the number or itself. It will go off or you will be so it will stay the same but it is not at first I will focus on the transcript path then I will say that. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Uuids', 'Name', 'Uuid', 'Id', 'Json', 'Dixie', 'Api', 'Etc'] \n",
      "\n",
      " community mapped:  [5, 5, 42, 2, 42, 2, 42, 48, 2, 42]\n",
      "\n",
      "\n",
      "\n",
      " So what is happening is essentially we are scraping for the right link, but even so what we used to do was when you so load the page. We try to capture all the cookies and pass it forward when we try to get the media from New link and that is returning a phone or three even in the browser right now. So if I just copy the media link given the blow to the initial page the cookies are remaining. So I think there are few headers missing in that one. I try to create a a try to load the tree. I tried to see all the network calls or happening whilst the page is loading while is trying to load the media on its own and it right and a copy the curl call for it. That seems to be working some trying to extract the headers from that and see what is dynamic what is static in figure out how to access them? media from Zoom by making that appropriate change in the coral \n",
      "\n",
      " Entities picked:  ['Web', 'Gypsy', 'Janus', 'Ios', 'App', 'Api', 'Internet', 'Dixie', 'Html', 'Id'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 5, 5, 2, 5, 48, 174, 2]\n",
      "\n",
      "\n",
      "\n",
      " So they have added more security secure layers is not the entire thing our solution for extracting the you videos robust enough to figure out at least get one video link. So that is working is capturing the right link except that it is returning a phone or three now even in spite of cookies. So there is some then this passing the book is that we have to do in order to get the media. \n",
      "\n",
      " Entities picked:  ['Janus', 'Web', 'Gypsy', 'Internet', 'Ios', 'Google', 'Api', 'Dixie', 'Youtube', 'Graham'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 5, 5, 5, 2, 48, 174, 25]\n",
      "\n",
      "\n",
      "\n",
      " Franklin one of the things that you have to do because after this after Coptic fixes is is at least once a week. You should do like a zoom come and make sure that it is we are able to you know, get the according as you keep changing it and it keeps breaking right? \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Pink', 'Janus', 'Bill', 'Beta', 'Zoom', 'Dixie', 'App', 'Jesus', 'Youtube'] \n",
      "\n",
      " community mapped:  [5, 42, 5, 3, 3, 5, 48, 5, 42, 174]\n",
      "\n",
      "\n",
      "\n",
      " So why does it feel like the buttons are at it? I will work on this published this car directly in the toolbar. I am checking on the website errors, like why it is happening again. And again, even though we increase the memory so really try to find out like why it is coming. So like I see what it is and I will get back to you like why it is like what could be the possible cause as for it \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Dixie', 'Internet', 'Ios', 'Web', 'Api', 'App', 'Sdk', 'Apis'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 5, 5, 5, 2, 5, 48, 2]\n",
      "\n",
      "\n",
      "\n",
      " The website are we have we get the logs in scalar right after we get the logs and the access log on gluing. We have a place where you can check all these logs. \n",
      "\n",
      " Entities picked:  ['Github', 'Janus', 'Names', 'Dixie', 'Workspace', 'Gypsy', 'Sdk', 'Api', 'Aps', 'Apis'] \n",
      "\n",
      " community mapped:  [6, 5, 48, 48, 48, 5, 48, 2, 2, 2]\n",
      "\n",
      "\n",
      "\n",
      " So for yesterday, I cannot see activity like in the analytics but earlier when we had checked that there was like a spike whenever the website went down. \n",
      "\n",
      " Entities picked:  ['Box', 'Internet', 'Janus', 'Screen', 'Graham', 'Gypsy', 'Ios', 'Postgres', 'Google', 'Mp4'] \n",
      "\n",
      " community mapped:  [25, 5, 5, 174, 25, 5, 5, 104, 5, 174]\n",
      "\n",
      "\n",
      "\n",
      " Not only will come as a test, like even default WordPress classes are some like so are throwing at us and then book like all random even like our caching plugin sometimes throws error. So the one which you tact was from the Minify that through the caching plugin, so \n",
      "\n",
      " Entities picked:  ['Ffmpeg', 'Web', 'Javascript', 'Gypsy', 'Janus', 'Ios', 'Python', 'Sdk', 'Electron', 'Json'] \n",
      "\n",
      " community mapped:  [3, 5, 3, 5, 5, 5, 3, 48, 3, 42]\n",
      "\n",
      "\n",
      "\n",
      " I do not know balance, but then you let when you deploy up to deploy both right got to be careful. \n",
      "\n",
      " Entities picked:  ['Janus', 'Dixie', 'Internet', 'S3', 'Sdk', 'Gypsy', 'AI', 'Names', 'App Center', 'Ios'] \n",
      "\n",
      " community mapped:  [5, 48, 5, 2, 48, 5, 48, 48, 3, 5]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, only thing we will have to be careful about is the image uploads so this and maybe we can like set of the SXnumberX upload directly and then this will be very easy to have like multiple like muscular kind of things images load balancers. \n",
      "\n",
      " Entities picked:  ['Sdk', 'S3', 'Web Sdk', 'Web', 'Python', 'Dixie', 'Gypsy', 'Janus', 'Ecs', 'Cdn'] \n",
      "\n",
      " community mapped:  [48, 2, 48, 5, 3, 48, 5, 5, 76, 2]\n",
      "\n",
      "\n",
      "\n",
      " okay, let us be careful of the website because user acquisition like we are doing campaigns and all so we need to make sure we are not losing losers because of upset and lips \n",
      "\n",
      " Entities picked:  ['Internet', 'Google', 'Names', 'Janus', 'Dixie', 'AI', 'Gypsy', 'Ios', 'Web', 'Slack'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 5, 48, 48, 5, 5, 5, 8]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, so regarding the cost for tourism the markers of set timing is showing properly the transcript also and the jumping part is also fine. Europe Auto skipping the post part is working fine, but in one to markers I observed that it was not working as expected. So I think of you case one two cases are saying I will just debugging that. Then I can go so I can give a bill today. So what will not work in it is since I am not integrated the websocket part for speeding up the Pod resume API. \n",
      "\n",
      " Entities picked:  ['Janus', 'Internet', 'Gypsy', 'Dixie', 'Ios', 'Api', 'Web', 'Sdk', 'Postgres', 'Google'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 48, 5, 2, 5, 48, 104, 5]\n",
      "\n",
      "\n",
      "\n",
      " If you need any does not repeat the whole thing so that you know, just because that is something that is important for us to push anyway, right? So once you have got the horns, you have the full thing integrated identical to how it should be. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Gypsy', 'Janus', 'Web', 'Names', 'Sdk', 'AI', 'Github', 'Ios', 'Api'] \n",
      "\n",
      " community mapped:  [48, 5, 5, 5, 48, 48, 48, 6, 5, 2]\n",
      "\n",
      "\n",
      "\n",
      " Actually, he can currently also tell like if he this a playback which was created through web then the positivism is immediately like book if a seems but yeah, he can test it together also. \n",
      "\n",
      " Entities picked:  ['Janus', 'Dixie', 'Gypsy', 'Api', 'Web', 'Ios', 'Names', 'Workspace', 'Internet', 'Sdk'] \n",
      "\n",
      " community mapped:  [5, 48, 5, 2, 5, 5, 48, 48, 5, 48]\n",
      "\n",
      "\n",
      "\n",
      " So after the coral test XnumberX and XnumberX and Q and do some basic right corner one or two tests on that if that works fine. So I mean one thing that is pending from my series like I need to test any to a duties for that for my implementation. Ask I am going to say you tease me for my boots. \n",
      "\n",
      " Entities picked:  ['Pink', 'Gypsy', 'Janus', 'Github', 'Jesus', 'Epa', 'Dixie', 'Web', 'Ios', 'Sdk'] \n",
      "\n",
      " community mapped:  [42, 5, 5, 6, 42, 8, 48, 5, 5, 48]\n",
      "\n",
      "\n",
      "\n",
      " I hope this trip years ago during that maybe binder today are on Thursday can do a test on this one also. \n",
      "\n",
      " Entities picked:  ['AI', 'Dixie', 'Pims', 'Ontology', 'Gypsy', 'Janus', 'Pink', 'Ios', 'Graham', 'Github'] \n",
      "\n",
      " community mapped:  [48, 48, 25, 124, 5, 5, 42, 5, 25, 6]\n",
      "\n",
      "\n",
      "\n",
      " Okay, I will let us around you will just do the same similar thing except if the providers also Thailand, right? \n",
      "\n",
      " Entities picked:  ['Janus', 'Apa', 'Aws', 'AI', 'Cdn', 'Sdk', 'Internet', 'Sigmoid', 'Aps', 'Names'] \n",
      "\n",
      " community mapped:  [5, 2, 2, 48, 2, 48, 5, 2, 2, 48]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, so I am going to do do not add a zoom Link in the remote and fun at the same time. \n",
      "\n",
      " Entities picked:  ['Zoom', 'Mla', 'Dixie', 'AI', 'Workspace', 'Chrome', 'Ios', 'Congress', 'Names', 'Web'] \n",
      "\n",
      " community mapped:  [5, 25, 48, 48, 48, 3, 5, 6, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " But the thing is it will exist between it will join the call at the exact time. It is eleven eleven to twelve will join a QA and we have the perfect silence buffer. It it late for that silence buffer, and then it will automatically in the \n",
      "\n",
      " Entities picked:  ['Janus', 'Id', 'Uuid', 'Context', 'Name', 'Uuids', 'Json', 'Gypsy', 'Api', 'Dixie'] \n",
      "\n",
      " community mapped:  [5, 2, 42, 41, 2, 42, 42, 5, 2, 48]\n",
      "\n",
      "\n",
      "\n",
      " Let is say there are if the that incline dialed in and it could not find anybody and it drops we still show that call as in our Netflix field. I think we might need to make those kind of you know. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Name', 'Web', 'Api', 'Dixie', 'Internet', 'Sdk', 'Id', 'Ios'] \n",
      "\n",
      " community mapped:  [5, 5, 2, 5, 2, 48, 5, 48, 2, 5]\n",
      "\n",
      "\n",
      "\n",
      " I mean in general is the number of participants in the caldwells, you know more than one then we can show right. \n",
      "\n",
      " Entities picked:  ['Youtube', 'America', 'Uuid', 'Name', 'Jesus', 'D', 'Janus', 'Gypsy', 'Id', 'Graph'] \n",
      "\n",
      " community mapped:  [174, 91, 42, 2, 42, 94, 5, 5, 2, 94]\n",
      "\n",
      "\n",
      "\n",
      " Nothing much happened since yesterday, but the primary thing is to on the key phrase filtration. We I am just trying to put quick key phrase  entity graph that would help which can go into such tanks key free service. So we will start there and then see how that will help with the filtration and to and then take it from there. I should have the Ether in ether keyphrase graph byebye today most it is there. I just need to do some tests on that and and on the other side regarding the custom Minds. We just realize that the data that we have been using was little noisy. That is the that is the that is the reason why the model was not consistent with the other models. So we did not is the data and then we are going to Arjun is going to train a new model that will that should improve And then make it make things clear for a customized. And on the I think that the first one is done sighs, so Trisha and just acknowledged currently, it will be right now. So the the in the communities may not make much sense because of the coverage but usually what I would expect in an optimal way would be to most of these iOS and under anything a general software engineering should take precedence. Otherwise, it will not cover most of the Ether engineer it will not rank. Because it is a semi but if we fix this ether engineering model, we should be able to revert it back if the things are in line with their domain models. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Ether Engineering', 'Dixie', 'Graham', 'Names', 'Web', 'Graph', 'Hr', 'Sdk'] \n",
      "\n",
      " community mapped:  [5, 5, 124, 48, 25, 48, 5, 94, 124, 48]\n",
      "\n",
      "\n",
      "\n",
      " On the recommended watches are so like whatever is needed for deployment is like ready for my site. So I am just waiting on waiting for the platform changes. so you like the only two changes that will we will be needing that is one is like graph backfilling in production or staging data and another is like Nazi when publication so once these are done then it should be meanwhile. I am also so I am testing two things in recommended water. So like locally and and having a graph dump of our engineering Channel which has like user entities and user connections based on summary highlights. So I am checking how it looks like with those as the insights. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Dixie', 'Web', 'Ios', 'Workspace', 'Sdk', 'App Store', 'Domain', 'Feature'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 5, 5, 48, 48, 5, 27, 6]\n",
      "\n",
      "\n",
      "\n",
      " I have not started but I am not sure if Karthik is also started. We have not discussed on them because if there is a dependency on platform changes for this to go into staging we should prioritize that right you have we should start working on it like a I was working on some other thing right like that simulate meeting I can respond because I can start working on that if Karthik is busy on something else. I will discuss what Shrunk the two tasks from platform said I did not get that. The one is the Matt is client with the other is backfilling Tusk, right? \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Ios', 'Web', 'Dixie', 'Api', 'Sdk', 'Web Sdk', 'Internet', 'Apa'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 5, 48, 2, 48, 48, 5, 2]\n",
      "\n",
      "\n",
      "\n",
      " Okay in general for a I just make sure that we are just touching these four channels right engineering Emily. \n",
      "\n",
      " Entities picked:  ['AI', 'Hr', 'Names', 'Domain', 'Ether Engineering', 'Dixie', 'Janus', 'Channel Minds', 'Workspace', 'Internet'] \n",
      "\n",
      " community mapped:  [48, 124, 48, 27, 124, 48, 5, 41, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " We just I just got the dump from when courts will be training a model and then we will look into how much data is actually required before we even start training us tomorrow for our customers and how to actually go about. \n",
      "\n",
      " Entities picked:  ['Gpt', 'Ecs', 'Hr', 'Gpd', 'Mla', 'AI', 'Turkish', 'Pim', 'S3', 'Ether Engineering'] \n",
      "\n",
      " community mapped:  [25, 76, 124, 25, 25, 48, 76, 25, 2, 124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " As soon as we click on this one, we used to create a marker and then subsidize a user to actually create a you know, start a screenshot. \n",
      "\n",
      " Entities picked:  ['Slack', 'Admin', 'Name', 'App', 'Screen', 'Login', 'Epa', 'Zoom', 'Id', 'Sigmoid'] \n",
      "\n",
      " community mapped:  [8, 8, 2, 5, 174, 8, 8, 5, 2, 2]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, I may be only a month even you can ask him to do and do not notice him on the phone. \n",
      "\n",
      " Entities picked:  ['AI', 'Dixie', 'Janus', 'Internet', 'Names', 'Pim', 'Apa', 'Api', 'Gypsy', 'Ios'] \n",
      "\n",
      " community mapped:  [48, 48, 5, 5, 48, 25, 2, 2, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " No, because the first Clincher said Karthik the second one said initial so it is not the same bug. This one is from the live cleaned screenshare markers are created from the lifeline. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Get', 'Janus', 'Json', 'Name', 'Ffmpeg', 'Uuids', 'Screen', 'Pink', 'App'] \n",
      "\n",
      " community mapped:  [5, 42, 5, 42, 2, 3, 42, 174, 42, 5]\n",
      "\n",
      "\n",
      "\n",
      " I think the some accidental click that happen that regard at least the markers in screenshot, but it does not actually stop the screenshot. \n",
      "\n",
      " Entities picked:  ['Safari', 'Iframe', 'App', 'Desktop', 'Html', 'Screen', 'Youtube', 'Zoom', 'Electron App', 'Gypsy'] \n",
      "\n",
      " community mapped:  [3, 3, 5, 3, 174, 174, 174, 5, 3, 5]\n",
      "\n",
      "\n",
      "\n",
      " Right but it should not have sir said stop and see the differences when we went from Janus which was full screen right where you had people video and content with you. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Css', 'House', 'Janus', 'Screen', 'Iframe', 'Youtube', 'App', 'Internet', 'Chrome'] \n",
      "\n",
      " community mapped:  [5, 3, 3, 5, 174, 3, 174, 5, 5, 3]\n",
      "\n",
      "\n",
      "\n",
      " So we say it does not matter whether you do not need to stop before you start in Janus. See you can have multiple people sharing at the same time because it is all going to the same video stream. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Dixie', 'Internet', 'Uuid', 'AI', 'Names', 'Id', 'Sql', 'Db'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 5, 42, 48, 48, 2, 104, 104]\n",
      "\n",
      "\n",
      "\n",
      " and Janice also the reason we did not want to have multiple screen share was because it was not conforming to the last n semantics so reasons why you did that because usually help people and you have content it is good to have only one Contender or conference at a time. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Web', 'Mp4', 'Google', 'Ios', 'Dixie', 'Names', 'Sdk', 'Internet'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 174, 5, 5, 48, 48, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " No, no, that is not doing any work of this is fine. The only thing is if you are spurious stop doing spelling then we need to see why. \n",
      "\n",
      " Entities picked:  ['Uuids', 'Uuid', 'Json', 'Name', 'D', 'Etc', 'Janus', 'Get', 'Id', 'Gypsy'] \n",
      "\n",
      " community mapped:  [42, 42, 42, 2, 94, 42, 5, 42, 2, 5]\n",
      "\n",
      "\n",
      "\n",
      " Can we have a kind of focused discussion on that cost CB started off with? This is my piano teacher back and talk about cast right? There is about three or four different things that we are trying and we are going to rewrite iOS core base everything at the same time result is none of the cast improvements are actually making it into production. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'AI', 'Internet', 'Graham', 'Dixie', 'Ether Engineering', 'Hr', 'Ios', 'Web'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 5, 25, 48, 124, 124, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " I am going on because we think it is how what XnumberXth of January right we will be trying to do this since end of December. But can we take a step back see from my perspective? You can pick either one of them complete them get them into production. Are you going to tell me which one we want to take first as a team one is this cast me pause resume playback hiding Which is a Broncos for out here for costume skills. The third is at the beginning of the cost you have this ether lab joining and throwing people off and people starting to talk before and all that. \n",
      "\n",
      " Entities picked:  ['Janus', 'AI', 'Dixie', 'Names', 'Gypsy', 'Internet', 'Sdk', 'Ether Engineering', 'Domain', 'Hr'] \n",
      "\n",
      " community mapped:  [5, 48, 48, 48, 5, 5, 48, 124, 27, 124]\n",
      "\n",
      "\n",
      "\n",
      " Fourth one which is spinning of the speaker, which we also need to take up. Now all of these for I want to see, you know, we right now trying to get everything address in web platform iOS and all of the same time. Now, I think we are tripping over each other on top of it in iOS. So what do you think is this in terms of how can we try to get at least one one of these features incrementally into production completely? \n",
      "\n",
      " Entities picked:  ['Ios', 'Web', 'Dixie', 'Gypsy', 'Janus', 'Internet', 'Sdk', 'AI', 'Web Sdk', 'App'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 5, 5, 5, 48, 48, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " May I speak Society discard draft changes are in staging right now. So what we can do is I will create release today and we can release these changes tomorrow in beta products with the router changes. I like I have tested the router changes thoroughly will test. Again, I will sit with me clean and a dentist again. I will create a doc and share in the channel and we will test again thoroughly with Franklin and like little later released tomorrow in the morning, beautiful. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Gypsy', 'AI', 'Names', 'Janus', 'Github', 'Feature', 'App Store', 'Workspace', 'Sdk'] \n",
      "\n",
      " community mapped:  [48, 5, 48, 48, 5, 6, 6, 5, 48, 48]\n",
      "\n",
      "\n",
      "\n",
      " But okay, so let us take let us take let us kill that cat first. So let us say we want to create publish this card same draft feature completely into production. A flexible side is all the innocent platform already in production, and it is only feature of life. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Names', 'Janus', 'Dixie', 'App', 'Web', 'App Store', 'Feature', 'Api', 'Ether Engineering'] \n",
      "\n",
      " community mapped:  [5, 48, 5, 48, 5, 5, 5, 6, 2, 124]\n",
      "\n",
      "\n",
      "\n",
      " It is only feature flag only the draft thing is feature flag rest of the tenders are available in production. \n",
      "\n",
      " Entities picked:  ['Github', 'Pr', 'Feature', 'Beta', 'Gypsy', 'App Store', 'Zoom', 'Web', 'Api', 'Slack'] \n",
      "\n",
      " community mapped:  [6, 6, 6, 3, 5, 5, 5, 5, 2, 8]\n",
      "\n",
      "\n",
      "\n",
      " What about save this card which has he said anything for separately or is it already combined with all the other changes that are going on? \n",
      "\n",
      " Entities picked:  ['Names', 'Janus', 'Dixie', 'Api', 'Gypsy', 'Slack', 'Sigmoid', 'Sdk', 'AI', 'Apis'] \n",
      "\n",
      " community mapped:  [48, 5, 48, 2, 5, 8, 2, 48, 48, 2]\n",
      "\n",
      "\n",
      "\n",
      " In other words when you were there be other problems that come because of this affair is separated enough only the same people. \n",
      "\n",
      " Entities picked:  ['Id', 'Uuid', 'Janus', 'Name', 'Domain', 'Ether Engineering', 'Json', 'Gypsy', 'Apis', 'Graph'] \n",
      "\n",
      " community mapped:  [2, 42, 5, 2, 27, 124, 42, 5, 2, 94]\n",
      "\n",
      "\n",
      "\n",
      " In web it is like separated in a flick all the pr has independent and we can take it off or keep it as it is. But in the master all the like timeline feature and also the save as draft features are in the Moscow. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Feature', 'Github', 'Names', 'Workspace', 'Web', 'App Store', 'Dixie', 'Pr', 'Janus'] \n",
      "\n",
      " community mapped:  [5, 6, 6, 48, 48, 5, 5, 48, 6, 5]\n",
      "\n",
      "\n",
      "\n",
      " So from their perspective, this is a now let me clarify what this means when we say publish this card and draft the draft is showing up all the things that are in graph showing up in Netflix you the right time? \n",
      "\n",
      " Entities picked:  ['Api', 'Slack', 'Apis', 'Apa', 'Janus', 'Epa', 'Id', 'Names', 'Graph', 'Meeting Id'] \n",
      "\n",
      " community mapped:  [2, 8, 2, 2, 5, 8, 2, 48, 94, 2]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, okay, and then going back and clicking on already saved Last One clicking on exert and publish all of that we have tested \n",
      "\n",
      " Entities picked:  ['Dixie', 'AI', 'Gypsy', 'Janus', 'Names', 'Screen', 'Pink', 'Sdk', 'Web', 'Github'] \n",
      "\n",
      " community mapped:  [48, 48, 5, 5, 48, 174, 42, 48, 5, 6]\n",
      "\n",
      "\n",
      "\n",
      " The cost we can just push the publish this card and save as draft to beat up Rod like we can cherrypick currently its smallest along with the navigation so we can either put like your test it thoroughly and done. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Names', 'App Center', 'AI', 'Janus', 'Chrome', 'Github', 'Workspace', 'Gypsy', 'Web'] \n",
      "\n",
      " community mapped:  [48, 48, 3, 48, 5, 3, 6, 48, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " I am sorry, but I ever said that we one of these changes related to the router here in a separate Branch. Let is do a good job of singing really and pushing it without the pressure that we have to release because changes. So if you can release the cost changes, especially the same discard whatever you have done into the current production current version of the app store. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Names', 'Janus', 'Sdk', 'Gypsy', 'Github', 'Workspace', 'AI', 'App Center', 'S3'] \n",
      "\n",
      " community mapped:  [48, 48, 5, 48, 5, 6, 48, 48, 3, 2]\n",
      "\n",
      "\n",
      "\n",
      " Only thing is those save as draft will not show up separately in the recent calls like then the Netflix mode. \n",
      "\n",
      " Entities picked:  ['Slack', 'App', 'Web', 'Workspace', 'Ios', 'Api', 'Gypsy', 'Janus', 'Dixie', 'Names'] \n",
      "\n",
      " community mapped:  [8, 5, 5, 48, 5, 2, 5, 5, 48, 48]\n",
      "\n",
      "\n",
      "\n",
      " Let me just lay back like platform more changes are required when we are going to try to get one branch temperature changes for same to publish graph Esther and push the sages. I mean push to production and then we do not know the same fibers, but that would not include the router changes. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Janus', 'Sdk', 'Names', 'Feature', 'Workspace', 'Gypsy', 'App Center', 'Github', 'Ios'] \n",
      "\n",
      " community mapped:  [48, 5, 48, 48, 6, 48, 5, 3, 6, 5]\n",
      "\n",
      "\n",
      "\n",
      " That is the only thing is will I think on production its feature flag, so we will just have to remove that for the draft. \n",
      "\n",
      " Entities picked:  ['Pr', 'Names', 'App Store', 'Github', 'Gypsy', 'Feature', 'Dixie', 'Pink', 'Beta', 'Xcode'] \n",
      "\n",
      " community mapped:  [6, 48, 5, 6, 5, 6, 48, 42, 3, 3]\n",
      "\n",
      "\n",
      "\n",
      " Yes, I we can we can take this criminal change the production. We can pick up the next one any of the for you spadamon. \n",
      "\n",
      " Entities picked:  ['AI', 'Dixie', 'Janus', 'Names', 'Gypsy', 'Pim', 'Internet', 'Sdk', 'Graham', 'Hr'] \n",
      "\n",
      " community mapped:  [48, 48, 5, 48, 5, 25, 5, 48, 25, 124]\n",
      "\n",
      "\n",
      "\n",
      " I do not think I was f on a landing zone of the country. Okay, so can we get all these changes into the staging so that at least you can do is I think rankings on his way so we can attach the suddenly in push to production of you. Okay, so we will take up this after this is done. \n",
      "\n",
      " Entities picked:  ['Dixie', 'AI', 'Janus', 'Gypsy', 'Names', 'Sdk', 'Internet', 'Pink', 'Github', 'Ios'] \n",
      "\n",
      " community mapped:  [48, 48, 5, 5, 48, 48, 5, 42, 6, 5]\n",
      "\n",
      "\n",
      "\n",
      " So like yesterday this is so we are now muting the audio using a we capture. Oh even in play back those marker of six are coming almost to the point. So but that is like I think Is doable so I will just create one cast call and you guys can like go through it and see but it is a very now the like pause resume. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Dixie', 'AI', 'Names', 'Internet', 'House', 'Web', 'Uuids', 'Uuid'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 48, 48, 5, 3, 5, 42, 42]\n",
      "\n",
      "\n",
      "\n",
      " We can just meet we are just using the overlay to cover the video and muting the audio directly in the recorder. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Gypsy', 'House', 'Janus', 'Web', 'Ios', 'Chrome', 'AI', 'Internet', 'Zoom'] \n",
      "\n",
      " community mapped:  [48, 5, 3, 5, 5, 5, 3, 48, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " So total food point is it like we have to show recording is like being set up and recording has started. So we are currently polling for to know whether the recorder has joined or not. So we will be doing that using web socket right going forward. \n",
      "\n",
      " Entities picked:  ['Websocket', 'Internet', 'Janus', 'Web', 'Ios', 'Gypsy', 'Websockets', 'Dixie', 'Apa', 'Api'] \n",
      "\n",
      " community mapped:  [38, 5, 5, 5, 5, 5, 38, 48, 2, 2]\n",
      "\n",
      "\n",
      "\n",
      " Yeah at this moment you can pull for that but like I am almost done with it. So by like postlunch you can directly listen for the websocket event. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Web', 'Api', 'Gypsy', 'Websocket', 'Janus', 'AI', 'Ios', 'App', 'Web Sdk'] \n",
      "\n",
      " community mapped:  [48, 5, 2, 5, 38, 5, 48, 5, 5, 48]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, so initially you have to check whether the recording has started like we were doing initially and instead like if it is not started instead of pulling we just wait for the websocket different. \n",
      "\n",
      " Entities picked:  ['Websocket', 'Dixie', 'Http', 'Websockets', 'Janus', 'Internet', 'Lambda', 'Api', 'Pim', 'Ios'] \n",
      "\n",
      " community mapped:  [38, 48, 38, 38, 5, 5, 2, 2, 25, 5]\n",
      "\n",
      "\n",
      "\n",
      " And that is the second one is to handle the cable line drawings after the current join in on going. \n",
      "\n",
      " Entities picked:  ['Janus', 'Dixie', 'Websocket', 'Web', 'Internet', 'Gypsy', 'Ios', 'Names', 'Workspace', 'Api'] \n",
      "\n",
      " community mapped:  [5, 48, 38, 5, 5, 5, 5, 48, 48, 2]\n",
      "\n",
      "\n",
      "\n",
      " I forgot the changes that we were discussing on how we handle that initial portion. So the idea there is that we will show a model of economic pillar that says please wait while the cost is being set up and it will come on then once the recorder joins. The model will go away and then we will just show a post that says ready to cast. All right, that is how we are doing it on the web. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Janus', 'AI', 'Gypsy', 'Web', 'Names', 'Workspace', 'Ios', 'Api', 'S3'] \n",
      "\n",
      " community mapped:  [48, 5, 48, 5, 5, 48, 48, 5, 2, 2]\n",
      "\n",
      "\n",
      "\n",
      " I think I have a couple of tasks to Meanwhile like the puzzle game and then removing the polling and using web socket and then I can \n",
      "\n",
      " Entities picked:  ['Web Sdk', 'Web', 'Ios', 'Websocket', 'Dixie', 'Gypsy', 'Internet', 'Api', 'Janus', 'Ios App'] \n",
      "\n",
      " community mapped:  [48, 5, 5, 38, 48, 5, 5, 2, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " Okay, he for iOS and my people if you going to say you will need to decide if it is easy for you to continue on one branch, which is another router changes make these changes for some production. But if you think they are already very close it all the router changes and if it will help accelerate the both of your working together. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Names', 'Janus', 'Workspace', 'Sdk', 'Ios', 'Github', 'AI', 'Gypsy', 'Web'] \n",
      "\n",
      " community mapped:  [48, 48, 5, 48, 48, 5, 6, 48, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " Yes, I I have added question detection in staging but I am getting some errors. So I am trying to figure out trying to solve that buggy. And secondly, there is one more case which Gothic pointed out in production testing. So I try to try some filters on it, but there is no one General Eric filter to filter out those kind of subjects. \n",
      "\n",
      " Entities picked:  ['Janus', 'Uuids', 'Ffmpeg', 'Gypsy', 'Json', 'Sdk', 'D', 'Internet', 'Ios', 'Dixie'] \n",
      "\n",
      " community mapped:  [5, 42, 3, 5, 42, 48, 94, 5, 5, 48]\n",
      "\n",
      "\n",
      "\n",
      " Currently I am working on that fixing that fixing the deployment deployment which I have depended agent called. This one is the problem is that this is the mismatch of number of There are too many well being returned like I have written three lives and it is not able to handle those three. So I will take straight and and then deployed to production. \n",
      "\n",
      " Entities picked:  ['Janus', 'Dixie', 'Gypsy', 'Box', 'Ios', 'Internet', 'Sdk', 'Workspace', 'Gps', 'Github'] \n",
      "\n",
      " community mapped:  [5, 48, 5, 25, 5, 5, 48, 48, 76, 6]\n",
      "\n",
      "\n",
      "\n",
      " So, you know just to take a step back goal is to filter out bad keywords, its highest priority, right? So if we are looking at that right where we are on that, I mean we were discussing earlier. \n",
      "\n",
      " Entities picked:  ['Uuid', 'Uuids', 'Graph', 'Json', 'Janus', 'D', 'Gypsy', 'Name', 'Names', 'Etc'] \n",
      "\n",
      " community mapped:  [42, 42, 94, 42, 5, 94, 5, 2, 48, 42]\n",
      "\n",
      "\n",
      "\n",
      " The thing is what we have done is we have added the patterns which forgiving like in that covers maximum number of noise, but the Tilt but filter which are covering only if I test it on a team meetings, they are giving their are able to filter out five or six bad bad action items in nXnumberX emitting. So we are not adding those filters right now because it will it will become difficult for us to maintain the maintain those things in the code \n",
      "\n",
      " Entities picked:  ['Names', 'Janus', 'Dixie', 'Gypsy', 'Uuids', 'Uuid', 'Workspace', 'Sdk', 'Graham', 'Postgres'] \n",
      "\n",
      " community mapped:  [48, 5, 48, 5, 42, 42, 48, 48, 25, 104]\n",
      "\n",
      "\n",
      "\n",
      " I had like three discussion on how we can like approach this. So one method that we were thinking of was like like you are taking the slack graph slack data is the base ground truth data and then try to like been an entity graph kind of structure with we have like we base all the other connection all the key phrase connections to The Entity. He is so and so basically like if there is something called Janus has an entity then we look for key phrase that are like Co occurring within a certain particular cycle of windows. So we might get like a noun phrases like recording record or surveys or recording client and all those stuff. So we EV form connections of this odd, and we try to and Like, you know keep reinforcing it like we tried to populate it every time as and genuine say if it is like rub then as in when it is like data comes in or if it is meeting graph. Then has not been meeting data comes in and then we put a filter that if the edge number of edge connection frequency if it is more than say four or five and use it. So this the connection between entity and key phrase is a valid one. So based on that we can whenever we get a highlight which Is like a bad word like a Sufi or something we go back and check in the graph. So ideally we expect that Sookie will have like lesser number of edge connections and we move. \n",
      "\n",
      " Entities picked:  ['Graph', 'Entity', 'Uuid', 'Id', 'Janus', 'Json', 'D', 'Context', 'Gypsy', 'Uuids'] \n",
      "\n",
      " community mapped:  [94, 94, 42, 2, 5, 42, 94, 41, 5, 42]\n",
      "\n",
      "\n",
      "\n",
      " That is correlated with the channel minded like because we cannot find a correlation between the key phase and the channel mind and see which one is outlier and things like that. \n",
      "\n",
      " Entities picked:  ['Channel Minds', 'Channel', 'Context', 'Websocket', 'Domain Mind', 'Mind', 'Workspace', 'Janus', 'Pim', 'Sql'] \n",
      "\n",
      " community mapped:  [41, 41, 41, 38, 27, 94, 48, 5, 25, 104]\n",
      "\n",
      "\n",
      "\n",
      " looking is a When we use mine for the when we use the models for these things like when we feature is the key phrases and do comparisons. They do get like lower scores, but we do not so if there is a highlight which has like five key phrases and out of which like four of them are bad. \n",
      "\n",
      " Entities picked:  ['Uuid', 'Uuids', 'Name', 'Names', 'Janus', 'Id', 'Postgres', 'Action Item', 'Gypsy', 'Json'] \n",
      "\n",
      " community mapped:  [42, 42, 2, 48, 5, 2, 104, 41, 5, 42]\n",
      "\n",
      "\n",
      "\n",
      " they still got like a score of said point four point five, and we do not have a way to Put a threshold and say that okay anything less than this is supposed to be bad. \n",
      "\n",
      " Entities picked:  ['Uuid', 'Uuids', 'Janus', 'Json', 'Pim', 'Graham', 'D', 'Internet', 'Google', 'Name'] \n",
      "\n",
      " community mapped:  [42, 42, 5, 42, 25, 25, 94, 5, 5, 2]\n",
      "\n",
      "\n",
      "\n",
      " Like feet Rising like feet Rising using Channel mind would be like a next step to further filter it but like we were like we still feature eyes and we still rank the key phrases and and we still get like some bad key phrases the somebody is so we thought like we will take one step back and just filter based on. Simple noun phrase occurrences and then apply Channel China mind on top of the next result. \n",
      "\n",
      " Entities picked:  ['Pim', 'Names', 'Graham', 'Channel Minds', 'Janus', 'Hr', 'Ether Engineering', 'Dixie', 'Mind', 'Ether Engine'] \n",
      "\n",
      " community mapped:  [25, 48, 25, 41, 5, 124, 124, 48, 94, 94]\n",
      "\n",
      "\n",
      "\n",
      " You know, but that is a big thing that I think in terms of getting done getting to our growth rate summary text. \n",
      "\n",
      " Entities picked:  ['Hr', 'Ether Engineering', 'AI', 'Janus', 'Ontology', 'Youtube', 'Gypsy', 'Internet', 'Google', 'Jesus'] \n",
      "\n",
      " community mapped:  [124, 124, 48, 5, 124, 174, 5, 5, 5, 42]\n",
      "\n",
      "\n",
      "\n",
      " We are getting rid of recommended Watchers in that case like channel. So after this call if you can go and put like a thumbs up or thumbs down so that will help me decide if you can for now if this is like a good version to be pushed in statement. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Names', 'Gypsy', 'Channel', 'Janus', 'Channel Minds', 'Github', 'Workspace', 'Congress', 'Web'] \n",
      "\n",
      " community mapped:  [48, 48, 5, 41, 5, 41, 6, 48, 6, 5]\n",
      "\n",
      "\n",
      "\n",
      " But we will do that and the focus will be on the highlights essentially right like whatever which is essentially in. \n",
      "\n",
      " Entities picked:  ['Mla', 'Names', 'Dixie', 'AI', 'Gypsy', 'Zoom', 'Domain Mind', 'Gpt', 'Web', 'Workspace'] \n",
      "\n",
      " community mapped:  [25, 48, 48, 48, 5, 5, 27, 25, 5, 48]\n",
      "\n",
      "\n",
      "\n",
      " That is the third is just to see how I created good it is for before pushing it to staging and production. \n",
      "\n",
      " Entities picked:  ['Pr', 'Feature', 'Github', 'Beta', 'Xcode', 'Dixie', 'Gypsy', 'App Center', 'App Store', 'Names'] \n",
      "\n",
      " community mapped:  [6, 6, 6, 3, 3, 48, 5, 3, 5, 48]\n",
      "\n",
      "\n",
      "\n",
      " Yes a so the storing part is then I mean it is almost an like all but one cases feeling currently. So for example, like if someone edits a single instance of a recurring event that storage part is working but if someone reads the same event, it is failing currently. So once that is done I will open up here for this the second part of this. Change would be retrieval of of an event when when a zoom start event is triggered. \n",
      "\n",
      " Entities picked:  ['Id', 'Context', 'Janus', 'Action', 'Gypsy', 'Name', 'Api', 'Postgres', 'Json', 'Web'] \n",
      "\n",
      " community mapped:  [2, 41, 5, 41, 5, 2, 2, 104, 42, 5]\n",
      "\n",
      "\n",
      "\n",
      " I was looking into the Dario pause and Risen King before that. I am writing a different Lambda for that with the table Yarns. And also I will be continuing continuing that work and maybe by end of day today I can raise it. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Dixie', 'AI', 'Apa', 'Lambda', 'Web', 'Ontology', 'Api', 'Internet'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 48, 2, 2, 5, 124, 2, 5]\n",
      "\n",
      "\n",
      "\n",
      " So I will start looking into the recommended Watchers implementation on the platform side. And besides that one also taking a look at this ongoing issue where in seating sometimes the record is do not join especially when the remember instances. There is a delay in by which Docker pull happens at which we are not able to talk to the recording excesses on time. So we are not in for scheduling guarantee scheduling of a call. That is a separate that is higher problem that we need to fix but I am trying to improve the probabilities of guaranteeing the recorder joints when there are res number of cooled instances also at the expense of increased time to time. \n",
      "\n",
      " Entities picked:  ['Janus', 'Websocket', 'Postgres', 'Workspace', 'Dixie', 'Gypsy', 'Internet', 'Lambda', 'Pim', 'Graham'] \n",
      "\n",
      " community mapped:  [5, 38, 104, 48, 48, 5, 5, 2, 25, 25]\n",
      "\n",
      "\n",
      "\n",
      " What are we finally concluding on what will be the give you want to increase? The number of retries have to make them there are two places one is the one the section that you posted about. You know, we do not do everything Lexi seconds sort of goes away and we just \n",
      "\n",
      " Entities picked:  ['Janus', 'AI', 'Dixie', 'Uuid', 'Gypsy', 'Id', 'Internet', 'Names', 'Api', 'Name'] \n",
      "\n",
      " community mapped:  [5, 48, 48, 42, 5, 2, 5, 48, 2, 2]\n",
      "\n",
      "\n",
      "\n",
      " So in that one the question is we want to return an error and increase the time to XnumberX seconds. So in do that one even as let us say it does still go through for any reason to start called we can add an expert like a back off of constant back of a five seconds before reason to think really dry times. So that we can account for the time for the instance to complete pulling the docker images from various startup. \n",
      "\n",
      " Entities picked:  ['Janus', 'Dixie', 'Gypsy', 'Sdk', 'Pim', 'Names', 'Workspace', 'Graham', 'Internet', 'Uuid'] \n",
      "\n",
      " community mapped:  [5, 48, 5, 48, 25, 48, 48, 25, 5, 42]\n",
      "\n",
      "\n",
      "\n",
      " Your birthday most of most of the times when when I when I will be decided under XnumberX second time or the interest was like changed best interest rate will change to running in XnumberX seconds. This is a precautionary we can we added it to XnumberX but like XnumberX of the problem is in that they are there the server starts around. XnumberX to XnumberX seconds after the in the instance goes to running state. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Uuid', 'Uuids', 'Ios', 'Workspace', 'Context', 'Ffmpeg', 'Postgres', 'Web'] \n",
      "\n",
      " community mapped:  [5, 5, 42, 42, 5, 48, 41, 3, 104, 5]\n",
      "\n",
      "\n",
      "\n",
      " But it at least if we have a log set for some reason easy to did not come up in XnumberX seconds. Also, then nothing should have a lot of the some errors. \n",
      "\n",
      " Entities picked:  ['Json', 'Ffmpeg', 'Uuids', 'D', 'Get', 'Gypsy', 'Janus', 'Etc', 'Uuid', 'Pink'] \n",
      "\n",
      " community mapped:  [42, 3, 42, 94, 42, 5, 5, 42, 42, 42]\n",
      "\n",
      "\n",
      "\n",
      " Just one second is I think we have like three machines running for Janus video bridge in production wrong with some, you know, DNS entries Etc. \n",
      "\n",
      " Entities picked:  ['Janus', 'Internet', 'Sdk', 'Gypsy', 'Ios', 'Google', 'Cdn', 'Graham', 'Web Sdk', 'Dixie'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 5, 5, 5, 2, 25, 48, 48]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, we can do it because right now and he will not because no one else is that this is purely what do you say disaster scenario insurance, right? \n",
      "\n",
      " Entities picked:  ['Janus', 'AI', 'Internet', 'Pink', 'Uuid', 'Dixie', 'Gypsy', 'Epa', 'Sdk', 'Api'] \n",
      "\n",
      " community mapped:  [5, 48, 5, 42, 42, 48, 5, 8, 48, 2]\n",
      "\n",
      "\n",
      "\n",
      " Janus is purely at because we have no plans to work with jealous any effort on it or anything only. We only reason we keep it is because if for whatever reason jitsi is goes down. \n",
      "\n",
      " Entities picked:  ['Janus', 'Dixie', 'Gypsy', 'Internet', 'Google', 'S3', 'Uuid', 'Graham', 'Workspace', 'Web'] \n",
      "\n",
      " community mapped:  [5, 48, 5, 5, 5, 2, 42, 25, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " With some effort we can move bring the arms back up. It may not again when I was as soon as lot of other things that will have to do then we can do that. The other option is to see you know, we would have used to discuss your strategy. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Ether Engineering', 'Web', 'Dixie', 'Ontology', 'Pink', 'Jesus', 'Names', 'Sdk'] \n",
      "\n",
      " community mapped:  [5, 5, 124, 5, 48, 124, 42, 42, 48, 48]\n",
      "\n",
      "\n",
      "\n",
      " The problem is right now all the costs changes all the new product data changes. Were not a lot of it is also stomachs and jitsi specific. \n",
      "\n",
      " Entities picked:  ['S3', 'Postgres', 'Dixie', 'Janus', 'Gypsy', 'Google', 'Sdk', 'Web', 'Graham', 'Internet'] \n",
      "\n",
      " community mapped:  [2, 104, 48, 5, 5, 5, 48, 5, 25, 5]\n",
      "\n",
      "\n",
      "\n",
      " Yes, because I am not SLE removing code from terraform bringing the instances back up into our formal not more than two. It is understanding the instance count to zero so that nothing is provisioned at least in a database and we just by switching the fly we can provision it back up. \n",
      "\n",
      " Entities picked:  ['Ebs', 'Ec2', 'S3', 'Ecs', 'Turkish', 'Aws', 'Lambda', 'Cdn', 'Lambdas', 'Janus'] \n",
      "\n",
      " community mapped:  [76, 76, 2, 76, 76, 2, 2, 2, 2, 5]\n",
      "\n",
      "\n",
      "\n",
      " So let us take it down to zero the effort of bringing and since from zero to one is as good as we wanted to scale it from ZXnumberX. \n",
      "\n",
      " Entities picked:  ['Ebs', 'Ecs', 'Pim', 'Janus', 'S3', 'Postgres', 'Pimps', 'Turkish', 'Graham', 'Dixie'] \n",
      "\n",
      " community mapped:  [76, 76, 25, 5, 2, 104, 25, 76, 25, 48]\n",
      "\n",
      "\n",
      "\n",
      " And also just so just to be safe say I will remove the jitsi provider in from the registry provider is screen API, so that front end clients get unsupported for any reason they call jitsi. \n",
      "\n",
      " Entities picked:  ['Janus', 'Web', 'Dixie', 'Api', 'Internet', 'Sdk', 'Google', 'Ios', 'Gypsy', 'Web Sdk'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 2, 5, 48, 5, 5, 5, 48]\n",
      "\n",
      "\n",
      "\n",
      " Okay, let us make sure it does not go the right on. So one thing for used to immediately focus on completely testing as draft or publish the squad on whereby and on iOS \n",
      "\n",
      " Entities picked:  ['Dixie', 'Gypsy', 'Janus', 'Ios', 'AI', 'Names', 'Web', 'Sdk', 'Github', 'Pim'] \n",
      "\n",
      " community mapped:  [48, 5, 5, 5, 48, 48, 5, 48, 6, 25]\n",
      "\n",
      "\n",
      "\n",
      " So if you can find me first part when that iron and ceremony on when that is this turbulence agent, then go through one full round of testing here is unblocked so that you can push it to production. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Janus', 'Feature', 'Workspace', 'Box', 'Gypsy', 'Ecs', 'Github', 'Pim', 'AI'] \n",
      "\n",
      " community mapped:  [48, 5, 6, 48, 25, 5, 76, 6, 25, 48]\n",
      "\n",
      "\n",
      "\n",
      " I recently upgraded the YouTube subscriber Library the ghost Library itself because it has some changes to improve the dating the new some some number new formats. So that should help with the issues at hand pointed out that some some YouTube link will not be allowed to be processed in nodes. Not rested against all of the links that go ahead try but frankly if we could if you go to staging testing and just look up the increaser Cullen had initiated nodes and it came back seeing that. Hey we cannot process somebody in the in the car in the last week so that you can just try the same links again and see if it is allowing you to run the notes. You get a somebody back and you it is allowing it to process the notes links you do. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Internet', 'Dixie', 'Graham', 'Ios', 'Web', 'Api', 'Google', 'Uuid'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 48, 25, 5, 5, 2, 5, 42]\n",
      "\n",
      "\n",
      "\n",
      " If we plant again like then staging to I will have to deploy the branch basically Master which we are planning to deploy on products. \n",
      "\n",
      " Entities picked:  ['Feature', 'Beta', 'App Store', 'Workspace', 'Pr', 'App Center', 'Names', 'Xcode', 'Github', 'Bill'] \n",
      "\n",
      " community mapped:  [6, 3, 5, 48, 6, 3, 48, 3, 6, 3]\n",
      "\n",
      "\n",
      "\n",
      " Because if I then deploy it and I will not be till the testing is done. I will not be able to like do my work because then my changes will not be there on recorder and life. \n",
      "\n",
      " Entities picked:  ['App Center', 'App Store', 'Gypsy', 'Dixie', 'Sdk', 'Beta', 'Github', 'Xcode', 'Congress', 'Feature'] \n",
      "\n",
      " community mapped:  [3, 5, 5, 48, 48, 3, 6, 3, 6, 6]\n",
      "\n",
      "\n",
      "\n",
      " After the gold one meeting you start depending on the branches that want to push to stay into and once placed on me and then you can go back and whatever we can do play whatever. \n",
      "\n",
      " Entities picked:  ['Feature', 'Github', 'Pr', 'Pink', 'Workspace', 'Jesus', 'Gypsy', 'Janus', 'Dixie', 'Names'] \n",
      "\n",
      " community mapped:  [6, 6, 6, 42, 48, 42, 5, 5, 48, 48]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, so the cars changes like this card publish things so their arms using but there might be issues because of the navigation be posting some issue is so I think Franklin can directly test on beat up rod. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Internet', 'Dixie', 'Chrome', 'Ios', 'Sdk', 'Web', 'Graham', 'Pim'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 48, 3, 5, 48, 5, 25, 25]\n",
      "\n",
      "\n",
      "\n",
      " Okay, just let me know which one prepares you for that is not know but the problem is a fraud because I will point the fraud. \n",
      "\n",
      " Entities picked:  ['Mooc', 'Auth', 'D', 'Pink', 'Json', 'Get', 'Uuids', 'Uuid', 'Etc', 'Uri'] \n",
      "\n",
      " community mapped:  [8, 8, 94, 42, 42, 42, 42, 42, 42, 8]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, I just noticed in staging to there is still the graphic visualization instance running the one that we used for Neptune. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Html', 'Dixie', 'Graham', 'Screen', 'Ios', 'Chrome', 'Internet', 'Web'] \n",
      "\n",
      " community mapped:  [5, 5, 174, 48, 25, 174, 5, 3, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " To SSH into emailing friends to one of them wearing translatable student when I saw least my graph is realization still running TXnumberX. \n",
      "\n",
      " Entities picked:  ['Janus', 'Internet', 'Ffmpeg', 'Ios', 'Uuid', 'Sdk', 'Postgres', 'Graham', 'Gypsy', 'Web'] \n",
      "\n",
      " community mapped:  [5, 5, 3, 5, 42, 48, 104, 25, 5, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " I know we have said we will be having a different apps after that. Like I did not get a heads up if I had to test iOS. \n",
      "\n",
      " Entities picked:  ['Ios', 'Web', 'App', 'Android', 'Gypsy', 'Ios App', 'Zoom', 'Sdk', 'Web Sdk', 'Electron'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 3, 5, 5, 5, 48, 48, 3]\n",
      "\n",
      "\n",
      "\n",
      " All right today, I guess the that the the other part is adding the form validation, but I guess we can add that in your when you give to get to it, right? \n",
      "\n",
      " Entities picked:  ['Pink', 'Gypsy', 'Janus', 'Dixie', 'Sdk', 'Jesus', 'Web', 'Api', 'Names', 'AI'] \n",
      "\n",
      " community mapped:  [42, 5, 5, 48, 48, 42, 5, 2, 48, 48]\n",
      "\n",
      "\n",
      "\n",
      " Please the memories to earlier it was set to default value of XnumberX MB. Now we have increased it to XnumberX XnumberX to M be so for PHP. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Dixie', 'Uuids', 'Csv', 'Names', 'Json', 'Name', 'Ffmpeg', 'Postgres'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 42, 43, 48, 42, 2, 3, 104]\n",
      "\n",
      "\n",
      "\n",
      " I am giving those things in the morning for both Mac and windows and beat the new build that error that resume toast message is fixed as well. \n",
      "\n",
      " Entities picked:  ['Windows', 'Mac', 'Chrome', 'App Center', 'Mac Os', 'Xcode', 'Github', 'Desktop', 'Pr', 'App'] \n",
      "\n",
      " community mapped:  [3, 3, 3, 3, 3, 3, 6, 3, 6, 5]\n",
      "\n",
      "\n",
      "\n",
      " Let is talk about this display the past resume same playback. So maybe just go into a little more detail on where we are on the web and iOS. \n",
      "\n",
      " Entities picked:  ['Ios', 'Web', 'Gypsy', 'Internet', 'App', 'Janus', 'Youtube', 'AI', 'Slack', 'Google'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 5, 5, 5, 174, 48, 8, 5]\n",
      "\n",
      "\n",
      "\n",
      " Okay, so I have deployed and the like basically pause embolism through that circuit and it is working like quite well there was only like in first there was it was the makah waited almost a second after the lake cause started. So for the attic is adding a like reducing the first marker offset by one second and adding a XnumberX second setting and so will verify that whether it is like seamless or not in past. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Graham', 'Dixie', 'Pim', 'Uuids', 'Internet', 'Names', 'Box', 'Sdk'] \n",
      "\n",
      " community mapped:  [5, 5, 25, 48, 25, 42, 5, 48, 25, 48]\n",
      "\n",
      "\n",
      "\n",
      " Okay, but all the I guess the only changes remaining to be tested in staging for the Rebels on the platform side is it that is also deployed the remind of the past markers using thing pause and resume your whatever partial said like, er. \n",
      "\n",
      " Entities picked:  ['Dixie', 'AI', 'Names', 'Janus', 'Gypsy', 'Chrome', 'Pim', 'Workspace', 'App Center', 'Ios'] \n",
      "\n",
      " community mapped:  [48, 48, 48, 5, 5, 3, 25, 48, 3, 5]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, so on Magnolia we had a discussion depend on the navigation still had some bugs and staging like the internet issues. So because of his job was was getting stuck at the first page. So now I am so I am already working on the father is impart. So I have integrated with ether just comment functions which part had written. So now the The markers are having the modified adjusted offset. So that time is showing properly the part left is when we reached a pause marker time. Like I have to skip and move it to the Playtime and then modify the timer at the top for this thing. Otherwise, the Marcus are coming properly and after that I have to take up the websocket integration so that it will be faster. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Dixie', 'Ios', 'Web', 'Internet', 'App', 'Api', 'Workspace', 'Websocket'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 5, 5, 5, 5, 2, 48, 38]\n",
      "\n",
      "\n",
      "\n",
      " Actually now when you pause since is that it has like is using websockets. So you will have to actually it will work for you because you are firing the pods called directly, but there would still be a delay when you pause from IOS. \n",
      "\n",
      " Entities picked:  ['Websocket', 'Websockets', 'Http', 'Web', 'Janus', 'Internet', 'Ios', 'Api', 'Microservice', 'App'] \n",
      "\n",
      " community mapped:  [38, 38, 38, 5, 5, 5, 5, 2, 38, 5]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, I was actually able to start on Witnesses who I need another two days minimum. I will try to computer the Monday or Tuesday with the websocket part. \n",
      "\n",
      " Entities picked:  ['AI', 'Internet', 'Janus', 'Dixie', 'Engineering Channel', 'Ios', 'Gypsy', 'Graham', 'Workspace', 'Gps'] \n",
      "\n",
      " community mapped:  [48, 5, 5, 48, 25, 5, 5, 25, 48, 76]\n",
      "\n",
      "\n",
      "\n",
      " So what we can do is see with the reason I am gonna mask in all these cases. I do not think we have like block users and I was yet, but if on a trip on the web if you feel like on Monday, we can have everything ready. I can also summon to plan starting the campaign on Monday. \n",
      "\n",
      " Entities picked:  ['Janus', 'AI', 'Dixie', 'Gypsy', 'Workspace', 'Names', 'Internet', 'Ios', 'Web', 'Api'] \n",
      "\n",
      " community mapped:  [5, 48, 48, 5, 48, 48, 5, 5, 5, 2]\n",
      "\n",
      "\n",
      "\n",
      " But because the websocket integration make Four Paws assume a pier it is a bit different from the logic. So if we can take that part of then I think we can be done by Monday. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Web', 'Ether Engineering', 'Api', 'Dixie', 'Sdk', 'Jesus', 'Json', 'Slack'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 124, 2, 48, 48, 42, 42, 8]\n",
      "\n",
      "\n",
      "\n",
      " We just focus on the experience they share right what the end user experience right and try to get it as seamless as possible. So we do not need to be high to release IOS app is what I am saying, like make sure that it is it works. So once this is done, then we can start to look at a couple of other things which is showing just handing the mole. \n",
      "\n",
      " Entities picked:  ['Web', 'Gypsy', 'Ios', 'Dixie', 'App', 'App Store', 'Zoom', 'Janus', 'Sdk', 'AI'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 48, 5, 5, 5, 5, 48, 48]\n",
      "\n",
      "\n",
      "\n",
      " I was working on the model showing up like it is almost done like I am taken the full from the partial has Mansion like it will be integrated with the web socket. \n",
      "\n",
      " Entities picked:  ['Web', 'Gypsy', 'Dixie', 'Gpt', 'Ios', 'Janus', 'Mla', 'Android', 'Zoom', 'App Store'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 25, 5, 5, 25, 3, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " Okay, okay, and for that are you know getting the proper even when you do start like when recording client stars and things like that? \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Web', 'Janus', 'Dixie', 'Api', 'Jesus', 'Sdk', 'Ios', 'AI', 'Internet'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 48, 2, 42, 48, 5, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, we are getting the events early like we discussed when recorder joins will fire. I have edit the code for it, but request not sending that so I am debugging that it should be a small fix so they will start getting that event. \n",
      "\n",
      " Entities picked:  ['Api', 'Janus', 'Websocket', 'Dixie', 'Gypsy', 'Apa', 'Id', 'Lambda', 'Websockets', 'Http'] \n",
      "\n",
      " community mapped:  [2, 5, 38, 48, 5, 2, 2, 2, 38, 38]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, so there is another able to pinch back end sense called recorder recording started it is that recording once it uploads a segment to the SXnumberX. Then it file exchange transitions from not static to started so that time they send the recording object. \n",
      "\n",
      " Entities picked:  ['Janus', 'Websocket', 'Websockets', 'Dixie', 'Gypsy', 'Postgres', 'Workspace', 'Pim', 'Web', 'S3'] \n",
      "\n",
      " community mapped:  [5, 38, 38, 48, 5, 104, 48, 25, 5, 2]\n",
      "\n",
      "\n",
      "\n",
      " So let us just say focus on these things related to you know related to the cost, right? So we will try to get the pasta Zoom corruptly back on the web Double D. And then Molly next week. Hopefully we can get all the modal dialogs and also the iOS playback. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Dixie', 'Janus', 'Ios', 'AI', 'Web', 'Internet', 'Graham', 'Hr', 'App'] \n",
      "\n",
      " community mapped:  [5, 48, 5, 5, 48, 5, 5, 25, 124, 5]\n",
      "\n",
      "\n",
      "\n",
      " Anything that will have to do for this cast is you need to push the a we capture changes that we have done recently production. \n",
      "\n",
      " Entities picked:  ['Names', 'Github', 'Feature', 'Gypsy', 'App Store', 'Pink', 'Janus', 'App Center', 'Dixie', 'Pr'] \n",
      "\n",
      " community mapped:  [48, 6, 6, 5, 5, 42, 5, 3, 48, 6]\n",
      "\n",
      "\n",
      "\n",
      " It is already there in staying for a couple of days we can and I just realized on smaller today is there with that implementation and fixing the ones that are than I we can push it to production. \n",
      "\n",
      " Entities picked:  ['Beta', 'App Center', 'Dixie', 'App Store', 'Github', 'Xcode', 'Gypsy', 'Feature', 'Box', 'Ios'] \n",
      "\n",
      " community mapped:  [3, 3, 48, 5, 6, 3, 5, 6, 25, 5]\n",
      "\n",
      "\n",
      "\n",
      " So one question that like some previously we used to use jitsi. Leave the car rejoin and all that right like all of that is not happening. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Uuids', 'Internet', 'Graph', 'Uuid', 'D', 'Name', 'Dixie', 'Ios'] \n",
      "\n",
      " community mapped:  [5, 5, 42, 5, 94, 42, 94, 2, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " Now the initial that immediate see the recorded is enjoying like it is continuously join, but we like put overlay of the video and we mute audio in recorder if through a we capture. \n",
      "\n",
      " Entities picked:  ['Janus', 'House', 'Gypsy', 'Graham', 'Screen', 'Youtube', 'Internet', 'Dixie', 'Pim', 'Ios'] \n",
      "\n",
      " community mapped:  [5, 3, 5, 25, 174, 174, 5, 48, 25, 5]\n",
      "\n",
      "\n",
      "\n",
      " Hundred hundred add to my question was when you remember what he had to do handle the case of hanging a call when you are past or something like that. \n",
      "\n",
      " Entities picked:  ['Uuid', 'Janus', 'Json', 'Id', 'Name', 'Pink', 'Uuids', 'Etc', 'Context', 'Api'] \n",
      "\n",
      " community mapped:  [42, 5, 42, 2, 2, 42, 42, 42, 41, 2]\n",
      "\n",
      "\n",
      "\n",
      " Diesel column so that the solvent should that so my question is did we hand we have to handle that case properly testing now. \n",
      "\n",
      " Entities picked:  ['Janus', 'Graham', 'AI', 'Pim', 'Sdk', 'Dixie', 'Internet', 'Ether Engineering', 'Names', 'Gypsy'] \n",
      "\n",
      " community mapped:  [5, 25, 48, 25, 48, 48, 5, 124, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " So it will still receive the participant left a flash participant leaves even in Cosmo it will end the call. \n",
      "\n",
      " Entities picked:  ['Event', 'Janus', 'Id', 'Uuid', 'Uuids', 'Name', 'Etc', 'Context', 'Json', 'D'] \n",
      "\n",
      " community mapped:  [41, 5, 2, 42, 42, 2, 42, 41, 42, 94]\n",
      "\n",
      "\n",
      "\n",
      " This part is done like handle exit when recorder has not yet joined like for is it is on the prod and I think Lonnie has also done. \n",
      "\n",
      " Entities picked:  ['Janus', 'Dixie', 'Gypsy', 'Gps', 'Workspace', 'Names', 'Ios', 'App', 'AI', 'Api'] \n",
      "\n",
      " community mapped:  [5, 48, 5, 76, 48, 48, 5, 5, 48, 2]\n",
      "\n",
      "\n",
      "\n",
      " Yes showing that ethercast has encountered and problem create start another cars and been on okay, we are exiting the call if the corded window in in one minute. \n",
      "\n",
      " Entities picked:  ['Janus', 'Internet', 'Pim', 'AI', 'Gps', 'Screen', 'Dixie', 'Graham', 'Workspace', 'Names'] \n",
      "\n",
      " community mapped:  [5, 5, 25, 48, 76, 174, 48, 25, 48, 48]\n",
      "\n",
      "\n",
      "\n",
      " Speaking of campaigns you mention you want to do the past campaign Monday. One thing we need to verify is because with the new campaign we will have to ask the people to reinstall the slack app if they are not done. So from the beginning of the situation, so we need to just check if the subscription of the user changes their easily upon Slack \n",
      "\n",
      " Entities picked:  ['Slack', 'Names', 'Apa', 'Name', 'Janus', 'Workspace', 'Google', 'Admin', 'Id', 'Internet'] \n",
      "\n",
      " community mapped:  [8, 48, 2, 2, 5, 48, 5, 8, 2, 5]\n",
      "\n",
      "\n",
      "\n",
      " From WordPress site because I have seen before we received transitions from pay plan to free Plan before when someone beans salsa to Satori go verify that is not happening again, okay. \n",
      "\n",
      " Entities picked:  ['Janus', 'Internet', 'Wordpress', 'Slack', 'Google', 'Names', 'AI', 'Pim', 'Dixie', 'Graham'] \n",
      "\n",
      " community mapped:  [5, 5, 3, 8, 5, 48, 48, 25, 48, 25]\n",
      "\n",
      "\n",
      "\n",
      " Looking into the light in the Lambda for marking the zoom related by resources as deleted that Lambda is been your average appear and energetic thing. I have tested it in staging to have taken a dump refers to a into database and stood in my local is working as expected that that we can merge. And one more thing is when most of the resources would be In the entry which are already deleted. So there is no check most of the SQL queries with deleted is for projects that I have added yesterday. So lies and deception has raised the pr for storing all the he said you have to share he has just having multiple scenarios editing the event and consoling my mother is things seem to be working fine. I will be and me and I think articles are for being that Pierre will be can try to merge is today and currently I am working on, you know, deleting the history resources for the deleted recordings because we are just marking them as deleted for more but we will have to eventually delete three objects. \n",
      "\n",
      " Entities picked:  ['Workspace', 'Janus', 'Names', 'Dixie', 'Gypsy', 'Github', 'S3', 'Gps', 'Postgres', 'Api'] \n",
      "\n",
      " community mapped:  [48, 5, 48, 48, 5, 6, 2, 76, 104, 2]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, I think there are few of us left on the from the Russian side. We need to know check for you know recurring rules when the start incomes and that that is that is still in imputation it. \n",
      "\n",
      " Entities picked:  ['Janus', 'Internet', 'Dixie', 'AI', 'Gypsy', 'Ios', 'Names', 'Google', 'Graham', 'Russian'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 48, 5, 5, 48, 5, 25, 91]\n",
      "\n",
      "\n",
      "\n",
      " And very understandable name is said we had a discussion how to creating an object of things that we have a recording deletion recording objects at can look into no generic meeting scheduling. There is a calendar API from Google we can check what are the events that I should be with me this minute. So by that we can we can start, you know, the disease has a hangout color can still go ahead and join a hangout called even though the \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Names', 'Api', 'Id', 'Name', 'Dixie', 'Workspace', 'Apa', 'AI'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 2, 2, 2, 48, 48, 2, 48]\n",
      "\n",
      "\n",
      "\n",
      " I mentioned that we are still trying to grow their approach to get started. What what I am doing is to create a keyword graph that with without the key verse ever recorded and then we do not show the keyword or any Associated subject if you do not see the key phrase at least in one or two meetings. It is so we will get started started that way and then see how it works out. So so what what it means is that every every mind could be associated with the Carolyn pizzas mind which is like for each animal have its own keep this graph accordingly. It will show the key phrases only if it is present in a few say it has occurred in at least one or two calls. So that way we can at least say sure that we do not see a bad words that Usually occur by very bad transcription and The Coincidence is in there that it is consistently pronounced that way. What what I am thinking too is to will put this in summaries first and then next turn the to the chapters little later. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Id', 'Uuid', 'Name', 'Names', 'Dixie', 'Graph', 'Workspace', 'Web'] \n",
      "\n",
      " community mapped:  [5, 5, 2, 42, 2, 48, 48, 94, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " Clarifying questions on those site you want to see what happens when you have the first columns in the channel. \n",
      "\n",
      " Entities picked:  ['Apis', 'Names', 'Name', 'Postgres', 'Id', 'Api', 'Aps', 'Sql', 'Db', 'Workspace'] \n",
      "\n",
      " community mapped:  [2, 48, 2, 104, 2, 2, 2, 104, 104, 48]\n",
      "\n",
      "\n",
      "\n",
      " Uhhuh for the first call we go with the main domain with as I said every mind would be associated with the key phrase graph the default Keepsake. That is the domain key phrase graph that means for now. \n",
      "\n",
      " Entities picked:  ['Graph', 'Entity', 'Context Id', 'Id', 'Name', 'Domain', 'Uuid', 'Channel', 'Uid', 'Etc'] \n",
      "\n",
      " community mapped:  [94, 94, 41, 2, 2, 27, 42, 41, 2, 42]\n",
      "\n",
      "\n",
      "\n",
      " So for the First Column we will do a mapping of the key phrase with whatever if this is they have been able to exact from the Mind from the model. \n",
      "\n",
      " Entities picked:  ['Entity', 'Mind', 'Context Id', 'Graph', 'Ether Engine', 'Action', 'Uuid', 'Entity Graph', 'Action Item', 'Context'] \n",
      "\n",
      " community mapped:  [94, 94, 41, 94, 94, 41, 42, 94, 41, 41]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, that will be a little unfair because we will not know we might not know lot of key phrases from the first call. So we will do that first and then I am going to figure out what we can do to reduce that impact of aggressive filtration. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Pim', 'Janus', 'Names', 'AI', 'Ebs', 'Mla', 'Workspace', 'Turkish', 'Sql'] \n",
      "\n",
      " community mapped:  [48, 25, 5, 48, 48, 76, 25, 48, 76, 104]\n",
      "\n",
      "\n",
      "\n",
      " Sorry, one other question is kind of related to that one. Like let us say you are starting to engineering starting a new project. So you are starting to talk about a bunch of new things in that case. \n",
      "\n",
      " Entities picked:  ['AI', 'Ether Engineering', 'Janus', 'Gypsy', 'Dixie', 'Jesus', 'Sdk', 'Internet', 'Web', 'Ontology'] \n",
      "\n",
      " community mapped:  [48, 124, 5, 5, 48, 42, 48, 5, 5, 124]\n",
      "\n",
      "\n",
      "\n",
      " I delete it will if there are new usually keep key phrases would be written and they are not entities. So if we if you do not find it we might have to filter out with the with the current approach because we do not know that if we if we if we do not know what the difference between like the Gypsy and Gypsy that they are the same unless we have a context about the them \n",
      "\n",
      " Entities picked:  ['Uuid', 'Id', 'Uuids', 'Entity', 'Name', 'Context', 'Uid', 'Context Id', 'Json', 'Etc'] \n",
      "\n",
      " community mapped:  [42, 2, 42, 94, 2, 41, 2, 41, 42, 42]\n",
      "\n",
      "\n",
      "\n",
      " I mean, this is just a discussion, especially with example, if you are summarizing YouTube and things like there may be a case where let us say, for example, you do not want to help each other. And so the first few times are assigned to summarize YouTube discussions. There may be something so we just have to see how well it works only in those cases. \n",
      "\n",
      " Entities picked:  ['Internet', 'Google', 'Janus', 'Gypsy', 'Web', 'Dixie', 'Apis', 'Ios', 'Api', 'Slack'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 5, 5, 48, 2, 5, 2, 8]\n",
      "\n",
      "\n",
      "\n",
      " Finally what rot are triangles inside your also transitioning to custom mind when adaptive mind speech three would update. Okay, so we should be able to find a fix for this aggressive behavior. I mean the twins will not blindly go by it self interest in even though we start and then observe the behavior will have to find an alternative for this approach both. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Mind', 'Ether Engineering', 'Dixie', 'Context', 'Domain Mind', 'Graham', 'Uuid', 'Web'] \n",
      "\n",
      " community mapped:  [5, 5, 94, 124, 48, 41, 27, 25, 42, 5]\n",
      "\n",
      "\n",
      "\n",
      " But because as you said it is not it is not really feasible to just you know, ignore all the key phrases if we if they do not occur in the in the dictionary. \n",
      "\n",
      " Entities picked:  ['Uuid', 'Uuids', 'Name', 'Json', 'Etc', 'Aps', 'Id', 'Uid', 'Janus', 'Names'] \n",
      "\n",
      " community mapped:  [42, 42, 2, 42, 42, 2, 2, 2, 5, 48]\n",
      "\n",
      "\n",
      "\n",
      " Yes, a so update from my side is like I like first time in like Esther standardizing the graph population and adding new connections and everything. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Janus', 'Gypsy', 'Names', 'Web', 'AI', 'Postgres', 'Graph', 'Pim', 'Sdk'] \n",
      "\n",
      " community mapped:  [48, 5, 5, 48, 5, 48, 104, 94, 25, 48]\n",
      "\n",
      "\n",
      "\n",
      " So then these new connections are basically like trying to interpret like implicit connections between users. So this info so this information comes from the grouping highlights, right? So when we give summaries which we show that when cut tree and chew gum spoke about something so I so I go and create connections between wankers free and Suba saying that they got group together then And then over the meeting or I am trying to increment this frequency count. I am also trying to infer if I named entity is ready to key phrase just by looking at the substring and all the so if there is like if you look at chapter there is this key phrase called SQL queries and SQL is it self identity so they both get connected saying that they are this key phrases related to introduce. \n",
      "\n",
      " Entities picked:  ['Id', 'Graph', 'Entity', 'Uuid', 'Sql', 'Mind', 'Context Id', 'Context', 'Name', 'Json'] \n",
      "\n",
      " community mapped:  [2, 94, 94, 42, 104, 94, 41, 41, 2, 42]\n",
      "\n",
      "\n",
      "\n",
      " So I am adding these connections and just trying to check on next teaching data if I can get some insights which can help in like filtering or recommendation and all this side. I see some good positive things like if there is like a new called which do not have any references, but if I go and check for references in the entire workspace, then we still find some Related keywords related entities so that way it is not very conclusive yet. But the like it kind of shows that if if you keep reinforcing then they could be a chance that we will have some solution to like cold start problems or better connections and building sites from video. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Janus', 'Gypsy', 'Sdk', 'Web', 'Ios', 'Internet', 'Workspace', 'Pim', 'Web Sdk'] \n",
      "\n",
      " community mapped:  [48, 5, 5, 48, 5, 5, 5, 48, 25, 48]\n",
      "\n",
      "\n",
      "\n",
      " So a bit from my side the main thing is like the enrichment of the mines so initially, right right now we have some kind of a ranking in place for all those default Minds which we currently have and we are moving away from that and looking into continuous enrichment of all these mines for each and every channel. So initially, we have only default mine and then one single ether mind. So right now we kind of have to have a specific set of files for every channel. \n",
      "\n",
      " Entities picked:  ['Channel Minds', 'Janus', 'Domain', 'Channel', 'Workspace', 'Names', 'Ether Engineering', 'Hr', 'Gypsy', 'Mind'] \n",
      "\n",
      " community mapped:  [41, 5, 27, 41, 48, 48, 124, 124, 5, 94]\n",
      "\n",
      "\n",
      "\n",
      " So right now at least in deployment currently, we have default minds and then after that we have customized Man which we manually have to think alike either engineering mind or if you want to have something separately camel in ether a moment. So right now what we are doing is like we are trying to have some specific set of files for each and every month for each and every channel basically. \n",
      "\n",
      " Entities picked:  ['Ether Engineering', 'Hr', 'Names', 'Domain', 'Workspace', 'Mind', 'Janus', 'S3', 'Channel Minds', 'Channel'] \n",
      "\n",
      " community mapped:  [124, 124, 48, 27, 48, 94, 5, 2, 41, 41]\n",
      "\n",
      "\n",
      "\n",
      " Bracket from from the domain wins every time when you channel is created size of currently. It is just a copy when you when you associate it is a mind to a channel but what we have what we do moving forward is along with the domain mind. Each side would have its own Associated additional pipe that carries recency and other other aspect of it. \n",
      "\n",
      " Entities picked:  ['Channel', 'Channel Minds', 'Domain', 'Context Id', 'Id', 'Domain Mind', 'Names', 'Context', 'Name', 'Entity'] \n",
      "\n",
      " community mapped:  [41, 41, 27, 41, 2, 27, 48, 41, 2, 94]\n",
      "\n",
      "\n",
      "\n",
      " So I was just wondering like see for example in either engineering today. Like for example, today we have in our call this call. Probably we are going to get about XnumberX XnumberX communities, right? Yeah the end of how do we get a feel for what our customers are going to experience? So should we switch either in this seat and lab should we switch it to just software engineering mind so that we get a feel for water real customers are going to audience dumbs of that is a summary communities. \n",
      "\n",
      " Entities picked:  ['Ether Engineering', 'Hr', 'Janus', 'Names', 'AI', 'Gypsy', 'Ontology', 'Dixie', 'Internet', 'Google'] \n",
      "\n",
      " community mapped:  [124, 124, 5, 48, 48, 5, 124, 48, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " At least for testing what we do you like we created a separate Channel with started with default SE mind and SC my ether graph and everything. And then after we ran an initial test just to get its what the user would feel when they just start the first call and we just progress with two more calls and in which all its data and the again ran the first call try to see how the enrichment process works like how the being effectively works. So that still works mainly because of the cause which we ran which were mainly podcast with and US accent. So for the problem with the Aether calls is the transcripts are in perfectly good because of basically how we speak and of the converse so that is something which we have yet to come. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Web', 'Dixie', 'Ios', 'Graham', 'Internet', 'Sdk', 'Pim', 'Workspace'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 48, 5, 25, 5, 48, 25, 48]\n",
      "\n",
      "\n",
      "\n",
      " I think we can reach our now and then see what the behavior is to see what what comes of SE mind is fine. Even for the ether engineering will be the behavior and then then see if we need to see can do minor modification just to make it Tyler made for this but otherwise from the flow perspective. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Dixie', 'Ether Engineering', 'Names', 'Web', 'Graham', 'Ios', 'AI', 'Domain Mind'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 124, 48, 5, 25, 5, 48, 27]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, so the that that is still going on that work wherein we can simulate the existing meeting and just produce the summary that part we have discussed and we have finalized the implementation approach. It user will be able to see the meetings which are actually Lee started and ended but recorder did not join. So those meetings when they click on it will be saying that recording is not available. \n",
      "\n",
      " Entities picked:  ['Workspace', 'Janus', 'Dixie', 'Gypsy', 'Internet', 'AI', 'Names', 'Apa', 'Api', 'Ios'] \n",
      "\n",
      " community mapped:  [48, 5, 48, 5, 5, 48, 48, 2, 2, 5]\n",
      "\n",
      "\n",
      "\n",
      " I am just fixing it like so the momenttomoment to join the meeting you just exited a collar you close a tab, right? So those things are also will show up in the recent meetings. \n",
      "\n",
      " Entities picked:  ['Janus', 'AI', 'Dixie', 'Gypsy', 'Workspace', 'Names', 'Screen', 'Ios', 'Internet', 'Web'] \n",
      "\n",
      " community mapped:  [5, 48, 48, 5, 48, 48, 174, 5, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " One is showing that basically a we used to do a parallel careers. Meaning we used to have a lot of causal connections like when users crawls down, right? We we can go up to let thousand meetings at a time at a time. So what the side effect of that is when you keep scrolling down there might be a delay in getting the meetings out basically in the recent meetings \n",
      "\n",
      " Entities picked:  ['Janus', 'Internet', 'Uuid', 'Id', 'Gypsy', 'Google', 'Name', 'Postgres', 'Names', 'Web'] \n",
      "\n",
      " community mapped:  [5, 5, 42, 2, 5, 5, 2, 104, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " So I guess it will work now, but it is a currently slow. Once we fix that computer moving the cursor approach it will all be solved. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Web', 'App', 'Ios', 'Dixie', 'Dom', 'Uuids', 'Sdk', 'Ffmpeg'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 5, 5, 48, 3, 42, 48, 3]\n",
      "\n",
      "\n",
      "\n",
      " So Anisha had faced one more issue related to internet connection. So like it was reverted on witness day because like Misha was having difficulty in continuing because she faced internet connection issue. \n",
      "\n",
      " Entities picked:  ['Internet', 'Janus', 'Ffmpeg', 'Screen', 'Gypsy', 'Ios', 'App', 'Webview', 'Web', 'Websocket'] \n",
      "\n",
      " community mapped:  [5, 5, 3, 174, 5, 5, 5, 3, 5, 38]\n",
      "\n",
      "\n",
      "\n",
      " Give the same as the and sing that is in parallel. Like Siwon only after we push all the cost related changes. We can then more than push sometimes like to second half of next week. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Dixie', 'Janus', 'Names', 'Workspace', 'Feature', 'Github', 'Sdk', 'Web', 'App Store'] \n",
      "\n",
      " community mapped:  [5, 48, 5, 48, 48, 6, 6, 48, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " The most of the modal dialog is small we can do because we have like a bunch of chemicals, right? So even if it I understand the conflict the the commission issue, but what is like given so you want me to assist the number but something simple some like can we what are what will happen? For example, if he appointed date like today is college like XnumberX Jan XnumberX XnumberX XnumberX years something so I know II just did XnumberX. I know that I need to also put the same name and the same day. I can put the time also, that is okay then it is like a number less. My point is why do not you just take a chance and just from the date? Hope you do not know that you  repeating same data. You can collect multiple meetings same at the same time is when we it does not include the time. \n",
      "\n",
      " Entities picked:  ['Uuid', 'Name', 'Uuids', 'Janus', 'Id', 'Json', 'Gypsy', 'Etc', 'D', 'Get'] \n",
      "\n",
      " community mapped:  [42, 2, 42, 5, 2, 42, 5, 42, 94, 42]\n",
      "\n",
      "\n",
      "\n",
      " Got if you just say XnumberXth June this thing but you can do meeting the same name on the same day. We are fixing and then we need to switch to room ID and jitsi using a different key. See if you are saying that I can never call to meeting names the same all the same way because of a new client sent in you will have to make a difference. \n",
      "\n",
      " Entities picked:  ['Name', 'Id', 'Uuid', 'Uid', 'Uuids', 'Json', 'Janus', 'Get', 'Aps', 'Meeting Id'] \n",
      "\n",
      " community mapped:  [2, 2, 42, 2, 42, 42, 5, 42, 2, 2]\n",
      "\n",
      "\n",
      "\n",
      " Reverted to the four digits random four digits like the chance of those random is very less. So and they were like that four digits for looking much better than this. \n",
      "\n",
      " Entities picked:  ['Uuids', 'D', 'Uuid', 'Json', 'Janus', 'Get', 'Pink', 'Gypsy', 'America', 'Graph'] \n",
      "\n",
      " community mapped:  [42, 94, 42, 42, 5, 42, 42, 5, 91, 94]\n",
      "\n",
      "\n",
      "\n",
      " My point is I am okay with pollution the likelihood of it happening is very low it will be even higher because we think that we enter in the meeting names and the set of I was not working before because we used to create uuid is to join the car as a room ID and the source of the rating and they would use the uuid this unique nonrepeatable. \n",
      "\n",
      " Entities picked:  ['Uuid', 'Uuids', 'Janus', 'Name', 'Id', 'Json', 'Postgres', 'Names', 'D', 'Workspace'] \n",
      "\n",
      " community mapped:  [42, 42, 5, 2, 2, 42, 104, 48, 94, 48]\n",
      "\n",
      "\n",
      "\n",
      " If you use two digits as I can use date and time if it is okay that is missing is better than something right? Okay, let us just do that for now so that in there was it? \n",
      "\n",
      " Entities picked:  ['Dixie', 'Janus', 'Uuids', 'Uuid', 'Name', 'Gypsy', 'Names', 'Json', 'Uid', 'AI'] \n",
      "\n",
      " community mapped:  [48, 5, 42, 42, 2, 5, 48, 42, 2, 48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " So fire breath, like I need to verify the pause ratio of set timing like yesterday after discussion with Makonnen Karthik. We decided that we will not send the pause and resume markers directly from Lifeline instead. Well send a websocket event to when someone calls has a Brazil is will send a websocket event and recorder will listen to it and once recorded changes the state from possibility of more they seem to like like was like basically position so it will create the marker with so I am waiting on for metal to add like for us to add egg. \n",
      "\n",
      " Entities picked:  ['Janus', 'Websocket', 'Dixie', 'Workspace', 'Gypsy', 'Http', 'Api', 'Websockets', 'Internet', 'Pim'] \n",
      "\n",
      " community mapped:  [5, 38, 48, 48, 5, 38, 2, 38, 5, 25]\n",
      "\n",
      "\n",
      "\n",
      " I think this shirt for so once it is done we tested out like if there is any like app like earlier, but it should be done like there should not be any other things required after that. \n",
      "\n",
      " Entities picked:  ['App Store', 'App Center', 'Beta', 'Xcode', 'App', 'Zoom', 'Android', 'Native', 'Github', 'Gypsy'] \n",
      "\n",
      " community mapped:  [5, 3, 3, 3, 5, 5, 3, 3, 6, 5]\n",
      "\n",
      "\n",
      "\n",
      " And I have merged bunnies Branch for graft and publish effect are a few changes required. So I am working on that pal, but that is done. \n",
      "\n",
      " Entities picked:  ['Github', 'Dixie', 'Workspace', 'Pims', 'Gypsy', 'App Store', 'Feature', 'Box', 'Ios', 'Beta'] \n",
      "\n",
      " community mapped:  [6, 48, 48, 25, 5, 5, 6, 25, 5, 3]\n",
      "\n",
      "\n",
      "\n",
      " So the one you know, one thing that I noticed when I was using cost is so let us take a cast as over and then I say if I say discard it should go away. The second is that if it is in draft mode or if it is published it should meet. Really show up in that clicks you so that I can click and view just to make sure things are okay. Periodically or do we also do it as soon as call event? For example, let us say I am doing a Castaway Hangar or did not do something. Is it possible to immediately refresh the Netflix you that it shows up rather than I am divine. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Name', 'Api', 'Id', 'Slack', 'App', 'Dixie', 'Web', 'Names'] \n",
      "\n",
      " community mapped:  [5, 5, 2, 2, 2, 8, 5, 48, 5, 48]\n",
      "\n",
      "\n",
      "\n",
      " Your recent calls every one minute we poll or to update the list and for ongoing calls. We show it immediately Whenever there is a joint life called the when the moment it shows on the join. \n",
      "\n",
      " Entities picked:  ['Id', 'Channel', 'Workspace', 'Websocket', 'Janus', 'Name', 'Apa', 'Postgres', 'Db', 'Channel Minds'] \n",
      "\n",
      " community mapped:  [2, 41, 48, 38, 5, 2, 2, 104, 104, 41]\n",
      "\n",
      "\n",
      "\n",
      " Right But be sure you understood the use case I am talking about right which is like the cost is over and I hunger discard whatever draft publish whatever something I finished and the status of the UI should up the dashboard should reflect what happened. And the only way that can happen is if we really refreshed both the channel list join list and if it is less \n",
      "\n",
      " Entities picked:  ['Workspace', 'Names', 'Janus', 'Dixie', 'Api', 'Gypsy', 'AI', 'Slack', 'Id', 'Name'] \n",
      "\n",
      " community mapped:  [48, 48, 5, 48, 2, 5, 48, 8, 2, 2]\n",
      "\n",
      "\n",
      "\n",
      " There is a backlog item that it shows up right away so I can quickly check and see whether it worked. \n",
      "\n",
      " Entities picked:  ['Pink', 'Gypsy', 'Engineering Channel', 'Action Item', 'Github', 'Name', 'Workspace', 'Dixie', 'Id', 'Names'] \n",
      "\n",
      " community mapped:  [42, 5, 25, 41, 6, 2, 48, 48, 2, 48]\n",
      "\n",
      "\n",
      "\n",
      " I guess you really want to finish this we can talk about things that we need to do to setting up past and so on and so forth on the recorder is joining. \n",
      "\n",
      " Entities picked:  ['AI', 'Dixie', 'Ecs', 'Ios', 'Gpt', 'Janus', 'Mla', 'Turkish', 'Workspace', 'Internet'] \n",
      "\n",
      " community mapped:  [48, 48, 76, 5, 25, 5, 25, 76, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " So I SRI and Junior weapon, or maybe I did not do do you have any recent update on that? \n",
      "\n",
      " Entities picked:  ['AI', 'Dixie', 'Mla', 'Ios', 'Internet', 'Graham', 'Janus', 'Sdk', 'Gypsy', 'Pim'] \n",
      "\n",
      " community mapped:  [48, 48, 25, 5, 5, 25, 5, 48, 5, 25]\n",
      "\n",
      "\n",
      "\n",
      " We also for every domain we are able to rank now. So we figured out how to perform communities on all the entries of this Ravine and ensure that there is enough separation between working in the domain. \n",
      "\n",
      " Entities picked:  ['Domain', 'Domain Mind', 'Id', 'Entity', 'Names', 'Channel', 'Ether Engineering', 'Domain Minds', 'Context Id', 'Mind'] \n",
      "\n",
      " community mapped:  [27, 27, 2, 94, 48, 41, 124, 27, 41, 94]\n",
      "\n",
      "\n",
      "\n",
      " Things out the domain so XnumberX is currently running tests and making sure that it is you know that the separation makes sense and segments are actually choosing the out of the main communities. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Gypsy', 'Graham', 'Janus', 'Names', 'Pim', 'Web', 'Domain Mind', 'Workspace', 'Ios'] \n",
      "\n",
      " community mapped:  [48, 5, 25, 5, 48, 25, 5, 27, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " So this is to filter out the out of domain noncontroversial summaries that we are facing last week sighs. But by next week we should have that filter also in production increasing the production and on the cases filtration. We are we are still discussing on how to populate this key phrases in the entry graph and then use that to To use that as a basis for to start with use that as a basis for filtration and like will also start with the with thought we discussed this morning about being able to correct it and then reflecting that back in the in the graph so that so that at least the phrases that come in summary should get corrected if they are Miss misspelled or Miss transcript. \n",
      "\n",
      " Entities picked:  ['Graham', 'Janus', 'Domain', 'Dixie', 'Names', 'Pim', 'Gypsy', 'Ether Engineering', 'AI', 'Hr'] \n",
      "\n",
      " community mapped:  [25, 5, 27, 48, 48, 25, 5, 124, 48, 124]\n",
      "\n",
      "\n",
      "\n",
      " So the way of thinking about it was if you see the chapter marker for this call you have hey guys, and hey Frenchy and all that stuff, right? Yeah, so I wanted to be able to or somebody on the team should be able to quickly go and delete those. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Dixie', 'Sdk', 'Jesus', 'Pink', 'Names', 'AI', 'Github', 'Api'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 48, 42, 42, 48, 48, 6, 2]\n",
      "\n",
      "\n",
      "\n",
      " the recommendation was like I have deployed this small new change with shows like who are the users in this segment and it recommends users other than the segment users. So it gives us like a better idea for even it becomes like a service kind of so like for when I was testing it locally the results seem quite consistent so excited I discuss with cardigan else. Like I informed him to for API helps to make changes to make it as a service only for our block space for now so we can get like a feedback on tropicals. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Janus', 'Gypsy', 'Workspace', 'Names', 'Api', 'Web', 'Ios', 'Internet', 'Graham'] \n",
      "\n",
      " community mapped:  [48, 5, 5, 48, 48, 2, 5, 5, 5, 25]\n",
      "\n",
      "\n",
      "\n",
      " So I have been looking at those at least in the ones that the recommendations are pretty good, especially if you remove your point folks are not part of the call and if we look at confidence of XnumberX a greater or something like that results seem to be pretty good especially forecast. I think it is going to be very useful because if we can actually mark it around the future that will cause because past is only one person. So almost everyone else is not going to be in the call and so suppose I leave a message about whatever right like I point out something related to recommendation engine and if you can automatically show that it will ping you or someone in the a team that is a pretty good example of example news is that we can market around \n",
      "\n",
      " Entities picked:  ['Ether Engineering', 'Ontology', 'Gypsy', 'Janus', 'Pink', 'Hr', 'Jesus', 'Uuid', 'D', 'Google'] \n",
      "\n",
      " community mapped:  [124, 124, 5, 5, 42, 124, 42, 42, 94, 5]\n",
      "\n",
      "\n",
      "\n",
      " So far this engineering seemed called we can see like recommendations are coming in for chapter markers as well. Yeah, and so right now I am like looking into things like how not to recommend stuff. Like if it is if the phrases are bad or if it is unrelated, then we should be a threshold on select by which we can like not recommend any user to that segment. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Names', 'AI', 'Janus', 'Google', 'Pim', 'Mla', 'Graham', 'Workspace', 'Gypsy'] \n",
      "\n",
      " community mapped:  [48, 48, 48, 5, 5, 25, 25, 25, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " So this will like so this is basically like dependent on filtering out bad keywords and see if that gets sold in the even this one will get like a positive. \n",
      "\n",
      " Entities picked:  ['Uuid', 'Uuids', 'D', 'Json', 'Graph', 'Id', 'Postgres', 'Name', 'Janus', 'Etc'] \n",
      "\n",
      " community mapped:  [42, 42, 94, 42, 94, 2, 104, 2, 5, 42]\n",
      "\n",
      "\n",
      "\n",
      " Even we need to build the integral of the main challenge. There is there is a lot of noise because of the bad transcription. So just trying to figure out how to make the entity graph more either granular so that so that we can based on the frequencies. We reduced a nice if we are able to figure that out. The bad cases is if that is if you work that out, I think half of the customer and problem is also interest and then the rest half is about how do we finetune the model specific to the mind? \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Dixie', 'Ether Engineering', 'Web', 'Graph', 'Entity', 'Sdk', 'Graham', 'Internet'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 124, 5, 94, 94, 48, 25, 5]\n",
      "\n",
      "\n",
      "\n",
      " He got it and right now just curious in terms of The Ether engineering mind itself. Is it something that was trained a while ago and it is continuous be the case. \n",
      "\n",
      " Entities picked:  ['Ether Engineering', 'Hr', 'Graham', 'Janus', 'Ontology', 'Mind', 'Gypsy', 'Deep Graham', 'Domain Mind', 'Dixie'] \n",
      "\n",
      " community mapped:  [124, 124, 25, 5, 124, 94, 5, 25, 27, 48]\n",
      "\n",
      "\n",
      "\n",
      " This the the mind gets got recently updated PSI, but the model got rain like the Snapchat was around one month the calls that happen like one month back. \n",
      "\n",
      " Entities picked:  ['Graham', 'Janus', 'Workspace', 'Engineering Channel', 'Internet', 'Gypsy', 'Dixie', 'Names', 'Box', 'Pim'] \n",
      "\n",
      " community mapped:  [25, 5, 48, 25, 5, 5, 48, 48, 25, 25]\n",
      "\n",
      "\n",
      "\n",
      " But the model we need not worry about the model because that can still be a little old but the mind can get mind is what gets updated more frequently. \n",
      "\n",
      " Entities picked:  ['Mind', 'Entity Graph', 'Entity', 'Ether Engine', 'Ether Engineering', 'Context', 'Knowledge Graph', 'Graph', 'Channel Minds', 'Domain Mind'] \n",
      "\n",
      " community mapped:  [94, 94, 94, 94, 124, 41, 94, 94, 41, 27]\n",
      "\n",
      "\n",
      "\n",
      " Now one of the things that we should keep in mind is how do we the backlog we should probably think about how we add that to the backlog that is that happens automatically, but every every channel that ether is installed, right? \n",
      "\n",
      " Entities picked:  ['Workspace', 'Names', 'Dixie', 'Sigmoid', 'Slack', 'Sdk', 'Janus', 'Web Sdk', 'Web', 'Internet'] \n",
      "\n",
      " community mapped:  [48, 48, 48, 2, 8, 48, 5, 48, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " I mean we have to make it not we can get access to slack made of course for all channels. We already do question is that there can be a manual intervention? \n",
      "\n",
      " Entities picked:  ['Slack', 'Congress', 'Sigmoid', 'Admin', 'Api', 'Sdk', 'Aps', 'Apis', 'Github', 'Janus'] \n",
      "\n",
      " community mapped:  [8, 6, 2, 8, 2, 48, 2, 2, 6, 5]\n",
      "\n",
      "\n",
      "\n",
      " It should not be because if you see currently Only The Ether is ether models are manually enter VDP. If you bring that to the same as domain models, which are dependent on into the graphs. \n",
      "\n",
      " Entities picked:  ['Graph', 'Mind', 'Entity', 'Domain', 'Uuid', 'Entity Graph', 'D', 'Etc', 'Ether Engineering', 'Id'] \n",
      "\n",
      " community mapped:  [94, 94, 94, 27, 42, 94, 94, 42, 124, 2]\n",
      "\n",
      "\n",
      "\n",
      " It is actually even currently we just do it manually, but but when we have this entity graph fully then we ideally there should not be any human intervention because it gets updated after every call without any periodic updates. \n",
      "\n",
      " Entities picked:  ['Entity', 'Graph', 'Postgres', 'Janus', 'S3', 'Id', 'Dixie', 'Api', 'Apis', 'Gypsy'] \n",
      "\n",
      " community mapped:  [94, 94, 104, 5, 2, 2, 48, 2, 2, 5]\n",
      "\n",
      "\n",
      "\n",
      " We just get to populate it after every call, but but eventually what happens is I mean the we are not involved even populations if you get populated after every week, every transcription actually got it. \n",
      "\n",
      " Entities picked:  ['Janus', 'Google', 'Gypsy', 'Internet', 'Graham', 'Postgres', 'Web', 'Dixie', 'Youtube', 'Uuid'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 5, 25, 104, 5, 48, 174, 42]\n",
      "\n",
      "\n",
      "\n",
      " Based on the history based on the final design is to be discussed yesterday. So I was writing few schema changes and service level code changes along with that this today. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Gypsy', 'Mla', 'Engineering Channel', 'Names', 'Web', 'Apis', 'AI', 'Ontology', 'Pims'] \n",
      "\n",
      " community mapped:  [48, 5, 25, 25, 48, 5, 2, 48, 124, 25]\n",
      "\n",
      "\n",
      "\n",
      " I think what would what is useful for us would be like a notification which says the meeting is going to start in the next five minutes or next two minutes. \n",
      "\n",
      " Entities picked:  ['Id', 'Workspace', 'Apa', 'Api', 'Slack', 'Janus', 'Name', 'Names', 'Sigmoid', 'Epa'] \n",
      "\n",
      " community mapped:  [2, 48, 2, 2, 8, 5, 2, 48, 2, 8]\n",
      "\n",
      "\n",
      "\n",
      " I just created a developer account there and try to register a Web book. If that is the case then I mean our implementation would be very straightforward like you just listen to those notifications and join the call. \n",
      "\n",
      " Entities picked:  ['Slack', 'Api', 'Apis', 'App', 'Apa', 'Admin', 'Web', 'Name', 'Id', 'Sigmoid'] \n",
      "\n",
      " community mapped:  [8, 2, 2, 5, 2, 8, 5, 2, 2, 2]\n",
      "\n",
      "\n",
      "\n",
      " So the question I had was on whether we should add a question marker should have a question. Yeah, if I if I installs slack, so I need to have slack installed for me to install Zoom, right? \n",
      "\n",
      " Entities picked:  ['Slack', 'Zoom', 'App', 'Sdk', 'Internet', 'Screen', 'Web', 'Sigmoid', 'Plan Api', 'Api'] \n",
      "\n",
      " community mapped:  [8, 5, 5, 48, 5, 174, 5, 2, 8, 2]\n",
      "\n",
      "\n",
      "\n",
      " We do not uninstall zoom in that case because it should be like a user driven even some like if you want to actually uninstall Zoom. Yeah, what do map but what we do is we make the customer as an act to so none of his meetings will you know get registered in our \n",
      "\n",
      " Entities picked:  ['Slack', 'Zoom', 'Apple', 'App', 'Admin', 'Name', 'Login', 'Admin Page', 'Sigmoid', 'Web'] \n",
      "\n",
      " community mapped:  [8, 5, 3, 5, 8, 2, 8, 8, 2, 5]\n",
      "\n",
      "\n",
      "\n",
      " So we almost like at one point we always check for the customer, you know activity customer is active or not. And he has that plan remaining planners are there or not? \n",
      "\n",
      " Entities picked:  ['Admin Page', 'Epa', 'Admin', 'Mooc', 'Psyche', 'Pink', 'Woocommerce', 'Uuid', 'Ether Engineering', 'Jesus'] \n",
      "\n",
      " community mapped:  [8, 8, 8, 8, 8, 42, 8, 42, 124, 42]\n",
      "\n",
      "\n",
      "\n",
      " Those checks will come in and then we will not be acting on upon but we might still receive the events from the customer. \n",
      "\n",
      " Entities picked:  ['Id', 'Api', 'Epa', 'Janus', 'Slack', 'Apis', 'Apa', 'Workspace', 'Uuid', 'Sigmoid'] \n",
      "\n",
      " community mapped:  [2, 2, 8, 5, 8, 2, 2, 48, 42, 2]\n",
      "\n",
      "\n",
      "\n",
      " Well, I guess you can do not go and similarly if you do not want zoom you also have to go into zoom and then uninstall. \n",
      "\n",
      " Entities picked:  ['Zoom', 'Device', 'App', 'Electron App', 'Apple', 'Windows', 'Iframe', 'Desktop', 'App Store', 'Chrome'] \n",
      "\n",
      " community mapped:  [5, 3, 5, 3, 3, 3, 3, 3, 5, 3]\n",
      "\n",
      "\n",
      "\n",
      " Also, we do not provide any link or admin page to uninsured because they can do it from the from their apps page will resume admin. \n",
      "\n",
      " Entities picked:  ['Admin', 'Point', 'Admins', 'Slack', 'Login', 'Apple', 'Admin Page', 'Auth', 'App', 'Cookie'] \n",
      "\n",
      " community mapped:  [8, 8, 8, 8, 8, 3, 8, 8, 5, 8]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, I think we are going to get we could not just use it as for the scenario because yeah. \n",
      "\n",
      " Entities picked:  ['AI', 'Dixie', 'Janus', 'Pim', 'Turkish', 'Names', 'Mla', 'Web', 'Api', 'Gypsy'] \n",
      "\n",
      " community mapped:  [48, 48, 5, 25, 76, 48, 25, 5, 2, 5]\n",
      "\n",
      "\n",
      "\n",
      " What we can do and she is was the basic cases are done. Maybe we can have like Franklin just go through a bunch of install uninstall type of test cases just to make sure they are seamless. \n",
      "\n",
      " Entities picked:  ['Pink', 'Github', 'Names', 'Gypsy', 'Dixie', 'Sdk', 'Janus', 'Workspace', 'App Store', 'Api'] \n",
      "\n",
      " community mapped:  [42, 6, 48, 5, 48, 48, 5, 48, 5, 2]\n",
      "\n",
      "\n",
      "\n",
      " One thing that I am not sure is also current current our Administration for selecting say be uninstalled the staging to app from all over space. \n",
      "\n",
      " Entities picked:  ['Workspace', 'Ios', 'Janus', 'Names', 'App', 'Slack', 'Sdk', 'Web', 'Internet', 'Gypsy'] \n",
      "\n",
      " community mapped:  [48, 5, 5, 48, 5, 8, 48, 5, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " There is there is a bug in the radiator which is you know, refactoring or statements whenever we save them. I thought they will fix it, but still not fixed the template they use our handy worse find out handlebar renderer for golang. \n",
      "\n",
      " Entities picked:  ['Pink', 'Javascript', 'Github', 'Gypsy', 'Json', 'Janus', 'Pr', 'Get', 'Ffmpeg', 'Sdk'] \n",
      "\n",
      " community mapped:  [42, 3, 6, 5, 42, 5, 6, 42, 3, 48]\n",
      "\n",
      "\n",
      "\n",
      " So like they have updated the pr like we were testing staging a pile like there was some issue with login. So let me decide it will merge the pr like I and then with integrating errors and out like Universal and custom links with router and all the internal out in logic with react router. So And I have done some speed ruminants and like we must appear. I will sit with partial Amnesia today and we will discuss the fear then we will merge and then the Shopkins create a branch out of it and then work on cast changes. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Dixie', 'Janus', 'Feature', 'Beta', 'Github', 'Sdk', 'Web', 'Ios', 'Pr'] \n",
      "\n",
      " community mapped:  [5, 48, 5, 6, 3, 6, 48, 5, 5, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " To test mobile desktop or web because the recorder joints and after that one cast that is the popup that says setting up your cast does not go away and it keeps loading. \n",
      "\n",
      " Entities picked:  ['Chrome', 'App', 'Desktop', 'Ios', 'Web', 'Zoom', 'Android', 'Gypsy', 'Safari', 'Internet'] \n",
      "\n",
      " community mapped:  [3, 5, 3, 5, 5, 5, 3, 5, 3, 5]\n",
      "\n",
      "\n",
      "\n",
      " But right now I am facing like when I do the call and if I want to watch it again, I am getting a chalice error. \n",
      "\n",
      " Entities picked:  ['D', 'Uuids', 'Ffmpeg', 'Json', 'Event', 'Get', 'Janus', 'Async', 'Gypsy', 'Context'] \n",
      "\n",
      " community mapped:  [94, 42, 3, 42, 41, 42, 5, 41, 5, 41]\n",
      "\n",
      "\n",
      "\n",
      " It is even if it is not even if you just apply whatever is in production and staging are not able to playback. \n",
      "\n",
      " Entities picked:  ['Janus', 'Dixie', 'Workspace', 'Sdk', 'Gypsy', 'Github', 'Ios', 'Congress', 'App Center', 'Web'] \n",
      "\n",
      " community mapped:  [5, 48, 48, 48, 5, 6, 5, 6, 3, 5]\n",
      "\n",
      "\n",
      "\n",
      " Well do a deploy and test it against is a big plus time like master was working will deployed once again and check. \n",
      "\n",
      " Entities picked:  ['Feature', 'Beta', 'Xcode', 'Pr', 'Bill', 'Pink', 'App Center', 'Github', 'Sdk', 'Box'] \n",
      "\n",
      " community mapped:  [6, 3, 3, 6, 3, 42, 3, 6, 48, 25]\n",
      "\n",
      "\n",
      "\n",
      " So yeah, let us do that just apply whatever it is in production like the latest code in staging let us master so that we know whether it is back in f a man. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Names', 'Xcode', 'AI', 'Janus', 'Pim', 'Gypsy', 'Workspace', 'Github', 'Sdk'] \n",
      "\n",
      " community mapped:  [48, 48, 3, 48, 5, 25, 5, 48, 6, 48]\n",
      "\n",
      "\n",
      "\n",
      " Oh, no it is because they promote more hits on the website and it was like because of the headset purposely like they were few memory issues and that is why like, uh, we were playing at us. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Internet', 'Screen', 'Google', 'Graham', 'Css', 'Ios', 'Dixie', 'Chrome'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 174, 5, 25, 3, 5, 48, 3]\n",
      "\n",
      "\n",
      "\n",
      " You know, like how many like what would be the typical memory requirement, right? Well well with like all like what is the memory we have allocated right now so long? \n",
      "\n",
      " Entities picked:  ['Context', 'Postgres', 'Janus', 'Mind', 'Lambdas', 'Sql', 'Lambda', 'Uuid', 'Internet', 'Hadoop'] \n",
      "\n",
      " community mapped:  [41, 104, 5, 94, 2, 104, 2, 42, 5, 94]\n",
      "\n",
      "\n",
      "\n",
      " Well, we have changed it to Alexis the easy to container has a XnumberXgp like limit XnumberX GB memory if we have made PHP Summer Olympic XnumberX GB \n",
      "\n",
      " Entities picked:  ['Python', 'Ffmpeg', 'Janus', 'Gypsy', 'Pim', 'Mac', 'Xcode', 'Sdk', 'Chrome', 'Linux'] \n",
      "\n",
      " community mapped:  [3, 3, 5, 5, 25, 3, 3, 48, 3, 239]\n",
      "\n",
      "\n",
      "\n",
      " No father Karthik and comfort like multiple letters so that I checked like what you can do. Otherwise, we answered the first part so that we will know whether it is getting the HTML file or not. We can take a call whether we want to switch from a particular engine. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Janus', 'Web', 'Gypsy', 'Names', 'Sdk', 'Api', 'Ios', 'Workspace', 'Apis'] \n",
      "\n",
      " community mapped:  [48, 5, 5, 5, 48, 48, 2, 5, 48, 2]\n",
      "\n",
      "\n",
      "\n",
      " Okay, I mean let us not do like this like when we said that it there is like how many simultaneous views are we talking about pages? \n",
      "\n",
      " Entities picked:  ['Safari', 'Iframe', 'Html', 'Youtube', 'App', 'D', 'Gypsy', 'Css', 'Get', 'Apple'] \n",
      "\n",
      " community mapped:  [3, 3, 174, 174, 5, 94, 5, 3, 42, 3]\n",
      "\n",
      "\n",
      "\n",
      " Third look if I have tried to calls like after deployment like I got the transcript completely from the calls. Mmm, and now like I will just check whether it is my back but I am short like it. So let us string error from the master Branch as well. Okay, so that is the only known issue it but that perspective. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Janus', 'Gypsy', 'Names', 'Internet', 'AI', 'Ios', 'Api', 'Sdk', 'Web'] \n",
      "\n",
      " community mapped:  [48, 5, 5, 48, 5, 48, 5, 2, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " Here is what I want you to do Franklin just to validate it. I want you to play the same file in production and in staging. Dont wait every large meeting do it for a small meeting. Okay about like one or two minutes of audio playback, OK and then is what you see in production and what you see in staging from a transcript perspective. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Dixie', 'Pink', 'Names', 'Github', 'Jesus', 'Sdk', 'Api', 'Name'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 42, 48, 6, 42, 48, 2, 2]\n",
      "\n",
      "\n",
      "\n",
      " Right, let him do this test just to show that we do not go back and visit transcript once and for all we know there is solid, right? \n",
      "\n",
      " Entities picked:  ['Dixie', 'AI', 'Janus', 'Pim', 'Names', 'Gypsy', 'Internet', 'Web', 'Sdk', 'Api'] \n",
      "\n",
      " community mapped:  [48, 48, 5, 25, 48, 5, 5, 5, 48, 2]\n",
      "\n",
      "\n",
      "\n",
      " The pause and play back cache partition, but this is remaining the ready to cast thing so using the socket and yeah till we get the event recording a sudden showing The View. \n",
      "\n",
      " Entities picked:  ['Janus', 'Websocket', 'Gypsy', 'Dixie', 'Screen', 'Internet', 'Graham', 'Uuids', 'Gps', 'Pim'] \n",
      "\n",
      " community mapped:  [5, 38, 5, 48, 174, 5, 25, 42, 76, 25]\n",
      "\n",
      "\n",
      "\n",
      " Well, so that Franklin kit can test and Report any box and I have some PR comments to it. It is the way and then I can also do this. This will be a small change the refresh the channel list and if you do not hang up, so right away. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Github', 'Name', 'Janus', 'Dixie', 'Pink', 'Api', 'Web', 'Slack', 'Names'] \n",
      "\n",
      " community mapped:  [5, 6, 2, 5, 48, 42, 2, 5, 8, 48]\n",
      "\n",
      "\n",
      "\n",
      " Yo, let is looking fine only the yesterday in back and like staging back and was not working. Okay, but I tested on Windows XnumberX or skipping and okay. They were some Corner Cases left like end of the call, you know if it is caused although the handle it should work for me. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Pink', 'Ios', 'Ffmpeg', 'Sdk', 'Screen', 'Dixie', 'Ios App', 'Internet'] \n",
      "\n",
      " community mapped:  [5, 5, 42, 5, 3, 48, 174, 48, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " I was not able to test the IOS app dancing because of the same staging should that is going on so I tried testing it. So once that is fixed I can say is a culture. \n",
      "\n",
      " Entities picked:  ['Pink', 'Gypsy', 'Pr', 'Bill', 'Feature', 'Ios App', 'Beta', 'Janus', 'Ios', 'Jesus'] \n",
      "\n",
      " community mapped:  [42, 5, 6, 3, 6, 5, 3, 5, 5, 42]\n",
      "\n",
      "\n",
      "\n",
      " Also, I am thanking her tested like the router changes and a performance changes in simulator on once like we are the okay with the cast changes. I will merge those changes to this crunched and then I can raise a pier and regarding the cast pinning view. I am doing that so Because as like Karthik suggested will not rebase and will merge from Ab c Upstream. So I will also update like the latest code from jitsi the first layer effects like a couple of pots and yeah, I will create a PR for that today. Like I will generate the SDK which it seizes the after the changes I will raise up there and then maybe we can test after giving a bit. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Gypsy', 'Janus', 'Names', 'Sdk', 'Github', 'Feature', 'App Store', 'Workspace', 'Web'] \n",
      "\n",
      " community mapped:  [48, 5, 5, 48, 48, 6, 6, 5, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " Okay, what you can do deep is first try to get the pinning changes done. Yeah with the existing build, right so that caused a proceedings and then you can work on other words. \n",
      "\n",
      " Entities picked:  ['Dixie', 'Xcode', 'Github', 'Names', 'Sdk', 'App Center', 'Gypsy', 'Beta', 'AI', 'Janus'] \n",
      "\n",
      " community mapped:  [48, 3, 6, 48, 48, 3, 5, 3, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " So there were a few fixes that that I had made on Tuesday have tested some basic cases as well. So once I am India is still open Tetris few comments. So yeah, once once franklyn starts to test it I can deploy. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Pink', 'Github', 'Dixie', 'Sdk', 'Pr', 'Ios', 'Feature', 'Web'] \n",
      "\n",
      " community mapped:  [5, 5, 42, 6, 48, 48, 6, 5, 6, 5]\n",
      "\n",
      "\n",
      "\n",
      " The idea is that the ball joints are not and tip. We do not want the end to a scenario where we wait for the summaries and all we just want to verify whether the body is joining at the scheduled time or not. \n",
      "\n",
      " Entities picked:  ['Janus', 'Uuid', 'Id', 'Name', 'Uuids', 'Internet', 'Gypsy', 'Json', 'Api', 'Context'] \n",
      "\n",
      " community mapped:  [5, 42, 2, 2, 42, 5, 5, 42, 2, 41]\n",
      "\n",
      "\n",
      "\n",
      " Okay, so maybe just spend some time so that we are not wasting time trying to identify test cases station, right you and Franklin sit down and decide what it is that we want test. \n",
      "\n",
      " Entities picked:  ['Pink', 'AI', 'Janus', 'Names', 'Hr', 'Ether Engineering', 'Dixie', 'Gypsy', 'Epa', 'Domain'] \n",
      "\n",
      " community mapped:  [42, 48, 5, 48, 124, 124, 48, 5, 8, 27]\n",
      "\n",
      "\n",
      "\n",
      " We have a we have this entity and key press graph the afar at least specifically detailed engineering channel. So with that shank is going to put that into the FIFA service and then see how how it is performing with respect to filtration. One time occurring or bad transcripts that happens within a single meeting. But but one in rare occasions, what is happening is there are cases where you work consistently spelled incorrectly across matings. So in that case what happens is they come as a they come as a usual usable knows that we cannot filter with the current approach yet. But this has happened this is this is rare, but this is one flaw that I have seen with the Also has been we will we will figure out how to address those problems. But with the current graph we should be able to remove most of the sorry. I am getting remove most of the bad transcripts that happens once or twice occasionally. \n",
      "\n",
      " Entities picked:  ['Janus', 'Uuid', 'Id', 'Graph', 'Gypsy', 'Uuids', 'Name', 'Json', 'Postgres', 'Api'] \n",
      "\n",
      " community mapped:  [5, 42, 2, 94, 5, 42, 2, 42, 104, 2]\n",
      "\n",
      "\n",
      "\n",
      " Yeah, that would be good for me to understand is if you are having any benchmarks for this, right, so let us say that I have been calls that have happened for which we have keywords. If you can kind of share what how it looks after the you know filtration are useful so that we see for how well looks like you are. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Jesus', 'Pink', 'Json', 'Uuid', 'Name', 'Id', 'Uuids', 'Names'] \n",
      "\n",
      " community mapped:  [5, 5, 42, 42, 42, 42, 2, 2, 42, 48]\n",
      "\n",
      "\n",
      "\n",
      " So what I will do is I will then I will take a few recent calls and try to filter out of line and then see what what was there earlier and then what is it? \n",
      "\n",
      " Entities picked:  ['Dixie', 'Graham', 'Pim', 'Janus', 'Gypsy', 'Workspace', 'AI', 'Deep Graham', 'Names', 'Internet'] \n",
      "\n",
      " community mapped:  [48, 25, 25, 5, 5, 48, 48, 25, 48, 5]\n",
      "\n",
      "\n",
      "\n",
      " And on the before we go on to the recommended Watchers on the on the custom minds and aggressive communities. We I think we have last week day before we had mentioned about having this bad transcriptions in the model, right? So so Arjun had signed a new model with the GitHub calls, which is currently testing. So if it aligns with our current Community approach, we should also be should be able to proceed further with the custom mines also in the same way without any modifications to the way we currently do the domain mines. So that is still in that still in testing Arjun can update on the test once he has \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Graham', 'Ether Engineering', 'Dixie', 'Hr', 'Ontology', 'Web', 'Sdk', 'Pink'] \n",
      "\n",
      " community mapped:  [5, 5, 25, 124, 48, 124, 124, 5, 48, 42]\n",
      "\n",
      "\n",
      "\n",
      " So by the way laughing, I will tell you funny things last night in my dream. I had a dream where one of the summary keywords us. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'AI', 'Ontology', 'Screen', 'Engineering Channel', 'Internet', 'Pink', 'Ether Engineering', 'Hr'] \n",
      "\n",
      " community mapped:  [5, 5, 48, 124, 174, 25, 5, 42, 124, 124]\n",
      "\n",
      "\n",
      "\n",
      " I have done the coding that was once I think of shank has to deploy latest recommendation service code to staging that it was failing right now in session. So once that is and I will be able to test and disappear, okay. \n",
      "\n",
      " Entities picked:  ['Pink', 'Janus', 'Gypsy', 'Feature', 'Dixie', 'Github', 'Sdk', 'Ios', 'Web', 'AI'] \n",
      "\n",
      " community mapped:  [42, 5, 5, 6, 48, 6, 48, 5, 5, 48]\n",
      "\n",
      "\n",
      "\n",
      " How would how it works and staging right, like do not worry too much about formatting and all that just make sure it is feature flag so that \n",
      "\n",
      " Entities picked:  ['Beta', 'Xcode', 'Pr', 'Bill', 'Feature', 'Pink', 'Github', 'App Store', 'Gypsy', 'Jesus'] \n",
      "\n",
      " community mapped:  [3, 3, 6, 3, 6, 42, 6, 5, 5, 42]\n",
      "\n",
      "\n",
      "\n",
      " And then just dump the results in staging and then we will see decide what we need to do to get it to production. \n",
      "\n",
      " Entities picked:  ['Dixie', 'S3', 'Names', 'AI', 'Hr', 'Workspace', 'Janus', 'App Center', 'Feature', 'App Store'] \n",
      "\n",
      " community mapped:  [48, 2, 48, 48, 124, 48, 5, 3, 6, 5]\n",
      "\n",
      "\n",
      "\n",
      " So let us just take aways our let us make sure that we are able to deploy stay, you know latest Master to staging and make sure staging is stable first and then deploy against only one is changes so that we are able to you know debuggers. \n",
      "\n",
      " Entities picked:  ['Beta', 'Feature', 'Bill', 'App Center', 'Xcode', 'Pr', 'Github', 'Sdk', 'Gypsy', 'Pink'] \n",
      "\n",
      " community mapped:  [3, 6, 3, 3, 3, 6, 6, 48, 5, 42]\n",
      "\n",
      "\n",
      "\n",
      " All right, and then if partial you want to stay back you addition to show you know, Marcia and Karthik irrigation system that we can talk about the website real quick. \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Web', 'Janus', 'Youtube', 'Pink', 'Dixie', 'Ontology', 'AI', 'Ios', 'Hr'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 174, 42, 48, 124, 48, 5, 124]\n",
      "\n",
      "\n",
      "\n",
      " Like we have a for example, the the theory we have is that when there are more than there are some n number of simultaneous page views. Question is what level of traffic can be sustained rate right now, right? So the fatal error that we would see in PHP are no more once it increase the memory allocation on Locker to about to the max of what the cvXnumberX machine provides which is XnumberX GB is Preservation and Max allocations to confine minimum resolution is XnumberX GB. So that is pretty much solving that class not found exception and Communication. So the next thing I did was I ran if you click on current usage tests bench tests with a sustained time period five minutes. So thousand requests per minute consistent energy coming in five minute period Sarkozy results success ratio, there was XnumberX XnumberX percent success. So between XnumberX finer tools fine arteries, but no fatal levels just that. So one thing we should see is if we can if it is possible. So this is redundancy may or may not help I mean, but that is in a very expensive given the machine size and how much traffic will be giving for that if it is possible to separate the static content like the static initial website like the careers or the main lips their page from Um the woocommerce part then and deploy that independent link that is one way to see that is true. She got the water playing tennis and I do not do any work on the website. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Internet', 'Dixie', 'Graham', 'Ios', 'Web', 'Postgres', 'Google', 'Websocket'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 48, 25, 5, 5, 104, 5, 38]\n",
      "\n",
      "\n",
      "\n",
      " Because there is so many other things in the product we have to do and I want to spend like another week or two and the website is right. So I want to get it to a reasonable stability where at least XnumberX to XnumberX simultaneous web used page views and work and everything will work. So in that case, let us see what how we are doing with the current memory size and then we even still running into issues will go to college. So I started with with XnumberX then XnumberX XnumberX also requests per minute that means \n",
      "\n",
      " Entities picked:  ['Gypsy', 'Janus', 'Web', 'Internet', 'Dixie', 'Ios', 'Graham', 'Google', 'Workspace', 'Sdk'] \n",
      "\n",
      " community mapped:  [5, 5, 5, 5, 48, 5, 25, 5, 48, 48]\n",
      "\n",
      "\n",
      "\n",
      " You guys say so even this and instead of like putting time into this if we run into issues to because these issues basically came when analytics was showing around XnumberX views and that time it was XnumberX and Beyond lie. \n",
      "\n",
      " Entities picked:  ['Janus', 'Gypsy', 'Graham', 'Google', 'Hr', 'Screen', 'D', 'Web', 'Postgres', 'Internet'] \n",
      "\n",
      " community mapped:  [5, 5, 25, 5, 124, 174, 94, 5, 104, 5]\n",
      "\n",
      "\n",
      "\n",
      " If we run into this again, I think instead of like putting our time into like checking out why it is happening because we know that it is because of the server itself. We just go ahead with like professional service for hosting and the all these things will not come up then. \n",
      "\n",
      " Entities picked:  ['Web', 'Internet', 'Api', 'Sdk', 'Apis', 'Janus', 'Ios', 'Dixie', 'Gypsy', 'App'] \n",
      "\n",
      " community mapped:  [5, 5, 2, 48, 2, 5, 5, 48, 5, 5]\n",
      "\n",
      "\n",
      "\n",
      " We are you know what we are I do not have any website business at all if they can avoid it. \n",
      "\n",
      " Entities picked:  ['Apple', 'Google', 'Internet', 'Psyche', 'Hr', 'Admin', 'Admin Page', 'Slack', 'Woocommerce', 'Web'] \n",
      "\n",
      " community mapped:  [3, 5, 5, 8, 124, 8, 8, 8, 8, 5]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset = os.listdir(\"../golden_dataset/ether/\")\n",
    "\n",
    "ranked = generate_gs([\"../golden_dataset/ether/\"+i for i in dataset if i!=\".ipynb_checkpoints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T07:22:39.599188Z",
     "start_time": "2020-01-17T07:22:39.163193Z"
    }
   },
   "outputs": [],
   "source": [
    "ranked_sorted = sorted(ranked.items(), key=lambda kv:kv[1], reverse=True)\n",
    "\n",
    "clusters = {}\n",
    "temp = []\n",
    "prev_com = 0\n",
    "for index,(word,cluster) in enumerate(sorted(com_map.items(), key=lambda kv:kv[1])):\n",
    "    if prev_com==cluster:\n",
    "        temp.append(word)\n",
    "        if index==len(com_map.items())-1:\n",
    "            clusters[cluster] = temp\n",
    "    else:\n",
    "        clusters[prev_com] = temp\n",
    "        temp = []\n",
    "        prev_com = cluster\n",
    "        temp.append(word)\n",
    "        if index==len(com_map.items())-1:\n",
    "            clusters[cluster] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-17T07:22:40.667342Z",
     "start_time": "2020-01-17T07:22:39.601766Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Community:  5   Freq:  799 \n",
      "\n",
      "['Web', 'Google', 'Ios', 'Janus', 'Zoom', 'Gypsy', 'App', 'Internet', 'App Store', 'Ios App']\n",
      "\n",
      "\n",
      "Community:  48   Freq:  507 \n",
      "\n",
      "['Sdk', 'Workspace', 'Names', 'Dixie', 'Web Sdk', 'AI']\n",
      "\n",
      "\n",
      "Community:  2   Freq:  242 \n",
      "\n",
      "['Api', 'S3', 'Lambda', 'Id', 'Lambda Function', 'Lambdas', 'Apa', 'Aws', 'Apis', 'Name', 'Aps', 'Cdn', 'Gateway', 'Sigmoid', 'Apa Gateway', 'Lambda Lambda', 'Uid', 'Lambda Functions', 'Cloudwatch', 'Meeting Id']\n",
      "\n",
      "\n",
      "Community:  42   Freq:  193 \n",
      "\n",
      "['Etc', 'Get', 'Json', 'Uuids', 'Uuid', 'Jesus', 'Pink']\n",
      "\n",
      "\n",
      "Community:  3   Freq:  126 \n",
      "\n",
      "['React', 'Electron', 'Windows', 'Python', 'Safari', 'Desktop', 'Device', 'Android', 'Wordpress', 'Chrome', 'Ffmpeg', 'Mac', 'Iframe', 'Native', 'Xcode', 'Javascript', 'Apple', 'Beta', 'Electron App', 'Webview', 'Php', 'Dom', 'Mac Os', 'Bill', 'App Center', 'React Native', 'House', 'Css']\n",
      "\n",
      "\n",
      "Community:  25   Freq:  119 \n",
      "\n",
      "['Gpt', 'Mla', 'Pims', 'Engineering Channel', 'Box', 'Pim', 'Graham', 'Deep Graham', 'Gpd', 'Pimps']\n",
      "\n",
      "\n",
      "Community:  6   Freq:  91 \n",
      "\n",
      "['Feature', 'Congress', 'Pr', 'Github']\n",
      "\n",
      "\n",
      "Community:  94   Freq:  70 \n",
      "\n",
      "['Knowledge Graph', 'Graph', 'Entity', 'D', 'Entity Graph', 'Mind', 'Ether Engine', 'Hadoop', 'Neo4j']\n",
      "\n",
      "\n",
      "Community:  124   Freq:  63 \n",
      "\n",
      "['Ether Engineering', 'Ontology', 'Hr']\n",
      "\n",
      "\n",
      "Community:  8   Freq:  61 \n",
      "\n",
      "['Slack', 'Epa', 'Admin', 'Login', 'Cookie', 'Auth', 'Admin Page', 'Uri', 'Point', 'Admins', 'Slack App', 'Mooc', 'Woocommerce', 'Plan Api', 'Create', 'Psyche']\n",
      "\n",
      "\n",
      "Community:  41   Freq:  50 \n",
      "\n",
      "['Context Id', 'Channel', 'Context', 'Channel Minds', 'Controller', 'Action', 'Event', 'Async', 'Action Item']\n",
      "\n",
      "\n",
      "Community:  174   Freq:  35 \n",
      "\n",
      "['Html', 'Youtube', 'Mp4', 'Screen']\n",
      "\n",
      "\n",
      "Community:  104   Freq:  34 \n",
      "\n",
      "['Db', 'Sql', 'Postgres', 'Rds']\n",
      "\n",
      "\n",
      "Community:  38   Freq:  29 \n",
      "\n",
      "['Http', 'Websocket', 'Websockets', 'Dsps', 'Microservice']\n",
      "\n",
      "\n",
      "Community:  76   Freq:  24 \n",
      "\n",
      "['Ebs', 'Turkish', 'Ecs', 'Ec2', 'Gps']\n",
      "\n",
      "\n",
      "Community:  27   Freq:  22 \n",
      "\n",
      "['Domain Minds', 'Domain', 'Domain Mind']\n",
      "\n",
      "\n",
      "Community:  91   Freq:  3 \n",
      "\n",
      "['America', 'Russia', 'Uk', 'Russian', 'Syria', 'France', 'Nato']\n",
      "\n",
      "\n",
      "Community:  43   Freq:  1 \n",
      "\n",
      "['Csv', 'Ml', 'Src']\n",
      "\n",
      "\n",
      "Community:  239   Freq:  1 \n",
      "\n",
      "['Node', 'Linux', 'Js']\n"
     ]
    }
   ],
   "source": [
    "for com, freq in ranked_sorted:\n",
    "    print (\"\\n\\nCommunity: \", com, \"  Freq: \", freq ,\"\\n\")\n",
    "    print (clusters[com])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:58:42.746658Z",
     "start_time": "2020-01-16T12:58:42.696609Z"
    }
   },
   "outputs": [],
   "source": [
    "com_map_selected = {}\n",
    "for index, (com, freq) in enumerate(ranked_sorted):\n",
    "    com_map_selected[com] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:58:46.812463Z",
     "start_time": "2020-01-16T12:58:46.763294Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(ranked, open(\"/home/ray__/ssd/minds/ether/gc.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:58:47.245605Z",
     "start_time": "2020-01-16T12:58:47.100690Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(com_map_selected, open(\"/home/ray__/ssd/minds/ether/ranked_com_selected.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:58:48.145123Z",
     "start_time": "2020-01-16T12:58:48.096766Z"
    }
   },
   "outputs": [],
   "source": [
    "lc = {}\n",
    "for cls, freq in ranked.items():\n",
    "    lc[cls] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:58:49.368805Z",
     "start_time": "2020-01-16T12:58:48.721963Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(lc, open(\"/home/ray__/ssd/minds/ether/lc.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T11:43:24.766553Z",
     "start_time": "2020-01-16T11:43:24.672773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Web',\n",
       " 'Google',\n",
       " 'Ios',\n",
       " 'Janus',\n",
       " 'Zoom',\n",
       " 'Gypsy',\n",
       " 'App',\n",
       " 'Internet',\n",
       " 'App Store',\n",
       " 'Ios App']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:41:48.208310Z",
     "start_time": "2020-01-16T12:41:48.203264Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "com_map = pickle.load(open(\"/home/ray__/ssd/minds/ether/com_map.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:43:00.156492Z",
     "start_time": "2020-01-16T12:43:00.134884Z"
    }
   },
   "outputs": [],
   "source": [
    "ranked_sorted = sorted(com_map.items(), key=lambda kv:kv[1], reverse=True)\n",
    "\n",
    "clusters = []\n",
    "temp = []\n",
    "prev_com = 0\n",
    "for index,(word,cluster) in enumerate(sorted(com_map.items(), key=lambda kv:kv[1])):\n",
    "    if prev_com==cluster:\n",
    "        temp.append(word)\n",
    "        if index==len(com_map.items())-1:\n",
    "            clusters.append(temp)\n",
    "    else:\n",
    "        clusters.append(temp)\n",
    "        temp = []\n",
    "        prev_com = cluster\n",
    "        temp.append(word)\n",
    "        if index==len(com_map.items())-1:\n",
    "            clusters.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:44:29.472744Z",
     "start_time": "2020-01-16T12:44:29.464267Z"
    }
   },
   "outputs": [],
   "source": [
    "com_map_refined = {}\n",
    "for ent,cls in com_map.items():\n",
    "    if len(clusters[cls])<=2:\n",
    "        continue\n",
    "    else:\n",
    "        com_map_refined[ent] = cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:44:45.676671Z",
     "start_time": "2020-01-16T12:44:45.669775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(com_map_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T14:22:15.174248Z",
     "start_time": "2020-01-08T14:22:14.504920Z"
    }
   },
   "outputs": [],
   "source": [
    "ent_check = \"Harvard Business Review\"\n",
    "ck = []\n",
    "for ent in ent_fv.keys():\n",
    "    ck.append((ent, 1-cosine(ent_fv[ent], ent_fv[ent_check])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T14:24:03.115899Z",
     "start_time": "2020-01-08T14:24:03.039837Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvard Business Review  ->  1.0\n",
      "Mckinsey  ->  0.8903322815895081\n",
      "Forbes  ->  0.8880045413970947\n",
      "Cio  ->  0.8862068057060242\n",
      "Engineering  ->  0.8720834255218506\n",
      "Digital  ->  0.87077397108078\n",
      "Software Engineering  ->  0.8672767877578735\n",
      "C-suite  ->  0.8627213835716248\n",
      "Man-month  ->  0.8586208820343018\n",
      "Gartner  ->  0.855764627456665\n",
      "American  ->  0.8541392087936401\n",
      "Software  ->  0.8452587723731995\n",
      "Agiles  ->  0.8377225995063782\n",
      "Agile Software  ->  0.8330069184303284\n",
      "10x  ->  0.8307514190673828\n",
      "Kodak  ->  0.8300771713256836\n",
      "Goldman Sachs  ->  0.8299168944358826\n",
      "Tech  ->  0.829247772693634\n",
      "Finance  ->  0.8286100625991821\n",
      "Agile  ->  0.8278757929801941\n",
      "North Star  ->  0.8275061249732971\n",
      "Org  ->  0.8253639936447144\n",
      "Accelerate  ->  0.8239707350730896\n",
      "Itil  ->  0.823801577091217\n",
      "Lean Startup  ->  0.8228476643562317\n",
      "Nasa  ->  0.8222607970237732\n",
      "Italian  ->  0.82192063331604\n",
      "Thoughtworks  ->  0.821498692035675\n",
      "Epfl  ->  0.820669412612915\n",
      "British  ->  0.8203485012054443\n"
     ]
    }
   ],
   "source": [
    "for ent, score in sorted(ck, key=lambda kv:kv[1], reverse=True)[:30]:\n",
    "    print (ent, \" -> \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:14:15.568122Z",
     "start_time": "2020-01-08T18:14:14.602193Z"
    }
   },
   "source": [
    "# testing sentence to entity similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T19:07:26.071775Z",
     "start_time": "2020-01-08T19:07:25.933420Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from extra_preprocess import preprocess_text\n",
    "\n",
    "#text = \"Everybody wants to create applications using the micro service architecture in this video. But rather than reading out a list of architectural Concepts, we will approach that a completely different place by me telling you a story that you really need to know. Let is go back in time to the old days when we did not have anything called micro services. Theyll go to a time when a computer was big enough to fill the room. Mr. Seen pictures of one of these things you would walk up to that computer. Fast forward a little bit to desktop machines again programs recite it on the desktop machine the programmer application contains all the instructions that are needed to execute. So when people would write applications for it their code would be compiled down in the whole thing would be installed on the computer in one go and it will be installed on the same computer think of purely desktop only apps. Okay, something like Microsoft Word or a text editor that you install on your I mean this has historically influence how we write code when we need to write an application. We would start a new code project and we would add functionality into that project need more functionality add more code. And so the size of the code base for any given application keeps increasing over time. So what starts as a small code base might end up turning into a large complex code base over time and people realize this.\"\n",
    "text = \"Nothing too fancy, but overtime web applications have started becoming better bigger quicker more complicated bigger scale bigger user base bigger everything and today we have web applications that can you know find something from the whole internet in a matter of milliseconds are fine. What cabs are available around you all over the world in milliseconds. These are incredible Feats if you think about it, and they need incredibly complicated code to be Developed and applied and this complexity becomes harder and harder to maintain. We have nice modular architecture on the code side of things is not that enough to handle the complexity during development time? Well with the type of applications we are talking about the complexity needs to be handled not just at the coding side of things. They also need to be handled at the runtime or execution side of things having a single thing that you would apply. Didnt work anymore this way of having a single application is called the monolithic application or monolithic architecture mono means single lithic means Stone single Stone monolithic. This is the Smosh basically What are some of the disadvantages of this monolithic model first the bigger the deployment the more challenging the deployment Let me Give an example. Let is say you want to push a new feature to your big monolithic application. So among all the code comments that you want to deploy is the single quote comment by this new guy. The company has just hired me not so sure about him. He probably does not know much and he still learning but his first code come it ever is still sitting there. Well, you need to test the whole thing before you would apply the whole application you never know which part of the application that come it might have broken. Well, I am exaggerating here, of course, but the fact remains.\"\n",
    "text_p = \" \".join(preprocess_text(text))\n",
    "\n",
    "text_pp = \" \".join(tp.preprocess(text, stop_words=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T19:07:28.262925Z",
     "start_time": "2020-01-08T19:07:27.046008Z"
    }
   },
   "outputs": [],
   "source": [
    "text_fv = gpt_model.get_text_feats(text)\n",
    "ck = []\n",
    "for ent in ent_fv.keys():\n",
    "    ck.append((ent, 1-cosine(ent_fv[ent], text_fv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T19:07:29.157499Z",
     "start_time": "2020-01-08T19:07:29.071270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Perfect',\n",
       " 'Harry',\n",
       " 'Know',\n",
       " 'Scotsman',\n",
       " 'Ugh',\n",
       " 'Open-closed',\n",
       " 'Greeks',\n",
       " 'Wait',\n",
       " 'Retval',\n",
       " 'Spaghetti Code']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i,j in sorted(ck, key=lambda kv:kv[1], reverse=True)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T19:07:32.769075Z",
     "start_time": "2020-01-08T19:07:31.538365Z"
    }
   },
   "outputs": [],
   "source": [
    "text_fv = gpt_model.get_text_feats(text_p)\n",
    "ck = []\n",
    "for ent in ent_fv.keys():\n",
    "    ck.append((ent, 1-cosine(ent_fv[ent], text_fv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T19:07:32.839574Z",
     "start_time": "2020-01-08T19:07:32.771931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Perfect',\n",
       " 'Harry',\n",
       " 'Know',\n",
       " 'Scotsman',\n",
       " 'Ugh',\n",
       " 'Open-closed',\n",
       " 'Greeks',\n",
       " 'Wait',\n",
       " 'Retval',\n",
       " 'Spaghetti Code']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i,j in sorted(ck, key=lambda kv:kv[1], reverse=True)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:31:05.951403Z",
     "start_time": "2020-01-08T18:31:05.022510Z"
    }
   },
   "outputs": [],
   "source": [
    "text_fv = gpt_model.get_text_feats(text_pp)\n",
    "ck = []\n",
    "for ent in ent_fv.keys():\n",
    "    ck.append((ent, 1-cosine(ent_fv[ent], text_fv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:31:09.098325Z",
     "start_time": "2020-01-08T18:31:09.038932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spaghetti Code',\n",
       " 'Perfect',\n",
       " 'Know',\n",
       " 'God',\n",
       " 'Ugh',\n",
       " 'Spaghetti',\n",
       " 'Netscapes',\n",
       " 'Universe',\n",
       " 'Smalltalk',\n",
       " 'Harry']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i,j in sorted(ck, key=lambda kv:kv[1], reverse=True)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:48:50.370912Z",
     "start_time": "2020-01-08T18:48:50.319486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the perfect is the enemy of the good.',\n",
       " 'the perfect is the enemy of the good.',\n",
       " \"'perfect is the enemy of good.'\",\n",
       " 'perfect we identified our first object role the entity loader ientityloader interface.',\n",
       " \"'perfect is the enemy of good.'\",\n",
       " 'finally, swift, originally from apple and used on ios, has begun to see some use in web backends as well, with vapor, kitura and perfect.',\n",
       " 'since then, a variety of other server frameworks have sprung up such as vapor and perfect, both of which seem to be building polished, feature-rich, and well documented tools for constructing a stack built around swift.',\n",
       " \"'the perfect is the enemy of the good'  voltaire.\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_dict['Perfect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
