{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:25:04.660436Z",
     "start_time": "2020-03-05T08:25:04.634247Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:25:45.380400Z",
     "start_time": "2020-03-05T08:25:09.726651Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ray__/ssd/BERT/\")\n",
    "sys.path.append(\"/home/ray__/CS/org/etherlabs/ai-engine/pkg/\")\n",
    "sys.path.append(\"../\")\n",
    "from gpt_feat_utils import GPT_Inference\n",
    "\n",
    "gpt_model = GPT_Inference(\"/home/ray__/ssd/BERT/models/ai/epoch3/\", device=\"cuda\")\n",
    "#gpt_model = GPT_Inference(\"/home/ray__/ssd/BERT/models/ether_v2/ether_googleJan13_groupsplit_withstop_4+w_gt3s_lr3e-5/\", device=\"cpu\")\n",
    "#gpt_model = GPT_Inference(\"/home/ray__/ssd/BERT/models/customer_service/epoch3/\", device=\"cpu\")\n",
    "#gpt_model = GPT_Inference(\"/home/ray__/ssd/BERT/models/sales/\", device=\"cuda\")\n",
    "# with open('../topic_testing/cullen_test.json','rb') as f:\n",
    "#     request = json.load(f)\n",
    "#     if isinstance(request, str):\n",
    "#         request = json.loads(request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:26:09.859996Z",
     "start_time": "2020-03-05T08:26:05.704807Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# # S.E\n",
    "# ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/se/se_entity_feats_v3.pkl\",\"rb\"))\n",
    "# sent_dict = pickle.load(open(\"/home/ray__/ssd/minds/se/se_sent_dict_v3.pkl\", \"rb\"))\n",
    "# # ent_graph = nx.read_gpickle(\"/home/ray__/ssd/minds/se/se_ent_graph_wcosine_pruned_v3.gpkl\")\n",
    "# # label_dict = pickle.load(open(\"/home/ray__/ssd/minds/ether/ether_single_label_dict_v1.pkl\",\"rb\"))\n",
    "# com_map = pickle.load(open(\"/home/ray__/ssd/minds/se/cw/com_map.pkl\", \"rb\"))\n",
    "# # #ranked_com = pickle.load(open(\"/home/ray__/ssd/minds/ether_se/ranked_com_selected_h.pkl\", \"rb\"))\n",
    "# noun_graph = pickle.load(open(\"/home/ray__/ssd/minds/se/se_noun_graph.pkl\", \"rb\"))\n",
    "# kp_entity_graph = pickle.load(open(\"/home/ray__/ssd/minds/se/se_ent_kp_graph_fromsd_posDBERT.pkl\", \"rb\"))\n",
    "\n",
    "# marketing -- current\n",
    "# ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/marketing/marketing_entity_feats_v3.pkl\",\"rb\"))\n",
    "# sent_dict = pickle.load(open(\"/home/ray__/ssd/minds/marketing/marketing_sent_dict_v3.pkl\", \"rb\"))\n",
    "# label_dict = pickle.load(open(\"/home/ray__/ssd/minds/marketing/marketing_label_dict_v3.pkl\", \"rb\"))\n",
    "# com_map = pickle.load(open(\"/home/ray__/ssd/minds/marketing/com_map.pkl\", \"rb\"))\n",
    "# #ranked_com = pickle.load(open(\"/home/ray__/ssd/minds/ether/ranked_com_h.pkl\", \"rb\"))\n",
    "# noun_graph = pickle.load(open(\"/home/ray__/ssd/minds/marketing/noun_graph.pkl\", \"rb\"))\n",
    "# #kp_entity_graph = pickle.load(open(\"/home/ray__/ssd/minds/marketing/entity_kp_graph_marketing_with_single_labels_v2.pkl\", \"rb\"))\n",
    "# kp_entity_graph = pickle.load(open(\"/home/ray__/ssd/minds/marketing/deployment/kp_entity_graph.pkl\",\"rb\"))\n",
    "\n",
    "# hr -- current\n",
    "# ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/hr/hr_entity_feats_v3.pkl\",\"rb\"))\n",
    "# sent_dict = pickle.load(open(\"/home/ray__/ssd/minds/hr/hr_sent_dict_v3.pkl\",\"rb\"))\n",
    "# label_dict = pickle.load(open(\"/home/ray__/ssd/minds/hr/hr_label_dict_v3.pkl\",\"rb\"))\n",
    "# com_map  = pickle.load(open(\"/home/ray__/ssd/minds/hr/com_map.pkl\",\"rb\"))\n",
    "# noun_graph = pickle.load(open(\"/home/ray__/ssd/minds/hr/noun_graph.pkl\",\"rb\"))\n",
    "# kp_entity_graph = pickle.load(open(\"/home/ray__/ssd/minds/hr/deployment/kp_entity_graph.pkl\",\"rb\"))\n",
    "\n",
    "# product -- current\n",
    "# ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/product/products_entity_feats_v3.pkl\",\"rb\"))\n",
    "# sent_dict = pickle.load(open(\"/home/ray__/ssd/minds/product/products_sent_dict_v3.pkl\",\"rb\"))\n",
    "# label_dict = pickle.load(open(\"/home/ray__/ssd/minds/product/products_label_dict_v3.pkl\",\"rb\"))\n",
    "# com_map  = pickle.load(open(\"/home/ray__/ssd/minds/product/com_map.pkl\",\"rb\"))\n",
    "# noun_graph = pickle.load(open(\"/home/ray__/ssd/minds/product/noun_graph.pkl\",\"rb\"))\n",
    "# kp_entity_graph = pickle.load(open(\"/home/ray__/ssd/minds/product/deployment/kp_entity_graph.pkl\",\"rb\"))\n",
    "\n",
    "# # ai -- current\n",
    "ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/ai/ai_entity.pkl\",\"rb\"))\n",
    "sent_dict = pickle.load(open(\"/home/ray__/ssd/minds/ai/ai_sent.pkl\", \"rb\"))\n",
    "com_map = pickle.load(open(\"/home/ray__/ssd/minds/ai/cw/com_map.pkl\",\"rb\"))\n",
    "#noun_graph = pickle.load(open(\"/home/ray__/ssd/minds/ai/noun_graph.pkl\", \"rb\"))\n",
    "kp_entity_graph = pickle.load(open(\"/home/ray__/ssd/minds/ai/kp_entity_graph.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "# sales -- current\n",
    "# ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/sales/sales_entity_feats_v3.pkl\",\"rb\"))\n",
    "# sent_dict = pickle.load(open(\"/home/ray__/ssd/minds/sales/sales_sent_dict_v3.pkl\",\"rb\"))\n",
    "# label_dict = pickle.load(open(\"/home/ray__/ssd/minds/sales/sales_label_dict_v3.pkl\",\"rb\"))\n",
    "# com_map  = pickle.load(open(\"/home/ray__/ssd/minds/sales/com_map.pkl\",\"rb\"))\n",
    "# noun_graph = pickle.load(open(\"/home/ray__/ssd/minds/sales/noun_graph.pkl\",\"rb\"))\n",
    "# kp_entity_graph = pickle.load(open(\"/home/ray__/ssd/minds/sales/deployment/kp_entity_graph.pkl\",\"rb\"))\n",
    "\n",
    "# Ether\n",
    "# ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/ether_se/ether_entity_feats_grp.pkl\",\"rb\"))\n",
    "# sent_dict = pickle.load(open(\"/home/ray__/ssd/minds/ether/ether_sent_dict_v1.pkl\", \"rb\"))\n",
    "# label_dict = pickle.load(open(\"/home/ray__/ssd/minds/ether/ether_single_label_dict_v1.pkl\",\"rb\"))\n",
    "# com_map = pickle.load(open(\"/home/ray__/ssd/minds/ether/com_map_h.pkl\", \"rb\"))\n",
    "# ranked_com = pickle.load(open(\"/home/ray__/ssd/minds/ether/ranked_com_h.pkl\", \"rb\"))\n",
    "# noun_graph = pickle.load(open(\"/home/ray__/ssd/minds/ether/meeting_noun_graph.pkl\", \"rb\"))\n",
    "# kp_entity_graph = pickle.load(open(\"/home/ray__/ssd/minds/ether/entity_kp_graph.pkl\", \"rb\"))\n",
    "\n",
    "#ether_se\n",
    "# ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/ether_se/se_ether_entity_feats_v3.pkl\",\"rb\"))\n",
    "# sent_dict = pickle.load(open(\"/home/ray__/ssd/minds/ether_se/ether_sent_dict_v3.pkl\", \"rb\"))\n",
    "# label_dict = pickle.load(open(\"/home/ray__/ssd/minds/ether_se/ether_single_label_dict_v4.pkl\", \"rb\"))\n",
    "# com_map = pickle.load(open(\"/home/ray__/ssd/minds/ether_se/com_map.pkl\", \"rb\"))\n",
    "# #ranked_com = pickle.load(open(\"/home/ray__/ssd/minds/ether_se/ranked_com_selected_h.pkl\", \"rb\"))\n",
    "# #noun_graph = pickle.load(open(\"/home/ray__/ssd/minds/ether_se/meeting_noun_graph.pkl\", \"rb\"))\n",
    "# kp_entity_graph = pickle.load(open(\"/home/ray__/ssd/minds/ether_se/se_ether_ent_kp_noun_graph_fromsd.pkl\", \"rb\"))\n",
    "\n",
    "# # customer_service\n",
    "# ent_fv_full = pickle.load(open(\"/home/ray__/ssd/minds/customer_service/entity.pkl\",\"rb\"))\n",
    "# sent_dict = pickle.load(open(\"/home/ray__/ssd/minds/customer_service/sent_dict.pkl\", \"rb\"))\n",
    "# label_dict = pickle.load(open(\"/home/ray__/ssd/minds/customer_service/label_dict.pkl\", \"rb\"))\n",
    "# kp_entity_graph = pickle.load(open(\"/home/ray__/ssd/minds/customer_service/kp_entity_graph.pkl\", \"rb\"))\n",
    "# com_map = pickle.load(open(\"/home/ray__/ssd/minds/customer_service/cw/com_map_v3.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "common_entities = ent_fv_full.keys() & com_map.keys()\n",
    "ent_fv = {}\n",
    "for ent in common_entities:\n",
    "    #if com_map[ent]!=1:\n",
    "    ent_fv[ent] = ent_fv_full[ent]\n",
    "\n",
    "# common_entities = ent_fv_full.keys() & com_map.keys()\n",
    "# ent_fv = {}\n",
    "# for ent in common_entities:\n",
    "#     if ent in sent_dict.keys() and label_dict[ent]=='PER':\n",
    "#         continue\n",
    "#     ent_fv[ent] = ent_fv_full[ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:26:26.107497Z",
     "start_time": "2020-03-05T08:26:09.862855Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /tmp/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /tmp/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /tmp/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import text_preprocessing.preprocess as tp\n",
    "from extra_preprocess import preprocess_text\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "sys.path.append(\"../helper_functions/\")\n",
    "from generate_controlled_top_10_ent import get_ent\n",
    "\n",
    "\n",
    "def get_entity_mapping(group_ent, ent_fv, com_map, kp_entity_graph):\n",
    "    group_ent_map = {}\n",
    "    for groupid, ent_list in group_ent.items():\n",
    "        #group_ent_map[groupid] = list(set([ent for ent in list(map(lambda kv:kv[0], group_ent[groupid]))]))\n",
    "        #group_ent_map[groupid] = [com_map[ent] for ent in group_ent_map[groupid] if ent in com_map.keys()]\n",
    "        group_ent_map[groupid] = [com_map[ent] for ent in list(map(lambda kv:kv[0], group_ent[groupid]))]\n",
    "        #group_ent_score[groupid] = [ranked_com[com] for com in group_ent_score[groupid] if com in ranked_com.keys()]\n",
    "    print (\"Group Ent map before filtering: \", group_ent_map)\n",
    "    return group_ent_map\n",
    "\n",
    "def filter_entity_community(group_ent_map, ent_fv, com_map, kp_entity_graph):\n",
    "    group_ent_map_filtered_intrm = {}\n",
    "    group_ent_map_filtered = {}\n",
    "    for groupid, ent_map in group_ent_map.items():\n",
    "        filtered_ent_map = []\n",
    "        if len(set(ent_map)) == len(ent_map):\n",
    "            group_ent_map_filtered[groupid] = []\n",
    "        else:\n",
    "            count_a = Counter(ent_map).most_common()\n",
    "            for i, count in count_a:\n",
    "                if count>2 :\n",
    "                    filtered_ent_map.append((i, count))\n",
    "            \n",
    "            group_ent_map_filtered[groupid] = filtered_ent_map\n",
    "    print (\"Group Ent map after filtering: \", group_ent_map_filtered)\n",
    "    return group_ent_map_filtered\n",
    "\n",
    "\n",
    "def generate_gs(set_list, from_csv=False):\n",
    "    com_freq = {}\n",
    "    for file in set_list:\n",
    "        if not from_csv:\n",
    "            req = json.load(open(file, \"r\"))\n",
    "            if isinstance(req, str):\n",
    "                request = json.loads(req)\n",
    "            else:\n",
    "                request = req\n",
    "        else:\n",
    "            request = {\"body\":file} \n",
    "        \n",
    "        group, group_ent = get_ent(request, ent_fv, com_map, kp_entity_graph)\n",
    "        group_ent_map = get_entity_mapping(group_ent, ent_fv, com_map, kp_entity_graph)\n",
    "        group_ent = filter_entity_community(group_ent_map, ent_fv, com_map, kp_entity_graph)\n",
    "        ent_score_sorted = {}\n",
    "        for groupid, ent_list in group_ent.items():\n",
    "            ent_score_sorted[groupid] = [i for i,j in ent_list]\n",
    "            for comm in ent_score_sorted[groupid]:\n",
    "                if comm in com_freq.keys():\n",
    "                    com_freq[comm] += 1\n",
    "                else:\n",
    "                    com_freq[comm] = 1\n",
    "            #print (\"\\n\\n\\n\", segments_map[segid][\"originalText\"], \"\\n\\n Entities picked: \", [i for i,j in sorted_score][:10], \"\\n\\n community mapped: \",  [com_map[i] for i in [i for i,j in sorted_score][:10]])\n",
    "        \n",
    "    return com_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:27:46.057359Z",
     "start_time": "2020-03-05T08:26:26.111407Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  None  for feature extraction\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 103, \"module\": \"scorer\", \"batches count\": 1, \"number of sentences\": 85, \"ts\": \"2020-03-05T08:26:26.978888Z\", \"msg\": \"computing in batches\"}\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 106, \"module\": \"scorer\", \"iteration count:\": 0, \"ts\": \"2020-03-05T08:26:26.979975Z\", \"msg\": \"getting feature vector\"}\n",
      "('So ideally the word embedding would figure out then that laughing laughter related since they are both related to joke.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('Broken up into two smaller words, so United and states are likely to appear together and then in the second and third example, we have a joke and laugh are kind of related words you laugh at a joke.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab')\n",
      "('So ideally the word embedding would figure out then that laughing laughter related since they are both related to joke.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('So ideally the word embedding would figure out then that laughing laughter related since they are both related to joke.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So ideally the word embedding would figure out then that laughing laughter related since they are both related to joke.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So the word Avec neural network in this case is just trying to picked context words from a word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So that is kind of where a word embedding gets its Knowledge from it learns things via context.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there is another way that context can help us and that is if two words happen to always appear in the same context at once.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab')\n",
      "('So that is kind of where a word embedding gets its Knowledge from it learns things via context.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('Our word embedding is going to have to learn the different forms of the same word or related.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab')\n",
      "('So that is kind of where a word embedding gets its Knowledge from it learns things via context.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('Today I am going to be talking about word embeddings, which I think are one of the coolest things you can do with machine learning right now and to explain why I think that I am just going to jump right in and give you an example of something you can do with word weddings.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('So that is kind of where a word embedding gets its Knowledge from it learns things via context.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So for example, if I put in a color it can tell me a bunch of other colors it never actually knew the idea of color going in it did not really know anything and piece together that all these words are related.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('So that is kind of where a word embedding gets its Knowledge from it learns things via context.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So pretty much every word embedding algorithm uses the idea of context and to show you what I mean.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('So that is kind of where a word embedding gets its Knowledge from it learns things via context.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So, you know, it goes right from that small Vector to the context.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So that is kind of where a word embedding gets its Knowledge from it learns things via context.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So here is a really simple picture of what a word to Veeck neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So that is kind of where a word embedding gets its Knowledge from it learns things via context.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('Looks like you essentially feed in a word and it produces in the middle.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So that is kind of where a word embedding gets its Knowledge from it learns things via context.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So the word Avec neural network in this case is just trying to picked context words from a word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('Our word embedding is going to have to learn the different forms of the same word or related.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab')\n",
      "('It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('Today I am going to be talking about word embeddings, which I think are one of the coolest things you can do with machine learning right now and to explain why I think that I am just going to jump right in and give you an example of something you can do with word weddings.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So the first approach I am going to be talking about is known as word Tyvek and it is probably the most famous kind of word of matting because it was the first word embedding to get the kind of impressive results that word embeddings stateoftheart word embeddings get today.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('Looks like you essentially feed in a word and it produces in the middle.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So the word Avec neural network in this case is just trying to picked context words from a word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they are very related words.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they are very related words.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they are very related words.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they are very related words.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('You know, how are we going to produce the set of vectors that work?', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they are very related words.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So for example, if I put in a color it can tell me a bunch of other colors it never actually knew the idea of color going in it did not really know anything and piece together that all these words are related.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So you are probably wondering how this actually worked because it is kind of baffling all I gave it was things that people typed on the internet.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('And in this video, I really want to explain why this works and how it works and just I think it is super cool.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So it says I painted the bench blank and were expected to fill in the blank.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So that is kind of just a really General overview of how weird to deck works and there is a lot more to it.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('I want the vector for Ferrari to be far away from The vector for hamburger and of course all these distances are relative, but you can kind of see what I mean that we want the closeness of these vectors to resemble the closeness of the words that they represent and in addition to this kind of idea of closeness.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('I will produce back out a vector maybe with a hundred thousand components in each neuron and that output Vector corresponds to a word as well.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('I want the vector for Ferrari to be far away from The vector for hamburger and of course all these distances are relative, but you can kind of see what I mean that we want the closeness of these vectors to resemble the closeness of the words that they represent and in addition to this kind of idea of closeness.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('Why do we expect that small Vector to actually be meeting?', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('I want the vector for Ferrari to be far away from The vector for hamburger and of course all these distances are relative, but you can kind of see what I mean that we want the closeness of these vectors to resemble the closeness of the words that they represent and in addition to this kind of idea of closeness.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('You know, how are we going to produce the set of vectors that work?', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('For example, if I do the vector from Man  the vector for woman and to subtract vectors, we just subtract each number from the corresponding number.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('For example, if I do the vector from Man  the vector for woman and to subtract vectors, we just subtract each number from the corresponding number.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('For example, if I do the vector from Man  the vector for woman and to subtract vectors, we just subtract each number from the corresponding number.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So if I take the vector from man and I subtract the vector for woman, I want that to somehow represent the difference between male and female and then if I add that Vector to the vector for for Queen, I want it to give me out something very very close to the vector for King.', '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5') ('So for example, if I put in a color it can tell me a bunch of other colors it never actually knew the idea of color going in it did not really know anything and piece together that all these words are related.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('But pretty much all of the methods utilized some amount of linear algebra.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('But pretty much all of the methods utilized some amount of linear algebra.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So essentially we are and devack is just a really simple neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So I am going to be just talking about matrices and Matrix multiplications dot products things like that.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('So I am going to be just talking about matrices and Matrix multiplications dot products things like that.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('So I am going to be just talking about matrices and Matrix multiplications dot products things like that.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('And if you do not know linear algebra Youre Not Really Gonna Get Much from this but that is why I kind of left it at the end of the video because some people will get something out of this and I think it is somewhat interesting.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So it is similar to training a neural network and it has really good results and it is extremely fast.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('And if you do not know linear algebra Youre Not Really Gonna Get Much from this but that is why I kind of left it at the end of the video because some people will get something out of this and I think it is somewhat interesting.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So essentially we are and devack is just a really simple neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('And if you do not know linear algebra Youre Not Really Gonna Get Much from this but that is why I kind of left it at the end of the video because some people will get something out of this and I think it is somewhat interesting.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('And if you have already seen my other videos on neural networks, you might actually already be able to implement or defect but I am just going to try to describe it here on a high level to give you an idea of how it works.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So basically a given cooccurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to cooccur.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('You can basically see that I get this massive Square Matrix, which is our cooccurrence matrix by multiplying a tall skinny Matrix and a short wide Matrix, you know, you think about how many entries are in the big Matrix?', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('You can basically see that I get this massive Square Matrix, which is our cooccurrence matrix by multiplying a tall skinny Matrix and a short wide Matrix, you know, you think about how many entries are in the big Matrix?', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('You can basically see that I get this massive Square Matrix, which is our cooccurrence matrix by multiplying a tall skinny Matrix and a short wide Matrix, you know, you think about how many entries are in the big Matrix?', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So, you know, it goes right from that small Vector to the context.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('You can basically see that I get this massive Square Matrix, which is our cooccurrence matrix by multiplying a tall skinny Matrix and a short wide Matrix, you know, you think about how many entries are in the big Matrix?', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('That is stored on the right side of this equation, which is you know, two relatively small matrices multiplied together.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('That is stored on the right side of this equation, which is you know, two relatively small matrices multiplied together.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So essentially we are and devack is just a really simple neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('That is stored on the right side of this equation, which is you know, two relatively small matrices multiplied together.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So I take the neuron for whatever word I want to feed in and I will set that neuron to one and I will set all the other neurons to XnumberX then the neural network will just use regular neural network stuff to produce a small Vector.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So by decomposing this big cooccurrence Matrix into these smaller ones were clearly compressing some information and in doing so hopefully we have to extract a lot of meaning from The Matrix so that we can do that compression and that should allow us to at least generate decent embeddings.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('I could have picked it from the short wide Matrix or I could even just decide to average these two vectors and use that as the embedding overall and there is actually a good reason to assume that these vectors would represent a decent amount of meaning.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('So by decomposing this big cooccurrence Matrix into these smaller ones were clearly compressing some information and in doing so hopefully we have to extract a lot of meaning from The Matrix so that we can do that compression and that should allow us to at least generate decent embeddings.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So by decomposing this big cooccurrence Matrix into these smaller ones were clearly compressing some information and in doing so hopefully we have to extract a lot of meaning from The Matrix so that we can do that compression and that should allow us to at least generate decent embeddings.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So by decomposing this big cooccurrence Matrix into these smaller ones were clearly compressing some information and in doing so hopefully we have to extract a lot of meaning from The Matrix so that we can do that compression and that should allow us to at least generate decent embeddings.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Once we have this Matrix decomposition, which I have not described exactly how we might find this yet, but you could imagine there is Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('I could have picked it from the short wide Matrix or I could even just decide to average these two vectors and use that as the embedding overall and there is actually a good reason to assume that these vectors would represent a decent amount of meaning.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('Once we have this Matrix decomposition, which I have not described exactly how we might find this yet, but you could imagine there is Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('Once we have this Matrix decomposition, which I have not described exactly how we might find this yet, but you could imagine there is Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('Once we have this Matrix decomposition, which I have not described exactly how we might find this yet, but you could imagine there is Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('Once we have this Matrix decomposition, which I have not described exactly how we might find this yet, but you could imagine there is Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So basically a given cooccurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to cooccur.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec')\n",
      "('But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('I will produce back out a vector maybe with a hundred thousand components in each neuron and that output Vector corresponds to a word as well.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So if I go into this tall skinny Matrix, and I grab the, you know entry, you know, the entry for a certain word that is going to give me a vector which is pretty small and', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So if I go into this tall skinny Matrix, and I grab the, you know entry, you know, the entry for a certain word that is going to give me a vector which is pretty small and', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13') ('Looks like you essentially feed in a word and it produces in the middle.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('It is XnumberX components and I can call that a word embedding for that for that word.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('It is XnumberX components and I can call that a word embedding for that for that word.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('And of course I did not have to pick it from the tall.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('I could have picked it from the short wide Matrix or I could even just decide to average these two vectors and use that as the embedding overall and there is actually a good reason to assume that these vectors would represent a decent amount of meaning.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('I could have picked it from the short wide Matrix or I could even just decide to average these two vectors and use that as the embedding overall and there is actually a good reason to assume that these vectors would represent a decent amount of meaning.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('Why do we expect that the middle layer when the word gets turned into a small Vector?', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So basically a given cooccurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to cooccur.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So basically a given cooccurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to cooccur.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So basically a given cooccurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to cooccur.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So now I will tell you a little bit about the particular method that I use to generate the word embeddings at the beginning of this video the method I used is known as glove which is short for Global vectors and it is a kind of cooccurrence decomposition method now, it is a little unique in that it decomposes the log the logarithm of the cooccurrence Matrix instead of the actual cooccurrence Matrix.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('And it also is weighted uses a model where certain entries in The cooccurrence Matrix are more important than others and you use gradient descent to learn the embedding.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('And it also is weighted uses a model where certain entries in The cooccurrence Matrix are more important than others and you use gradient descent to learn the embedding.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So I will have a link to the actual original word defect paper and the description if you want to read more about it.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So I really like glove I had a lot more fun implementing glove than I did implementing word Avec and I will certainly have a link to that paper in the description that describes glove.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('So I really like glove I had a lot more fun implementing glove than I did implementing word Avec and I will certainly have a link to that paper in the description that describes glove.', '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec') ('So I will have a link to the actual original word defect paper and the description if you want to read more about it.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0')\n",
      "('I painted the bench green something like that and already we can see that, you know, if a word can show up in this context it is likely to be a color but unfortunately that is not always true.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('So for example, if I put in a color it can tell me a bunch of other colors it never actually knew the idea of color going in it did not really know anything and piece together that all these words are related.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('I painted the bench green something like that and already we can see that, you know, if a word can show up in this context it is likely to be a color but unfortunately that is not always true.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('I painted the bench green something like that and already we can see that, you know, if a word can show up in this context it is likely to be a color but unfortunately that is not always true.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('It was yellow and I am going to feed as input the word yellow to the word Tyvek neural network, and I am going to try to get it as output to give me all the other words that were in the Tweet.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('I painted the bench green something like that and already we can see that, you know, if a word can show up in this context it is likely to be a color but unfortunately that is not always true.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('So the word Avec neural network in this case is just trying to picked context words from a word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there is another way that context can help us and that is if two words happen to always appear in the same context at once.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('So you are probably wondering how this actually worked because it is kind of baffling all I gave it was things that people typed on the internet.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there is another way that context can help us and that is if two words happen to always appear in the same context at once.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('So pretty much every word embedding algorithm uses the idea of context and to show you what I mean.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there is another way that context can help us and that is if two words happen to always appear in the same context at once.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('Looks like you essentially feed in a word and it produces in the middle.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there is another way that context can help us and that is if two words happen to always appear in the same context at once.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So here are three different sentences that will help us understand this idea.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('So here are three different sentences that will help us understand this idea.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('So here is an example of a sentence where there is a word missing.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('So here are three different sentences that will help us understand this idea.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('So here is a really simple picture of what a word to Veeck neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So here are three different sentences that will help us understand this idea.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So here are three different sentences that will help us understand this idea.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('So the word Avec neural network in this case is just trying to picked context words from a word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('And the first sentence I actually have two examples Donald and Trump are likely to appear together because ones the first name of a person and one is the last name of that same person.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('I fed it directly into a word embedding algorithm which was able to figure out tons of relationships between words just from the raw things that people happen to type on the internet.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('And the first sentence I actually have two examples Donald and Trump are likely to appear together because ones the first name of a person and one is the last name of that same person.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('And the first sentence I actually have two examples Donald and Trump are likely to appear together because ones the first name of a person and one is the last name of that same person.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('I mean I chose English, but I could have chosen any language and it still would have been just as successful.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('And the first sentence I actually have two examples Donald and Trump are likely to appear together because ones the first name of a person and one is the last name of that same person.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('What I have done is I have picked out a random tweet from my Corpus and I picked out a random word from within that tweet.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('We also have United States which is kind of just one logical.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('You know, how do I represent that for the neural network?', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Broken up into two smaller words, so United and states are likely to appear together and then in the second and third example, we have a joke and laugh are kind of related words you laugh at a joke.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('So they are also likely to appear in the same context.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('I am just going to give you a really simple example.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('And ideally since we knew nothing about English going in our algorithm.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('I fed it directly into a word embedding algorithm which was able to figure out tons of relationships between words just from the raw things that people happen to type on the internet.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('And ideally since we knew nothing about English going in our algorithm.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('Our word embedding is going to have to learn the different forms of the same word or related.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('Today I am going to be talking about word embeddings, which I think are one of the coolest things you can do with machine learning right now and to explain why I think that I am just going to jump right in and give you an example of something you can do with word weddings.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('Our word embedding is going to have to learn the different forms of the same word or related.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('So pretty much every word embedding algorithm uses the idea of context and to show you what I mean.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('Our word embedding is going to have to learn the different forms of the same word or related.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('So here is an example of a sentence where there is a word missing.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('Our word embedding is going to have to learn the different forms of the same word or related.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('So the first approach I am going to be talking about is known as word Tyvek and it is probably the most famous kind of word of matting because it was the first word embedding to get the kind of impressive results that word embeddings stateoftheart word embeddings get today.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Our word embedding is going to have to learn the different forms of the same word or related.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('Looks like you essentially feed in a word and it produces in the middle.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Our word embedding is going to have to learn the different forms of the same word or related.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Our word embedding is going to have to learn the different forms of the same word or related.', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('So the word Avec neural network in this case is just trying to picked context words from a word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('It has to learn that laughed is somehow related to laugh and what you can see is, you know, these examples give you an idea of how the model might be able to do that because you can see laughed appears with the word joke in the second sentence and', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('It has to learn that laughed is somehow related to laugh and what you can see is, you know, these examples give you an idea of how the model might be able to do that because you can see laughed appears with the word joke in the second sentence and', '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab') ('And in this video, I really want to explain why this works and how it works and just I think it is super cool.', '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0')\n",
      "('Blair with XnumberX neurons and then using more neural network magic.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('And if you have already seen my other videos on neural networks, you might actually already be able to implement or defect but I am just going to try to describe it here on a high level to give you an idea of how it works.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Blair with XnumberX neurons and then using more neural network magic.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So here is a really simple picture of what a word to Veeck neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('I will produce back out a vector maybe with a hundred thousand components in each neuron and that output Vector corresponds to a word as well.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('I will produce back out a vector maybe with a hundred thousand components in each neuron and that output Vector corresponds to a word as well.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So I take the neuron for whatever word I want to feed in and I will set that neuron to one and I will set all the other neurons to XnumberX then the neural network will just use regular neural network stuff to produce a small Vector.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('And I want every neuron to be set that is in the context and I want every neuron that was not in the context not to be set.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So essentially we are and devack is just a really simple neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('And I want every neuron to be set that is in the context and I want every neuron that was not in the context not to be set.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('And if you have already seen my other videos on neural networks, you might actually already be able to implement or defect but I am just going to try to describe it here on a high level to give you an idea of how it works.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('And I want every neuron to be set that is in the context and I want every neuron that was not in the context not to be set.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('You know, how do I represent that for the neural network?', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('And I want every neuron to be set that is in the context and I want every neuron that was not in the context not to be set.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('Well, basically the network has a different input neuron for each different word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Why do we expect that the middle layer when the word gets turned into a small Vector?', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('Well, basically the network has a different input neuron for each different word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Why do we expect that small Vector to actually be meeting?', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('You know, how are we going to produce the set of vectors that work?', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Well, the answer is that that small Vector is all the network has to figure out the context.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('You know, how are we going to produce the set of vectors that work?', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Well, the answer is that that small Vector is all the network has to figure out the context.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('Well, the answer is that that small Vector is all the network has to figure out the context.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('Well, basically the network has a different input neuron for each different word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So, you know, it goes right from that small Vector to the context.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('You know, how are we going to produce the set of vectors that work?', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So, you know, it goes right from that small Vector to the context.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('Looks like you essentially feed in a word and it produces in the middle.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So, you know, it goes right from that small Vector to the context.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So here is a really simple picture of what a word to Veeck neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So if two words have very similar contexts, it is really helpful for the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So the word Avec neural network in this case is just trying to picked context words from a word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So essentially this model is just forcing the middle layer of the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('And if you have already seen my other videos on neural networks, you might actually already be able to implement or defect but I am just going to try to describe it here on a high level to give you an idea of how it works.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So essentially this model is just forcing the middle layer of the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So here is a really simple picture of what a word to Veeck neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So essentially this model is just forcing the middle layer of the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('I am actually going to give an example of something we might ask a word Tyvek neural network to do.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So essentially this model is just forcing the middle layer of the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('You know, how do I represent that for the neural network?', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So essentially this model is just forcing the middle layer of the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('Well, basically the network has a different input neuron for each different word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So essentially this model is just forcing the middle layer of the neural network.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So I take the neuron for whatever word I want to feed in and I will set that neuron to one and I will set all the other neurons to XnumberX then the neural network will just use regular neural network stuff to produce a small Vector.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So the word Avec neural network in this case is just trying to picked context words from a word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So that is kind of just a really General overview of how weird to deck works and there is a lot more to it.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So essentially we are and devack is just a really simple neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So that is kind of just a really General overview of how weird to deck works and there is a lot more to it.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('And if you have already seen my other videos on neural networks, you might actually already be able to implement or defect but I am just going to try to describe it here on a high level to give you an idea of how it works.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So I will have a link to the actual original word defect paper and the description if you want to read more about it.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So here is a really simple picture of what a word to Veeck neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So I will have a link to the actual original word defect paper and the description if you want to read more about it.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('It was yellow and I am going to feed as input the word yellow to the word Tyvek neural network, and I am going to try to get it as output to give me all the other words that were in the Tweet.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So the first approach I am going to be talking about is known as word Tyvek and it is probably the most famous kind of word of matting because it was the first word embedding to get the kind of impressive results that word embeddings stateoftheart word embeddings get today.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So here is a really simple picture of what a word to Veeck neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So the word Avec neural network in this case is just trying to picked context words from a word.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "('So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.', '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0') ('So here is a really simple picture of what a word to Veeck neural network.', '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c')\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 390, \"module\": \"grouper_segments\", \"outlier threshold is : \": 0.47543230652809143, \"ts\": \"2020-03-05T08:26:31.165518Z\", \"msg\": \"Outlier Score\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Community Algorithm\n",
      "cluster before alteration=========>\n",
      "So ideally the word embedding would figure out then that laughing laughter related since they are both related to joke.\n",
      "So that is kind of where a word embedding gets its Knowledge from it learns things via context.\n",
      "It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.\n",
      "We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they are very related words.\n",
      "Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger.\n",
      "I want the vector for Ferrari to be far away from The vector for hamburger and of course all these distances are relative, but you can kind of see what I mean that we want the closeness of these vectors to resemble the closeness of the words that they represent and in addition to this kind of idea of closeness.\n",
      "For example, if I do the vector from Man  the vector for woman and to subtract vectors, we just subtract each number from the corresponding number.\n",
      "So if I take the vector from man and I subtract the vector for woman, I want that to somehow represent the difference between male and female and then if I add that Vector to the vector for for Queen, I want it to give me out something very very close to the vector for King.\n",
      "So I you know, I want these vectors to be related and I want the differences between vectors to also carry some meaning and you know, I might add other constraints but the idea is I just want to encode as\n",
      "cluster before alteration=========>\n",
      "Broken up into two smaller words, so United and states are likely to appear together and then in the second and third example, we have a joke and laugh are kind of related words you laugh at a joke.\n",
      "Now, there is one subtle thing that I would like to point out in this example, which is that laughed and laughed our might be different words, like laughs is the present tense and laughed as the past tense and likewise, you know, we could think about joke.\n",
      "It has to learn that laughed is somehow related to laugh and what you can see is, you know, these examples give you an idea of how the model might be able to do that because you can see laughed appears with the word joke in the second sentence and\n",
      "You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there is another way that context can help us and that is if two words happen to always appear in the same context at once.\n",
      "Our word embedding is going to have to learn the different forms of the same word or related.\n",
      "I painted the bench green something like that and already we can see that, you know, if a word can show up in this context it is likely to be a color but unfortunately that is not always true.\n",
      "So here are three different sentences that will help us understand this idea.\n",
      "And the first sentence I actually have two examples Donald and Trump are likely to appear together because ones the first name of a person and one is the last name of that same person.\n",
      "We also have United States which is kind of just one logical.\n",
      "So they are also likely to appear in the same context.\n",
      "And ideally since we knew nothing about English going in our algorithm.\n",
      "cluster before alteration=========>\n",
      "You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.\n",
      "Today I am going to be talking about word embeddings, which I think are one of the coolest things you can do with machine learning right now and to explain why I think that I am just going to jump right in and give you an example of something you can do with word weddings.\n",
      "So for example, if I put in a color it can tell me a bunch of other colors it never actually knew the idea of color going in it did not really know anything and piece together that all these words are related.\n",
      "So pretty much every word embedding algorithm uses the idea of context and to show you what I mean.\n",
      "So you are probably wondering how this actually worked because it is kind of baffling all I gave it was things that people typed on the internet.\n",
      "And in this video, I really want to explain why this works and how it works and just I think it is super cool.\n",
      "So it says I painted the bench blank and were expected to fill in the blank.\n",
      "I am just going to give you a really simple example.\n",
      "So here is an example of a sentence where there is a word missing.\n",
      "I fed it directly into a word embedding algorithm which was able to figure out tons of relationships between words just from the raw things that people happen to type on the internet.\n",
      "I mean I chose English, but I could have chosen any language and it still would have been just as successful.\n",
      "So a few months ago, I set up a program that just downloads tweets a people write tweets on Twitter and saves them to a file and after running this program from there to I had collected a massive file with over five gigabytes of tweets after I can press is it so I got a massive amount of data and it is just raw data that people typed on the internet after getting all this data.\n",
      "So it is pretty impressive that it was able to do this just from raw text.\n",
      "cluster before alteration=========>\n",
      "So if two words have very similar contexts, it is really helpful for the neural network.\n",
      "So, you know, it goes right from that small Vector to the context.\n",
      "To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.\n",
      "So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.\n",
      "So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.\n",
      "If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense.\n",
      "So that is kind of just a really General overview of how weird to deck works and there is a lot more to it.\n",
      "I will produce back out a vector maybe with a hundred thousand components in each neuron and that output Vector corresponds to a word as well.\n",
      "Why do we expect that small Vector to actually be meeting?\n",
      "Why do we expect that the middle layer when the word gets turned into a small Vector?\n",
      "So I will have a link to the actual original word defect paper and the description if you want to read more about it.\n",
      "So essentially this model is just forcing the middle layer of the neural network.\n",
      "Blair with XnumberX neurons and then using more neural network magic.\n",
      "And I want every neuron to be set that is in the context and I want every neuron that was not in the context not to be set.\n",
      "Well, the answer is that that small Vector is all the network has to figure out the context.\n",
      "For example with Twitter data, we might just Loop through all the tweets go through all the words and all those tweets and for every time two words occur in the same.\n",
      "cluster before alteration=========>\n",
      "So the word Avec neural network in this case is just trying to picked context words from a word.\n",
      "So here is a really simple picture of what a word to Veeck neural network.\n",
      "Looks like you essentially feed in a word and it produces in the middle.\n",
      "It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.\n",
      "So the first approach I am going to be talking about is known as word Tyvek and it is probably the most famous kind of word of matting because it was the first word embedding to get the kind of impressive results that word embeddings stateoftheart word embeddings get today.\n",
      "You know, how are we going to produce the set of vectors that work?\n",
      "So essentially we are and devack is just a really simple neural network.\n",
      "And if you have already seen my other videos on neural networks, you might actually already be able to implement or defect but I am just going to try to describe it here on a high level to give you an idea of how it works.\n",
      "So I take the neuron for whatever word I want to feed in and I will set that neuron to one and I will set all the other neurons to XnumberX then the neural network will just use regular neural network stuff to produce a small Vector.\n",
      "It was yellow and I am going to feed as input the word yellow to the word Tyvek neural network, and I am going to try to get it as output to give me all the other words that were in the Tweet.\n",
      "What I have done is I have picked out a random tweet from my Corpus and I picked out a random word from within that tweet.\n",
      "You know, how do I represent that for the neural network?\n",
      "Well, basically the network has a different input neuron for each different word.\n",
      "I am actually going to give an example of something we might ask a word Tyvek neural network to do.\n",
      "How are we going to you know solve for vectors for all of the words that ever appear on Twitter?\n",
      "So how exactly is it that I feed in the word yellow and I get out all these contacts words.\n",
      "cluster before alteration=========>\n",
      "I could have picked it from the short wide Matrix or I could even just decide to average these two vectors and use that as the embedding overall and there is actually a good reason to assume that these vectors would represent a decent amount of meaning.\n",
      "So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings.\n",
      "So basically a given cooccurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to cooccur.\n",
      "Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.\n",
      "So it is similar to training a neural network and it has really good results and it is extremely fast.\n",
      "It is XnumberX components and I can call that a word embedding for that for that word.\n",
      "And of course I did not have to pick it from the tall.\n",
      "So now I will tell you a little bit about the particular method that I use to generate the word embeddings at the beginning of this video the method I used is known as glove which is short for Global vectors and it is a kind of cooccurrence decomposition method now, it is a little unique in that it decomposes the log the logarithm of the cooccurrence Matrix instead of the actual cooccurrence Matrix.\n",
      "And it also is weighted uses a model where certain entries in The cooccurrence Matrix are more important than others and you use gradient descent to learn the embedding.\n",
      "So I really like glove I had a lot more fun implementing glove than I did implementing word Avec and I will certainly have a link to that paper in the description that describes glove.\n",
      "cluster before alteration=========>\n",
      "But pretty much all of the methods utilized some amount of linear algebra.\n",
      "So I am going to be just talking about matrices and Matrix multiplications dot products things like that.\n",
      "And if you do not know linear algebra Youre Not Really Gonna Get Much from this but that is why I kind of left it at the end of the video because some people will get something out of this and I think it is somewhat interesting.\n",
      "So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.\n",
      "You can basically see that I get this massive Square Matrix, which is our cooccurrence matrix by multiplying a tall skinny Matrix and a short wide Matrix, you know, you think about how many entries are in the big Matrix?\n",
      "That is stored on the right side of this equation, which is you know, two relatively small matrices multiplied together.\n",
      "So by decomposing this big cooccurrence Matrix into these smaller ones were clearly compressing some information and in doing so hopefully we have to extract a lot of meaning from The Matrix so that we can do that compression and that should allow us to at least generate decent embeddings.\n",
      "Once we have this Matrix decomposition, which I have not described exactly how we might find this yet, but you could imagine there is Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that.\n",
      "But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.\n",
      "So if I go into this tall skinny Matrix, and I grab the, you know entry, you know, the entry for a certain word that is going to give me a vector which is pretty small and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "So ideally the word embedding would figure out then that laughing laughter related since they are both related to joke.\n",
      "So that is kind of where a word embedding gets its Knowledge from it learns things via context.\n",
      "It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.\n",
      "We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they are very related words.\n",
      "Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger.\n",
      "I want the vector for Ferrari to be far away from The vector for hamburger and of course all these distances are relative, but you can kind of see what I mean that we want the closeness of these vectors to resemble the closeness of the words that they represent and in addition to this kind of idea of closeness.\n",
      "For example, if I do the vector from Man  the vector for woman and to subtract vectors, we just subtract each number from the corresponding number.\n",
      "So if I take the vector from man and I subtract the vector for woman, I want that to somehow represent the difference between male and female and then if I add that Vector to the vector for for Queen, I want it to give me out something very very close to the vector for King.\n",
      "So I you know, I want these vectors to be related and I want the differences between vectors to also carry some meaning and you know, I might add other constraints but the idea is I just want to encode as\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Broken up into two smaller words, so United and states are likely to appear together and then in the second and third example, we have a joke and laugh are kind of related words you laugh at a joke.\n",
      "Now, there is one subtle thing that I would like to point out in this example, which is that laughed and laughed our might be different words, like laughs is the present tense and laughed as the past tense and likewise, you know, we could think about joke.\n",
      "It has to learn that laughed is somehow related to laugh and what you can see is, you know, these examples give you an idea of how the model might be able to do that because you can see laughed appears with the word joke in the second sentence and\n",
      "You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there is another way that context can help us and that is if two words happen to always appear in the same context at once.\n",
      "Our word embedding is going to have to learn the different forms of the same word or related.\n",
      "I painted the bench green something like that and already we can see that, you know, if a word can show up in this context it is likely to be a color but unfortunately that is not always true.\n",
      "So here are three different sentences that will help us understand this idea.\n",
      "And the first sentence I actually have two examples Donald and Trump are likely to appear together because ones the first name of a person and one is the last name of that same person.\n",
      "We also have United States which is kind of just one logical.\n",
      "So they are also likely to appear in the same context.\n",
      "And ideally since we knew nothing about English going in our algorithm.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.\n",
      "Today I am going to be talking about word embeddings, which I think are one of the coolest things you can do with machine learning right now and to explain why I think that I am just going to jump right in and give you an example of something you can do with word weddings.\n",
      "So for example, if I put in a color it can tell me a bunch of other colors it never actually knew the idea of color going in it did not really know anything and piece together that all these words are related.\n",
      "So pretty much every word embedding algorithm uses the idea of context and to show you what I mean.\n",
      "So you are probably wondering how this actually worked because it is kind of baffling all I gave it was things that people typed on the internet.\n",
      "And in this video, I really want to explain why this works and how it works and just I think it is super cool.\n",
      "So it says I painted the bench blank and were expected to fill in the blank.\n",
      "I am just going to give you a really simple example.\n",
      "So here is an example of a sentence where there is a word missing.\n",
      "I fed it directly into a word embedding algorithm which was able to figure out tons of relationships between words just from the raw things that people happen to type on the internet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I mean I chose English, but I could have chosen any language and it still would have been just as successful.\n",
      "So a few months ago, I set up a program that just downloads tweets a people write tweets on Twitter and saves them to a file and after running this program from there to I had collected a massive file with over five gigabytes of tweets after I can press is it so I got a massive amount of data and it is just raw data that people typed on the internet after getting all this data.\n",
      "So it is pretty impressive that it was able to do this just from raw text.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "So if two words have very similar contexts, it is really helpful for the neural network.\n",
      "So, you know, it goes right from that small Vector to the context.\n",
      "To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.\n",
      "So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.\n",
      "So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.\n",
      "If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense.\n",
      "So that is kind of just a really General overview of how weird to deck works and there is a lot more to it.\n",
      "I will produce back out a vector maybe with a hundred thousand components in each neuron and that output Vector corresponds to a word as well.\n",
      "Why do we expect that small Vector to actually be meeting?\n",
      "Why do we expect that the middle layer when the word gets turned into a small Vector?\n",
      "So I will have a link to the actual original word defect paper and the description if you want to read more about it.\n",
      "So essentially this model is just forcing the middle layer of the neural network.\n",
      "Blair with XnumberX neurons and then using more neural network magic.\n",
      "And I want every neuron to be set that is in the context and I want every neuron that was not in the context not to be set.\n",
      "Well, the answer is that that small Vector is all the network has to figure out the context.\n",
      "For example with Twitter data, we might just Loop through all the tweets go through all the words and all those tweets and for every time two words occur in the same.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "So the word Avec neural network in this case is just trying to picked context words from a word.\n",
      "So here is a really simple picture of what a word to Veeck neural network.\n",
      "Looks like you essentially feed in a word and it produces in the middle.\n",
      "It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.\n",
      "So the first approach I am going to be talking about is known as word Tyvek and it is probably the most famous kind of word of matting because it was the first word embedding to get the kind of impressive results that word embeddings stateoftheart word embeddings get today.\n",
      "You know, how are we going to produce the set of vectors that work?\n",
      "So essentially we are and devack is just a really simple neural network.\n",
      "And if you have already seen my other videos on neural networks, you might actually already be able to implement or defect but I am just going to try to describe it here on a high level to give you an idea of how it works.\n",
      "So I take the neuron for whatever word I want to feed in and I will set that neuron to one and I will set all the other neurons to XnumberX then the neural network will just use regular neural network stuff to produce a small Vector.\n",
      "It was yellow and I am going to feed as input the word yellow to the word Tyvek neural network, and I am going to try to get it as output to give me all the other words that were in the Tweet.\n",
      "What I have done is I have picked out a random tweet from my Corpus and I picked out a random word from within that tweet.\n",
      "You know, how do I represent that for the neural network?\n",
      "Well, basically the network has a different input neuron for each different word.\n",
      "I am actually going to give an example of something we might ask a word Tyvek neural network to do.\n",
      "How are we going to you know solve for vectors for all of the words that ever appear on Twitter?\n",
      "So how exactly is it that I feed in the word yellow and I get out all these contacts words.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "I could have picked it from the short wide Matrix or I could even just decide to average these two vectors and use that as the embedding overall and there is actually a good reason to assume that these vectors would represent a decent amount of meaning.\n",
      "So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings.\n",
      "So basically a given cooccurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to cooccur.\n",
      "Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.\n",
      "So it is similar to training a neural network and it has really good results and it is extremely fast.\n",
      "It is XnumberX components and I can call that a word embedding for that for that word.\n",
      "And of course I did not have to pick it from the tall.\n",
      "So now I will tell you a little bit about the particular method that I use to generate the word embeddings at the beginning of this video the method I used is known as glove which is short for Global vectors and it is a kind of cooccurrence decomposition method now, it is a little unique in that it decomposes the log the logarithm of the cooccurrence Matrix instead of the actual cooccurrence Matrix.\n",
      "And it also is weighted uses a model where certain entries in The cooccurrence Matrix are more important than others and you use gradient descent to learn the embedding.\n",
      "So I really like glove I had a lot more fun implementing glove than I did implementing word Avec and I will certainly have a link to that paper in the description that describes glove.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "But pretty much all of the methods utilized some amount of linear algebra.\n",
      "So I am going to be just talking about matrices and Matrix multiplications dot products things like that.\n",
      "And if you do not know linear algebra Youre Not Really Gonna Get Much from this but that is why I kind of left it at the end of the video because some people will get something out of this and I think it is somewhat interesting.\n",
      "So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.\n",
      "You can basically see that I get this massive Square Matrix, which is our cooccurrence matrix by multiplying a tall skinny Matrix and a short wide Matrix, you know, you think about how many entries are in the big Matrix?\n",
      "That is stored on the right side of this equation, which is you know, two relatively small matrices multiplied together.\n",
      "So by decomposing this big cooccurrence Matrix into these smaller ones were clearly compressing some information and in doing so hopefully we have to extract a lot of meaning from The Matrix so that we can do that compression and that should allow us to at least generate decent embeddings.\n",
      "Once we have this Matrix decomposition, which I have not described exactly how we might find this yet, but you could imagine there is Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that.\n",
      "But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.\n",
      "So if I go into this tall skinny Matrix, and I grab the, you know entry, you know, the entry for a certain word that is going to give me a vector which is pretty small and\n",
      "\n",
      "\n",
      "Checking Timerange --------------\n",
      "--------------\n",
      "Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger. 39cb0782-85d4-4e1b-ad99-ca933adf62b5 \n",
      "\n",
      "We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they are very related words. 39cb0782-85d4-4e1b-ad99-ca933adf62b5 \n",
      "\n",
      "So that is kind of where a word embedding gets its Knowledge from it learns things via context. 39cb0782-85d4-4e1b-ad99-ca933adf62b5 \n",
      "\n",
      "So if I take the vector from man and I subtract the vector for woman, I want that to somehow represent the difference between male and female and then if I add that Vector to the vector for for Queen, I want it to give me out something very very close to the vector for King. 39cb0782-85d4-4e1b-ad99-ca933adf62b5 \n",
      "\n",
      "For example, if I do the vector from Man  the vector for woman and to subtract vectors, we just subtract each number from the corresponding number. 39cb0782-85d4-4e1b-ad99-ca933adf62b5 \n",
      "\n",
      "So I you know, I want these vectors to be related and I want the differences between vectors to also carry some meaning and you know, I might add other constraints but the idea is I just want to encode as 39cb0782-85d4-4e1b-ad99-ca933adf62b5 \n",
      "\n",
      "So ideally the word embedding would figure out then that laughing laughter related since they are both related to joke. 39cb0782-85d4-4e1b-ad99-ca933adf62b5 \n",
      "\n",
      "It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good. 39cb0782-85d4-4e1b-ad99-ca933adf62b5 \n",
      "\n",
      "I want the vector for Ferrari to be far away from The vector for hamburger and of course all these distances are relative, but you can kind of see what I mean that we want the closeness of these vectors to resemble the closeness of the words that they represent and in addition to this kind of idea of closeness. 39cb0782-85d4-4e1b-ad99-ca933adf62b5 \n",
      "\n",
      "--------------\n",
      "It has to learn that laughed is somehow related to laugh and what you can see is, you know, these examples give you an idea of how the model might be able to do that because you can see laughed appears with the word joke in the second sentence and 7162f0aa-7409-4369-93b8-bf0b0a6c6bab \n",
      "\n",
      "Our word embedding is going to have to learn the different forms of the same word or related. 7162f0aa-7409-4369-93b8-bf0b0a6c6bab \n",
      "\n",
      "Now, there is one subtle thing that I would like to point out in this example, which is that laughed and laughed our might be different words, like laughs is the present tense and laughed as the past tense and likewise, you know, we could think about joke. 7162f0aa-7409-4369-93b8-bf0b0a6c6bab \n",
      "\n",
      "And the first sentence I actually have two examples Donald and Trump are likely to appear together because ones the first name of a person and one is the last name of that same person. 7162f0aa-7409-4369-93b8-bf0b0a6c6bab \n",
      "\n",
      "So here are three different sentences that will help us understand this idea. 7162f0aa-7409-4369-93b8-bf0b0a6c6bab \n",
      "\n",
      "And ideally since we knew nothing about English going in our algorithm. 7162f0aa-7409-4369-93b8-bf0b0a6c6bab \n",
      "\n",
      "So they are also likely to appear in the same context. 7162f0aa-7409-4369-93b8-bf0b0a6c6bab \n",
      "\n",
      "Broken up into two smaller words, so United and states are likely to appear together and then in the second and third example, we have a joke and laugh are kind of related words you laugh at a joke. 7162f0aa-7409-4369-93b8-bf0b0a6c6bab \n",
      "\n",
      "You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there is another way that context can help us and that is if two words happen to always appear in the same context at once. 7162f0aa-7409-4369-93b8-bf0b0a6c6bab \n",
      "\n",
      "I painted the bench green something like that and already we can see that, you know, if a word can show up in this context it is likely to be a color but unfortunately that is not always true. 7162f0aa-7409-4369-93b8-bf0b0a6c6bab \n",
      "\n",
      "We also have United States which is kind of just one logical. 7162f0aa-7409-4369-93b8-bf0b0a6c6bab \n",
      "\n",
      "--------------\n",
      "I am just going to give you a really simple example. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "So pretty much every word embedding algorithm uses the idea of context and to show you what I mean. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "So a few months ago, I set up a program that just downloads tweets a people write tweets on Twitter and saves them to a file and after running this program from there to I had collected a massive file with over five gigabytes of tweets after I can press is it so I got a massive amount of data and it is just raw data that people typed on the internet after getting all this data. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "I fed it directly into a word embedding algorithm which was able to figure out tons of relationships between words just from the raw things that people happen to type on the internet. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "So for example, if I put in a color it can tell me a bunch of other colors it never actually knew the idea of color going in it did not really know anything and piece together that all these words are related. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "I mean I chose English, but I could have chosen any language and it still would have been just as successful. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "So you are probably wondering how this actually worked because it is kind of baffling all I gave it was things that people typed on the internet. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "So it says I painted the bench blank and were expected to fill in the blank. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "So it is pretty impressive that it was able to do this just from raw text. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "So here is an example of a sentence where there is a word missing. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "Today I am going to be talking about word embeddings, which I think are one of the coolest things you can do with machine learning right now and to explain why I think that I am just going to jump right in and give you an example of something you can do with word weddings. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "And in this video, I really want to explain why this works and how it works and just I think it is super cool. d3b26266-0fcd-4c48-9d10-b07f3caba0f0 \n",
      "\n",
      "--------------\n",
      "If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "So, you know, it goes right from that small Vector to the context. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "Why do we expect that small Vector to actually be meeting? e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "For example with Twitter data, we might just Loop through all the tweets go through all the words and all those tweets and for every time two words occur in the same. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "So I will have a link to the actual original word defect paper and the description if you want to read more about it. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "Why do we expect that the middle layer when the word gets turned into a small Vector? e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "So essentially this model is just forcing the middle layer of the neural network. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "So if two words have very similar contexts, it is really helpful for the neural network. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "And I want every neuron to be set that is in the context and I want every neuron that was not in the context not to be set. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "So that is kind of just a really General overview of how weird to deck works and there is a lot more to it. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "Well, the answer is that that small Vector is all the network has to figure out the context. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "I will produce back out a vector maybe with a hundred thousand components in each neuron and that output Vector corresponds to a word as well. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "Blair with XnumberX neurons and then using more neural network magic. e1b6ba78-88cf-441e-833c-60ba230190a0 \n",
      "\n",
      "--------------\n",
      "Looks like you essentially feed in a word and it produces in the middle. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "So the word Avec neural network in this case is just trying to picked context words from a word. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "You know, how do I represent that for the neural network? f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "So how exactly is it that I feed in the word yellow and I get out all these contacts words. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "So here is a really simple picture of what a word to Veeck neural network. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "And if you have already seen my other videos on neural networks, you might actually already be able to implement or defect but I am just going to try to describe it here on a high level to give you an idea of how it works. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "So essentially we are and devack is just a really simple neural network. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "How are we going to you know solve for vectors for all of the words that ever appear on Twitter? f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "I am actually going to give an example of something we might ask a word Tyvek neural network to do. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "What I have done is I have picked out a random tweet from my Corpus and I picked out a random word from within that tweet. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "It was yellow and I am going to feed as input the word yellow to the word Tyvek neural network, and I am going to try to get it as output to give me all the other words that were in the Tweet. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "So the first approach I am going to be talking about is known as word Tyvek and it is probably the most famous kind of word of matting because it was the first word embedding to get the kind of impressive results that word embeddings stateoftheart word embeddings get today. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "So I take the neuron for whatever word I want to feed in and I will set that neuron to one and I will set all the other neurons to XnumberX then the neural network will just use regular neural network stuff to produce a small Vector. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "Well, basically the network has a different input neuron for each different word. f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "You know, how are we going to produce the set of vectors that work? f3cc5613-f30d-4bce-948a-1a1d991c0c3c \n",
      "\n",
      "--------------\n",
      "And of course I did not have to pick it from the tall. 49d74149-4a6c-4d9c-83dd-11b8fd5142ec \n",
      "\n",
      "So now I will tell you a little bit about the particular method that I use to generate the word embeddings at the beginning of this video the method I used is known as glove which is short for Global vectors and it is a kind of cooccurrence decomposition method now, it is a little unique in that it decomposes the log the logarithm of the cooccurrence Matrix instead of the actual cooccurrence Matrix. 49d74149-4a6c-4d9c-83dd-11b8fd5142ec \n",
      "\n",
      "So basically a given cooccurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to cooccur. 49d74149-4a6c-4d9c-83dd-11b8fd5142ec \n",
      "\n",
      "So I really like glove I had a lot more fun implementing glove than I did implementing word Avec and I will certainly have a link to that paper in the description that describes glove. 49d74149-4a6c-4d9c-83dd-11b8fd5142ec \n",
      "\n",
      "So it is similar to training a neural network and it has really good results and it is extremely fast. 49d74149-4a6c-4d9c-83dd-11b8fd5142ec \n",
      "\n",
      "So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings. 49d74149-4a6c-4d9c-83dd-11b8fd5142ec \n",
      "\n",
      "I could have picked it from the short wide Matrix or I could even just decide to average these two vectors and use that as the embedding overall and there is actually a good reason to assume that these vectors would represent a decent amount of meaning. 49d74149-4a6c-4d9c-83dd-11b8fd5142ec \n",
      "\n",
      "And it also is weighted uses a model where certain entries in The cooccurrence Matrix are more important than others and you use gradient descent to learn the embedding. 49d74149-4a6c-4d9c-83dd-11b8fd5142ec \n",
      "\n",
      "Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix. 49d74149-4a6c-4d9c-83dd-11b8fd5142ec \n",
      "\n",
      "It is XnumberX components and I can call that a word embedding for that for that word. 49d74149-4a6c-4d9c-83dd-11b8fd5142ec \n",
      "\n",
      "--------------\n",
      "So I am going to be just talking about matrices and Matrix multiplications dot products things like that. 3b23e820-dc5a-41ef-a6d8-9fa398f84c13 \n",
      "\n",
      "So if I go into this tall skinny Matrix, and I grab the, you know entry, you know, the entry for a certain word that is going to give me a vector which is pretty small and 3b23e820-dc5a-41ef-a6d8-9fa398f84c13 \n",
      "\n",
      "And if you do not know linear algebra Youre Not Really Gonna Get Much from this but that is why I kind of left it at the end of the video because some people will get something out of this and I think it is somewhat interesting. 3b23e820-dc5a-41ef-a6d8-9fa398f84c13 \n",
      "\n",
      "But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word. 3b23e820-dc5a-41ef-a6d8-9fa398f84c13 \n",
      "\n",
      "So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices. 3b23e820-dc5a-41ef-a6d8-9fa398f84c13 \n",
      "\n",
      "That is stored on the right side of this equation, which is you know, two relatively small matrices multiplied together. 3b23e820-dc5a-41ef-a6d8-9fa398f84c13 \n",
      "\n",
      "But pretty much all of the methods utilized some amount of linear algebra. 3b23e820-dc5a-41ef-a6d8-9fa398f84c13 \n",
      "\n",
      "You can basically see that I get this massive Square Matrix, which is our cooccurrence matrix by multiplying a tall skinny Matrix and a short wide Matrix, you know, you think about how many entries are in the big Matrix? 3b23e820-dc5a-41ef-a6d8-9fa398f84c13 \n",
      "\n",
      "Once we have this Matrix decomposition, which I have not described exactly how we might find this yet, but you could imagine there is Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that. 3b23e820-dc5a-41ef-a6d8-9fa398f84c13 \n",
      "\n",
      "So by decomposing this big cooccurrence Matrix into these smaller ones were clearly compressing some information and in doing so hopefully we have to extract a lot of meaning from The Matrix so that we can do that compression and that should allow us to at least generate decent embeddings. 3b23e820-dc5a-41ef-a6d8-9fa398f84c13 \n",
      "\n",
      "<---------------->\n",
      "order difference: 0\n",
      "Relevant sentence:  Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger.    =====    We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they are very related words.\n",
      "order difference: 0\n",
      "Relevant sentence:  We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they are very related words.    =====    So that is kind of where a word embedding gets its Knowledge from it learns things via context.\n",
      "order difference: 0\n",
      "Relevant sentence:  So that is kind of where a word embedding gets its Knowledge from it learns things via context.    =====    So if I take the vector from man and I subtract the vector for woman, I want that to somehow represent the difference between male and female and then if I add that Vector to the vector for for Queen, I want it to give me out something very very close to the vector for King.\n",
      "order difference: 0\n",
      "Relevant sentence:  So if I take the vector from man and I subtract the vector for woman, I want that to somehow represent the difference between male and female and then if I add that Vector to the vector for for Queen, I want it to give me out something very very close to the vector for King.    =====    For example, if I do the vector from Man  the vector for woman and to subtract vectors, we just subtract each number from the corresponding number.\n",
      "order difference: 0\n",
      "Relevant sentence:  For example, if I do the vector from Man  the vector for woman and to subtract vectors, we just subtract each number from the corresponding number.    =====    So I you know, I want these vectors to be related and I want the differences between vectors to also carry some meaning and you know, I might add other constraints but the idea is I just want to encode as\n",
      "order difference: 0\n",
      "Relevant sentence:  So I you know, I want these vectors to be related and I want the differences between vectors to also carry some meaning and you know, I might add other constraints but the idea is I just want to encode as    =====    So ideally the word embedding would figure out then that laughing laughter related since they are both related to joke.\n",
      "order difference: 0\n",
      "Relevant sentence:  So ideally the word embedding would figure out then that laughing laughter related since they are both related to joke.    =====    It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.\n",
      "order difference: 0\n",
      "Relevant sentence:  It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we are after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say XnumberX numbers and those numbers would describe the word and forward embedding to be good.    =====    I want the vector for Ferrari to be far away from The vector for hamburger and of course all these distances are relative, but you can kind of see what I mean that we want the closeness of these vectors to resemble the closeness of the words that they represent and in addition to this kind of idea of closeness.\n",
      "order difference: 0\n",
      "Relevant sentence:  It has to learn that laughed is somehow related to laugh and what you can see is, you know, these examples give you an idea of how the model might be able to do that because you can see laughed appears with the word joke in the second sentence and    =====    Our word embedding is going to have to learn the different forms of the same word or related.\n",
      "order difference: 0\n",
      "Relevant sentence:  Our word embedding is going to have to learn the different forms of the same word or related.    =====    Now, there is one subtle thing that I would like to point out in this example, which is that laughed and laughed our might be different words, like laughs is the present tense and laughed as the past tense and likewise, you know, we could think about joke.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now, there is one subtle thing that I would like to point out in this example, which is that laughed and laughed our might be different words, like laughs is the present tense and laughed as the past tense and likewise, you know, we could think about joke.    =====    And the first sentence I actually have two examples Donald and Trump are likely to appear together because ones the first name of a person and one is the last name of that same person.\n",
      "order difference: 0\n",
      "Relevant sentence:  And the first sentence I actually have two examples Donald and Trump are likely to appear together because ones the first name of a person and one is the last name of that same person.    =====    So here are three different sentences that will help us understand this idea.\n",
      "order difference: 0\n",
      "Relevant sentence:  So here are three different sentences that will help us understand this idea.    =====    And ideally since we knew nothing about English going in our algorithm.\n",
      "order difference: 0\n",
      "Relevant sentence:  And ideally since we knew nothing about English going in our algorithm.    =====    So they are also likely to appear in the same context.\n",
      "order difference: 0\n",
      "Relevant sentence:  So they are also likely to appear in the same context.    =====    Broken up into two smaller words, so United and states are likely to appear together and then in the second and third example, we have a joke and laugh are kind of related words you laugh at a joke.\n",
      "order difference: 0\n",
      "Relevant sentence:  Broken up into two smaller words, so United and states are likely to appear together and then in the second and third example, we have a joke and laugh are kind of related words you laugh at a joke.    =====    You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there is another way that context can help us and that is if two words happen to always appear in the same context at once.\n",
      "order difference: 0\n",
      "Relevant sentence:  You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there is another way that context can help us and that is if two words happen to always appear in the same context at once.    =====    I painted the bench green something like that and already we can see that, you know, if a word can show up in this context it is likely to be a color but unfortunately that is not always true.\n",
      "order difference: 0\n",
      "Relevant sentence:  I painted the bench green something like that and already we can see that, you know, if a word can show up in this context it is likely to be a color but unfortunately that is not always true.    =====    We also have United States which is kind of just one logical.\n",
      "order difference: 0\n",
      "Relevant sentence:  I am just going to give you a really simple example.    =====    So pretty much every word embedding algorithm uses the idea of context and to show you what I mean.\n",
      "order difference: 0\n",
      "Relevant sentence:  So pretty much every word embedding algorithm uses the idea of context and to show you what I mean.    =====    So a few months ago, I set up a program that just downloads tweets a people write tweets on Twitter and saves them to a file and after running this program from there to I had collected a massive file with over five gigabytes of tweets after I can press is it so I got a massive amount of data and it is just raw data that people typed on the internet after getting all this data.\n",
      "order difference: 0\n",
      "Relevant sentence:  So a few months ago, I set up a program that just downloads tweets a people write tweets on Twitter and saves them to a file and after running this program from there to I had collected a massive file with over five gigabytes of tweets after I can press is it so I got a massive amount of data and it is just raw data that people typed on the internet after getting all this data.    =====    I fed it directly into a word embedding algorithm which was able to figure out tons of relationships between words just from the raw things that people happen to type on the internet.\n",
      "order difference: 0\n",
      "Relevant sentence:  I fed it directly into a word embedding algorithm which was able to figure out tons of relationships between words just from the raw things that people happen to type on the internet.    =====    So for example, if I put in a color it can tell me a bunch of other colors it never actually knew the idea of color going in it did not really know anything and piece together that all these words are related.\n",
      "order difference: 0\n",
      "Relevant sentence:  So for example, if I put in a color it can tell me a bunch of other colors it never actually knew the idea of color going in it did not really know anything and piece together that all these words are related.    =====    I mean I chose English, but I could have chosen any language and it still would have been just as successful.\n",
      "order difference: 0\n",
      "Relevant sentence:  I mean I chose English, but I could have chosen any language and it still would have been just as successful.    =====    So you are probably wondering how this actually worked because it is kind of baffling all I gave it was things that people typed on the internet.\n",
      "order difference: 0\n",
      "Relevant sentence:  So you are probably wondering how this actually worked because it is kind of baffling all I gave it was things that people typed on the internet.    =====    You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.\n",
      "order difference: 0\n",
      "Relevant sentence:  You can see I can put in other things like a kind of And I get other kinds of food out and I will actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words.    =====    So it says I painted the bench blank and were expected to fill in the blank.\n",
      "order difference: 0\n",
      "Relevant sentence:  So it says I painted the bench blank and were expected to fill in the blank.    =====    So it is pretty impressive that it was able to do this just from raw text.\n",
      "order difference: 0\n",
      "Relevant sentence:  So it is pretty impressive that it was able to do this just from raw text.    =====    So here is an example of a sentence where there is a word missing.\n",
      "order difference: 0\n",
      "Relevant sentence:  So here is an example of a sentence where there is a word missing.    =====    Today I am going to be talking about word embeddings, which I think are one of the coolest things you can do with machine learning right now and to explain why I think that I am just going to jump right in and give you an example of something you can do with word weddings.\n",
      "order difference: 0\n",
      "Relevant sentence:  Today I am going to be talking about word embeddings, which I think are one of the coolest things you can do with machine learning right now and to explain why I think that I am just going to jump right in and give you an example of something you can do with word weddings.    =====    And in this video, I really want to explain why this works and how it works and just I think it is super cool.\n",
      "order difference: 0\n",
      "Relevant sentence:  If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense.    =====    So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.\n",
      "order difference: 0\n",
      "Relevant sentence:  So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this.    =====    So, you know, it goes right from that small Vector to the context.\n",
      "order difference: 0\n",
      "Relevant sentence:  So, you know, it goes right from that small Vector to the context.    =====    Why do we expect that small Vector to actually be meeting?\n",
      "order difference: 0\n",
      "Relevant sentence:  Why do we expect that small Vector to actually be meeting?    =====    For example with Twitter data, we might just Loop through all the tweets go through all the words and all those tweets and for every time two words occur in the same.\n",
      "order difference: 0\n",
      "Relevant sentence:  For example with Twitter data, we might just Loop through all the tweets go through all the words and all those tweets and for every time two words occur in the same.    =====    So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.\n",
      "order difference: 0\n",
      "Relevant sentence:  So here is a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing.    =====    So I will have a link to the actual original word defect paper and the description if you want to read more about it.\n",
      "order difference: 0\n",
      "Relevant sentence:  So I will have a link to the actual original word defect paper and the description if you want to read more about it.    =====    Why do we expect that the middle layer when the word gets turned into a small Vector?\n",
      "order difference: 0\n",
      "Relevant sentence:  Why do we expect that the middle layer when the word gets turned into a small Vector?    =====    So essentially this model is just forcing the middle layer of the neural network.\n",
      "order difference: 0\n",
      "Relevant sentence:  So essentially this model is just forcing the middle layer of the neural network.    =====    So if two words have very similar contexts, it is really helpful for the neural network.\n",
      "order difference: 0\n",
      "Relevant sentence:  So if two words have very similar contexts, it is really helpful for the neural network.    =====    And I want every neuron to be set that is in the context and I want every neuron that was not in the context not to be set.\n",
      "order difference: 0\n",
      "Relevant sentence:  And I want every neuron to be set that is in the context and I want every neuron that was not in the context not to be set.    =====    So that is kind of just a really General overview of how weird to deck works and there is a lot more to it.\n",
      "order difference: 0\n",
      "Relevant sentence:  So that is kind of just a really General overview of how weird to deck works and there is a lot more to it.    =====    Well, the answer is that that small Vector is all the network has to figure out the context.\n",
      "order difference: 0\n",
      "Relevant sentence:  Well, the answer is that that small Vector is all the network has to figure out the context.    =====    To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.\n",
      "order difference: 0\n",
      "Relevant sentence:  To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that is what is easiest for the network to do.    =====    I will produce back out a vector maybe with a hundred thousand components in each neuron and that output Vector corresponds to a word as well.\n",
      "order difference: 0\n",
      "Relevant sentence:  I will produce back out a vector maybe with a hundred thousand components in each neuron and that output Vector corresponds to a word as well.    =====    Blair with XnumberX neurons and then using more neural network magic.\n",
      "order difference: 0\n",
      "Relevant sentence:  Looks like you essentially feed in a word and it produces in the middle.    =====    So the word Avec neural network in this case is just trying to picked context words from a word.\n",
      "order difference: 0\n",
      "Relevant sentence:  So the word Avec neural network in this case is just trying to picked context words from a word.    =====    You know, how do I represent that for the neural network?\n",
      "order difference: 0\n",
      "Relevant sentence:  You know, how do I represent that for the neural network?    =====    So how exactly is it that I feed in the word yellow and I get out all these contacts words.\n",
      "order difference: 0\n",
      "Relevant sentence:  So how exactly is it that I feed in the word yellow and I get out all these contacts words.    =====    So here is a really simple picture of what a word to Veeck neural network.\n",
      "order difference: 0\n",
      "Relevant sentence:  So here is a really simple picture of what a word to Veeck neural network.    =====    And if you have already seen my other videos on neural networks, you might actually already be able to implement or defect but I am just going to try to describe it here on a high level to give you an idea of how it works.\n",
      "order difference: 0\n",
      "Relevant sentence:  And if you have already seen my other videos on neural networks, you might actually already be able to implement or defect but I am just going to try to describe it here on a high level to give you an idea of how it works.    =====    It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.\n",
      "order difference: 0\n",
      "Relevant sentence:  It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail.    =====    So essentially we are and devack is just a really simple neural network.\n",
      "order difference: 0\n",
      "Relevant sentence:  So essentially we are and devack is just a really simple neural network.    =====    How are we going to you know solve for vectors for all of the words that ever appear on Twitter?\n",
      "order difference: 0\n",
      "Relevant sentence:  How are we going to you know solve for vectors for all of the words that ever appear on Twitter?    =====    I am actually going to give an example of something we might ask a word Tyvek neural network to do.\n",
      "order difference: 0\n",
      "Relevant sentence:  I am actually going to give an example of something we might ask a word Tyvek neural network to do.    =====    What I have done is I have picked out a random tweet from my Corpus and I picked out a random word from within that tweet.\n",
      "order difference: 0\n",
      "Relevant sentence:  What I have done is I have picked out a random tweet from my Corpus and I picked out a random word from within that tweet.    =====    It was yellow and I am going to feed as input the word yellow to the word Tyvek neural network, and I am going to try to get it as output to give me all the other words that were in the Tweet.\n",
      "order difference: 0\n",
      "Relevant sentence:  It was yellow and I am going to feed as input the word yellow to the word Tyvek neural network, and I am going to try to get it as output to give me all the other words that were in the Tweet.    =====    So the first approach I am going to be talking about is known as word Tyvek and it is probably the most famous kind of word of matting because it was the first word embedding to get the kind of impressive results that word embeddings stateoftheart word embeddings get today.\n",
      "order difference: 0\n",
      "Relevant sentence:  So the first approach I am going to be talking about is known as word Tyvek and it is probably the most famous kind of word of matting because it was the first word embedding to get the kind of impressive results that word embeddings stateoftheart word embeddings get today.    =====    So I take the neuron for whatever word I want to feed in and I will set that neuron to one and I will set all the other neurons to XnumberX then the neural network will just use regular neural network stuff to produce a small Vector.\n",
      "order difference: 0\n",
      "Relevant sentence:  So I take the neuron for whatever word I want to feed in and I will set that neuron to one and I will set all the other neurons to XnumberX then the neural network will just use regular neural network stuff to produce a small Vector.    =====    Well, basically the network has a different input neuron for each different word.\n",
      "order difference: 0\n",
      "Relevant sentence:  Well, basically the network has a different input neuron for each different word.    =====    You know, how are we going to produce the set of vectors that work?\n",
      "order difference: 0\n",
      "Relevant sentence:  And of course I did not have to pick it from the tall.    =====    So now I will tell you a little bit about the particular method that I use to generate the word embeddings at the beginning of this video the method I used is known as glove which is short for Global vectors and it is a kind of cooccurrence decomposition method now, it is a little unique in that it decomposes the log the logarithm of the cooccurrence Matrix instead of the actual cooccurrence Matrix.\n",
      "order difference: 0\n",
      "Relevant sentence:  So now I will tell you a little bit about the particular method that I use to generate the word embeddings at the beginning of this video the method I used is known as glove which is short for Global vectors and it is a kind of cooccurrence decomposition method now, it is a little unique in that it decomposes the log the logarithm of the cooccurrence Matrix instead of the actual cooccurrence Matrix.    =====    So basically a given cooccurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to cooccur.\n",
      "order difference: 0\n",
      "Relevant sentence:  So basically a given cooccurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to cooccur.    =====    So I really like glove I had a lot more fun implementing glove than I did implementing word Avec and I will certainly have a link to that paper in the description that describes glove.\n",
      "order difference: 0\n",
      "Relevant sentence:  So I really like glove I had a lot more fun implementing glove than I did implementing word Avec and I will certainly have a link to that paper in the description that describes glove.    =====    So it is similar to training a neural network and it has really good results and it is extremely fast.\n",
      "order difference: 0\n",
      "Relevant sentence:  So it is similar to training a neural network and it has really good results and it is extremely fast.    =====    So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings.\n",
      "order difference: 0\n",
      "Relevant sentence:  So I have gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings.    =====    I could have picked it from the short wide Matrix or I could even just decide to average these two vectors and use that as the embedding overall and there is actually a good reason to assume that these vectors would represent a decent amount of meaning.\n",
      "order difference: 0\n",
      "Relevant sentence:  I could have picked it from the short wide Matrix or I could even just decide to average these two vectors and use that as the embedding overall and there is actually a good reason to assume that these vectors would represent a decent amount of meaning.    =====    And it also is weighted uses a model where certain entries in The cooccurrence Matrix are more important than others and you use gradient descent to learn the embedding.\n",
      "order difference: 0\n",
      "Relevant sentence:  And it also is weighted uses a model where certain entries in The cooccurrence Matrix are more important than others and you use gradient descent to learn the embedding.    =====    Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.\n",
      "order difference: 0\n",
      "Relevant sentence:  Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix.    =====    It is XnumberX components and I can call that a word embedding for that for that word.\n",
      "order difference: 0\n",
      "Relevant sentence:  So I am going to be just talking about matrices and Matrix multiplications dot products things like that.    =====    So if I go into this tall skinny Matrix, and I grab the, you know entry, you know, the entry for a certain word that is going to give me a vector which is pretty small and\n",
      "order difference: 0\n",
      "Relevant sentence:  So if I go into this tall skinny Matrix, and I grab the, you know entry, you know, the entry for a certain word that is going to give me a vector which is pretty small and    =====    And if you do not know linear algebra Youre Not Really Gonna Get Much from this but that is why I kind of left it at the end of the video because some people will get something out of this and I think it is somewhat interesting.\n",
      "order difference: 0\n",
      "Relevant sentence:  And if you do not know linear algebra Youre Not Really Gonna Get Much from this but that is why I kind of left it at the end of the video because some people will get something out of this and I think it is somewhat interesting.    =====    But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.\n",
      "order difference: 0\n",
      "Relevant sentence:  But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big cooccurrence Matrix each row and each column corresponds to a word.    =====    So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.\n",
      "order difference: 0\n",
      "Relevant sentence:  So probably the simplest approach To generating word embeddings with the cooccurrence Matrix is to just decompose The cooccurrence Matrix into the product of two much smaller matrices.    =====    That is stored on the right side of this equation, which is you know, two relatively small matrices multiplied together.\n",
      "order difference: 0\n",
      "Relevant sentence:  That is stored on the right side of this equation, which is you know, two relatively small matrices multiplied together.    =====    But pretty much all of the methods utilized some amount of linear algebra.\n",
      "order difference: 0\n",
      "Relevant sentence:  But pretty much all of the methods utilized some amount of linear algebra.    =====    You can basically see that I get this massive Square Matrix, which is our cooccurrence matrix by multiplying a tall skinny Matrix and a short wide Matrix, you know, you think about how many entries are in the big Matrix?\n",
      "order difference: 0\n",
      "Relevant sentence:  You can basically see that I get this massive Square Matrix, which is our cooccurrence matrix by multiplying a tall skinny Matrix and a short wide Matrix, you know, you think about how many entries are in the big Matrix?    =====    Once we have this Matrix decomposition, which I have not described exactly how we might find this yet, but you could imagine there is Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that.\n",
      "order difference: 0\n",
      "Relevant sentence:  Once we have this Matrix decomposition, which I have not described exactly how we might find this yet, but you could imagine there is Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that.    =====    So by decomposing this big cooccurrence Matrix into these smaller ones were clearly compressing some information and in doing so hopefully we have to extract a lot of meaning from The Matrix so that we can do that compression and that should allow us to at least generate decent embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6)]\n",
      "[[[\"Appears with the word joke in the third sentence. So ideally the word embedding would figure out then that laughing laughter related since they're both related to joke. So that's kind of where a word embedding gets its Knowledge from it learns things via context. It sees, you know, what words occur near other words, but what is the word embedding actually do I still have to kind of formalize what what we're after so in one sentence a word embedding just converts words into vectors so you might give Vin a word like hamburger and you would get out a list of say 64 numbers and those numbers would describe the word and forward embedding to be good. We kind of require that the vectors carry some meaning so if I put in hamburger and cheeseburger into my model, I want those vectors to be very close to each other because they're very related words. Whereas if I put in something else like Ferrari like a kind of car totally unrelated to hamburger. I want the vector for Ferrari to be far away from The vector for hamburger and of course all these distances are relative, but you can kind of see what I mean that we want the closeness of these vectors to resemble the closeness of the words that they represent and in addition to this kind of idea of closeness. We might also want there to be even more structure. For example, if I do the vector from Man - the vector for woman and to subtract vectors, we just subtract each number from the corresponding number. Of the other Vector. So if I take the vector from man and I subtract the vector for woman, I want that to somehow represent the difference between male and female and then if I add that Vector to the vector for for Queen, I want it to give me out something very very close to the vector for King. So I you know, I want these vectors to be related and I want the differences between vectors to also carry some meaning and you know, I might add other constraints but the idea is I just want to encode as \"], '2020-01-31T07:57:53Z', '716067a60a1a4034abc49a12ecafb39b', '39cb0782-85d4-4e1b-ad99-ca933adf62b5']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Is to put a color right? I painted The Bench red. I painted the bench green something like that and already we can see that, you know, if a word can show up in this context it's likely to be a color but unfortunately that's not always true. You could also say I painted the bench today today is not a color but the main takeaway is that context is really kind of closely related to meaning so that was an example where multiple different words could go into the same context and we presume that those words are somehow ow related at least a lot of them are but there's another way that context can help us and that's if two words happen to always appear in the same context at once. So here are three different sentences that will help us understand this idea. And the first sentence I actually have two examples Donald and Trump are likely to appear together because ones the first name of a person and one's the last name of that same person. So those words are closely related. We also have United States which is kind of just one logical. Broken up into two smaller words, so United and states are likely to appear together and then in the second and third example, we have a joke and laugh are kind of related words you laugh at a joke. So they're also likely to appear in the same context. Now, there's one subtle thing that I'd like to point out in this example, which is that laughed and laughed our might be different words, like laughs is the present tense and laughed as the past tense and likewise, you know, we could think about joke. Versus jokes like one is plural one is singular. These are different forms of the same word. And ideally since we knew nothing about English going in our algorithm. Our word embedding is going to have to learn the different forms of the same word or related. It has to learn that laughed is somehow related to laugh and what you can see is, you know, these examples give you an idea of how the model might be able to do that because you can see laughed appears with the word joke in the second sentence and \"], '2020-01-31T07:55:53Z', '716067a60a1a4034abc49a12ecafb39b', '7162f0aa-7409-4369-93b8-bf0b0a6c6bab']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Today I'm going to be talking about word embeddings, which I think are one of the coolest things you can do with machine learning right now and to explain why I think that I'm just going to jump right in and give you an example of something you can do with word weddings. So a few months ago, I set up a program that just downloads tweets a people write tweets on Twitter and saves them to a file and after running this program from there to I had collected a massive file with over five gigabytes of tweets after I can press is it so I got a massive amount of data and it's just raw data that people typed on the internet after getting all this data. I fed it directly into a word embedding algorithm which was able to figure out tons of relationships between words just from the raw things that people happen to type on the internet. So for example, if I put in a color it can tell me a bunch of other colors it never actually knew the idea of color going in it didn't really know anything and piece together that all these words are related. You can see I can put in other things like a kind of And I get other kinds of food out and I'll actually have a link in the description so that you can try this out yourself just to see how well it really learned the relationships between words. So you're probably wondering how this actually worked because it's kind of baffling all I gave it was things that people typed on the internet. I didn't give it a dictionary. I didn't give it a list of synonyms. I mean I chose English, but I could have chosen any language and it still would have been just as successful. So it's pretty impressive that it was able to do this just from raw text. And in this video, I really want to explain why this works and how it works and just I think it's super cool. So I want to share it with you. So pretty much every word embedding algorithm uses the idea of context and to show you what I mean. I'm just going to give you a really simple example. So here is an example of a sentence where there's a word missing. So it says I painted the bench blank and were expected to fill in the blank. So the obvious thing \"], '2020-01-31T07:53:53Z', '716067a60a1a4034abc49a12ecafb39b', 'd3b26266-0fcd-4c48-9d10-b07f3caba0f0']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Blair with 64 neurons and then using more neural network magic. I'll produce back out a vector maybe with a hundred thousand components in each neuron and that output Vector corresponds to a word as well. And I want every neuron to be set that is in the context and I want every neuron that wasn't in the context not to be set. So why does this work? Why do we expect that the middle layer when the word gets turned into a small Vector? Why do we expect that small Vector to actually be meeting? Awful. Well, the answer is that that small Vector is all the network has to figure out the context. So, you know, it goes right from that small Vector to the context. So if two words have very similar contexts, it's really helpful for the neural network. If if the small Vector is similar for those two words because if it produces a similar output a similar Vector makes sense. So essentially this model is just forcing the middle layer of the neural network. To correspond to meaning you know close words words with similar contexts will have closed vectors in the middle of the network just because that's what's easiest for the network to do. So that's kind of just a really General overview of how weird to deck works and there's a lot more to it. So I'll have a link to the actual original word defect paper and the description if you want to read more about it. So besides word Tyvek, there are a bunch of other ways to generate word embeddings and the majority of them are based on this. Thing called The co-occurrence Matrix. So here's a really simple example of what this might look like basically both the rows and the columns correspond to different words and the entry at any given point in The Matrix just counts how many times those two words happen in the same context so you can imagine how we might generate this thing. For example with Twitter data, we might just Loop through all the tweets go through all the words and all those tweets and for every time two words occur in the same. \"], '2020-01-31T08:01:53Z', '716067a60a1a4034abc49a12ecafb39b', 'e1b6ba78-88cf-441e-833c-60ba230190a0']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Meaning as I can into the vectors. So how are we actually going to do that? How are we going to you know solve for vectors for all of the words that ever appear on Twitter? You know, how are we going to produce the set of vectors that work? So the first approach I'm going to be talking about is known as word Tyvek and it's probably the most famous kind of word of matting because it was the first word embedding to get the kind of impressive results that word embeddings state-of-the-art word embeddings get today. So essentially we're and devack is just a really simple neural network. And if you've already seen my other videos on neural networks, you might actually already be able to implement or defect but I'm just going to try to describe it here on a high level to give you an idea of how it works. So here's a really simple picture of what a word to Veeck neural network. Looks like you essentially feed in a word and it produces in the middle. It produces a small Vector which is a word embedding and then it produces as output something like a context and to But this is in a little more detail. I'm actually going to give an example of something we might ask a word Tyvek neural network to do. What I've done is I've picked out a random tweet from my Corpus and I picked out a random word from within that tweet. In this case. It was yellow and I'm going to feed as input the word yellow to the word Tyvek neural network, and I'm going to try to get it as output to give me all the other words that were in the Tweet. So the word Avec neural network in this case is just trying to picked context words from a word. So how exactly is it that I feed in the word yellow and I get out all these contacts words. You know, how do I represent that for the neural network? Well, basically the network has a different input neuron for each different word. So I take the neuron for whatever word I want to feed in and I'll set that neuron to one and I'll set all the other neurons to 0 then the neural network will just use regular neural network stuff to produce a small Vector. Basically, that's just a \"], '2020-01-31T07:59:53Z', '716067a60a1a4034abc49a12ecafb39b', 'f3cc5613-f30d-4bce-948a-1a1d991c0c3c']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"It's 64 components and I can call that a word embedding for that for that word. And of course I didn't have to pick it from the tall. Skinny Matrix. I could have picked it from the short wide Matrix or I could even just decide to average these two vectors and use that as the embedding overall and there's actually a good reason to assume that these vectors would represent a decent amount of meaning. The reason is that an entry in the big co-occurrence. Matrix is equivalent to the dot product of a word Vector taken from the tall skinny Matrix in a word Vector taken from the wide short Matrix. So basically a given co-occurrence is approximated by a DOT product between two word vectors basically, so if I use these words vectors what it tells me is that now the dot product represents How likely towards our to co-occur. So I've gotten this structure in my vectors, you know correlation and vectors corresponds to correlation and Text so that is why you might expect Matrix decompositions to give you a good embeddings. So now I'll tell you a little bit about the particular method that I use to generate the word embeddings at the beginning of this video the method I used is known as glove which is short for Global vectors and it is a kind of co-occurrence decomposition method now, it's a little unique in that it decomposes the log the logarithm of the co-occurrence Matrix instead of the actual co-occurrence Matrix. And it also is weighted uses a model where certain entries in The co-occurrence Matrix are more important than others and you use gradient descent to learn the embedding. So it's similar to training a neural network and it has really good results and it's extremely fast. So I really like glove I had a lot more fun implementing glove than I did implementing word Avec and I will certainly have a link to that paper in the description that describes glove. \"], '2020-01-31T08:05:53Z', '716067a60a1a4034abc49a12ecafb39b', '49d74149-4a6c-4d9c-83dd-11b8fd5142ec']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Just add one to that entry in this Matrix. Now different methods will use this Matrix in different ways. But pretty much all of the methods utilized some amount of linear algebra. So I'm going to be just talking about matrices and Matrix multiplications dot products things like that. And if you don't know linear algebra You're Not Really Gonna Get Much from this but that's why I kind of left it at the end of the video because some people will get something out of this and I think it's somewhat interesting. So probably the simplest approach To generating word embeddings with the co-occurrence Matrix is to just decompose The co-occurrence Matrix into the product of two much smaller matrices. So I've drawn out the picture here. You can basically see that I get this massive Square Matrix, which is our co-occurrence matrix by multiplying a tall skinny Matrix and a short wide Matrix, you know, you think about how many entries are in the big Matrix? There's a hundred thousand squared. That's a lot more. That is stored on the right side of this equation, which is you know, two relatively small matrices multiplied together. So by decomposing this big co-occurrence Matrix into these smaller ones were clearly compressing some information and in doing so hopefully we have to extract a lot of meaning from The Matrix so that we can do that compression and that should allow us to at least generate decent embeddings. Once we have this Matrix decomposition, which I haven't described exactly how we might find this yet, but you could imagine there's Plenty of methods in linear algebra to decompose a matrix like singular value decomposition or you could use gradient descent or something like that. But once you have this Matrix decomposition, we actually get word vectors pretty much for free for example in the big in the big co-occurrence Matrix each row and each column corresponds to a word. So if I go into this tall skinny Matrix, and I grab the, you know entry, you know, the entry for a certain word that is going to give me a vector which is pretty small and \"], '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3b23e820-dc5a-41ef-a6d8-9fa398f84c13']] \n",
      "\n",
      "\n",
      "6\n",
      "Before Merging 7\n",
      "[]\n",
      "After Merging 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Ent map before filtering:  {'2': [2613, 2613, 56, 1049, 2613, 3216, 3216, 3036, 3216, 926, 56, 2288, 56, 3367, 360, 6611, 926, 2613, 2613, 452, 2613, 2613, 3822, 4865, 2613, 1049, 4865, 2613, 3036, 4452], '1': [2613, 2613, 3524, 1049, 2613, 876, 398, 2613, 4499, 1049], '0': [1049, 2613, 1049, 1296, 2613, 6171, 1669, 695, 5655, 143, 1049, 1982, 143, 1049, 2613, 1296, 5655, 6171, 1669, 695], '4': [1049, 2613, 2613, 1049, 1049, 4447, 5140, 4865, 5140, 1049, 3216, 3216, 3216, 3216, 3216, 4003, 413, 3216, 3216, 4006, 3216, 3216, 1049, 3216, 3216, 3216, 2613, 1049, 3216, 4447], '3': [5249, 103, 3216, 4774, 3378, 5249, 143, 5249, 3216, 1049, 1049, 3378, 4003, 103, 1049, 1049, 3216, 3216, 5249, 1049, 2613, 1049, 1049, 5140, 1049, 5140, 2613, 5080, 3378, 2613], '6': [1951, 5299, 3861, 3216, 1951, 3798, 3216, 926, 27, 103, 103, 5299, 3401, 143, 3798, 5249, 3216, 27, 1951, 3216, 103, 3401, 3798, 5249, 3524, 3524, 3146, 5249, 27, 1951], '5': [5249, 103, 143, 1049, 1049, 852, 3524, 2613, 3216, 3319]}\n",
      "Group Ent map after filtering:  {'2': [(2613, 9), (56, 3), (3216, 3)], '1': [(2613, 4)], '0': [(1049, 4), (2613, 3)], '4': [(3216, 13), (1049, 6), (2613, 3)], '3': [(1049, 8), (5249, 4), (3216, 4), (3378, 3), (2613, 3)], '6': [(1951, 4), (3216, 4), (3798, 3), (27, 3), (103, 3), (5249, 3)], '5': []}\n",
      "Using  None  for feature extraction\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 103, \"module\": \"scorer\", \"batches count\": 1, \"number of sentences\": 51, \"ts\": \"2020-03-05T08:26:39.984774Z\", \"msg\": \"computing in batches\"}\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 106, \"module\": \"scorer\", \"iteration count:\": 0, \"ts\": \"2020-03-05T08:26:39.986454Z\", \"msg\": \"getting feature vector\"}\n",
      "('That we just went through with the filter will happen to this new output with the next layers filters.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('All right, so this is our input This input will be passed to a convolutional layer as just discussed.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('That we just went through with the filter will happen to this new output with the next layers filters.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('Weve specified this layer to only have one filter and this filter is going to convolve across each XnumberXxXnumberX block of pixels from the input.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('That we just went through with the filter will happen to this new output with the next layers filters.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('So after This filter has convolved the entire input will be left with a new representation of our input which is going to be made up of the entire Matrix of those stored dot products.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('That we just went through with the filter will happen to this new output with the next layers filters.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('This is what will then be passed to the next layer as input in this same.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('That we just went through with the filter will happen to this new output with the next layers filters.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('So really we should say that the filter is going to convolve across each three', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('But as mentioned we can think of these filters as pattern detectors.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('So one type of quote pattern that a filter could detect could be edges and images.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('But as mentioned we can think of these filters as pattern detectors.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('Now these simple and kind of geometric filters are what we would see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('So we cannot really observe any specific pattern that was picked out from our filter in the example.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('If we look at the formula for each of these cells we can see that it is just indeed the dot product of the filter with each three by three section of pixels from the input.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('We just looked at an Excel but let us show our original image of the XnumberX here and now let us say we have four three by three filters for our convolutional layer and these filters are filled with the values.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('Weve specified this layer to only have one filter and this filter is going to convolve across each XnumberXxXnumberX block of pixels from the input.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('We just looked at an Excel but let us show our original image of the XnumberX here and now let us say we have four three by three filters for our convolutional layer and these filters are filled with the values.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('So if we can involved our original image of a XnumberX with each of these four filters individually, this is what the output would look like for each filter.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('So after This filter has convolved the entire input will be left with a new representation of our input which is going to be made up of the entire Matrix of those stored dot products.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('So if we can involved our original image of a XnumberX with each of these four filters individually, this is what the output would look like for each filter.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('The brightest pixels can be interpreted as what the filter has detected.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('So this first one we can see to text top horizontal Edge.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('So here we have our Matrix representation of an image of a XnumberX from the mnist data set the values in this Matrix are the individual pixels from the image.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('So this first one we can see to text top horizontal Edge.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('So look we would just take the dot product of the filter here with this first three by three block and then we would store it over here.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('Now these filters are really basic and just to take edges.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('Now these simple and kind of geometric filters are what we would see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('Now these filters are really basic and just to take edges.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('So really we should say that the filter is going to convolve across each three', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('And as we go even further into our layers, the filters are able to detect much more complex patterns, like these dog faces being interpreted in this filter or even the bird legs detected in this one.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('Now these simple and kind of geometric filters are what we would see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('Alright, so now if you are interested in seeing how to work with CNN is and code then check out the CNN and finetuning videos and my Karis deep learning.', '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6') ('So I would like to share that with you all to I have also linked to his lecture in the description of this video.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('Video, we will be discussing convolutional neural networks a convolutional neural network also known as a CNN or conf net is an artificial neural network that has so far been most popularly used analyzing images.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('I am going to use an example that Jeremy Howard used in one of his lectures for fast AI his example really gave me a lot of insight behind what was going on within a convolutional layer.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('Although image analysis has been the most widespread use of CNN is they can all be used for other data analysis or classification problems as well.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('The filters are able to detect even more sophisticated objects, like full dogs cats lizards and birds to understand what is actually happening here with these convolutional layers and their respective filters.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('Well a CNN now CNN is can and usually do have other non convolutional layers as well.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('Now these simple and kind of geometric filters are what we would see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('So what do these convolutional layers do just like any other layer a convolutional layer receives input then transforms the input in some way and then outputs the transform input to the next layer with a convolutional layer this transformation is a convolution operation will come back to this operation in a bit for now.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('All right, so this is our input This input will be passed to a convolutional layer as just discussed.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('So what do these convolutional layers do just like any other layer a convolutional layer receives input then transforms the input in some way and then outputs the transform input to the next layer with a convolutional layer this transformation is a convolution operation will come back to this operation in a bit for now.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('Let is now assume that the first hidden layer in our model is a convolutional layer as mentioned earlier when adding a convolutional layer to a model.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('So what do these convolutional layers do just like any other layer a convolutional layer receives input then transforms the input in some way and then outputs the transform input to the next layer with a convolutional layer this transformation is a convolution operation will come back to this operation in a bit for now.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('Let is look at a high level idea of what convolutional layers are doing as mentioned earlier convolutional neural networks are able to take patterns and images more precisely.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('Now these simple and kind of geometric filters are what we would see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('Well, actually, let us be a little more precise than that with each convolutional layer.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('All right, so this is our input This input will be passed to a convolutional layer as just discussed.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc')\n",
      "('Well, actually, let us be a little more precise than that with each convolutional layer.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('Let is now assume that the first hidden layer in our model is a convolutional layer as mentioned earlier when adding a convolutional layer to a model.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('Well, actually, let us be a little more precise than that with each convolutional layer.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('So for this first convolutional layer in this example of hours, we are going to specify that.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('Well, actually, let us be a little more precise than that with each convolutional layer.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('The layer should have and will speak technically about what a filter is in just a few moments, but for now, I understand that these filters are actually what detect the patterns now when I say that the filters are able to detect patterns what precisely do I mean by patterns.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('So one type of quote pattern that a filter could detect could be edges and images.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('The layer should have and will speak technically about what a filter is in just a few moments, but for now, I understand that these filters are actually what detect the patterns now when I say that the filters are able to detect patterns what precisely do I mean by patterns.', '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47') ('Now these simple and kind of geometric filters are what we would see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('Three block of pixels from the input to actually illustrate this.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('All right, so this is our input This input will be passed to a convolutional layer as just discussed.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('Let is now assume that the first hidden layer in our model is a convolutional layer as mentioned earlier when adding a convolutional layer to a model.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('All right, so this is our input This input will be passed to a convolutional layer as just discussed.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('So for this first convolutional layer in this example of hours, we are going to specify that.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('Weve specified this layer to only have one filter and this filter is going to convolve across each XnumberXxXnumberX block of pixels from the input.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('We want the layer to contain one filter of size XnumberX by XnumberX.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('Weve specified this layer to only have one filter and this filter is going to convolve across each XnumberXxXnumberX block of pixels from the input.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('So here is our three by three filter of random numbers here when the filter first lands on the first three by three block of pixels the dot product of the filter itself with the XnumberXXXnumberX block of pixels from the input will be computed and stored this will occur for each three by three set of pixels that the Involves.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('If we look at the formula for each of these cells we can see that it is just indeed the dot product of the filter with each three by three section of pixels from the input.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('The layer to have a filter can technically just be thought of as a relatively small Matrix for which we decide the number of rows and number of columns that this Matrix has and the values within the Matrix are initialized with Some numbers.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('If we look at the formula for each of these cells we can see that it is just indeed the dot product of the filter with each three by three section of pixels from the input.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('So after This filter has convolved the entire input will be left with a new representation of our input which is going to be made up of the entire Matrix of those stored dot products.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('The layer to have a filter can technically just be thought of as a relatively small Matrix for which we decide the number of rows and number of columns that this Matrix has and the values within the Matrix are initialized with Some numbers.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('So after This filter has convolved the entire input will be left with a new representation of our input which is going to be made up of the entire Matrix of those stored dot products.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('This Matrix of dot products is going to be the output of this layer and is represented here.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('Let is now assume that the first hidden layer in our model is a convolutional layer as mentioned earlier when adding a convolutional layer to a model.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('This Matrix of dot products is going to be the output of this layer and is represented here.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('The layer to have a filter can technically just be thought of as a relatively small Matrix for which we decide the number of rows and number of columns that this Matrix has and the values within the Matrix are initialized with Some numbers.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('This is what will then be passed to the next layer as input in this same.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('Let is now assume that the first hidden layer in our model is a convolutional layer as mentioned earlier when adding a convolutional layer to a model.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('This is what will then be passed to the next layer as input in this same.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('The layer to have a filter can technically just be thought of as a relatively small Matrix for which we decide the number of rows and number of columns that this Matrix has and the values within the Matrix are initialized with Some numbers.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "('This is what will then be passed to the next layer as input in this same.', '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc') ('Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.', '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d')\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 390, \"module\": \"grouper_segments\", \"outlier threshold is : \": 0.6645093560218811, \"ts\": \"2020-03-05T08:26:41.172106Z\", \"msg\": \"Outlier Score\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Community Algorithm\n",
      "cluster before alteration=========>\n",
      "That we just went through with the filter will happen to this new output with the next layers filters.\n",
      "But as mentioned we can think of these filters as pattern detectors.\n",
      "So we cannot really observe any specific pattern that was picked out from our filter in the example.\n",
      "We just looked at an Excel but let us show our original image of the XnumberX here and now let us say we have four three by three filters for our convolutional layer and these filters are filled with the values.\n",
      "You see here and this These values can be represented visually as these filters where the  ones correspond to black ones correspond to White and zeros correspond to gray.\n",
      "So if we can involved our original image of a XnumberX with each of these four filters individually, this is what the output would look like for each filter.\n",
      "We can see that all four of these filters are detecting edges in the output.\n",
      "The brightest pixels can be interpreted as what the filter has detected.\n",
      "So this first one we can see to text top horizontal Edge.\n",
      "Is of the seven and that is indicated by the brightest pixels here II detects left vertical edges again being displayed with the brightest pixels, the third detects bottom horizontal edges and the fourth detects right vertical edges.\n",
      "Now these filters are really basic and just to take edges.\n",
      "We may see towards the start of our Network more complex filters would be located deeper in the network and would gradually be able to detect more sophisticated patterns like the one shown here we can see The shapes that the filters on the left detected from the images on the right this one here detect circles and this one at the bottom is detecting corners.\n",
      "And as we go even further into our layers, the filters are able to detect much more complex patterns, like these dog faces being interpreted in this filter or even the bird legs detected in this one.\n",
      "Alright, so now if you are interested in seeing how to work with CNN is and code then check out the CNN and finetuning videos and my Karis deep learning.\n",
      "cluster before alteration=========>\n",
      "All right, so this is our input This input will be passed to a convolutional layer as just discussed.\n",
      "Weve specified this layer to only have one filter and this filter is going to convolve across each XnumberXxXnumberX block of pixels from the input.\n",
      "So after This filter has convolved the entire input will be left with a new representation of our input which is going to be made up of the entire Matrix of those stored dot products.\n",
      "This is what will then be passed to the next layer as input in this same.\n",
      "If we look at the formula for each of these cells we can see that it is just indeed the dot product of the filter with each three by three section of pixels from the input.\n",
      "So here we have our Matrix representation of an image of a XnumberX from the mnist data set the values in this Matrix are the individual pixels from the image.\n",
      "So look we would just take the dot product of the filter here with this first three by three block and then we would store it over here.\n",
      "So I would like to share that with you all to I have also linked to his lecture in the description of this video.\n",
      "I am going to use an example that Jeremy Howard used in one of his lectures for fast AI his example really gave me a lot of insight behind what was going on within a convolutional layer.\n",
      "Three block of pixels from the input to actually illustrate this.\n",
      "So here is our three by three filter of random numbers here when the filter first lands on the first three by three block of pixels the dot product of the filter itself with the XnumberXXXnumberX block of pixels from the input will be computed and stored this will occur for each three by three set of pixels that the Involves.\n",
      "Now we slide to the next three by three block take the dot product and then store the value here.\n",
      "So here we have this first value is the dot product of this input with this filter and then if I click on another random value over here, we can see that this value is the dot product of the filter with this input.\n",
      "This Matrix of dot products is going to be the output of this layer and is represented here.\n",
      "cluster before alteration=========>\n",
      "So really we should say that the filter is going to convolve across each three\n",
      "So one type of quote pattern that a filter could detect could be edges and images.\n",
      "Now these simple and kind of geometric filters are what we would see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects.\n",
      "Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.\n",
      "The filters are able to detect even more sophisticated objects, like full dogs cats lizards and birds to understand what is actually happening here with these convolutional layers and their respective filters.\n",
      "Let is now assume that the first hidden layer in our model is a convolutional layer as mentioned earlier when adding a convolutional layer to a model.\n",
      "So for this first convolutional layer in this example of hours, we are going to specify that.\n",
      "We want the layer to contain one filter of size XnumberX by XnumberX.\n",
      "The layer to have a filter can technically just be thought of as a relatively small Matrix for which we decide the number of rows and number of columns that this Matrix has and the values within the Matrix are initialized with Some numbers.\n",
      "For example some filters May detect Corners some may detect circles other squares.\n",
      "Like Eyes Ears hair or fur feathers scales and beaks even and an even deeper layers.\n",
      "So say we have a convolutional neural network that is accepting images of handwritten digits like from the amnesty.\n",
      "Set and our network is classifying them into their respective categories of whether the images of a XnumberX XnumberX XnumberX Etc.\n",
      "cluster before alteration=========>\n",
      "Although image analysis has been the most widespread use of CNN is they can all be used for other data analysis or classification problems as well.\n",
      "Well a CNN now CNN is can and usually do have other non convolutional layers as well.\n",
      "Video, we will be discussing convolutional neural networks a convolutional neural network also known as a CNN or conf net is an artificial neural network that has so far been most popularly used analyzing images.\n",
      "Generally we can think of a CNS an artificial neural network that has some type of specialization for being able to pick out or detect patterns and make sense of Them this pattern detection is what makes CNN so useful for image analysis.\n",
      "So if a CNN is just some form of an artificial neural network what differentiates it from just a standard multilayer perceptron or MLP?\n",
      "So what do these convolutional layers do just like any other layer a convolutional layer receives input then transforms the input in some way and then outputs the transform input to the next layer with a convolutional layer this transformation is a convolution operation will come back to this operation in a bit for now.\n",
      "Let is look at a high level idea of what convolutional layers are doing as mentioned earlier convolutional neural networks are able to take patterns and images more precisely.\n",
      "Well, actually, let us be a little more precise than that with each convolutional layer.\n",
      "The layer should have and will speak technically about what a filter is in just a few moments, but for now, I understand that these filters are actually what detect the patterns now when I say that the filters are able to detect patterns what precisely do I mean by patterns.\n",
      "Well, think about how much may be going on in any single image.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "That we just went through with the filter will happen to this new output with the next layers filters.\n",
      "But as mentioned we can think of these filters as pattern detectors.\n",
      "So we cannot really observe any specific pattern that was picked out from our filter in the example.\n",
      "We just looked at an Excel but let us show our original image of the XnumberX here and now let us say we have four three by three filters for our convolutional layer and these filters are filled with the values.\n",
      "You see here and this These values can be represented visually as these filters where the  ones correspond to black ones correspond to White and zeros correspond to gray.\n",
      "So if we can involved our original image of a XnumberX with each of these four filters individually, this is what the output would look like for each filter.\n",
      "We can see that all four of these filters are detecting edges in the output.\n",
      "The brightest pixels can be interpreted as what the filter has detected.\n",
      "So this first one we can see to text top horizontal Edge.\n",
      "Is of the seven and that is indicated by the brightest pixels here II detects left vertical edges again being displayed with the brightest pixels, the third detects bottom horizontal edges and the fourth detects right vertical edges.\n",
      "Now these filters are really basic and just to take edges.\n",
      "We may see towards the start of our Network more complex filters would be located deeper in the network and would gradually be able to detect more sophisticated patterns like the one shown here we can see The shapes that the filters on the left detected from the images on the right this one here detect circles and this one at the bottom is detecting corners.\n",
      "And as we go even further into our layers, the filters are able to detect much more complex patterns, like these dog faces being interpreted in this filter or even the bird legs detected in this one.\n",
      "Alright, so now if you are interested in seeing how to work with CNN is and code then check out the CNN and finetuning videos and my Karis deep learning.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "All right, so this is our input This input will be passed to a convolutional layer as just discussed.\n",
      "Weve specified this layer to only have one filter and this filter is going to convolve across each XnumberXxXnumberX block of pixels from the input.\n",
      "So after This filter has convolved the entire input will be left with a new representation of our input which is going to be made up of the entire Matrix of those stored dot products.\n",
      "This is what will then be passed to the next layer as input in this same.\n",
      "If we look at the formula for each of these cells we can see that it is just indeed the dot product of the filter with each three by three section of pixels from the input.\n",
      "So here we have our Matrix representation of an image of a XnumberX from the mnist data set the values in this Matrix are the individual pixels from the image.\n",
      "So look we would just take the dot product of the filter here with this first three by three block and then we would store it over here.\n",
      "So I would like to share that with you all to I have also linked to his lecture in the description of this video.\n",
      "I am going to use an example that Jeremy Howard used in one of his lectures for fast AI his example really gave me a lot of insight behind what was going on within a convolutional layer.\n",
      "Three block of pixels from the input to actually illustrate this.\n",
      "So here is our three by three filter of random numbers here when the filter first lands on the first three by three block of pixels the dot product of the filter itself with the XnumberXXXnumberX block of pixels from the input will be computed and stored this will occur for each three by three set of pixels that the Involves.\n",
      "Now we slide to the next three by three block take the dot product and then store the value here.\n",
      "So here we have this first value is the dot product of this input with this filter and then if I click on another random value over here, we can see that this value is the dot product of the filter with this input.\n",
      "This Matrix of dot products is going to be the output of this layer and is represented here.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "So really we should say that the filter is going to convolve across each three\n",
      "So one type of quote pattern that a filter could detect could be edges and images.\n",
      "Now these simple and kind of geometric filters are what we would see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects.\n",
      "Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.\n",
      "The filters are able to detect even more sophisticated objects, like full dogs cats lizards and birds to understand what is actually happening here with these convolutional layers and their respective filters.\n",
      "Let is now assume that the first hidden layer in our model is a convolutional layer as mentioned earlier when adding a convolutional layer to a model.\n",
      "So for this first convolutional layer in this example of hours, we are going to specify that.\n",
      "We want the layer to contain one filter of size XnumberX by XnumberX.\n",
      "The layer to have a filter can technically just be thought of as a relatively small Matrix for which we decide the number of rows and number of columns that this Matrix has and the values within the Matrix are initialized with Some numbers.\n",
      "For example some filters May detect Corners some may detect circles other squares.\n",
      "Like Eyes Ears hair or fur feathers scales and beaks even and an even deeper layers.\n",
      "So say we have a convolutional neural network that is accepting images of handwritten digits like from the amnesty.\n",
      "Set and our network is classifying them into their respective categories of whether the images of a XnumberX XnumberX XnumberX Etc.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Although image analysis has been the most widespread use of CNN is they can all be used for other data analysis or classification problems as well.\n",
      "Well a CNN now CNN is can and usually do have other non convolutional layers as well.\n",
      "Video, we will be discussing convolutional neural networks a convolutional neural network also known as a CNN or conf net is an artificial neural network that has so far been most popularly used analyzing images.\n",
      "Generally we can think of a CNS an artificial neural network that has some type of specialization for being able to pick out or detect patterns and make sense of Them this pattern detection is what makes CNN so useful for image analysis.\n",
      "So if a CNN is just some form of an artificial neural network what differentiates it from just a standard multilayer perceptron or MLP?\n",
      "So what do these convolutional layers do just like any other layer a convolutional layer receives input then transforms the input in some way and then outputs the transform input to the next layer with a convolutional layer this transformation is a convolution operation will come back to this operation in a bit for now.\n",
      "Let is look at a high level idea of what convolutional layers are doing as mentioned earlier convolutional neural networks are able to take patterns and images more precisely.\n",
      "Well, actually, let us be a little more precise than that with each convolutional layer.\n",
      "The layer should have and will speak technically about what a filter is in just a few moments, but for now, I understand that these filters are actually what detect the patterns now when I say that the filters are able to detect patterns what precisely do I mean by patterns.\n",
      "Well, think about how much may be going on in any single image.\n",
      "\n",
      "\n",
      "Checking Timerange --------------\n",
      "--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So if we can involved our original image of a XnumberX with each of these four filters individually, this is what the output would look like for each filter. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "That we just went through with the filter will happen to this new output with the next layers filters. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "The brightest pixels can be interpreted as what the filter has detected. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "But as mentioned we can think of these filters as pattern detectors. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "So this first one we can see to text top horizontal Edge. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "We may see towards the start of our Network more complex filters would be located deeper in the network and would gradually be able to detect more sophisticated patterns like the one shown here we can see The shapes that the filters on the left detected from the images on the right this one here detect circles and this one at the bottom is detecting corners. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "And as we go even further into our layers, the filters are able to detect much more complex patterns, like these dog faces being interpreted in this filter or even the bird legs detected in this one. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "Is of the seven and that is indicated by the brightest pixels here II detects left vertical edges again being displayed with the brightest pixels, the third detects bottom horizontal edges and the fourth detects right vertical edges. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "Now these filters are really basic and just to take edges. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "So we cannot really observe any specific pattern that was picked out from our filter in the example. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "We just looked at an Excel but let us show our original image of the XnumberX here and now let us say we have four three by three filters for our convolutional layer and these filters are filled with the values. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "You see here and this These values can be represented visually as these filters where the  ones correspond to black ones correspond to White and zeros correspond to gray. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "Alright, so now if you are interested in seeing how to work with CNN is and code then check out the CNN and finetuning videos and my Karis deep learning. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "We can see that all four of these filters are detecting edges in the output. 2ea7614f-7029-4e24-8f06-9b8550296ce6 \n",
      "\n",
      "--------------\n",
      "So I would like to share that with you all to I have also linked to his lecture in the description of this video. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "If we look at the formula for each of these cells we can see that it is just indeed the dot product of the filter with each three by three section of pixels from the input. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "I am going to use an example that Jeremy Howard used in one of his lectures for fast AI his example really gave me a lot of insight behind what was going on within a convolutional layer. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "This is what will then be passed to the next layer as input in this same. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "Three block of pixels from the input to actually illustrate this. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "So here is our three by three filter of random numbers here when the filter first lands on the first three by three block of pixels the dot product of the filter itself with the XnumberXXXnumberX block of pixels from the input will be computed and stored this will occur for each three by three set of pixels that the Involves. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "So look we would just take the dot product of the filter here with this first three by three block and then we would store it over here. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "So here we have our Matrix representation of an image of a XnumberX from the mnist data set the values in this Matrix are the individual pixels from the image. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "Now we slide to the next three by three block take the dot product and then store the value here. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "This Matrix of dot products is going to be the output of this layer and is represented here. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "So here we have this first value is the dot product of this input with this filter and then if I click on another random value over here, we can see that this value is the dot product of the filter with this input. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "So after This filter has convolved the entire input will be left with a new representation of our input which is going to be made up of the entire Matrix of those stored dot products. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "All right, so this is our input This input will be passed to a convolutional layer as just discussed. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "Weve specified this layer to only have one filter and this filter is going to convolve across each XnumberXxXnumberX block of pixels from the input. 686a3a00-f4ce-4d3f-abd4-7164d5d956bc \n",
      "\n",
      "--------------\n",
      "So really we should say that the filter is going to convolve across each three 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "For example some filters May detect Corners some may detect circles other squares. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "Like Eyes Ears hair or fur feathers scales and beaks even and an even deeper layers. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "The layer to have a filter can technically just be thought of as a relatively small Matrix for which we decide the number of rows and number of columns that this Matrix has and the values within the Matrix are initialized with Some numbers. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "So one type of quote pattern that a filter could detect could be edges and images. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "The filters are able to detect even more sophisticated objects, like full dogs cats lizards and birds to understand what is actually happening here with these convolutional layers and their respective filters. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "Now these simple and kind of geometric filters are what we would see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "Let is now assume that the first hidden layer in our model is a convolutional layer as mentioned earlier when adding a convolutional layer to a model. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "So for this first convolutional layer in this example of hours, we are going to specify that. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "Set and our network is classifying them into their respective categories of whether the images of a XnumberX XnumberX XnumberX Etc. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "We want the layer to contain one filter of size XnumberX by XnumberX. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "So say we have a convolutional neural network that is accepting images of handwritten digits like from the amnesty. 80ee6252-1baa-4c19-9b0b-0da30037997d \n",
      "\n",
      "--------------\n",
      "Although image analysis has been the most widespread use of CNN is they can all be used for other data analysis or classification problems as well. 544706de-3064-4549-a86f-b4b8dd7c7a47 \n",
      "\n",
      "So what do these convolutional layers do just like any other layer a convolutional layer receives input then transforms the input in some way and then outputs the transform input to the next layer with a convolutional layer this transformation is a convolution operation will come back to this operation in a bit for now. 544706de-3064-4549-a86f-b4b8dd7c7a47 \n",
      "\n",
      "Well, actually, let us be a little more precise than that with each convolutional layer. 544706de-3064-4549-a86f-b4b8dd7c7a47 \n",
      "\n",
      "Generally we can think of a CNS an artificial neural network that has some type of specialization for being able to pick out or detect patterns and make sense of Them this pattern detection is what makes CNN so useful for image analysis. 544706de-3064-4549-a86f-b4b8dd7c7a47 \n",
      "\n",
      "Well a CNN now CNN is can and usually do have other non convolutional layers as well. 544706de-3064-4549-a86f-b4b8dd7c7a47 \n",
      "\n",
      "Let is look at a high level idea of what convolutional layers are doing as mentioned earlier convolutional neural networks are able to take patterns and images more precisely. 544706de-3064-4549-a86f-b4b8dd7c7a47 \n",
      "\n",
      "Well, think about how much may be going on in any single image. 544706de-3064-4549-a86f-b4b8dd7c7a47 \n",
      "\n",
      "Video, we will be discussing convolutional neural networks a convolutional neural network also known as a CNN or conf net is an artificial neural network that has so far been most popularly used analyzing images. 544706de-3064-4549-a86f-b4b8dd7c7a47 \n",
      "\n",
      "So if a CNN is just some form of an artificial neural network what differentiates it from just a standard multilayer perceptron or MLP? 544706de-3064-4549-a86f-b4b8dd7c7a47 \n",
      "\n",
      "The layer should have and will speak technically about what a filter is in just a few moments, but for now, I understand that these filters are actually what detect the patterns now when I say that the filters are able to detect patterns what precisely do I mean by patterns. 544706de-3064-4549-a86f-b4b8dd7c7a47 \n",
      "\n",
      "<---------------->\n",
      "order difference: 0\n",
      "Relevant sentence:  So if we can involved our original image of a XnumberX with each of these four filters individually, this is what the output would look like for each filter.    =====    That we just went through with the filter will happen to this new output with the next layers filters.\n",
      "order difference: 0\n",
      "Relevant sentence:  That we just went through with the filter will happen to this new output with the next layers filters.    =====    The brightest pixels can be interpreted as what the filter has detected.\n",
      "order difference: 0\n",
      "Relevant sentence:  The brightest pixels can be interpreted as what the filter has detected.    =====    But as mentioned we can think of these filters as pattern detectors.\n",
      "order difference: 0\n",
      "Relevant sentence:  But as mentioned we can think of these filters as pattern detectors.    =====    So this first one we can see to text top horizontal Edge.\n",
      "order difference: 0\n",
      "Relevant sentence:  So this first one we can see to text top horizontal Edge.    =====    We may see towards the start of our Network more complex filters would be located deeper in the network and would gradually be able to detect more sophisticated patterns like the one shown here we can see The shapes that the filters on the left detected from the images on the right this one here detect circles and this one at the bottom is detecting corners.\n",
      "order difference: 0\n",
      "Relevant sentence:  We may see towards the start of our Network more complex filters would be located deeper in the network and would gradually be able to detect more sophisticated patterns like the one shown here we can see The shapes that the filters on the left detected from the images on the right this one here detect circles and this one at the bottom is detecting corners.    =====    And as we go even further into our layers, the filters are able to detect much more complex patterns, like these dog faces being interpreted in this filter or even the bird legs detected in this one.\n",
      "order difference: 0\n",
      "Relevant sentence:  And as we go even further into our layers, the filters are able to detect much more complex patterns, like these dog faces being interpreted in this filter or even the bird legs detected in this one.    =====    Is of the seven and that is indicated by the brightest pixels here II detects left vertical edges again being displayed with the brightest pixels, the third detects bottom horizontal edges and the fourth detects right vertical edges.\n",
      "order difference: 0\n",
      "Relevant sentence:  Is of the seven and that is indicated by the brightest pixels here II detects left vertical edges again being displayed with the brightest pixels, the third detects bottom horizontal edges and the fourth detects right vertical edges.    =====    Now these filters are really basic and just to take edges.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now these filters are really basic and just to take edges.    =====    So we cannot really observe any specific pattern that was picked out from our filter in the example.\n",
      "order difference: 0\n",
      "Relevant sentence:  So we cannot really observe any specific pattern that was picked out from our filter in the example.    =====    We just looked at an Excel but let us show our original image of the XnumberX here and now let us say we have four three by three filters for our convolutional layer and these filters are filled with the values.\n",
      "order difference: 0\n",
      "Relevant sentence:  We just looked at an Excel but let us show our original image of the XnumberX here and now let us say we have four three by three filters for our convolutional layer and these filters are filled with the values.    =====    You see here and this These values can be represented visually as these filters where the  ones correspond to black ones correspond to White and zeros correspond to gray.\n",
      "order difference: 0\n",
      "Relevant sentence:  You see here and this These values can be represented visually as these filters where the  ones correspond to black ones correspond to White and zeros correspond to gray.    =====    Alright, so now if you are interested in seeing how to work with CNN is and code then check out the CNN and finetuning videos and my Karis deep learning.\n",
      "order difference: 0\n",
      "Relevant sentence:  Alright, so now if you are interested in seeing how to work with CNN is and code then check out the CNN and finetuning videos and my Karis deep learning.    =====    We can see that all four of these filters are detecting edges in the output.\n",
      "order difference: 0\n",
      "Relevant sentence:  So I would like to share that with you all to I have also linked to his lecture in the description of this video.    =====    If we look at the formula for each of these cells we can see that it is just indeed the dot product of the filter with each three by three section of pixels from the input.\n",
      "order difference: 0\n",
      "Relevant sentence:  If we look at the formula for each of these cells we can see that it is just indeed the dot product of the filter with each three by three section of pixels from the input.    =====    I am going to use an example that Jeremy Howard used in one of his lectures for fast AI his example really gave me a lot of insight behind what was going on within a convolutional layer.\n",
      "order difference: 0\n",
      "Relevant sentence:  I am going to use an example that Jeremy Howard used in one of his lectures for fast AI his example really gave me a lot of insight behind what was going on within a convolutional layer.    =====    This is what will then be passed to the next layer as input in this same.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is what will then be passed to the next layer as input in this same.    =====    Three block of pixels from the input to actually illustrate this.\n",
      "order difference: 0\n",
      "Relevant sentence:  Three block of pixels from the input to actually illustrate this.    =====    So here is our three by three filter of random numbers here when the filter first lands on the first three by three block of pixels the dot product of the filter itself with the XnumberXXXnumberX block of pixels from the input will be computed and stored this will occur for each three by three set of pixels that the Involves.\n",
      "order difference: 0\n",
      "Relevant sentence:  So here is our three by three filter of random numbers here when the filter first lands on the first three by three block of pixels the dot product of the filter itself with the XnumberXXXnumberX block of pixels from the input will be computed and stored this will occur for each three by three set of pixels that the Involves.    =====    So look we would just take the dot product of the filter here with this first three by three block and then we would store it over here.\n",
      "order difference: 0\n",
      "Relevant sentence:  So look we would just take the dot product of the filter here with this first three by three block and then we would store it over here.    =====    So here we have our Matrix representation of an image of a XnumberX from the mnist data set the values in this Matrix are the individual pixels from the image.\n",
      "order difference: 0\n",
      "Relevant sentence:  So here we have our Matrix representation of an image of a XnumberX from the mnist data set the values in this Matrix are the individual pixels from the image.    =====    Now we slide to the next three by three block take the dot product and then store the value here.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now we slide to the next three by three block take the dot product and then store the value here.    =====    This Matrix of dot products is going to be the output of this layer and is represented here.\n",
      "order difference: 0\n",
      "Relevant sentence:  This Matrix of dot products is going to be the output of this layer and is represented here.    =====    So here we have this first value is the dot product of this input with this filter and then if I click on another random value over here, we can see that this value is the dot product of the filter with this input.\n",
      "order difference: 0\n",
      "Relevant sentence:  So here we have this first value is the dot product of this input with this filter and then if I click on another random value over here, we can see that this value is the dot product of the filter with this input.    =====    So after This filter has convolved the entire input will be left with a new representation of our input which is going to be made up of the entire Matrix of those stored dot products.\n",
      "order difference: 0\n",
      "Relevant sentence:  So after This filter has convolved the entire input will be left with a new representation of our input which is going to be made up of the entire Matrix of those stored dot products.    =====    All right, so this is our input This input will be passed to a convolutional layer as just discussed.\n",
      "order difference: 0\n",
      "Relevant sentence:  All right, so this is our input This input will be passed to a convolutional layer as just discussed.    =====    Weve specified this layer to only have one filter and this filter is going to convolve across each XnumberXxXnumberX block of pixels from the input.\n",
      "order difference: 0\n",
      "Relevant sentence:  So really we should say that the filter is going to convolve across each three    =====    For example some filters May detect Corners some may detect circles other squares.\n",
      "order difference: 0\n",
      "Relevant sentence:  For example some filters May detect Corners some may detect circles other squares.    =====    Like Eyes Ears hair or fur feathers scales and beaks even and an even deeper layers.\n",
      "order difference: 0\n",
      "Relevant sentence:  Like Eyes Ears hair or fur feathers scales and beaks even and an even deeper layers.    =====    The layer to have a filter can technically just be thought of as a relatively small Matrix for which we decide the number of rows and number of columns that this Matrix has and the values within the Matrix are initialized with Some numbers.\n",
      "order difference: 0\n",
      "Relevant sentence:  The layer to have a filter can technically just be thought of as a relatively small Matrix for which we decide the number of rows and number of columns that this Matrix has and the values within the Matrix are initialized with Some numbers.    =====    So one type of quote pattern that a filter could detect could be edges and images.\n",
      "order difference: 0\n",
      "Relevant sentence:  So one type of quote pattern that a filter could detect could be edges and images.    =====    Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image.    =====    The filters are able to detect even more sophisticated objects, like full dogs cats lizards and birds to understand what is actually happening here with these convolutional layers and their respective filters.\n",
      "order difference: 0\n",
      "Relevant sentence:  The filters are able to detect even more sophisticated objects, like full dogs cats lizards and birds to understand what is actually happening here with these convolutional layers and their respective filters.    =====    Now these simple and kind of geometric filters are what we would see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now these simple and kind of geometric filters are what we would see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects.    =====    Let is now assume that the first hidden layer in our model is a convolutional layer as mentioned earlier when adding a convolutional layer to a model.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is now assume that the first hidden layer in our model is a convolutional layer as mentioned earlier when adding a convolutional layer to a model.    =====    So for this first convolutional layer in this example of hours, we are going to specify that.\n",
      "order difference: 0\n",
      "Relevant sentence:  So for this first convolutional layer in this example of hours, we are going to specify that.    =====    Set and our network is classifying them into their respective categories of whether the images of a XnumberX XnumberX XnumberX Etc.\n",
      "order difference: 0\n",
      "Relevant sentence:  Set and our network is classifying them into their respective categories of whether the images of a XnumberX XnumberX XnumberX Etc.    =====    We want the layer to contain one filter of size XnumberX by XnumberX.\n",
      "order difference: 0\n",
      "Relevant sentence:  We want the layer to contain one filter of size XnumberX by XnumberX.    =====    So say we have a convolutional neural network that is accepting images of handwritten digits like from the amnesty.\n",
      "order difference: 0\n",
      "Relevant sentence:  Although image analysis has been the most widespread use of CNN is they can all be used for other data analysis or classification problems as well.    =====    So what do these convolutional layers do just like any other layer a convolutional layer receives input then transforms the input in some way and then outputs the transform input to the next layer with a convolutional layer this transformation is a convolution operation will come back to this operation in a bit for now.\n",
      "order difference: 0\n",
      "Relevant sentence:  So what do these convolutional layers do just like any other layer a convolutional layer receives input then transforms the input in some way and then outputs the transform input to the next layer with a convolutional layer this transformation is a convolution operation will come back to this operation in a bit for now.    =====    Well, actually, let us be a little more precise than that with each convolutional layer.\n",
      "order difference: 0\n",
      "Relevant sentence:  Well, actually, let us be a little more precise than that with each convolutional layer.    =====    Generally we can think of a CNS an artificial neural network that has some type of specialization for being able to pick out or detect patterns and make sense of Them this pattern detection is what makes CNN so useful for image analysis.\n",
      "order difference: 0\n",
      "Relevant sentence:  Generally we can think of a CNS an artificial neural network that has some type of specialization for being able to pick out or detect patterns and make sense of Them this pattern detection is what makes CNN so useful for image analysis.    =====    Well a CNN now CNN is can and usually do have other non convolutional layers as well.\n",
      "order difference: 0\n",
      "Relevant sentence:  Well a CNN now CNN is can and usually do have other non convolutional layers as well.    =====    Let is look at a high level idea of what convolutional layers are doing as mentioned earlier convolutional neural networks are able to take patterns and images more precisely.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is look at a high level idea of what convolutional layers are doing as mentioned earlier convolutional neural networks are able to take patterns and images more precisely.    =====    Well, think about how much may be going on in any single image.\n",
      "order difference: 0\n",
      "Relevant sentence:  Well, think about how much may be going on in any single image.    =====    Video, we will be discussing convolutional neural networks a convolutional neural network also known as a CNN or conf net is an artificial neural network that has so far been most popularly used analyzing images.\n",
      "order difference: 0\n",
      "Relevant sentence:  Video, we will be discussing convolutional neural networks a convolutional neural network also known as a CNN or conf net is an artificial neural network that has so far been most popularly used analyzing images.    =====    So if a CNN is just some form of an artificial neural network what differentiates it from just a standard multilayer perceptron or MLP?\n",
      "order difference: 0\n",
      "Relevant sentence:  So if a CNN is just some form of an artificial neural network what differentiates it from just a standard multilayer perceptron or MLP?    =====    The layer should have and will speak technically about what a filter is in just a few moments, but for now, I understand that these filters are actually what detect the patterns now when I say that the filters are able to detect patterns what precisely do I mean by patterns.\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3)]\n",
      "[[[\"That we just went through with the filter will happen to this new output with the next layers filters. Now, this was just a very simple illustration. But as mentioned we can think of these filters as pattern detectors. So we can't really observe any specific pattern that was picked out from our filter in the example. We just looked at an Excel but let's show our original image of the 7 here and now let's say we have four three by three filters for our convolutional layer and these filters are filled with the values. You see here and this These values can be represented visually as these filters where the - ones correspond to black ones correspond to White and zeros correspond to gray. So if we can involved our original image of a 7 with each of these four filters individually, this is what the output would look like for each filter. We can see that all four of these filters are detecting edges in the output. The brightest pixels can be interpreted as what the filter has detected. So this first one we can see to text top horizontal Edge. Is of the seven and that's indicated by the brightest pixels here II detects left vertical edges again being displayed with the brightest pixels, the third detects bottom horizontal edges and the fourth detects right vertical edges. Now these filters are really basic and just to take edges. These are filters. We may see towards the start of our Network more complex filters would be located deeper in the network and would gradually be able to detect more sophisticated patterns like the one shown here we can see The shapes that the filters on the left detected from the images on the right this one here detect circles and this one at the bottom is detecting corners. And as we go even further into our layers, the filters are able to detect much more complex patterns, like these dog faces being interpreted in this filter or even the bird legs detected in this one. Alright, so now if you're interested in seeing how to work with CNN's and code then check out the CNN and fine-tuning videos and my Karis deep learning. \"], '2020-01-31T07:55:13Z', '716067a60a1a4034abc49a12ecafb39b', '2ea7614f-7029-4e24-8f06-9b8550296ce6']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Three block of pixels from the input to actually illustrate this. I'm going to use an example that Jeremy Howard used in one of his lectures for fast AI his example really gave me a lot of insight behind what was going on within a convolutional layer. So I'd like to share that with you all to I've also linked to his lecture in the description of this video. So here we have our Matrix representation of an image of a 7 from the mnist data set the values in this Matrix are the individual pixels from the image. All right, so this is our input This input will be passed to a convolutional layer as just discussed. We've specified this layer to only have one filter and this filter is going to convolve across each 3x3 block of pixels from the input. So here's our three by three filter of random numbers here when the filter first lands on the first three by three block of pixels the dot product of the filter itself with the 3X3 block of pixels from the input will be computed and stored this will occur for each three by three set of pixels that the Involves. So look we would just take the dot product of the filter here with this first three by three block and then we'd store it over here. Now we slide to the next three by three block take the dot product and then store the value here. If we look at the formula for each of these cells we can see that it is just indeed the dot product of the filter with each three by three section of pixels from the input. So here we have this first value is the dot product of this input with this filter and then if I click on another random value over here, we can see that this value is the dot product of the filter with this input. So after This filter has convolved the entire input will be left with a new representation of our input which is going to be made up of the entire Matrix of those stored dot products. We got from the filter. This Matrix of dot products is going to be the output of this layer and is represented here. This is what will then be passed to the next layer as input in this same. \"], '2020-01-31T07:53:13Z', '716067a60a1a4034abc49a12ecafb39b', '686a3a00-f4ce-4d3f-abd4-7164d5d956bc']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Multiple edges shapes textures objects Etc. So one type of quote pattern that a filter could detect could be edges and images. So this filter would be called an edge detector. For example some filters May detect Corners some may detect circles other squares. Now these simple and kind of geometric filters are what we'd see at the start of our Network the deeper our Network goes the more sophisticated these filters become so in later layers rather An edges and simple shapes our filters may be able to detect specific objects. Like Eyes Ears hair or fur feathers scales and beaks even and an even deeper layers. The filters are able to detect even more sophisticated objects, like full dogs cats lizards and birds to understand what's actually happening here with these convolutional layers and their respective filters. Let's look at an example. So say we have a convolutional neural network that's accepting images of handwritten digits like from the amnesty. Set and our network is classifying them into their respective categories of whether the images of a 1 2 3 Etc. Let's now assume that the first hidden layer in our model is a convolutional layer as mentioned earlier when adding a convolutional layer to a model. We also have to specify how many filters we want. The layer to have a filter can technically just be thought of as a relatively small Matrix for which we decide the number of rows and number of columns that this Matrix has and the values within the Matrix are initialized with Some numbers. So for this first convolutional layer in this example of hours, we're going to specify that. We want the layer to contain one filter of size 3 by 3. Now when this convolutional layer receives input the filter will slide over each three by three set of pixels from the input itself until it slid over every three by three block of pixels from the entire image. This sliding is actually referred to as convolving. So really we should say that the filter is going to convolve across each three \"], '2020-01-31T07:51:13Z', '716067a60a1a4034abc49a12ecafb39b', '80ee6252-1baa-4c19-9b0b-0da30037997d']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Video, we'll be discussing convolutional neural networks a convolutional neural network also known as a CNN or conf net is an artificial neural network that has so far been most popularly used analyzing images. Although image analysis has been the most widespread use of CNN's they can all be used for other data analysis or classification problems as well. Most. Generally we can think of a CNS an artificial neural network that has some type of specialization for being able to pick out or detect patterns and make sense of Them this pattern detection is what makes CNN so useful for image analysis. So if a CNN is just some form of an artificial neural network what differentiates it from just a standard multi-layer perceptron or MLP? Well, a CNN has hidden layers called convolutional layers. And these layers are precisely what makes a CNN. Well a CNN now CNN's can and usually do have other non convolutional layers as well. But the basis of a CNN is the convolutional layers. All right. So what do these convolutional layers do just like any other layer a convolutional layer receives input then transforms the input in some way and then outputs the transform input to the next layer with a convolutional layer this transformation is a convolution operation will come back to this operation in a bit for now. Let's look at a high level idea of what convolutional layers are doing as mentioned earlier convolutional neural networks are able to take patterns and images more precisely. The convolutional layers are able to detect patterns. Well, actually, let's be a little more precise than that with each convolutional layer. We need to specify the number of filters. The layer should have and will speak technically about what a filter is in just a few moments, but for now, I understand that these filters are actually what detect the patterns now when I say that the filters are able to detect patterns what precisely do I mean by patterns. Well, think about how much may be going on in any single image. \"], '2020-01-31T07:49:13Z', '716067a60a1a4034abc49a12ecafb39b', '544706de-3064-4549-a86f-b4b8dd7c7a47']] \n",
      "\n",
      "\n",
      "3\n",
      "Before Merging 4\n",
      "[]\n",
      "After Merging 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Ent map before filtering:  {'3': [3216, 3216, 3216, 3216, 3216, 4003, 3216, 3216, 3216, 4725, 3216, 6402, 3216, 3216, 3216, 3216, 3216, 3216, 4003, 3216], '2': [4006, 3378, 693, 3216, 3899, 3216, 818, 4006, 3216, 44, 3216, 3216, 3216, 3216, 3216, 4006, 44, 4006, 4006, 4006], '1': [3741, 3216, 3216, 2987, 5249, 1933, 103, 3712, 5465, 852, 5249, 3741, 3216, 852, 103, 3216, 27, 5907, 818, 1420], '0': [3524, 3216, 4006, 4821, 3378, 1933, 4774, 1658, 4943, 3216]}\n",
      "Group Ent map after filtering:  {'3': [(3216, 16)], '2': [(3216, 8), (4006, 6)], '1': [(3216, 4)], '0': []}\n",
      "Using  None  for feature extraction\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 103, \"module\": \"scorer\", \"batches count\": 1, \"number of sentences\": 100, \"ts\": \"2020-03-05T08:26:46.124189Z\", \"msg\": \"computing in batches\"}\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 106, \"module\": \"scorer\", \"iteration count:\": 0, \"ts\": \"2020-03-05T08:26:46.125251Z\", \"msg\": \"getting feature vector\"}\n",
      "('Sometimes it is going to randomly select a whole sequence of actions that actually lead to scoring a goal.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('Sometimes it is going to randomly select a whole sequence of actions that actually lead to scoring a goal.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And while this obviously makes it easier for your policy to converge to the desired Behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('Sometimes it is going to randomly select a whole sequence of actions that actually lead to scoring a goal.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So if you are looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('Sometimes it is going to randomly select a whole sequence of actions that actually lead to scoring a goal.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('Sometimes it is going to randomly select a whole sequence of actions that actually lead to scoring a goal.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('Sometimes it is going to randomly select a whole sequence of actions that actually lead to scoring a goal.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So if you are looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And now obviously since our agent has not learned anything useful yet.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Now one of the simplest ways to train a policy network is a method called.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And now obviously since our agent has not learned anything useful yet.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('And so what policy gradients are going to do is that That for every episode where we got a positive reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so what policy gradients are going to do is that That for every episode where we got a positive reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so what policy gradients are going to do is that That for every episode where we got a positive reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so what policy gradients are going to do is that That for every episode where we got a positive reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so what policy gradients are going to do is that That for every episode where we got a positive reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so what policy gradients are going to do is that That for every episode where we got a positive reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And so what policy gradients are going to do is that That for every episode where we got a positive reward.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Now one of the simplest ways to train a policy network is a method called.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('Were going to use the normal gradients to increase the probability of those actions in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('Were going to use the normal gradients to increase the probability of those actions in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('Were going to use the normal gradients to increase the probability of those actions in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Now one of the simplest ways to train a policy network is a method called.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('Were going to use the normal gradients to increase the probability of those actions in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('But whenever we got a negative reward, we are going to apply the same gradient.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('But whenever we got a negative reward, we are going to apply the same gradient.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('But whenever we got a negative reward, we are going to apply the same gradient.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('But whenever we got a negative reward, we are going to apply the same gradient.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('But whenever we got a negative reward, we are going to apply the same gradient.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Now one of the simplest ways to train a policy network is a method called.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('But whenever we got a negative reward, we are going to apply the same gradient.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So if you are looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It produces a random output action, you know, either up or down you send that action back to the game engine in the game engine produces the next frame and this is how the loop continues and the network in this case.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Now one of the simplest ways to train a policy network is a method called.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So in a sense our agent is learning how to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So in a sense our agent is learning how to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So in a sense our agent is learning how to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So in a sense our agent is learning how to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('So in a sense our agent is learning how to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('So in a sense our agent is learning how to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And now obviously since our agent has not learned anything useful yet.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('So in a sense our agent is learning how to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So imagine you want to train a neural network to play the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So in a sense our agent is learning how to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So in a sense our agent is learning how to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('I know this was a very quick introduction to reinforcement learning.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('I know this was a very quick introduction to reinforcement learning.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is never going to get to see any of that reward and I think it is important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('I know this was a very quick introduction to reinforcement learning.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('I know this was a very quick introduction to reinforcement learning.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('I know this was a very quick introduction to reinforcement learning.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we cannot use supervised learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('I know this was a very quick introduction to reinforcement learning.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('I know this was a very quick introduction to reinforcement learning.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So the framework in reinforcement learning is actually surprisingly similar to the normal frame work in supervised learning so', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('All right, so we can use policy gradients that train a neural network to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('All right, so we can use policy gradients that train a neural network to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('All right, so we can use policy gradients that train a neural network to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Now one of the simplest ways to train a policy network is a method called.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('All right, so we can use policy gradients that train a neural network to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So imagine you want to train a neural network to play the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('All right, so we can use policy gradients that train a neural network to play the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('But as always there are a few very significant downsides to using these methods.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('But as always there are a few very significant downsides to using these methods.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is never going to get to see any of that reward and I think it is important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('But as always there are a few very significant downsides to using these methods.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So firstly we were taping is a custom process that needs to be redone for every new environment.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('But as always there are a few very significant downsides to using these methods.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It could be a fully connected network, but you can obviously apply convolutions there as well.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('But as always there are a few very significant downsides to using these methods.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('But as always there are a few very significant downsides to using these methods.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('We can actually train that neural network to replicate the actions of the human gamer, but there are two significant downside to this approach.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So if you are looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And now obviously since our agent has not learned anything useful yet.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So imagine you want to train a neural network to play the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('What you would do in a supervised setting is you would have a good human gamer play the game of pong for a couple of hours and you would create a data set where you log all of the frames that that human is seeing on the screen as well as the actions that he takes in response to those frames.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So if you are looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It produces a random output action, you know, either up or down you send that action back to the game engine in the game engine produces the next frame and this is how the loop continues and the network in this case.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('What you would do in a supervised setting is you would have a good human gamer play the game of pong for a couple of hours and you would create a data set where you log all of the frames that that human is seeing on the screen as well as the actions that he takes in response to those frames.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So the problem with policy gradients is that our policy gradient is going to', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So the problem with policy gradients is that our policy gradient is going to', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So the problem with policy gradients is that our policy gradient is going to', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So the problem with policy gradients is that our policy gradient is going to', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So the problem with policy gradients is that our policy gradient is going to', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So the problem with policy gradients is that our policy gradient is going to', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So the problem with policy gradients is that our policy gradient is going to', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a')\n",
      "('So the problem with policy gradients is that our policy gradient is going to', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('So the problem with policy gradients is that our policy gradient is going to', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('Now one of the simplest ways to train a policy network is a method called.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('So the problem with policy gradients is that our policy gradient is going to', '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('But remember that actually the most part of that episode we were doing really well.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So we are just going to run a whole bunch of those game frames through your network select random actions feed them back into the engine and just create a whole bunch of random pong games.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('But remember that actually the most part of that episode we were doing really well.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('But remember that actually the most part of that episode we were doing really well.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('And there is a lot of very hard engineering going on behind the scenes.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Now one of the simplest ways to train a policy network is a method called.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So we do not really want to decrease the likelihood of those actions and in reinforcement learning.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So reward shipping is the process of manually designing a reward function that needs to guide your policy to some desired Behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('It is never going to get to see any of that reward and I think it is important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So it turns out that reward shaping is actually surprisingly difficult in a lot of cases when you', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So a famous example is the game Montezuma is Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('So a famous example is the game Montezuma is Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('So a famous example is the game Montezuma is Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So if you are looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('So a famous example is the game Montezuma is Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('So a famous example is the game Montezuma is Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('And while this obviously makes it easier for your policy to converge to the desired Behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Now one of the simplest ways to train a policy network is a method called.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And so your policy gradient is never going to see a single positive reward.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So it has no idea what to do and the same case applies to robotic control where for example you would like to train in a row.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('It is never going to get to see any of that reward and I think it is important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('So it has no idea what to do and the same case applies to robotic control where for example you would like to train in a row.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e')\n",
      "('So it has no idea what to do and the same case applies to robotic control where for example you would like to train in a row.', '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a') ('So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('From The Amazing results and vintage Atari games deep mines Victory with alphago stunning breakthroughs in robotic arm manipulation and even beating professional players at what do you want to build up?', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('It is going to lose most of those games, but the thing is that', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a')\n",
      "('From The Amazing results and vintage Atari games deep mines Victory with alphago stunning breakthroughs in robotic arm manipulation and even beating professional players at what do you want to build up?', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('And so in this video, I want to introduce you guys to the whole subfield in machine learning that is called reinforcement learning which I think is one of the most promising directions to actually get to very intelligent robotic Behavior.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('The successes of supervised deep learning have continued to pile up and people from many different backgrounds have started using deep neural Nets to solve a wide range of new tie.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('And so in this video, I want to introduce you guys to the whole subfield in machine learning that is called reinforcement learning which I think is one of the most promising directions to actually get to very intelligent robotic Behavior.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('The successes of supervised deep learning have continued to pile up and people from many different backgrounds have started using deep neural Nets to solve a wide range of new tie.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So in the most common machine learning applications people use what we call supervised learning and this means that you give an input to your neural network model, but you know the output that your model should produce and therefore you can compute gradients using something like the back propagation.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('The successes of supervised deep learning have continued to pile up and people from many different backgrounds have started using deep neural Nets to solve a wide range of new tie.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So the framework in reinforcement learning is actually surprisingly similar to the normal frame work in supervised learning so', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('Tasks including how to learn intelligent behavior in complex Dynamic environments.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So in this episode, I will give a general introduction into the field of reinforcement learning as well as an overview of the most challenging problems that we are facing today.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we cannot use supervised learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So in this episode, I will give a general introduction into the field of reinforcement learning as well as an overview of the most challenging problems that we are facing today.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So in this episode, I will give a general introduction into the field of reinforcement learning as well as an overview of the most challenging problems that we are facing today.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So the framework in reinforcement learning is actually surprisingly similar to the normal frame work in supervised learning so', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('If you are looking for a solid introduction into the field of deep reinforcement learning then this episode is exactly what you are looking for.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('If you are looking for a solid introduction into the field of deep reinforcement learning then this episode is exactly what you are looking for.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we cannot use supervised learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('If you are looking for a solid introduction into the field of deep reinforcement learning then this episode is exactly what you are looking for.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('If you are looking for a solid introduction into the field of deep reinforcement learning then this episode is exactly what you are looking for.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So the framework in reinforcement learning is actually surprisingly similar to the normal frame work in supervised learning so', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('I think is a very important one it basically says that the robots we have been building for decades now are physically perfectly capable of doing a wide range of useful tasks, but the problem is that we cannot embed them with the needed intelligence to do those things.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So it turns out that having a robot learn how to do something very simple like picking up.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So in the most common machine learning applications people use what we call supervised learning and this means that you give an input to your neural network model, but you know the output that your model should produce and therefore you can compute gradients using something like the back propagation.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So it turns out that having a robot learn how to do something very simple like picking up.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So imagine you want to train a neural network to play the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So it turns out that having a robot learn how to do something very simple like picking up.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we cannot use supervised learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So it turns out that having a robot learn how to do something very simple like picking up.', '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('Farm to pick up an object and stack it on to something else.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('Dont get me wrong, but there is not a lot of self driven Behavior.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b')\n",
      "('Well, the typical robot has about seven joints that it can move.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So it is a relatively High action space and if you only give it a positive reward when it is actually successfully stacked a block well by doing random exploration.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So it is a relatively High action space and if you only give it a positive reward when it is actually successfully stacked a block well by doing random exploration.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('It is never going to get to see any of that reward and I think it is important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Ice cold Michelle Pfeiffer that white gold this one for them hood girls them good girls straight and so the traditional approach to solve this issue of sparse rewards has been the use of reward shaping.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So reward shipping is the process of manually designing a reward function that needs to guide your policy to some desired Behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So reward shipping is the process of manually designing a reward function that needs to guide your policy to some desired Behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So reward shipping is the process of manually designing a reward function that needs to guide your policy to some desired Behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And while this obviously makes it easier for your policy to converge to the desired Behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And while this obviously makes it easier for your policy to converge to the desired Behavior.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So firstly we were taping is a custom process that needs to be redone for every new environment.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So if you are looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Is that reward shaping suffers from what we call the alignment problem.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Is that reward shaping suffers from what we call the alignment problem.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Is that reward shaping suffers from what we call the alignment problem.', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So it turns out that reward shaping is actually surprisingly difficult in a lot of cases when you', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So it turns out that reward shaping is actually surprisingly difficult in a lot of cases when you', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So it turns out that reward shaping is actually surprisingly difficult in a lot of cases when you', '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e') ('I mean, look at this shaped reward function for a robotic control task.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('It is either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('Now one of the simplest ways to train a policy network is a method called.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('It produces a random output action, you know, either up or down you send that action back to the game engine in the game engine produces the next frame and this is how the loop continues and the network in this case.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('It is either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('It could be a fully connected network, but you can obviously apply convolutions there as well.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So in the most common machine learning applications people use what we call supervised learning and this means that you give an input to your neural network model, but you know the output that your model should produce and therefore you can compute gradients using something like the back propagation.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('It could be a fully connected network, but you can obviously apply convolutions there as well.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So whether he is pushing the up Arrow or down arrow and we can then feed those input frames through a very simple neural network that at the output Produce two simple actions.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So the framework in reinforcement learning is actually surprisingly similar to the normal frame work in supervised learning so', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('The only feedback that we are going to give it is the scoreboard in the game.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('It is either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So in order to train or policy Network, the first thing we are going to do is collect a bunch of experience.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So in the most common machine learning applications people use what we call supervised learning and this means that you give an input to your neural network model, but you know the output that your model should produce and therefore you can compute gradients using something like the back propagation.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So in order to train or policy Network, the first thing we are going to do is collect a bunch of experience.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So imagine you want to train a neural network to play the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So in order to train or policy Network, the first thing we are going to do is collect a bunch of experience.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So in order to train or policy Network, the first thing we are going to do is collect a bunch of experience.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we cannot use supervised learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So in order to train or policy Network, the first thing we are going to do is collect a bunch of experience.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So we are just going to run a whole bunch of those game frames through your network select random actions feed them back into the engine and just create a whole bunch of random pong games.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So imagine you want to train a neural network to play the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So we are just going to run a whole bunch of those game frames through your network select random actions feed them back into the engine and just create a whole bunch of random pong games.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('What you would do in a supervised setting is you would have a good human gamer play the game of pong for a couple of hours and you would create a data set where you log all of the frames that that human is seeing on the screen as well as the actions that he takes in response to those frames.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('So we are just going to run a whole bunch of those game frames through your network select random actions feed them back into the engine and just create a whole bunch of random pong games.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('It is either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('And now obviously since our agent has not learned anything useful yet.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('It is going to lose most of those games, but the thing is that', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('It is going to lose most of those games, but the thing is that', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a') ('So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908')\n",
      "('Support me on patreon for which I would just like to say.', '2020-01-31T08:09:21Z', '716067a60a1a4034abc49a12ecafb39b', '98d73e6d-9061-460e-84fd-2137177746f0') ('Company, but the media images they have created might be a little bit confusing to a lot of people that do not know what is going on behind the seats.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b')\n",
      "('Support me on patreon for which I would just like to say.', '2020-01-31T08:09:21Z', '716067a60a1a4034abc49a12ecafb39b', '98d73e6d-9061-460e-84fd-2137177746f0') ('So I personally believe that most if not all technological progress is beneficial in the long run as long as we can make sure that there are no monopolies that can maintain or enforce their power with the Views of AI well anyway enough politics for one video.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b')\n",
      "('Support me on patreon for which I would just like to say.', '2020-01-31T08:09:21Z', '716067a60a1a4034abc49a12ecafb39b', '98d73e6d-9061-460e-84fd-2137177746f0') ('I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Support me on patreon for which I would just like to say.', '2020-01-31T08:09:21Z', '716067a60a1a4034abc49a12ecafb39b', '98d73e6d-9061-460e-84fd-2137177746f0') ('And there is a lot of very hard engineering going on behind the scenes.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('I am doing these videos completely in my spare time and knowing that there is people out there that appreciate this content really feels great.', '2020-01-31T08:09:21Z', '716067a60a1a4034abc49a12ecafb39b', '98d73e6d-9061-460e-84fd-2137177746f0') ('Company, but the media images they have created might be a little bit confusing to a lot of people that do not know what is going on behind the seats.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b')\n",
      "('I am doing these videos completely in my spare time and knowing that there is people out there that appreciate this content really feels great.', '2020-01-31T08:09:21Z', '716067a60a1a4034abc49a12ecafb39b', '98d73e6d-9061-460e-84fd-2137177746f0') ('We see in technology, but on the other hand, I also feel like the media is focusing way too much on the negative side of these Technologies simply because people fear what they do not understand and well fear sells more advertisement than Utopias.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b')\n",
      "('I am doing these videos completely in my spare time and knowing that there is people out there that appreciate this content really feels great.', '2020-01-31T08:09:21Z', '716067a60a1a4034abc49a12ecafb39b', '98d73e6d-9061-460e-84fd-2137177746f0') ('So this video is an introduction into deep reinforcement learning and an overview of the most challenging problems that we are facing in the field in the next video.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b')\n",
      "('I am doing these videos completely in my spare time and knowing that there is people out there that appreciate this content really feels great.', '2020-01-31T08:09:21Z', '716067a60a1a4034abc49a12ecafb39b', '98d73e6d-9061-460e-84fd-2137177746f0') ('And there is a lot of very hard engineering going on behind the scenes.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('I am doing these videos completely in my spare time and knowing that there is people out there that appreciate this content really feels great.', '2020-01-31T08:09:21Z', '716067a60a1a4034abc49a12ecafb39b', '98d73e6d-9061-460e-84fd-2137177746f0') ('So I think that one of the biggest challenges in navigating our digital landscape is Discerning truth from fiction in this ocean of Click bait that is powered by the advertisement industry.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Dont get me wrong, but there is not a lot of self driven Behavior.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Dont get me wrong, but there is not a lot of self driven Behavior.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('So I think if you go out on the streets and you asked a thousand people with the most advanced robots today are but they would probably', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('There is not a lot of intelligent decision making going on in those robots.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('There is not a lot of intelligent decision making going on in those robots.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('And there is a lot of very hard engineering going on behind the scenes.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('There is not a lot of intelligent decision making going on in those robots.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('And I think the atlas robot from Boston Dynamics is a very clear example of what I mean.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('There is not a lot of intelligent decision making going on in those robots.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('So I think if you go out on the streets and you asked a thousand people with the most advanced robots today are but they would probably', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Company, but the media images they have created might be a little bit confusing to a lot of people that do not know what is going on behind the seats.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('And there is a lot of very hard engineering going on behind the scenes.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('Company, but the media images they have created might be a little bit confusing to a lot of people that do not know what is going on behind the seats.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('And I think the atlas robot from Boston Dynamics is a very clear example of what I mean.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('I think we should not be negligible of the potential risks that these Technologies can bring so I think it is very good at a lot more people are getting involved in the whole AI Safety Research because this is going to become very fundamental threats, like autonomous weapons and mass surveillance are to be taken very seriously.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('And so the only hope we have is that International law is going to be somewhat able to keep up with the rapid progress.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('And there is a lot of very hard engineering going on behind the scenes.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So I personally believe that most if not all technological progress is beneficial in the long run as long as we can make sure that there are no monopolies that can maintain or enforce their power with the Views of AI well anyway enough politics for one video.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So I personally believe that most if not all technological progress is beneficial in the long run as long as we can make sure that there are no monopolies that can maintain or enforce their power with the Views of AI well anyway enough politics for one video.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('And I think the atlas robot from Boston Dynamics is a very clear example of what I mean.', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "('So this video is an introduction into deep reinforcement learning and an overview of the most challenging problems that we are facing in the field in the next video.', '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b') ('So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?', '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0')\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 390, \"module\": \"grouper_segments\", \"outlier threshold is : \": 0.4544264078140259, \"ts\": \"2020-03-05T08:26:48.966322Z\", \"msg\": \"Outlier Score\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Community Algorithm\n",
      "cluster before alteration=========>\n",
      "Sometimes it is going to randomly select a whole sequence of actions that actually lead to scoring a goal.\n",
      "And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.\n",
      "We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.\n",
      "And so what policy gradients are going to do is that That for every episode where we got a positive reward.\n",
      "Were going to use the normal gradients to increase the probability of those actions in the future.\n",
      "But whenever we got a negative reward, we are going to apply the same gradient.\n",
      "Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.\n",
      "And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.\n",
      "So in a sense our agent is learning how to play the game of pong.\n",
      "I know this was a very quick introduction to reinforcement learning.\n",
      "So if you want to read up a bit and spend a little bit more timing thinking about the details, I really recommend to read and red carpet these blog post punk from pixels.\n",
      "All right, so we can use policy gradients that train a neural network to play the game of pong.\n",
      "But as always there are a few very significant downsides to using these methods.\n",
      "So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.\n",
      "But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.\n",
      "So the problem with policy gradients is that our policy gradient is going to\n",
      "cluster before alteration=========>\n",
      "So we do not really want to decrease the likelihood of those actions and in reinforcement learning.\n",
      "That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.\n",
      "It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.\n",
      "We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.\n",
      "And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.\n",
      "And so your policy gradient is never going to see a single positive reward.\n",
      "It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.\n",
      "And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.\n",
      "But remember that actually the most part of that episode we were doing really well.\n",
      "So a famous example is the game Montezuma is Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level.\n",
      "So it has no idea what to do and the same case applies to robotic control where for example you would like to train in a row.\n",
      "cluster before alteration=========>\n",
      "And while this obviously makes it easier for your policy to converge to the desired Behavior.\n",
      "So if you are looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games.\n",
      "So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.\n",
      "It is never going to get to see any of that reward and I think it is important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation.\n",
      "Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.\n",
      "So firstly we were taping is a custom process that needs to be redone for every new environment.\n",
      "So reward shipping is the process of manually designing a reward function that needs to guide your policy to some desired Behavior.\n",
      "So it turns out that reward shaping is actually surprisingly difficult in a lot of cases when you\n",
      "Farm to pick up an object and stack it on to something else.\n",
      "Well, the typical robot has about seven joints that it can move.\n",
      "So it is a relatively High action space and if you only give it a positive reward when it is actually successfully stacked a block well by doing random exploration.\n",
      "Ice cold Michelle Pfeiffer that white gold this one for them hood girls them good girls straight and so the traditional approach to solve this issue of sparse rewards has been the use of reward shaping.\n",
      "Is that reward shaping suffers from what we call the alignment problem.\n",
      "cluster before alteration=========>\n",
      "And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.\n",
      "Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.\n",
      "So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.\n",
      "And now obviously since our agent has not learned anything useful yet.\n",
      "Now one of the simplest ways to train a policy network is a method called.\n",
      "It produces a random output action, you know, either up or down you send that action back to the game engine in the game engine produces the next frame and this is how the loop continues and the network in this case.\n",
      "It could be a fully connected network, but you can obviously apply convolutions there as well.\n",
      "So we are just going to run a whole bunch of those game frames through your network select random actions feed them back into the engine and just create a whole bunch of random pong games.\n",
      "It is going to lose most of those games, but the thing is that\n",
      "So the approaching policy gradients is that you start out with a completely random Network you feed that Network frame from the game engine.\n",
      "The only feedback that we are going to give it is the scoreboard in the game.\n",
      "So in order to train or policy Network, the first thing we are going to do is collect a bunch of experience.\n",
      "cluster before alteration=========>\n",
      "It is either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation.\n",
      "So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong.\n",
      "So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.\n",
      "So imagine you want to train a neural network to play the game of pong.\n",
      "And so in this video, I want to introduce you guys to the whole subfield in machine learning that is called reinforcement learning which I think is one of the most promising directions to actually get to very intelligent robotic Behavior.\n",
      "For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we cannot use supervised learning.\n",
      "So the framework in reinforcement learning is actually surprisingly similar to the normal frame work in supervised learning so\n",
      "We can actually train that neural network to replicate the actions of the human gamer, but there are two significant downside to this approach.\n",
      "What you would do in a supervised setting is you would have a good human gamer play the game of pong for a couple of hours and you would create a data set where you log all of the frames that that human is seeing on the screen as well as the actions that he takes in response to those frames.\n",
      "So in the most common machine learning applications people use what we call supervised learning and this means that you give an input to your neural network model, but you know the output that your model should produce and therefore you can compute gradients using something like the back propagation.\n",
      "So whether he is pushing the up Arrow or down arrow and we can then feed those input frames through a very simple neural network that at the output Produce two simple actions.\n",
      "cluster before alteration=========>\n",
      "So in this episode, I will give a general introduction into the field of reinforcement learning as well as an overview of the most challenging problems that we are facing today.\n",
      "If you are looking for a solid introduction into the field of deep reinforcement learning then this episode is exactly what you are looking for.\n",
      "From The Amazing results and vintage Atari games deep mines Victory with alphago stunning breakthroughs in robotic arm manipulation and even beating professional players at what do you want to build up?\n",
      "The field reinforcement learning has literally exploded in recent years every city impressive breakthrough on the imagenet classification challenge in XnumberX.\n",
      "The successes of supervised deep learning have continued to pile up and people from many different backgrounds have started using deep neural Nets to solve a wide range of new tie.\n",
      "Tasks including how to learn intelligent behavior in complex Dynamic environments.\n",
      "So nips XnumberX Peter at Beale gave me very inspiring demo in front of a large audience of some of the brightest Minds in Ai and machine learning.\n",
      "So you showed this video where a robot is cleaning a living room bringing somebody a bottle of beer and basically doing a whole range of mundane tasks that robots in SciFi movies can do without question and then End of the video Peter revealed that the robots actions were actually entirely remote controlled by a human operator and the takeaway from this demo.\n",
      "I think is a very important one it basically says that the robots we have been building for decades now are physically perfectly capable of doing a wide range of useful tasks, but the problem is that we cannot embed them with the needed intelligence to do those things.\n",
      "So basically creating useful stateoftheart robotics is a software Challenge and not a hardware.\n",
      "So it turns out that having a robot learn how to do something very simple like picking up.\n",
      "cluster before alteration=========>\n",
      "So this video is an introduction into deep reinforcement learning and an overview of the most challenging problems that we are facing in the field in the next video.\n",
      "There is not a lot of intelligent decision making going on in those robots.\n",
      "Dont get me wrong, but there is not a lot of self driven Behavior.\n",
      "Company, but the media images they have created might be a little bit confusing to a lot of people that do not know what is going on behind the seats.\n",
      "So I personally believe that most if not all technological progress is beneficial in the long run as long as we can make sure that there are no monopolies that can maintain or enforce their power with the Views of AI well anyway enough politics for one video.\n",
      "We see in technology, but on the other hand, I also feel like the media is focusing way too much on the negative side of these Technologies simply because people fear what they do not understand and well fear sells more advertisement than Utopias.\n",
      "If you look at their previous papers in their research track record.\n",
      "I will dive into some of the most recent approaches that try to tackle these problems as sample efficiency and these parts reward setting so specifically I will cover a few technical papers dealing with approaches like auxiliary reward settings intrinsic curiosity hindsight experience replay and so on.\n",
      "At last from Boston Dynamics because everybody has seen the video where he does a backflip, but the reality is that if you think about what is what Boston Dynamics is actually doing well, it is very likely that there is not a lot of deep learning going on there.\n",
      "But nonetheless if you look at the progress of research that is going on.\n",
      "I think we should not be negligible of the potential risks that these Technologies can bring so I think it is very good at a lot more people are getting involved in the whole AI Safety Research because this is going to become very fundamental threats, like autonomous weapons and mass surveillance are to be taken very seriously.\n",
      "And so the only hope we have is that International law is going to be somewhat able to keep up with the rapid progress.\n",
      "cluster before alteration=========>\n",
      "I am doing these videos completely in my spare time and knowing that there is people out there that appreciate this content really feels great.\n",
      "Dont forget to subscribe and I would love to see you again in the next episode of archive insights.\n",
      "Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.\n",
      "And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.\n",
      "So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.\n",
      "You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.\n",
      "So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?\n",
      "And there is a lot of very hard engineering going on behind the scenes.\n",
      "I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition.\n",
      "I mean, look at this shaped reward function for a robotic control task.\n",
      "So I think if you go out on the streets and you asked a thousand people with the most advanced robots today are but they would probably\n",
      "And I think the atlas robot from Boston Dynamics is a very clear example of what I mean.\n",
      "Support me on patreon for which I would just like to say.\n",
      "So I think that one of the biggest challenges in navigating our digital landscape is Discerning truth from fiction in this ocean of Click bait that is powered by the advertisement industry.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Sometimes it is going to randomly select a whole sequence of actions that actually lead to scoring a goal.\n",
      "And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.\n",
      "We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.\n",
      "And so what policy gradients are going to do is that That for every episode where we got a positive reward.\n",
      "Were going to use the normal gradients to increase the probability of those actions in the future.\n",
      "But whenever we got a negative reward, we are going to apply the same gradient.\n",
      "Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.\n",
      "And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.\n",
      "So in a sense our agent is learning how to play the game of pong.\n",
      "I know this was a very quick introduction to reinforcement learning.\n",
      "So if you want to read up a bit and spend a little bit more timing thinking about the details, I really recommend to read and red carpet these blog post punk from pixels.\n",
      "All right, so we can use policy gradients that train a neural network to play the game of pong.\n",
      "But as always there are a few very significant downsides to using these methods.\n",
      "So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.\n",
      "But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.\n",
      "So the problem with policy gradients is that our policy gradient is going to\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "So we do not really want to decrease the likelihood of those actions and in reinforcement learning.\n",
      "That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.\n",
      "It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.\n",
      "We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.\n",
      "And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.\n",
      "And so your policy gradient is never going to see a single positive reward.\n",
      "It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.\n",
      "And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.\n",
      "But remember that actually the most part of that episode we were doing really well.\n",
      "So a famous example is the game Montezuma is Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level.\n",
      "So it has no idea what to do and the same case applies to robotic control where for example you would like to train in a row.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "And while this obviously makes it easier for your policy to converge to the desired Behavior.\n",
      "So if you are looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games.\n",
      "So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.\n",
      "It is never going to get to see any of that reward and I think it is important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation.\n",
      "Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.\n",
      "So firstly we were taping is a custom process that needs to be redone for every new environment.\n",
      "So reward shipping is the process of manually designing a reward function that needs to guide your policy to some desired Behavior.\n",
      "So it turns out that reward shaping is actually surprisingly difficult in a lot of cases when you\n",
      "Farm to pick up an object and stack it on to something else.\n",
      "Well, the typical robot has about seven joints that it can move.\n",
      "So it is a relatively High action space and if you only give it a positive reward when it is actually successfully stacked a block well by doing random exploration.\n",
      "Ice cold Michelle Pfeiffer that white gold this one for them hood girls them good girls straight and so the traditional approach to solve this issue of sparse rewards has been the use of reward shaping.\n",
      "Is that reward shaping suffers from what we call the alignment problem.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.\n",
      "Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.\n",
      "So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.\n",
      "And now obviously since our agent has not learned anything useful yet.\n",
      "Now one of the simplest ways to train a policy network is a method called.\n",
      "It produces a random output action, you know, either up or down you send that action back to the game engine in the game engine produces the next frame and this is how the loop continues and the network in this case.\n",
      "It could be a fully connected network, but you can obviously apply convolutions there as well.\n",
      "So we are just going to run a whole bunch of those game frames through your network select random actions feed them back into the engine and just create a whole bunch of random pong games.\n",
      "It is going to lose most of those games, but the thing is that\n",
      "So the approaching policy gradients is that you start out with a completely random Network you feed that Network frame from the game engine.\n",
      "The only feedback that we are going to give it is the scoreboard in the game.\n",
      "So in order to train or policy Network, the first thing we are going to do is collect a bunch of experience.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "It is either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation.\n",
      "So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong.\n",
      "So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.\n",
      "So imagine you want to train a neural network to play the game of pong.\n",
      "And so in this video, I want to introduce you guys to the whole subfield in machine learning that is called reinforcement learning which I think is one of the most promising directions to actually get to very intelligent robotic Behavior.\n",
      "For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we cannot use supervised learning.\n",
      "So the framework in reinforcement learning is actually surprisingly similar to the normal frame work in supervised learning so\n",
      "We can actually train that neural network to replicate the actions of the human gamer, but there are two significant downside to this approach.\n",
      "What you would do in a supervised setting is you would have a good human gamer play the game of pong for a couple of hours and you would create a data set where you log all of the frames that that human is seeing on the screen as well as the actions that he takes in response to those frames.\n",
      "So in the most common machine learning applications people use what we call supervised learning and this means that you give an input to your neural network model, but you know the output that your model should produce and therefore you can compute gradients using something like the back propagation.\n",
      "So whether he is pushing the up Arrow or down arrow and we can then feed those input frames through a very simple neural network that at the output Produce two simple actions.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "So in this episode, I will give a general introduction into the field of reinforcement learning as well as an overview of the most challenging problems that we are facing today.\n",
      "If you are looking for a solid introduction into the field of deep reinforcement learning then this episode is exactly what you are looking for.\n",
      "From The Amazing results and vintage Atari games deep mines Victory with alphago stunning breakthroughs in robotic arm manipulation and even beating professional players at what do you want to build up?\n",
      "The field reinforcement learning has literally exploded in recent years every city impressive breakthrough on the imagenet classification challenge in XnumberX.\n",
      "The successes of supervised deep learning have continued to pile up and people from many different backgrounds have started using deep neural Nets to solve a wide range of new tie.\n",
      "Tasks including how to learn intelligent behavior in complex Dynamic environments.\n",
      "So nips XnumberX Peter at Beale gave me very inspiring demo in front of a large audience of some of the brightest Minds in Ai and machine learning.\n",
      "So you showed this video where a robot is cleaning a living room bringing somebody a bottle of beer and basically doing a whole range of mundane tasks that robots in SciFi movies can do without question and then End of the video Peter revealed that the robots actions were actually entirely remote controlled by a human operator and the takeaway from this demo.\n",
      "I think is a very important one it basically says that the robots we have been building for decades now are physically perfectly capable of doing a wide range of useful tasks, but the problem is that we cannot embed them with the needed intelligence to do those things.\n",
      "So basically creating useful stateoftheart robotics is a software Challenge and not a hardware.\n",
      "So it turns out that having a robot learn how to do something very simple like picking up.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "So this video is an introduction into deep reinforcement learning and an overview of the most challenging problems that we are facing in the field in the next video.\n",
      "There is not a lot of intelligent decision making going on in those robots.\n",
      "Dont get me wrong, but there is not a lot of self driven Behavior.\n",
      "Company, but the media images they have created might be a little bit confusing to a lot of people that do not know what is going on behind the seats.\n",
      "So I personally believe that most if not all technological progress is beneficial in the long run as long as we can make sure that there are no monopolies that can maintain or enforce their power with the Views of AI well anyway enough politics for one video.\n",
      "We see in technology, but on the other hand, I also feel like the media is focusing way too much on the negative side of these Technologies simply because people fear what they do not understand and well fear sells more advertisement than Utopias.\n",
      "If you look at their previous papers in their research track record.\n",
      "I will dive into some of the most recent approaches that try to tackle these problems as sample efficiency and these parts reward setting so specifically I will cover a few technical papers dealing with approaches like auxiliary reward settings intrinsic curiosity hindsight experience replay and so on.\n",
      "At last from Boston Dynamics because everybody has seen the video where he does a backflip, but the reality is that if you think about what is what Boston Dynamics is actually doing well, it is very likely that there is not a lot of deep learning going on there.\n",
      "But nonetheless if you look at the progress of research that is going on.\n",
      "I think we should not be negligible of the potential risks that these Technologies can bring so I think it is very good at a lot more people are getting involved in the whole AI Safety Research because this is going to become very fundamental threats, like autonomous weapons and mass surveillance are to be taken very seriously.\n",
      "And so the only hope we have is that International law is going to be somewhat able to keep up with the rapid progress.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "I am doing these videos completely in my spare time and knowing that there is people out there that appreciate this content really feels great.\n",
      "Dont forget to subscribe and I would love to see you again in the next episode of archive insights.\n",
      "Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.\n",
      "And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.\n",
      "So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.\n",
      "You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.\n",
      "So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?\n",
      "And there is a lot of very hard engineering going on behind the scenes.\n",
      "I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition.\n",
      "I mean, look at this shaped reward function for a robotic control task.\n",
      "So I think if you go out on the streets and you asked a thousand people with the most advanced robots today are but they would probably\n",
      "And I think the atlas robot from Boston Dynamics is a very clear example of what I mean.\n",
      "Support me on patreon for which I would just like to say.\n",
      "So I think that one of the biggest challenges in navigating our digital landscape is Discerning truth from fiction in this ocean of Click bait that is powered by the advertisement industry.\n",
      "\n",
      "\n",
      "Checking Timerange --------------\n",
      "--------------\n",
      "And so what policy gradients are going to do is that That for every episode where we got a positive reward. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "Were going to use the normal gradients to increase the probability of those actions in the future. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "But whenever we got a negative reward, we are going to apply the same gradient. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "But as always there are a few very significant downsides to using these methods. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "So the problem with policy gradients is that our policy gradient is going to 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "So if you want to read up a bit and spend a little bit more timing thinking about the details, I really recommend to read and red carpet these blog post punk from pixels. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "I know this was a very quick introduction to reinforcement learning. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "Sometimes it is going to randomly select a whole sequence of actions that actually lead to scoring a goal. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "All right, so we can use policy gradients that train a neural network to play the game of pong. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "So in a sense our agent is learning how to play the game of pong. 0396879a-033e-4dea-a186-bddb5887378e \n",
      "\n",
      "--------------\n",
      "And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely. 0a51398a-64b1-45f6-8ed2-1ad78aec5e4a \n",
      "\n",
      "It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action. 0a51398a-64b1-45f6-8ed2-1ad78aec5e4a \n",
      "\n",
      "It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior. 0a51398a-64b1-45f6-8ed2-1ad78aec5e4a \n",
      "\n",
      "That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future. 0a51398a-64b1-45f6-8ed2-1ad78aec5e4a \n",
      "\n",
      "So we do not really want to decrease the likelihood of those actions and in reinforcement learning. 0a51398a-64b1-45f6-8ed2-1ad78aec5e4a \n",
      "\n",
      "So a famous example is the game Montezuma is Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level. 0a51398a-64b1-45f6-8ed2-1ad78aec5e4a \n",
      "\n",
      "And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated. 0a51398a-64b1-45f6-8ed2-1ad78aec5e4a \n",
      "\n",
      "And so your policy gradient is never going to see a single positive reward. 0a51398a-64b1-45f6-8ed2-1ad78aec5e4a \n",
      "\n",
      "We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else. 0a51398a-64b1-45f6-8ed2-1ad78aec5e4a \n",
      "\n",
      "So it has no idea what to do and the same case applies to robotic control where for example you would like to train in a row. 0a51398a-64b1-45f6-8ed2-1ad78aec5e4a \n",
      "\n",
      "But remember that actually the most part of that episode we were doing really well. 0a51398a-64b1-45f6-8ed2-1ad78aec5e4a \n",
      "\n",
      "--------------\n",
      "So if you are looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "So reward shipping is the process of manually designing a reward function that needs to guide your policy to some desired Behavior. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "Farm to pick up an object and stack it on to something else. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "So it is a relatively High action space and if you only give it a positive reward when it is actually successfully stacked a block well by doing random exploration. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "Ice cold Michelle Pfeiffer that white gold this one for them hood girls them good girls straight and so the traditional approach to solve this issue of sparse rewards has been the use of reward shaping. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "It is never going to get to see any of that reward and I think it is important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "Well, the typical robot has about seven joints that it can move. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "So firstly we were taping is a custom process that needs to be redone for every new environment. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "Is that reward shaping suffers from what we call the alignment problem. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "And while this obviously makes it easier for your policy to converge to the desired Behavior. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior. 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "So it turns out that reward shaping is actually surprisingly difficult in a lot of cases when you 14d5dec1-c18a-4389-8d76-4335306d578e \n",
      "\n",
      "--------------\n",
      "So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible. 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "So the approaching policy gradients is that you start out with a completely random Network you feed that Network frame from the game engine. 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "Now one of the simplest ways to train a policy network is a method called. 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "It could be a fully connected network, but you can obviously apply convolutions there as well. 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "It is going to lose most of those games, but the thing is that 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "So in order to train or policy Network, the first thing we are going to do is collect a bunch of experience. 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "So we are just going to run a whole bunch of those game frames through your network select random actions feed them back into the engine and just create a whole bunch of random pong games. 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "And now obviously since our agent has not learned anything useful yet. 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "It produces a random output action, you know, either up or down you send that action back to the game engine in the game engine produces the next frame and this is how the loop continues and the network in this case. 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself. 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "The only feedback that we are going to give it is the scoreboard in the game. 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network. 635d06e9-b7bb-4d6a-9720-5a9eb205e62a \n",
      "\n",
      "--------------\n",
      "So the framework in reinforcement learning is actually surprisingly similar to the normal frame work in supervised learning so ebdb1aa6-0290-4a65-8a95-43c05626d908 \n",
      "\n",
      "We can actually train that neural network to replicate the actions of the human gamer, but there are two significant downside to this approach. ebdb1aa6-0290-4a65-8a95-43c05626d908 \n",
      "\n",
      "So imagine you want to train a neural network to play the game of pong. ebdb1aa6-0290-4a65-8a95-43c05626d908 \n",
      "\n",
      "For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we cannot use supervised learning. ebdb1aa6-0290-4a65-8a95-43c05626d908 \n",
      "\n",
      "So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning. ebdb1aa6-0290-4a65-8a95-43c05626d908 \n",
      "\n",
      "What you would do in a supervised setting is you would have a good human gamer play the game of pong for a couple of hours and you would create a data set where you log all of the frames that that human is seeing on the screen as well as the actions that he takes in response to those frames. ebdb1aa6-0290-4a65-8a95-43c05626d908 \n",
      "\n",
      "And so in this video, I want to introduce you guys to the whole subfield in machine learning that is called reinforcement learning which I think is one of the most promising directions to actually get to very intelligent robotic Behavior. ebdb1aa6-0290-4a65-8a95-43c05626d908 \n",
      "\n",
      "So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong. ebdb1aa6-0290-4a65-8a95-43c05626d908 \n",
      "\n",
      "So whether he is pushing the up Arrow or down arrow and we can then feed those input frames through a very simple neural network that at the output Produce two simple actions. ebdb1aa6-0290-4a65-8a95-43c05626d908 \n",
      "\n",
      "So in the most common machine learning applications people use what we call supervised learning and this means that you give an input to your neural network model, but you know the output that your model should produce and therefore you can compute gradients using something like the back propagation. ebdb1aa6-0290-4a65-8a95-43c05626d908 \n",
      "\n",
      "It is either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation. ebdb1aa6-0290-4a65-8a95-43c05626d908 \n",
      "\n",
      "--------------\n",
      "So in this episode, I will give a general introduction into the field of reinforcement learning as well as an overview of the most challenging problems that we are facing today. 0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db \n",
      "\n",
      "So it turns out that having a robot learn how to do something very simple like picking up. 0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db \n",
      "\n",
      "So nips XnumberX Peter at Beale gave me very inspiring demo in front of a large audience of some of the brightest Minds in Ai and machine learning. 0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db \n",
      "\n",
      "The successes of supervised deep learning have continued to pile up and people from many different backgrounds have started using deep neural Nets to solve a wide range of new tie. 0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db \n",
      "\n",
      "Tasks including how to learn intelligent behavior in complex Dynamic environments. 0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db \n",
      "\n",
      "So basically creating useful stateoftheart robotics is a software Challenge and not a hardware. 0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db \n",
      "\n",
      "I think is a very important one it basically says that the robots we have been building for decades now are physically perfectly capable of doing a wide range of useful tasks, but the problem is that we cannot embed them with the needed intelligence to do those things. 0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db \n",
      "\n",
      "So you showed this video where a robot is cleaning a living room bringing somebody a bottle of beer and basically doing a whole range of mundane tasks that robots in SciFi movies can do without question and then End of the video Peter revealed that the robots actions were actually entirely remote controlled by a human operator and the takeaway from this demo. 0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db \n",
      "\n",
      "If you are looking for a solid introduction into the field of deep reinforcement learning then this episode is exactly what you are looking for. 0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db \n",
      "\n",
      "The field reinforcement learning has literally exploded in recent years every city impressive breakthrough on the imagenet classification challenge in XnumberX. 0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db \n",
      "\n",
      "From The Amazing results and vintage Atari games deep mines Victory with alphago stunning breakthroughs in robotic arm manipulation and even beating professional players at what do you want to build up? 0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db \n",
      "\n",
      "--------------\n",
      "So this video is an introduction into deep reinforcement learning and an overview of the most challenging problems that we are facing in the field in the next video. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "Dont get me wrong, but there is not a lot of self driven Behavior. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "And so the only hope we have is that International law is going to be somewhat able to keep up with the rapid progress. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "So I personally believe that most if not all technological progress is beneficial in the long run as long as we can make sure that there are no monopolies that can maintain or enforce their power with the Views of AI well anyway enough politics for one video. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "But nonetheless if you look at the progress of research that is going on. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "Company, but the media images they have created might be a little bit confusing to a lot of people that do not know what is going on behind the seats. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "We see in technology, but on the other hand, I also feel like the media is focusing way too much on the negative side of these Technologies simply because people fear what they do not understand and well fear sells more advertisement than Utopias. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "I will dive into some of the most recent approaches that try to tackle these problems as sample efficiency and these parts reward setting so specifically I will cover a few technical papers dealing with approaches like auxiliary reward settings intrinsic curiosity hindsight experience replay and so on. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "At last from Boston Dynamics because everybody has seen the video where he does a backflip, but the reality is that if you think about what is what Boston Dynamics is actually doing well, it is very likely that there is not a lot of deep learning going on there. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "If you look at their previous papers in their research track record. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "I think we should not be negligible of the potential risks that these Technologies can bring so I think it is very good at a lot more people are getting involved in the whole AI Safety Research because this is going to become very fundamental threats, like autonomous weapons and mass surveillance are to be taken very seriously. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "There is not a lot of intelligent decision making going on in those robots. d425094a-088e-4acc-a972-6560fc8dac2b \n",
      "\n",
      "--------------\n",
      "So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping. dba1aa05-b75d-4a7e-a2ff-40daf4b580e0 \n",
      "\n",
      "I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition. dba1aa05-b75d-4a7e-a2ff-40daf4b580e0 \n",
      "\n",
      "So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today? dba1aa05-b75d-4a7e-a2ff-40daf4b580e0 \n",
      "\n",
      "And there is a lot of very hard engineering going on behind the scenes. dba1aa05-b75d-4a7e-a2ff-40daf4b580e0 \n",
      "\n",
      "So I think that one of the biggest challenges in navigating our digital landscape is Discerning truth from fiction in this ocean of Click bait that is powered by the advertisement industry. dba1aa05-b75d-4a7e-a2ff-40daf4b580e0 \n",
      "\n",
      "So I think if you go out on the streets and you asked a thousand people with the most advanced robots today are but they would probably dba1aa05-b75d-4a7e-a2ff-40daf4b580e0 \n",
      "\n",
      "And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong. dba1aa05-b75d-4a7e-a2ff-40daf4b580e0 \n",
      "\n",
      "You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation. dba1aa05-b75d-4a7e-a2ff-40daf4b580e0 \n",
      "\n",
      "I mean, look at this shaped reward function for a robotic control task. dba1aa05-b75d-4a7e-a2ff-40daf4b580e0 \n",
      "\n",
      "And I think the atlas robot from Boston Dynamics is a very clear example of what I mean. dba1aa05-b75d-4a7e-a2ff-40daf4b580e0 \n",
      "\n",
      "Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do. dba1aa05-b75d-4a7e-a2ff-40daf4b580e0 \n",
      "\n",
      "Dont forget to subscribe and I would love to see you again in the next episode of archive insights. 98d73e6d-9061-460e-84fd-2137177746f0 \n",
      "\n",
      "Support me on patreon for which I would just like to say. 98d73e6d-9061-460e-84fd-2137177746f0 \n",
      "\n",
      "I am doing these videos completely in my spare time and knowing that there is people out there that appreciate this content really feels great. 98d73e6d-9061-460e-84fd-2137177746f0 \n",
      "\n",
      "<---------------->\n",
      "order difference: 0\n",
      "Relevant sentence:  And so what policy gradients are going to do is that That for every episode where we got a positive reward.    =====    Were going to use the normal gradients to increase the probability of those actions in the future.\n",
      "order difference: 0\n",
      "Relevant sentence:  Were going to use the normal gradients to increase the probability of those actions in the future.    =====    But whenever we got a negative reward, we are going to apply the same gradient.\n",
      "order difference: 0\n",
      "Relevant sentence:  But whenever we got a negative reward, we are going to apply the same gradient.    =====    But as always there are a few very significant downsides to using these methods.\n",
      "order difference: 0\n",
      "Relevant sentence:  But as always there are a few very significant downsides to using these methods.    =====    So the problem with policy gradients is that our policy gradient is going to\n",
      "order difference: 0\n",
      "Relevant sentence:  So the problem with policy gradients is that our policy gradient is going to    =====    Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.\n",
      "order difference: 0\n",
      "Relevant sentence:  Were going to multiply it with XnumberX and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future.    =====    So if you want to read up a bit and spend a little bit more timing thinking about the details, I really recommend to read and red carpet these blog post punk from pixels.\n",
      "order difference: 0\n",
      "Relevant sentence:  So if you want to read up a bit and spend a little bit more timing thinking about the details, I really recommend to read and red carpet these blog post punk from pixels.    =====    So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.\n",
      "order difference: 0\n",
      "Relevant sentence:  So imagine that your agent has been training for a while and it is actually doing a pretty decent job at playing the game of pong.    =====    I know this was a very quick introduction to reinforcement learning.\n",
      "order difference: 0\n",
      "Relevant sentence:  I know this was a very quick introduction to reinforcement learning.    =====    Sometimes it is going to randomly select a whole sequence of actions that actually lead to scoring a goal.\n",
      "order difference: 0\n",
      "Relevant sentence:  Sometimes it is going to randomly select a whole sequence of actions that actually lead to scoring a goal.    =====    And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.\n",
      "order difference: 0\n",
      "Relevant sentence:  And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely.    =====    But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.\n",
      "order difference: 0\n",
      "Relevant sentence:  But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty.    =====    We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.\n",
      "order difference: 0\n",
      "Relevant sentence:  We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial.    =====    All right, so we can use policy gradients that train a neural network to play the game of pong.\n",
      "order difference: 0\n",
      "Relevant sentence:  All right, so we can use policy gradients that train a neural network to play the game of pong.    =====    And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.\n",
      "order difference: 0\n",
      "Relevant sentence:  And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward.    =====    So in a sense our agent is learning how to play the game of pong.\n",
      "order difference: 0\n",
      "Relevant sentence:  And I have made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely.    =====    It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem is not Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action.    =====    It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.\n",
      "order difference: 0\n",
      "Relevant sentence:  It does not really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior.    =====    That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.\n",
      "order difference: 0\n",
      "Relevant sentence:  That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future.    =====    So we do not really want to decrease the likelihood of those actions and in reinforcement learning.\n",
      "order difference: 0\n",
      "Relevant sentence:  So we do not really want to decrease the likelihood of those actions and in reinforcement learning.    =====    So a famous example is the game Montezuma is Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level.\n",
      "order difference: 0\n",
      "Relevant sentence:  So a famous example is the game Montezuma is Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level.    =====    And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.\n",
      "order difference: 0\n",
      "Relevant sentence:  And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated.    =====    And so your policy gradient is never going to see a single positive reward.\n",
      "order difference: 0\n",
      "Relevant sentence:  And so your policy gradient is never going to see a single positive reward.    =====    We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.\n",
      "order difference: 0\n",
      "Relevant sentence:  We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it is only the actions right before it hits the ball that are truly important everything else.    =====    So it has no idea what to do and the same case applies to robotic control where for example you would like to train in a row.\n",
      "order difference: 0\n",
      "Relevant sentence:  So it has no idea what to do and the same case applies to robotic control where for example you would like to train in a row.    =====    But remember that actually the most part of that episode we were doing really well.\n",
      "order difference: 0\n",
      "Relevant sentence:  So if you are looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games.    =====    So reward shipping is the process of manually designing a reward function that needs to guide your policy to some desired Behavior.\n",
      "order difference: 0\n",
      "Relevant sentence:  So reward shipping is the process of manually designing a reward function that needs to guide your policy to some desired Behavior.    =====    Farm to pick up an object and stack it on to something else.\n",
      "order difference: 0\n",
      "Relevant sentence:  Farm to pick up an object and stack it on to something else.    =====    So it is a relatively High action space and if you only give it a positive reward when it is actually successfully stacked a block well by doing random exploration.\n",
      "order difference: 0\n",
      "Relevant sentence:  So it is a relatively High action space and if you only give it a positive reward when it is actually successfully stacked a block well by doing random exploration.    =====    Ice cold Michelle Pfeiffer that white gold this one for them hood girls them good girls straight and so the traditional approach to solve this issue of sparse rewards has been the use of reward shaping.\n",
      "order difference: 0\n",
      "Relevant sentence:  Ice cold Michelle Pfeiffer that white gold this one for them hood girls them good girls straight and so the traditional approach to solve this issue of sparse rewards has been the use of reward shaping.    =====    Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.\n",
      "order difference: 0\n",
      "Relevant sentence:  Whereas in reinforcement learning setting you are having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult.    =====    It is never going to get to see any of that reward and I think it is important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is never going to get to see any of that reward and I think it is important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation.    =====    Well, the typical robot has about seven joints that it can move.\n",
      "order difference: 0\n",
      "Relevant sentence:  Well, the typical robot has about seven joints that it can move.    =====    So firstly we were taping is a custom process that needs to be redone for every new environment.\n",
      "order difference: 0\n",
      "Relevant sentence:  So firstly we were taping is a custom process that needs to be redone for every new environment.    =====    Is that reward shaping suffers from what we call the alignment problem.\n",
      "order difference: 0\n",
      "Relevant sentence:  Is that reward shaping suffers from what we call the alignment problem.    =====    And while this obviously makes it easier for your policy to converge to the desired Behavior.\n",
      "order difference: 0\n",
      "Relevant sentence:  And while this obviously makes it easier for your policy to converge to the desired Behavior.    =====    So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.\n",
      "order difference: 0\n",
      "Relevant sentence:  So in the case of Montezuma is Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior.    =====    So it turns out that reward shaping is actually surprisingly difficult in a lot of cases when you\n",
      "order difference: 0\n",
      "Relevant sentence:  So whenever our agent manages to score a goal it will see where reward of plus XnumberX and if the opponent is Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible.    =====    So the approaching policy gradients is that you start out with a completely random Network you feed that Network frame from the game engine.\n",
      "order difference: 0\n",
      "Relevant sentence:  So the approaching policy gradients is that you start out with a completely random Network you feed that Network frame from the game engine.    =====    Now one of the simplest ways to train a policy network is a method called.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now one of the simplest ways to train a policy network is a method called.    =====    It could be a fully connected network, but you can obviously apply convolutions there as well.\n",
      "order difference: 0\n",
      "Relevant sentence:  It could be a fully connected network, but you can obviously apply convolutions there as well.    =====    It is going to lose most of those games, but the thing is that\n",
      "order difference: 0\n",
      "Relevant sentence:  It is going to lose most of those games, but the thing is that    =====    So in order to train or policy Network, the first thing we are going to do is collect a bunch of experience.\n",
      "order difference: 0\n",
      "Relevant sentence:  So in order to train or policy Network, the first thing we are going to do is collect a bunch of experience.    =====    So we are just going to run a whole bunch of those game frames through your network select random actions feed them back into the engine and just create a whole bunch of random pong games.\n",
      "order difference: 0\n",
      "Relevant sentence:  So we are just going to run a whole bunch of those game frames through your network select random actions feed them back into the engine and just create a whole bunch of random pong games.    =====    And now obviously since our agent has not learned anything useful yet.\n",
      "order difference: 0\n",
      "Relevant sentence:  And now obviously since our agent has not learned anything useful yet.    =====    It produces a random output action, you know, either up or down you send that action back to the game engine in the game engine produces the next frame and this is how the loop continues and the network in this case.\n",
      "order difference: 0\n",
      "Relevant sentence:  It produces a random output action, you know, either up or down you send that action back to the game engine in the game engine produces the next frame and this is how the loop continues and the network in this case.    =====    And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.\n",
      "order difference: 0\n",
      "Relevant sentence:  And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you are not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself.    =====    The only feedback that we are going to give it is the scoreboard in the game.\n",
      "order difference: 0\n",
      "Relevant sentence:  The only feedback that we are going to give it is the scoreboard in the game.    =====    Well have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we do not actually know the target label so we do not know in any situation whether we should have gone up or down because we do not have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network.\n",
      "order difference: 0\n",
      "Relevant sentence:  So the framework in reinforcement learning is actually surprisingly similar to the normal frame work in supervised learning so    =====    We can actually train that neural network to replicate the actions of the human gamer, but there are two significant downside to this approach.\n",
      "order difference: 0\n",
      "Relevant sentence:  We can actually train that neural network to replicate the actions of the human gamer, but there are two significant downside to this approach.    =====    So imagine you want to train a neural network to play the game of pong.\n",
      "order difference: 0\n",
      "Relevant sentence:  So imagine you want to train a neural network to play the game of pong.    =====    For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we cannot use supervised learning.\n",
      "order difference: 0\n",
      "Relevant sentence:  For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we cannot use supervised learning.    =====    So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.\n",
      "order difference: 0\n",
      "Relevant sentence:  So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning.    =====    What you would do in a supervised setting is you would have a good human gamer play the game of pong for a couple of hours and you would create a data set where you log all of the frames that that human is seeing on the screen as well as the actions that he takes in response to those frames.\n",
      "order difference: 0\n",
      "Relevant sentence:  What you would do in a supervised setting is you would have a good human gamer play the game of pong for a couple of hours and you would create a data set where you log all of the frames that that human is seeing on the screen as well as the actions that he takes in response to those frames.    =====    And so in this video, I want to introduce you guys to the whole subfield in machine learning that is called reinforcement learning which I think is one of the most promising directions to actually get to very intelligent robotic Behavior.\n",
      "order difference: 0\n",
      "Relevant sentence:  And so in this video, I want to introduce you guys to the whole subfield in machine learning that is called reinforcement learning which I think is one of the most promising directions to actually get to very intelligent robotic Behavior.    =====    So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong.\n",
      "order difference: 0\n",
      "Relevant sentence:  So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong.    =====    So whether he is pushing the up Arrow or down arrow and we can then feed those input frames through a very simple neural network that at the output Produce two simple actions.\n",
      "order difference: 0\n",
      "Relevant sentence:  So whether he is pushing the up Arrow or down arrow and we can then feed those input frames through a very simple neural network that at the output Produce two simple actions.    =====    So in the most common machine learning applications people use what we call supervised learning and this means that you give an input to your neural network model, but you know the output that your model should produce and therefore you can compute gradients using something like the back propagation.\n",
      "order difference: 0\n",
      "Relevant sentence:  So in the most common machine learning applications people use what we call supervised learning and this means that you give an input to your neural network model, but you know the output that your model should produce and therefore you can compute gradients using something like the back propagation.    =====    It is either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation.\n",
      "order difference: 0\n",
      "Relevant sentence:  So in this episode, I will give a general introduction into the field of reinforcement learning as well as an overview of the most challenging problems that we are facing today.    =====    So it turns out that having a robot learn how to do something very simple like picking up.\n",
      "order difference: 0\n",
      "Relevant sentence:  So it turns out that having a robot learn how to do something very simple like picking up.    =====    So nips XnumberX Peter at Beale gave me very inspiring demo in front of a large audience of some of the brightest Minds in Ai and machine learning.\n",
      "order difference: 0\n",
      "Relevant sentence:  So nips XnumberX Peter at Beale gave me very inspiring demo in front of a large audience of some of the brightest Minds in Ai and machine learning.    =====    The successes of supervised deep learning have continued to pile up and people from many different backgrounds have started using deep neural Nets to solve a wide range of new tie.\n",
      "order difference: 0\n",
      "Relevant sentence:  The successes of supervised deep learning have continued to pile up and people from many different backgrounds have started using deep neural Nets to solve a wide range of new tie.    =====    Tasks including how to learn intelligent behavior in complex Dynamic environments.\n",
      "order difference: 0\n",
      "Relevant sentence:  Tasks including how to learn intelligent behavior in complex Dynamic environments.    =====    So basically creating useful stateoftheart robotics is a software Challenge and not a hardware.\n",
      "order difference: 0\n",
      "Relevant sentence:  So basically creating useful stateoftheart robotics is a software Challenge and not a hardware.    =====    I think is a very important one it basically says that the robots we have been building for decades now are physically perfectly capable of doing a wide range of useful tasks, but the problem is that we cannot embed them with the needed intelligence to do those things.\n",
      "order difference: 0\n",
      "Relevant sentence:  I think is a very important one it basically says that the robots we have been building for decades now are physically perfectly capable of doing a wide range of useful tasks, but the problem is that we cannot embed them with the needed intelligence to do those things.    =====    So you showed this video where a robot is cleaning a living room bringing somebody a bottle of beer and basically doing a whole range of mundane tasks that robots in SciFi movies can do without question and then End of the video Peter revealed that the robots actions were actually entirely remote controlled by a human operator and the takeaway from this demo.\n",
      "order difference: 0\n",
      "Relevant sentence:  So you showed this video where a robot is cleaning a living room bringing somebody a bottle of beer and basically doing a whole range of mundane tasks that robots in SciFi movies can do without question and then End of the video Peter revealed that the robots actions were actually entirely remote controlled by a human operator and the takeaway from this demo.    =====    If you are looking for a solid introduction into the field of deep reinforcement learning then this episode is exactly what you are looking for.\n",
      "order difference: 0\n",
      "Relevant sentence:  If you are looking for a solid introduction into the field of deep reinforcement learning then this episode is exactly what you are looking for.    =====    The field reinforcement learning has literally exploded in recent years every city impressive breakthrough on the imagenet classification challenge in XnumberX.\n",
      "order difference: 0\n",
      "Relevant sentence:  The field reinforcement learning has literally exploded in recent years every city impressive breakthrough on the imagenet classification challenge in XnumberX.    =====    From The Amazing results and vintage Atari games deep mines Victory with alphago stunning breakthroughs in robotic arm manipulation and even beating professional players at what do you want to build up?\n",
      "order difference: 0\n",
      "Relevant sentence:  So this video is an introduction into deep reinforcement learning and an overview of the most challenging problems that we are facing in the field in the next video.    =====    Dont get me wrong, but there is not a lot of self driven Behavior.\n",
      "order difference: 0\n",
      "Relevant sentence:  Dont get me wrong, but there is not a lot of self driven Behavior.    =====    And so the only hope we have is that International law is going to be somewhat able to keep up with the rapid progress.\n",
      "order difference: 0\n",
      "Relevant sentence:  And so the only hope we have is that International law is going to be somewhat able to keep up with the rapid progress.    =====    So I personally believe that most if not all technological progress is beneficial in the long run as long as we can make sure that there are no monopolies that can maintain or enforce their power with the Views of AI well anyway enough politics for one video.\n",
      "order difference: 0\n",
      "Relevant sentence:  So I personally believe that most if not all technological progress is beneficial in the long run as long as we can make sure that there are no monopolies that can maintain or enforce their power with the Views of AI well anyway enough politics for one video.    =====    But nonetheless if you look at the progress of research that is going on.\n",
      "order difference: 0\n",
      "Relevant sentence:  But nonetheless if you look at the progress of research that is going on.    =====    Company, but the media images they have created might be a little bit confusing to a lot of people that do not know what is going on behind the seats.\n",
      "order difference: 0\n",
      "Relevant sentence:  Company, but the media images they have created might be a little bit confusing to a lot of people that do not know what is going on behind the seats.    =====    We see in technology, but on the other hand, I also feel like the media is focusing way too much on the negative side of these Technologies simply because people fear what they do not understand and well fear sells more advertisement than Utopias.\n",
      "order difference: 0\n",
      "Relevant sentence:  We see in technology, but on the other hand, I also feel like the media is focusing way too much on the negative side of these Technologies simply because people fear what they do not understand and well fear sells more advertisement than Utopias.    =====    I will dive into some of the most recent approaches that try to tackle these problems as sample efficiency and these parts reward setting so specifically I will cover a few technical papers dealing with approaches like auxiliary reward settings intrinsic curiosity hindsight experience replay and so on.\n",
      "order difference: 0\n",
      "Relevant sentence:  I will dive into some of the most recent approaches that try to tackle these problems as sample efficiency and these parts reward setting so specifically I will cover a few technical papers dealing with approaches like auxiliary reward settings intrinsic curiosity hindsight experience replay and so on.    =====    At last from Boston Dynamics because everybody has seen the video where he does a backflip, but the reality is that if you think about what is what Boston Dynamics is actually doing well, it is very likely that there is not a lot of deep learning going on there.\n",
      "order difference: 0\n",
      "Relevant sentence:  At last from Boston Dynamics because everybody has seen the video where he does a backflip, but the reality is that if you think about what is what Boston Dynamics is actually doing well, it is very likely that there is not a lot of deep learning going on there.    =====    If you look at their previous papers in their research track record.\n",
      "order difference: 0\n",
      "Relevant sentence:  If you look at their previous papers in their research track record.    =====    I think we should not be negligible of the potential risks that these Technologies can bring so I think it is very good at a lot more people are getting involved in the whole AI Safety Research because this is going to become very fundamental threats, like autonomous weapons and mass surveillance are to be taken very seriously.\n",
      "order difference: 0\n",
      "Relevant sentence:  I think we should not be negligible of the potential risks that these Technologies can bring so I think it is very good at a lot more people are getting involved in the whole AI Safety Research because this is going to become very fundamental threats, like autonomous weapons and mass surveillance are to be taken very seriously.    =====    There is not a lot of intelligent decision making going on in those robots.\n",
      "order difference: 0\n",
      "Relevant sentence:  So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping.    =====    I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition.\n",
      "order difference: 0\n",
      "Relevant sentence:  I do not even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition.    =====    So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?\n",
      "order difference: 0\n",
      "Relevant sentence:  So the situation that we are in right now is that we know that it is really hard to train in a sparse reward setting but at the same time it is also very tricky to shape a reward function and we do not always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today?    =====    And there is a lot of very hard engineering going on behind the scenes.\n",
      "order difference: 0\n",
      "Relevant sentence:  And there is a lot of very hard engineering going on behind the scenes.    =====    So I think that one of the biggest challenges in navigating our digital landscape is Discerning truth from fiction in this ocean of Click bait that is powered by the advertisement industry.\n",
      "order difference: 0\n",
      "Relevant sentence:  So I think that one of the biggest challenges in navigating our digital landscape is Discerning truth from fiction in this ocean of Click bait that is powered by the advertisement industry.    =====    So I think if you go out on the streets and you asked a thousand people with the most advanced robots today are but they would probably\n",
      "order difference: 0\n",
      "Relevant sentence:  So I think if you go out on the streets and you asked a thousand people with the most advanced robots today are but they would probably    =====    And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.\n",
      "order difference: 0\n",
      "Relevant sentence:  And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there is a lot of funny cases where reward shaping goes terribly wrong.    =====    You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.\n",
      "order difference: 0\n",
      "Relevant sentence:  You do not want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation.    =====    I mean, look at this shaped reward function for a robotic control task.\n",
      "order difference: 0\n",
      "Relevant sentence:  I mean, look at this shaped reward function for a robotic control task.    =====    And I think the atlas robot from Boston Dynamics is a very clear example of what I mean.\n",
      "order difference: 0\n",
      "Relevant sentence:  And I think the atlas robot from Boston Dynamics is a very clear example of what I mean.    =====    Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.\n",
      "Not Relevant sentence:  Shape your reward function your agent will find some very surprising way to make sure that it is getting a lot of reward but not doing at all what you want to do.    !=    Dont forget to subscribe and I would love to see you again in the next episode of archive insights.\n",
      "order difference: 2\n",
      "order difference: 0\n",
      "Relevant sentence:  Dont forget to subscribe and I would love to see you again in the next episode of archive insights.    =====    Support me on patreon for which I would just like to say.\n",
      "order difference: 0\n",
      "Relevant sentence:  Support me on patreon for which I would just like to say.    =====    I am doing these videos completely in my spare time and knowing that there is people out there that appreciate this content really feels great.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (8, 6), (7, 7)]\n",
      "[[[\"Sometimes our agent might get lucky. Sometimes it's going to randomly select a whole sequence of actions that actually lead to scoring a goal. And in this case our agent is going to receive a reward and now a key thing to understand is that for every episode regardless of whether we want a positive or A negative reward. We can already compute the gradients that would make the actions that our agents has chosen more likely in the future and this is very crucial. And so what policy gradients are going to do is that That for every episode where we got a positive reward. We're going to use the normal gradients to increase the probability of those actions in the future. But whenever we got a negative reward, we're going to apply the same gradient. We're going to multiply it with -1 and this minus sign will make sure that in the future all the actions that we took in a very bad episode are going to be less likely in the future. And so the result is that while training our policy Network the actions that lead to negative Rewards are slowly going to be filtered out and the actions that lead to positive rewards are going to become more and more likely. So in a sense our agent is learning how to play the game of pong. Now. I know this was a very quick introduction to reinforcement learning. So if you want to read up a bit and spend a little bit more timing thinking about the details, I really recommend to read and red carpet these blog post punk from pixels. It does a phenomenal job at explaining all the details. All right, so we can use policy gradients that train a neural network to play the game of pong. That's amazing. Right? Well, yes it is. But as always there are a few very significant downsides to using these methods. Let's go back to pong one more time. So imagine that your agent has been training for a while and it's actually doing a pretty decent job at playing the game of pong. It's bouncing the ball back and forth. But then at the end of the episode, it makes a mistake it lets the ball through and it gets a negative penalty. So the problem with policy gradients is that our policy gradient is going to \"], '2020-01-31T07:59:21Z', '716067a60a1a4034abc49a12ecafb39b', '0396879a-033e-4dea-a186-bddb5887378e']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"That since we lost that episode all of the actions that we took there must be bad actions and is going to reduce the likelihood of taking those actions in the future. But remember that actually the most part of that episode we were doing really well. So we don't really want to decrease the likelihood of those actions and in reinforcement learning. This is called the credit assignment problem. It's where if you get a reward at the end of your episode will what are the exact actions that led to that specific reward and this problem isn't Tightly related to the fact that we have what we call a sparse reward setting so instead of getting a reward for every single action. We only get a reward after an entire episode and our agent needs to figure out what part of its action sequence were causing the reward that it eventually gets so in the case of palm, for example, our agent should learn that it's only the actions right before it hits the ball that are truly important everything else. Once the ball is flying off. It doesn't really matter for the eventual reward and so the result of this First reward setting is that in reinforcement learning algorithms are typically very sample inefficient, which means that you have to give them a ton of training time before they can learn some useful behavior. And I've made a previous video to compare the sample efficiency of reinforcement learning algorithms with human learning that goes much deeper into why this is the case and now it turns out that in some extreme cases the sparse reward setting actually fails completely. So a famous example is the game Montezuma's Revenge where the goal of the agent is to navigate a each of ladders jump over a skull grab a key and then actually navigate to the door to in order to get to the next level. And the problem here is that by taking random actions your agent is never going to see a single reward because you know, the sequence of actions that it needs to take to get that reward is just too complicated. It's never going to get there with random actions. And so your policy gradient is never going to see a single positive reward. So it has no idea what to do and the same case applies to robotic control where for example you would like to train in a row. \"], '2020-01-31T08:01:21Z', '716067a60a1a4034abc49a12ecafb39b', '0a51398a-64b1-45f6-8ed2-1ad78aec5e4a']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Farm to pick up an object and stack it on to something else. Well, the typical robot has about seven joints that it can move. So it's a relatively High action space and if you only give it a positive reward when it's actually successfully stacked a block well by doing random exploration. It's never going to get to see any of that reward and I think it's important to compare this with the traditional supervised deep learning successes that we get in something like computer vision, for example, so the reason computer vision works, so well, is that for every single input frame you have a Target label and this lets you do very efficient gradient descent with something like backpropagation. Whereas in reinforcement learning setting you're having to deal with this very big problem of sparse reward setting and this is why you know computer vision is showing some very impressive results while something as simple as stacking one block onto another seems very difficult. Even for state-of-the-art keep learning. Ice cold Michelle Pfeiffer that white gold this one for them hood girls them good girls straight and so the traditional approach to solve this issue of sparse rewards has been the use of reward shaping. So reward shipping is the process of manually designing a reward function that needs to guide your policy to some desired Behavior. So in the case of Montezuma's Revenge, for example, you could give your agent or reward every single time it manages to avoid this call or reach the key and these extra rewards will guide your policy to some desired behavior. And while this obviously makes it easier for your policy to converge to the desired Behavior. There are some significant downsides to reward shaping. So firstly we were taping is a custom process that needs to be redone for every new environment. You want to train a policy. So if you're looking at The Benchmark of Atari, for example, well, you would have to craft a new reward function for every single one of those games. That's just not scalable the second problem. Is that reward shaping suffers from what we call the alignment problem. So it turns out that reward shaping is actually surprisingly difficult in a lot of cases when you \"], '2020-01-31T08:03:21Z', '716067a60a1a4034abc49a12ecafb39b', '14d5dec1-c18a-4389-8d76-4335306d578e']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"We'll have an input frame we run it through some neural network model and the network produces an output action either up or down but the only difference here is that now we don't actually know the target label so we don't know in any situation whether we should have gone up or down because we don't have a data set to train on and in reinforcement learning the network that transforms input frames to Output actions is called the policy Network. Now one of the simplest ways to train a policy network is a method called. Elysee gradients. So the approaching policy gradients is that you start out with a completely random Network you feed that Network frame from the game engine. It produces a random output action, you know, either up or down you send that action back to the game engine in the game engine produces the next frame and this is how the loop continues and the network in this case. It could be a fully connected network, but you can obviously apply convolutions there as well. And now in reality the output of your network is going to consist of two numbers the probability of going up and the probability of going Down and what you will do while training is actually sample from the distribution so that you're not always going to repeat the same exact actions and this will allow your agent to sort of explore the environment a bit randomly and hopefully discover better rewards and better Behavior now importantly because we want to enable our agent to learn entirely by itself. The only feedback that we're going to give it is the scoreboard in the game. So whenever our agent manages to score a goal it will see where reward of plus 1 and if the opponent's Court Gold then our agent will receive a penalty of minus one and the entire goal of the agent is to optimize its policy to receive as much reward as possible. So in order to train or policy Network, the first thing we're going to do is collect a bunch of experience. So we're just going to run a whole bunch of those game frames through your network select random actions feed them back into the engine and just create a whole bunch of random pong games. And now obviously since our agent hasn't learned anything useful yet. It's going to lose most of those games, but the thing is that \"], '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', '635d06e9-b7bb-4d6a-9720-5a9eb205e62a']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Beer can be a very challenging task. And so in this video, I want to introduce you guys to the whole subfield in machine learning that's called reinforcement learning which I think is one of the most promising directions to actually get to very intelligent robotic Behavior. So in the most common machine learning applications people use what we call supervised learning and this means that you give an input to your neural network model, but you know the output that your model should produce and therefore you can compute gradients using something like the back propagation. Algorithm to train that Network to produce your outputs. So imagine you want to train a neural network to play the game of pong. What you would do in a supervised setting is you would have a good human gamer play the game of pong for a couple of hours and you would create a data set where you log all of the frames that that human is seeing on the screen as well as the actions that he takes in response to those frames. So whether he's pushing the up Arrow or down arrow and we can then feed those input frames through a very simple neural network that at the output Produce two simple actions. It's either going to select the up action or the down action and by simply training on the data set of the human gameplay using something like backpropagation. We can actually train that neural network to replicate the actions of the human gamer, but there are two significant downside to this approach. So on the one hand if you want to do supervised learning you have to create a data set to train on which is not always a very easy thing to do and on the other hand if you train your neural network model to Simply imitate the actions of the One player will then by definition your agent can never be better at playing the game of pong. Then that human gamer. For example, if you want to train a neural net to be better at playing the game of gold and the best human then by definition we can't use supervised learning. So is there a way to have an agent learn to play a game entirely by itself or fortunately there is and this is called reinforcement learning. So the framework in reinforcement learning is actually surprisingly similar to the normal frame work in supervised learning so \"], '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'ebdb1aa6-0290-4a65-8a95-43c05626d908']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"From The Amazing results and vintage Atari games deep mines Victory with alphago stunning breakthroughs in robotic arm manipulation and even beating professional players at what do you want to build up? The field reinforcement learning has literally exploded in recent years every city impressive breakthrough on the imagenet classification challenge in 2012. The successes of supervised deep learning have continued to pile up and people from many different backgrounds have started using deep neural Nets to solve a wide range of new tie. Tasks including how to learn intelligent behavior in complex Dynamic environments. So in this episode, I will give a general introduction into the field of reinforcement learning as well as an overview of the most challenging problems that we're facing today. If you're looking for a solid introduction into the field of deep reinforcement learning then this episode is exactly what you're looking for. My name is Xander and welcome to Archive insights. So nips 2017 Peter at Beale gave me very inspiring demo in front of a large audience of some of the brightest Minds in Ai and machine learning. So you showed this video where a robot is cleaning a living room bringing somebody a bottle of beer and basically doing a whole range of mundane tasks that robots in Sci-Fi movies can do without question and then End of the video Peter revealed that the robots actions were actually entirely remote controlled by a human operator and the takeaway from this demo. I think is a very important one it basically says that the robots we've been building for decades now are physically perfectly capable of doing a wide range of useful tasks, but the problem is that we can't embed them with the needed intelligence to do those things. So basically creating useful state-of-the-art robotics is a software Challenge and not a hardware. Problem. So it turns out that having a robot learn how to do something very simple like picking up. \"], '2020-01-31T07:53:21Z', '716067a60a1a4034abc49a12ecafb39b', '0e0d7cbc-b2df-44f5-b9a7-4df0d5b007db']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\".2. At last from Boston Dynamics because everybody has seen the video where he does a backflip, but the reality is that if you think about what's what Boston Dynamics is actually doing well, it's very likely that there's not a lot of deep learning going on there. If you look at their previous papers in their research track record. Well, they're doing a lot of very Advanced robotics. Don't get me wrong, but there's not a lot of self driven Behavior. There's not a lot of intelligent decision making going on in those robots. So don't get me wrong. Boston Dynamics is a very impressive robotics. Company, but the media images they've created might be a little bit confusing to a lot of people that don't know what's going on behind the seats. But nonetheless if you look at the progress of research that is going on. I think we should not be negligible of the potential risks that these Technologies can bring so I think it's very good at a lot more people are getting involved in the whole AI Safety Research because this is going to become very fundamental threats, like autonomous weapons and mass surveillance are to be taken very seriously. And so the only hope we have is that International law is going to be somewhat able to keep up with the rapid progress. We see in technology, but on the other hand, I also feel like the media is focusing way too much on the negative side of these Technologies simply because people fear what they don't understand and well fear sells more advertisement than Utopias. So I personally believe that most if not all technological progress is beneficial in the long run as long as we can make sure that there are no monopolies that can maintain or enforce their power with the Views of AI well anyway enough politics for one video. So this video is an introduction into deep reinforcement learning and an overview of the most challenging problems that we're facing in the field in the next video. I will dive into some of the most recent approaches that try to tackle these problems as sample efficiency and these parts reward setting so specifically I will cover a few technical papers dealing with approaches like auxiliary reward settings intrinsic curiosity hindsight experience replay and so on. I've also seen that a few people have \"], '2020-01-31T08:07:21Z', '716067a60a1a4034abc49a12ecafb39b', 'd425094a-088e-4acc-a972-6560fc8dac2b']] \n",
      "\n",
      "\n",
      "[[[\"Support me on patreon for which I would just like to say. Thank you very much. I mean it really means a big deal to me. I'm doing these videos completely in my spare time and knowing that there's people out there that appreciate this content really feels great. So thank you very much. Thanks for watching. Don't forget to subscribe and I'd love to see you again in the next episode of archive insights. \"], '2020-01-31T08:09:21Z', '716067a60a1a4034abc49a12ecafb39b', '98d73e6d-9061-460e-84fd-2137177746f0']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Shape your reward function your agent will find some very surprising way to make sure that it's getting a lot of reward but not doing at all what you want to do. And in a sense the policy is just over fitting to that specific reward function that you designed while not generalizing to the intended behavior that you had in mind and there's a lot of funny cases where reward shaping goes terribly wrong. So here for example, the agent was trained to do jumping and the reward function was the distance from its feet to the ground and what this agent has learned is to Simply grow a very tall body and do some kind of a back flip to make sure that its feet are very far from the ground to give you one final idea of how hard it can be to do reward shaping. I mean, look at this shaped reward function for a robotic control task. I don't even want to know how long the people from this paper spent on designing this specific reward function to get the behavior that they wanted and finally in some cases like alphago for example by definition. You don't want to do any reward shaping because this will Train your policy to the behavior of humans, which is not exactly optimal in every situation. So the situation that we're in right now is that we know that it's really hard to train in a sparse reward setting but at the same time it's also very tricky to shape a reward function and we don't always want to do that and to end this video I would like to note that a lot of media stories picture reinforcement learning as some kind of a magical AI sauce that lets the agent learn on itself or improve upon its previous version, but the reality is Is that most of these breakthroughs are actually the work of some of the brightest Minds Alive today? And there's a lot of very hard engineering going on behind the scenes. So I think that one of the biggest challenges in navigating our digital landscape is Discerning truth from fiction in this ocean of Click bait that is powered by the advertisement industry. And I think the atlas robot from Boston Dynamics is a very clear example of what I mean. So I think if you go out on the streets and you asked a thousand people with the most advanced robots today are but they would probably \"], '2020-01-31T08:05:21Z', '716067a60a1a4034abc49a12ecafb39b', 'dba1aa05-b75d-4a7e-a2ff-40daf4b580e0']] \n",
      "\n",
      "\n",
      "7\n",
      "Before Merging 9\n",
      "[(6, 8)]\n",
      "\n",
      "\n",
      ".2. At last from Boston Dynamics because everybody has seen the video where he does a backflip, but the reality is that if you think about what's what Boston Dynamics is actually doing well, it's very likely that there's not a lot of deep learning going on there. If you look at their previous papers in their research track record. Well, they're doing a lot of very Advanced robotics. Don't get me wrong, but there's not a lot of self driven Behavior. There's not a lot of intelligent decision making going on in those robots. So don't get me wrong. Boston Dynamics is a very impressive robotics. Company, but the media images they've created might be a little bit confusing to a lot of people that don't know what's going on behind the seats. But nonetheless if you look at the progress of research that is going on. I think we should not be negligible of the potential risks that these Technologies can bring so I think it's very good at a lot more people are getting involved in the whole AI Safety Research because this is going to become very fundamental threats, like autonomous weapons and mass surveillance are to be taken very seriously. And so the only hope we have is that International law is going to be somewhat able to keep up with the rapid progress. We see in technology, but on the other hand, I also feel like the media is focusing way too much on the negative side of these Technologies simply because people fear what they don't understand and well fear sells more advertisement than Utopias. So I personally believe that most if not all technological progress is beneficial in the long run as long as we can make sure that there are no monopolies that can maintain or enforce their power with the Views of AI well anyway enough politics for one video. So this video is an introduction into deep reinforcement learning and an overview of the most challenging problems that we're facing in the field in the next video. I will dive into some of the most recent approaches that try to tackle these problems as sample efficiency and these parts reward setting so specifically I will cover a few technical papers dealing with approaches like auxiliary reward settings intrinsic curiosity hindsight experience replay and so on. I've also seen that a few people have \n",
      "\n",
      "\n",
      "Support me on patreon for which I would just like to say. Thank you very much. I mean it really means a big deal to me. I'm doing these videos completely in my spare time and knowing that there's people out there that appreciate this content really feels great. So thank you very much. Thanks for watching. Don't forget to subscribe and I'd love to see you again in the next episode of archive insights. \n",
      "After Merging 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Ent map before filtering:  {'6': [6496, 6496, 6496, 4627, 2697, 2697, 3216, 3216, 3216, 2697], '5': [6496, 6496, 3216, 3216, 6496, 3216, 4627, 2697, 4627, 3216], '4': [4712, 3216, 726, 449, 818, 3216, 5249, 3216, 5299, 4627], '1': [5327, 2267, 449, 5249, 5249, 3798, 3216, 27, 3216, 1758, 726, 2267, 5327, 449, 5249, 4627, 5249, 895, 2489, 27, 2267, 5327, 3798, 5249, 4627, 449, 3216, 3216, 3216, 5249, 4627, 4627, 4627, 68, 3216, 726, 449, 1758, 3216, 103], '2': [6496, 6496, 4627, 4627, 6496, 4627, 4627, 449, 3899, 4627], '3': [3216, 3216, 3216, 2697, 4627, 3216, 2267, 6496, 6496, 3216, 4627, 6496, 4627, 6496, 6496, 4627, 4627, 449, 895, 4627], '7': [6496, 6496, 6496, 6496, 6496, 6496, 4627, 4627, 4627, 6496, 6496, 6496, 6496, 6496, 6496, 4627, 4627, 4627, 56, 3899, 642, 68, 6496, 748, 6204, 6496, 2697, 70, 103, 70], '0': [70, 70, 895, 8, 241, 1616, 463, 241, 81, 211, 598, 891, 113, 1233, 139, 366, 591, 1625, 635, 4909, 70, 2111, 131, 459, 79, 1175, 79, 66, 70, 824, 6496, 6496, 4627, 2697, 3216, 6496, 2697, 6204, 3216, 3216]}\n",
      "Group Ent map after filtering:  {'6': [(6496, 3), (2697, 3), (3216, 3)], '5': [(3216, 4), (6496, 3)], '4': [(3216, 3)], '1': [(3216, 7), (5249, 6), (4627, 5), (449, 4), (5327, 3), (2267, 3)], '2': [(4627, 5), (6496, 3)], '3': [(4627, 6), (3216, 5), (6496, 5)], '7': [(6496, 14), (4627, 6)], '0': [(70, 4), (6496, 3), (3216, 3)]}\n",
      "Using  None  for feature extraction\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 103, \"module\": \"scorer\", \"batches count\": 1, \"number of sentences\": 158, \"ts\": \"2020-03-05T08:27:00.384320Z\", \"msg\": \"computing in batches\"}\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 106, \"module\": \"scorer\", \"iteration count:\": 0, \"ts\": \"2020-03-05T08:27:00.385572Z\", \"msg\": \"getting feature vector\"}\n",
      "('You can still predict what is going to be for dinner tonight by thinking back two days ago think what was for dinner then so what would be predicted for you last night?', '2020-01-31T07:53:23Z', '716067a60a1a4034abc49a12ecafb39b', '0055bc75-2657-4694-81d2-0e4564df65ce') ('We set everything equal to XnumberX except for the dinner item that we predict.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece')\n",
      "('You can still predict what is going to be for dinner tonight by thinking back two days ago think what was for dinner then so what would be predicted for you last night?', '2020-01-31T07:53:23Z', '716067a60a1a4034abc49a12ecafb39b', '0055bc75-2657-4694-81d2-0e4564df65ce') ('So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday.', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('And then you can use that prediction in turn to make a prediction for tonight.', '2020-01-31T07:53:23Z', '716067a60a1a4034abc49a12ecafb39b', '0055bc75-2657-4694-81d2-0e4564df65ce') ('You still cannot get much better than chance predictions on dinner as is often the case with complicated machine learning problems.', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('So we make use of not only our actual information from yesterday.', '2020-01-31T07:53:23Z', '716067a60a1a4034abc49a12ecafb39b', '0055bc75-2657-4694-81d2-0e4564df65ce') ('We can also have Vector for statements like, it is Tuesday in order to encode this kind of information what we do is we make a List of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true.', '2020-01-31T07:54:21Z', '716067a60a1a4034abc49a12ecafb39b', 'b9d07405-49d7-48e9-a8bf-284e87ecb9c2')\n",
      "('So we make use of not only our actual information from yesterday.', '2020-01-31T07:53:23Z', '716067a60a1a4034abc49a12ecafb39b', '0055bc75-2657-4694-81d2-0e4564df65ce') ('So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday.', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('So at this point it is helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if I want to describe the weather to you for a given day, I could say Say the high XnumberX degrees Fahrenheit the lows XnumberX wins XnumberX miles an hour.', '2020-01-31T07:53:23Z', '716067a60a1a4034abc49a12ecafb39b', '0055bc75-2657-4694-81d2-0e4564df65ce') ('The vector is the reason that it is useful is vectors list of numbers are computers native language.', '2020-01-31T07:54:21Z', '716067a60a1a4034abc49a12ecafb39b', 'b9d07405-49d7-48e9-a8bf-284e87ecb9c2')\n",
      "('So at this point it is helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if I want to describe the weather to you for a given day, I could say Say the high XnumberX degrees Fahrenheit the lows XnumberX wins XnumberX miles an hour.', '2020-01-31T07:53:23Z', '716067a60a1a4034abc49a12ecafb39b', '0055bc75-2657-4694-81d2-0e4564df65ce') ('We can also have Vector for statements like, it is Tuesday in order to encode this kind of information what we do is we make a List of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true.', '2020-01-31T07:54:21Z', '716067a60a1a4034abc49a12ecafb39b', 'b9d07405-49d7-48e9-a8bf-284e87ecb9c2')\n",
      "('So at this point it is helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if I want to describe the weather to you for a given day, I could say Say the high XnumberX degrees Fahrenheit the lows XnumberX wins XnumberX miles an hour.', '2020-01-31T07:53:23Z', '716067a60a1a4034abc49a12ecafb39b', '0055bc75-2657-4694-81d2-0e4564df65ce') ('This format is called one hot and coding and it is very common to see a long Vector of zeros with just one element being one.', '2020-01-31T07:54:21Z', '716067a60a1a4034abc49a12ecafb39b', 'b9d07405-49d7-48e9-a8bf-284e87ecb9c2')\n",
      "('So at this point it is helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if I want to describe the weather to you for a given day, I could say Say the high XnumberX degrees Fahrenheit the lows XnumberX wins XnumberX miles an hour.', '2020-01-31T07:53:23Z', '716067a60a1a4034abc49a12ecafb39b', '0055bc75-2657-4694-81d2-0e4564df65ce') ('We can make a one hot Vector for our prediction for dinner tonight.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece')\n",
      "('There is going to be a quarter inch of rain and the relative humidity is XnumberX percent.', '2020-01-31T07:53:23Z', '716067a60a1a4034abc49a12ecafb39b', '0055bc75-2657-4694-81d2-0e4564df65ce') ('We can make a one hot Vector for our prediction for dinner tonight.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece')\n",
      "('And by a similar method anytime we come across the word saw or a period we know that a name has to come after that.', '2020-01-31T07:58:40Z', '716067a60a1a4034abc49a12ecafb39b', '0b0dc278-ae3e-4774-8f76-aca2c6c4fbbf') ('We would expect that to vote heavily for the word saw or for a period because those are the two words in our dictionary that can follow a name.', '2020-01-31T07:58:04Z', '716067a60a1a4034abc49a12ecafb39b', '3f6d0dea-c156-48f4-95ad-fe274d7c1490')\n",
      "('So it will learn to vote very strongly for a name Jane Doug or spot.', '2020-01-31T07:58:40Z', '716067a60a1a4034abc49a12ecafb39b', '0b0dc278-ae3e-4774-8f76-aca2c6c4fbbf') ('You can imagine if in the course of that processing say something got voted for twice.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352')\n",
      "('So it will learn to vote very strongly for a name Jane Doug or spot.', '2020-01-31T07:58:40Z', '716067a60a1a4034abc49a12ecafb39b', '0b0dc278-ae3e-4774-8f76-aca2c6c4fbbf') ('early, if we had predicted a name on the previous time step, we would expect those to vote also for the word saw or for a period', '2020-01-31T07:58:29Z', '716067a60a1a4034abc49a12ecafb39b', 'f05c0675-3618-4081-bbf3-c46f76f3e736')\n",
      "('So it will learn to vote very strongly for a name Jane Doug or spot.', '2020-01-31T07:58:40Z', '716067a60a1a4034abc49a12ecafb39b', '0b0dc278-ae3e-4774-8f76-aca2c6c4fbbf') ('Then it does not use the information from further back and it is subject to these types of mistakes.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('Applications of machine learning have gotten a lot of traction in the last few years.', '2020-01-31T07:50:13Z', '716067a60a1a4034abc49a12ecafb39b', '1368a07c-94f3-478d-a444-3027d5ffabb0') ('And you learn how to predict what is going to be for dinner tonight?', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('There is a couple of big categories that have had wins one is identifying pictures the equivalent of finding cats on the internet and any problem that can be made to look like that and the other is sequence to sequence deflation is can be speech to text or one language to another most of the former are done with convolutional neural networks.', '2020-01-31T07:50:13Z', '716067a60a1a4034abc49a12ecafb39b', '1368a07c-94f3-478d-a444-3027d5ffabb0') ('So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday.', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('Most of the latter are done with recurrent neural networks particularly long short term memory to give an example of how long shortterm memory works.', '2020-01-31T07:50:13Z', '716067a60a1a4034abc49a12ecafb39b', '1368a07c-94f3-478d-a444-3027d5ffabb0') ('The trouble is that your network does not work very well despite carefully choosing your inputs and training it thoroughly.', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('Most of the latter are done with recurrent neural networks particularly long short term memory to give an example of how long shortterm memory works.', '2020-01-31T07:50:13Z', '716067a60a1a4034abc49a12ecafb39b', '1368a07c-94f3-478d-a444-3027d5ffabb0') ('So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday.', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('Let is say for a minute that you are a very lucky apartment dweller and you have a flatmate who loves to cook dinner every night.', '2020-01-31T07:50:13Z', '716067a60a1a4034abc49a12ecafb39b', '1368a07c-94f3-478d-a444-3027d5ffabb0') ('It becomes a very simple voting process and and it is right all the time because your flatmate is incredibly consistent.', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('Now if you are new to neural networks, I highly recommend you take a minute and stopped to watch the how neural networks work tutorial.', '2020-01-31T07:50:13Z', '716067a60a1a4034abc49a12ecafb39b', '1368a07c-94f3-478d-a444-3027d5ffabb0') ('The trouble is that your network does not work very well despite carefully choosing your inputs and training it thoroughly.', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('Now if you are new to neural networks, I highly recommend you take a minute and stopped to watch the how neural networks work tutorial.', '2020-01-31T07:50:13Z', '716067a60a1a4034abc49a12ecafb39b', '1368a07c-94f3-478d-a444-3027d5ffabb0') ('So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday.', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.', '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1') ('So to do this, we replace our food vectors with our dictionary vectors here again, it is just a list of numbers representing each of the words.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', 'a8e01ed7-f7df-4351-971d-2265654b4b34')\n",
      "('We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.', '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1') ('And we similarly can represent our predictions and our predictions from yesterday.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', 'a8e01ed7-f7df-4351-971d-2265654b4b34')\n",
      "('We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.', '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1') ('The vector is the reason that it is useful is vectors list of numbers are computers native language.', '2020-01-31T07:54:21Z', '716067a60a1a4034abc49a12ecafb39b', 'b9d07405-49d7-48e9-a8bf-284e87ecb9c2')\n",
      "('We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.', '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1') ('We can also have Vector for statements like, it is Tuesday in order to encode this kind of information what we do is we make a List of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true.', '2020-01-31T07:54:21Z', '716067a60a1a4034abc49a12ecafb39b', 'b9d07405-49d7-48e9-a8bf-284e87ecb9c2')\n",
      "('We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.', '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1') ('This format is called one hot and coding and it is very common to see a long Vector of zeros with just one element being one.', '2020-01-31T07:54:21Z', '716067a60a1a4034abc49a12ecafb39b', 'b9d07405-49d7-48e9-a8bf-284e87ecb9c2')\n",
      "('We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.', '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1') ('Complete our picture we can show how the prediction for today will get recycled the dotted line there means hold onto it for a day and then reuse it tomorrow and it becomes our yesterday is predictions tomorrow.', '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf')\n",
      "('We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.', '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1') ('We just ignore the new information part and we can unwrap or unwind this Vector in time until we do have some information to base it on and then just play it forward.', '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf')\n",
      "('We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.', '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1') ('We set everything equal to XnumberX except for the dinner item that we predict.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece')\n",
      "('And the neural network is just connections between every element in each of those input vectors to every element in the output connector.', '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1') ('And the task of the neural network is to put these together in the right order to make a good children is book.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', 'a8e01ed7-f7df-4351-971d-2265654b4b34')\n",
      "('And the neural network is just connections between every element in each of those input vectors to every element in the output connector.', '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1') ('We just ignore the new information part and we can unwrap or unwind this Vector in time until we do have some information to base it on and then just play it forward.', '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf')\n",
      "('And the neural network is just connections between every element in each of those input vectors to every element in the output connector.', '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1') ('So this is a nice simple example that showed recurrent neural networks now to show how they do not meet all of our needs.', '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf')\n",
      "('Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Youve probably noticed though when we are combining our predictions with our memories.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So we want a little filter to keep our memories inside and let our predictions get out.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('And that is we add another gate for that to do selection.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So we just squash it to be careful to make sure it never gets out of control.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('We select just a few to release as the prediction for that moment.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So we will skip over it for now and this prediction of Saw not Doug is passed forward and again for the purposes of Simplicity.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('I will pass over ignoring and attention and this example and we will take those predictions forward.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('The other thing that happened is our previous set of possibilities.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('This is an intention mechanism it lets things that are not immediately relevant be set aside.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Youve probably noticed though when we are combining our predictions with our memories.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('This is an intention mechanism it lets things that are not immediately relevant be set aside.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So we want a little filter to keep our memories inside and let our predictions get out.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('This is an intention mechanism it lets things that are not immediately relevant be set aside.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So we just squash it to be careful to make sure it never gets out of control.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('This is an intention mechanism it lets things that are not immediately relevant be set aside.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('This is an intention mechanism it lets things that are not immediately relevant be set aside.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('The other thing that happened is our previous set of possibilities.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So if they do not Cloud the predictions in memory going forward It has its own neural network and its own Logistics squashing function and its own gating activity right here.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Weve also introduced another squashing function here since we do an addition here.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('So if they do not Cloud the predictions in memory going forward It has its own neural network and its own Logistics squashing function and its own gating activity right here.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So we just squash it to be careful to make sure it never gets out of control.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('So if they do not Cloud the predictions in memory going forward It has its own neural network and its own Logistics squashing function and its own gating activity right here.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Now long short term memory has a lot of pieces a lot of bits that were together and it is a little much to wrap your head around it all at once.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Youve probably noticed though when we are combining our predictions with our memories.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Now long short term memory has a lot of pieces a lot of bits that were together and it is a little much to wrap your head around it all at once.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Now long short term memory has a lot of pieces a lot of bits that were together and it is a little much to wrap your head around it all at once.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Each of these things when to forget and when to let things out of our memory are learned by their own neural networks.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('So what we will do is take a very simple example and step through it just to illustrate how a couple of these pieces work.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Youve probably noticed though when we are combining our predictions with our memories.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('So what we will do is take a very simple example and step through it just to illustrate how a couple of these pieces work.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So we want a little filter to keep our memories inside and let our predictions get out.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('So what we will do is take a very simple example and step through it just to illustrate how a couple of these pieces work.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So we will skip over it for now and this prediction of Saw not Doug is passed forward and again for the purposes of Simplicity.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So what we will do is take a very simple example and step through it just to illustrate how a couple of these pieces work.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('I will pass over ignoring and attention and this example and we will take those predictions forward.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So what we will do is take a very simple example and step through it just to illustrate how a couple of these pieces work.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('So what we will do is take a very simple example and step through it just to illustrate how a couple of these pieces work.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('It is admittedly an overly simplistic example, and feel free to poke holes at it later.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Youve probably noticed though when we are combining our predictions with our memories.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('It is admittedly an overly simplistic example, and feel free to poke holes at it later.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Weve also introduced another squashing function here since we do an addition here.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('It is admittedly an overly simplistic example, and feel free to poke holes at it later.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('I will pass over ignoring and attention and this example and we will take those predictions forward.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('It is admittedly an overly simplistic example, and feel free to poke holes at it later.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('When you get to that point then you know, you are ready to move on to the next level of material.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Each of these things when to forget and when to let things out of our memory are learned by their own neural networks.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('When you get to that point then you know, you are ready to move on to the next level of material.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('So we are now in the process of writing our children is book.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('I will pass over ignoring and attention and this example and we will take those predictions forward.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So we are now in the process of writing our children is book.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('And for the purposes of demonstration, we will assume that this LS TM has been trained on our children is books examples that we want to mimic and all of the appropriate votes and weights in those neural networks have been learned now will show it in action.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('We select just a few to release as the prediction for that moment.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('And for the purposes of demonstration, we will assume that this LS TM has been trained on our children is books examples that we want to mimic and all of the appropriate votes and weights in those neural networks have been learned now will show it in action.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('I will pass over ignoring and attention and this example and we will take those predictions forward.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('And for the purposes of demonstration, we will assume that this LS TM has been trained on our children is books examples that we want to mimic and all of the appropriate votes and weights in those neural networks have been learned now will show it in action.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('So so far our story so far is Jane saucepot period Doug so Doug is the most recent word that is occurred in our story and also not surprisingly for this time step.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('This is learned that the word saw is a great guess to make for next word, but it is also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So so far our story so far is Jane saucepot period Doug so Doug is the most recent word that is occurred in our story and also not surprisingly for this time step.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So the fact that there is a vote for not Doug gets blocked here and the word saw get sent out as the prediction for the next time step.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So so far our story so far is Jane saucepot period Doug so Doug is the most recent word that is occurred in our story and also not surprisingly for this time step.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('We now predict that the words Doug Jane or spot might come next.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('The name is Doug Jane and spot where all predicted as viable options.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Youve probably noticed though when we are combining our predictions with our memories.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('The name is Doug Jane and spot where all predicted as viable options.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('We select just a few to release as the prediction for that moment.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('The name is Doug Jane and spot where all predicted as viable options.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('The first one of these make some predictions given that the word Doug just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('The name is Doug Jane and spot where all predicted as viable options.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So it makes a positive prediction for Saul and a negative.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('The name is Doug Jane and spot where all predicted as viable options.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('The name is Doug Jane and spot where all predicted as viable options.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('I will pass over ignoring and attention and this example and we will take those predictions forward.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('The name is Doug Jane and spot where all predicted as viable options.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('The other thing that happened is our previous set of possibilities.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So we have our New information which is the word Doug we have our recent prediction.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So we want a little filter to keep our memories inside and let our predictions get out.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('So we have our New information which is the word Doug we have our recent prediction.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('This is learned that the word saw is a great guess to make for next word, but it is also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So we have our New information which is the word Doug we have our recent prediction.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So it makes a positive prediction for Saul and a negative.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So we have our New information which is the word Doug we have our recent prediction.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So we have our New information which is the word Doug we have our recent prediction.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('We now predict that the words Doug Jane or spot might come next.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So we have our New information which is the word Doug we have our recent prediction.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('I will pass over ignoring and attention and this example and we will take those predictions forward.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So we have our New information which is the word Doug we have our recent prediction.', '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('When we introduced all of these together what we get, we still have the combination of our previous predictions in our new information those vectors get past and we make predictions based on them.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('The way it works is you start with two vectors of equal size and you go down each one.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1')\n",
      "('When we introduced all of these together what we get, we still have the combination of our previous predictions in our new information those vectors get past and we make predictions based on them.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('And so your output Vector is the same size of each of your input vectors.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1')\n",
      "('When we introduced all of these together what we get, we still have the combination of our previous predictions in our new information those vectors get past and we make predictions based on them.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('When we introduced all of these together what we get, we still have the combination of our previous predictions in our new information those vectors get past and we make predictions based on them.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1')\n",
      "('Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('Youve probably noticed though when we are combining our predictions with our memories.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('So we want a little filter to keep our memories inside and let our predictions get out.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('We select just a few to release as the prediction for that moment.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Some of them are remembered the ones that are remembered are added back into the prediction.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('So we want a little filter to keep our memories inside and let our predictions get out.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Some of them are remembered the ones that are remembered are added back into the prediction.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Some of them are remembered the ones that are remembered are added back into the prediction.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('We select just a few to release as the prediction for that moment.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('So now we have not just prediction for a predictions plus the memories that we have accumulated and that we have not chosen to forget.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('We may not necessarily want to release all of those memories out as new predictions each time.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('So now we have not just prediction for a predictions plus the memories that we have accumulated and that we have not chosen to forget.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('So we want a little filter to keep our memories inside and let our predictions get out.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('So now we have not just prediction for a predictions plus the memories that we have accumulated and that we have not chosen to forget.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('So now we have not just prediction for a predictions plus the memories that we have accumulated and that we have not chosen to forget.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('We select just a few to release as the prediction for that moment.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('So now we have not just prediction for a predictions plus the memories that we have accumulated and that we have not chosen to forget.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('Each of these things when to forget and when to let things out of our memory are learned by their own neural networks.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('Now there is an entirely separate neural network here that learns when to forget what based on what we are seeing right now.', '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1') ('Youve probably noticed though when we are combining our predictions with our memories.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37')\n",
      "('After training this neural network and teaching it what to do.', '2020-01-31T07:58:04Z', '716067a60a1a4034abc49a12ecafb39b', '3f6d0dea-c156-48f4-95ad-fe274d7c1490') ('And the task of the neural network is to put these together in the right order to make a good children is book.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', 'a8e01ed7-f7df-4351-971d-2265654b4b34')\n",
      "('For instance anytime a name comes up Jane Doug or spot.', '2020-01-31T07:58:04Z', '716067a60a1a4034abc49a12ecafb39b', '3f6d0dea-c156-48f4-95ad-fe274d7c1490') ('early, if we had predicted a name on the previous time step, we would expect those to vote also for the word saw or for a period', '2020-01-31T07:58:29Z', '716067a60a1a4034abc49a12ecafb39b', 'f05c0675-3618-4081-bbf3-c46f76f3e736')\n",
      "('We would expect that to vote heavily for the word saw or for a period because those are the two words in our dictionary that can follow a name.', '2020-01-31T07:58:04Z', '716067a60a1a4034abc49a12ecafb39b', '3f6d0dea-c156-48f4-95ad-fe274d7c1490') ('Dictionary is small just the words Doug Jane spot saw and a period.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', 'a8e01ed7-f7df-4351-971d-2265654b4b34')\n",
      "('We would expect that to vote heavily for the word saw or for a period because those are the two words in our dictionary that can follow a name.', '2020-01-31T07:58:04Z', '716067a60a1a4034abc49a12ecafb39b', '3f6d0dea-c156-48f4-95ad-fe274d7c1490') ('So to do this, we replace our food vectors with our dictionary vectors here again, it is just a list of numbers representing each of the words.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', 'a8e01ed7-f7df-4351-971d-2265654b4b34')\n",
      "('The word saw and not Doug that we were maintaining internally get passed to a forgetting gate.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('The word saw and not Doug that we were maintaining internally get passed to a forgetting gate.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('The other thing that happened is our previous set of possibilities.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('The word saw and not Doug that we were maintaining internally get passed to a forgetting gate.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('Now the forgetting gate says hey my last word that came that occurred was the word saw based on my past experience.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('This is learned that the word saw is a great guess to make for next word, but it is also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('Now the forgetting gate says hey my last word that came that occurred was the word saw based on my past experience.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('Now the forgetting gate says hey my last word that came that occurred was the word saw based on my past experience.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('The word saw is our most recent word and our most recent prediction.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('Now the forgetting gate says hey my last word that came that occurred was the word saw based on my past experience.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('But I want to keep any predictions having to do with names.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('I will not step through it, but it is encouraging that something that looks so complex expressed mathematically actually makes it fairly straightforward.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3')\n",
      "('But I want to keep any predictions having to do with names.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('The first one of these make some predictions given that the word Doug just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('But I want to keep any predictions having to do with names.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('So it makes a positive prediction for Saul and a negative.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('But I want to keep any predictions having to do with names.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('But I want to keep any predictions having to do with names.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('So the fact that there is a vote for not Doug gets blocked here and the word saw get sent out as the prediction for the next time step.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('But I want to keep any predictions having to do with names.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('The word saw is our most recent word and our most recent prediction.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('But I want to keep any predictions having to do with names.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('I will pass over ignoring and attention and this example and we will take those predictions forward.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So it forgets saw holds onto the vote for not Doug.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('The first one of these make some predictions given that the word Doug just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So it forgets saw holds onto the vote for not Doug.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('So we will skip over it for now and this prediction of Saw not Doug is passed forward and again for the purposes of Simplicity.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('So it forgets saw holds onto the vote for not Doug.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('We have a positive vote for dog an Get a vote for Doug and so they cancel each other out.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('The first one of these make some predictions given that the word Doug just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('We have a positive vote for dog an Get a vote for Doug and so they cancel each other out.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('Addiction for Doug it says I do not expect to see dog in the near future.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('We have a positive vote for dog an Get a vote for Doug and so they cancel each other out.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('So we will skip over it for now and this prediction of Saw not Doug is passed forward and again for the purposes of Simplicity.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('We have a positive vote for dog an Get a vote for Doug and so they cancel each other out.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('We have a positive vote for dog an Get a vote for Doug and so they cancel each other out.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('We now predict that the words Doug Jane or spot might come next.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('It looks like is that they find the higherlevel idea and translate it from one mode of expression to another just using the bits and pieces that we just walked through.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3')\n",
      "('It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('It takes them and uses that been to predict what text what word is being spoken and it can use the history the recent history of words to make a better guess for what is going to come next.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3')\n",
      "('It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('The first one of these make some predictions given that the word Doug just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('This is learned that the word saw is a great guess to make for next word, but it is also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('The word saw is our most recent word and our most recent prediction.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('They get passed forward to all of these neural networks, and we get a new set of predictions because the word saw just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('I will pass over ignoring and attention and this example and we will take those predictions forward.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('Then we get predictions of only Jane and spot not Doug this avoids the Dog Saw Doug period type of error and the other errors that we saw.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('The first one of these make some predictions given that the word Doug just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('Then we get predictions of only Jane and spot not Doug this avoids the Dog Saw Doug period type of error and the other errors that we saw.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('This is learned that the word saw is a great guess to make for next word, but it is also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('Then we get predictions of only Jane and spot not Doug this avoids the Dog Saw Doug period type of error and the other errors that we saw.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('So it makes a positive prediction for Saul and a negative.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('What this shows is that long shortterm memory can look back two three many time steps and use that information to make good predictions about what is going to happen next now to be fair to vanilla recurrent neural networks.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('It looks like is that they find the higherlevel idea and translate it from one mode of expression to another just using the bits and pieces that we just walked through.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3')\n",
      "('What this shows is that long shortterm memory can look back two three many time steps and use that information to make good predictions about what is going to happen next now to be fair to vanilla recurrent neural networks.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('I will not step through it, but it is encouraging that something that looks so complex expressed mathematically actually makes it fairly straightforward.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3')\n",
      "('What this shows is that long shortterm memory can look back two three many time steps and use that information to make good predictions about what is going to happen next now to be fair to vanilla recurrent neural networks.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('They get passed forward to all of these neural networks, and we get a new set of predictions because the word saw just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('What this shows is that long shortterm memory can look back two three many time steps and use that information to make good predictions about what is going to happen next now to be fair to vanilla recurrent neural networks.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('They can actually look back several time steps as well, but not very many.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('It looks like is that they find the higherlevel idea and translate it from one mode of expression to another just using the bits and pieces that we just walked through.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3')\n",
      "('They can actually look back several time steps as well, but not very many.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('L spms are a great fit for any information that is embedded in time audio video on my favorite application of all forces robotics robotics is nothing more than an agent taking in information from a set of sensors and then based on that information making a decision and carrying out an action.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3')\n",
      "('They can actually look back several time steps as well, but not very many.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('It is inherently sequential and actions taken now can influence what is sent and what should be done many times steps down the line.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3')\n",
      "('They can actually look back several time steps as well, but not very many.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('They get passed forward to all of these neural networks, and we get a new set of predictions because the word saw just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('They can actually look back several time steps as well, but not very many.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('The other thing that happened is our previous set of possibilities.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('They can actually look back several time steps as well, but not very many.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('LST M can look back many times steps and has shown that successfully.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('I will not step through it, but it is encouraging that something that looks so complex expressed mathematically actually makes it fairly straightforward.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3')\n",
      "('LST M can look back many times steps and has shown that successfully.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('Also, there are a collection of really good tutorials and discussions other ways of explaining lsdm so that you may find helpful as well.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3')\n",
      "('LST M can look back many times steps and has shown that successfully.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('I would also strongly encourage you to visit Andre carpet these blog posts showing examples of what L stm is can do in text.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3')\n",
      "('LST M can look back many times steps and has shown that successfully.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('I will pass over ignoring and attention and this example and we will take those predictions forward.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('LST M can look back many times steps and has shown that successfully.', '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('It takes them and uses that been to predict what text what word is being spoken and it can use the history the recent history of words to make a better guess for what is going to come next.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3') ('It is a phrase to phrase or even in some cases a sentence to sentence process LS TMS are able to represent those grammar structures that are specific to each language and', '2020-01-31T08:13:38Z', '716067a60a1a4034abc49a12ecafb39b', 'a1772ba2-b967-4483-b518-6fe281080e37')\n",
      "('I would also strongly encourage you to visit Andre carpet these blog posts showing examples of what L stm is can do in text.', '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3') ('It is a phrase to phrase or even in some cases a sentence to sentence process LS TMS are able to represent those grammar structures that are specific to each language and', '2020-01-31T08:13:38Z', '716067a60a1a4034abc49a12ecafb39b', 'a1772ba2-b967-4483-b518-6fe281080e37')\n",
      "('The way it works is you start with two vectors of equal size and you go down each one.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e')\n",
      "('The way it works is you start with two vectors of equal size and you go down each one.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e')\n",
      "('The way it works is you start with two vectors of equal size and you go down each one.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('The way it works is you start with two vectors of equal size and you go down each one.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('Now in order to do gating it is nice to have a value that you know is always between XnumberX and XnumberX so we introduce another squashing function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('The way it works is you start with two vectors of equal size and you go down each one.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('This will represent with a circle with a flat bottom and this is It is called The logistic function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('The way it works is you start with two vectors of equal size and you go down each one.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('It is very similar to the other squashing function the hyperbolic tangent except that it just goes between XnumberX and XnumberX instead of minus XnumberX and XnumberX.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('The way it works is you start with two vectors of equal size and you go down each one.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e')\n",
      "('You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e')\n",
      "('You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('Now in order to do gating it is nice to have a value that you know is always between XnumberX and XnumberX so we introduce another squashing function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('This will represent with a circle with a flat bottom and this is It is called The logistic function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('So XnumberX plus XnumberX equals XnumberX, then you go to the next element XnumberX plus XnumberX equals XnumberX.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('So XnumberX plus XnumberX equals XnumberX, then you go to the next element XnumberX plus XnumberX equals XnumberX.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('This will represent with a circle with a flat bottom and this is It is called The logistic function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('And so your output Vector is the same size of each of your input vectors.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e')\n",
      "('And so your output Vector is the same size of each of your input vectors.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('And so your output Vector is the same size of each of your input vectors.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('Now in order to do gating it is nice to have a value that you know is always between XnumberX and XnumberX so we introduce another squashing function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('And so your output Vector is the same size of each of your input vectors.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('This will represent with a circle with a flat bottom and this is It is called The logistic function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('And so your output Vector is the same size of each of your input vectors.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('Just a list of numbers same length, but it is the some element by element of the two.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e')\n",
      "('Just a list of numbers same length, but it is the some element by element of the two.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e')\n",
      "('Just a list of numbers same length, but it is the some element by element of the two.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('And this case we will just assign the number two that XnumberX.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Just a list of numbers same length, but it is the some element by element of the two.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('Now in order to do gating it is nice to have a value that you know is always between XnumberX and XnumberX so we introduce another squashing function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Just a list of numbers same length, but it is the some element by element of the two.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('This will represent with a circle with a flat bottom and this is It is called The logistic function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Youve probably guessed the X in the circle is element by element multiplication.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e')\n",
      "('Youve probably guessed the X in the circle is element by element multiplication.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('And this case we will just assign the number two that XnumberX.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Youve probably guessed the X in the circle is element by element multiplication.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('So in this case an open gate an open faucet would be a one and a closed faucet would be a zero and the way this works with element wise multiplication.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Youve probably guessed the X in the circle is element by element multiplication.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Youve probably guessed the X in the circle is element by element multiplication.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('And then with the gating value of XnumberX the single was passed through but it is smaller.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Youve probably guessed the X in the circle is element by element multiplication.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('Now in order to do gating it is nice to have a value that you know is always between XnumberX and XnumberX so we introduce another squashing function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Youve probably guessed the X in the circle is element by element multiplication.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('This will represent with a circle with a flat bottom and this is It is called The logistic function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Youve probably guessed the X in the circle is element by element multiplication.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('It is very similar to the other squashing function the hyperbolic tangent except that it just goes between XnumberX and XnumberX instead of minus XnumberX and XnumberX.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('It is just like Edition except instead of adding you multiply for instance.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e')\n",
      "('Three times six gives you a first element of XnumberX XnumberX times XnumberX gives you XnumberX again.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('And this case we will just assign the number two that XnumberX.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Three times six gives you a first element of XnumberX XnumberX times XnumberX gives you XnumberX again.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('The output Vector is the same size of each of the input vectors.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('The output Vector is the same size of each of the input vectors.', '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1') ('This will represent with a circle with a flat bottom and this is It is called The logistic function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('In this form and this formulation, we have a recurrent neural network for Simplicity.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352')\n",
      "('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('I will take the vectors in the weights and collapse them down to that little symbol with the dots in the arrows the dots in the lines connecting them and there is one more symbol we have not talked about yet.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352')\n",
      "('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('How it works is you take all of your votes coming out and you subject them to this squashing function.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352')\n",
      "('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('RXnumberX and similarly if you put in a big negative number, then what you will get out will be very close to minus XnumberX.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352')\n",
      "('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('By ensuring that it is always less than XnumberX but more than minus XnumberX you can multiply it as many times as you want.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352')\n",
      "('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('So you may have noticed our neural network in its current state is subject to some mistakes.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352')\n",
      "('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('This will represent with a circle with a flat bottom and this is It is called The logistic function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Weve introduced here one is another squashing function this one with a flat bottom.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('I will take the vectors in the weights and collapse them down to that little symbol with the dots in the arrows the dots in the lines connecting them and there is one more symbol we have not talked about yet.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352')\n",
      "('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('This is a squashing function and it just helps the network to behave.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352')\n",
      "('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('So you may have noticed our neural network in its current state is subject to some mistakes.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352')\n",
      "('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('And this case we will just assign the number two that XnumberX.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('So in this case an open gate an open faucet would be a one and a closed faucet would be a zero and the way this works with element wise multiplication.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('Now in order to do gating it is nice to have a value that you know is always between XnumberX and XnumberX so we introduce another squashing function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('This will represent with a circle with a flat bottom and this is It is called The logistic function.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('One is an X in a circle and one is a cross in a circle.', '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e') ('It is very similar to the other squashing function the hyperbolic tangent except that it just goes between XnumberX and XnumberX instead of minus XnumberX and XnumberX.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4')\n",
      "('Youve probably noticed though when we are combining our predictions with our memories.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('Youve probably noticed though when we are combining our predictions with our memories.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('We may not necessarily want to release all of those memories out as new predictions each time.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('So we want a little filter to keep our memories inside and let our predictions get out.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('So we want a little filter to keep our memories inside and let our predictions get out.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('And that is we add another gate for that to do selection.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('Oceans can be used to vote on what all the gates should be.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('What should be kept internal and what should be released as a prediction.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('It is possible that things could become greater than XnumberX or smaller than XnumberX.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('So we just squash it to be careful to make sure it never gets out of control.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('So we just squash it to be careful to make sure it never gets out of control.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "('Each of these things when to forget and when to let things out of our memory are learned by their own neural networks.', '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('It is really useful in some surprisingly practical applications if I have text in one language, and I want to translate it to text to another language L stm is work very well, even though translation is not a word to word process.', '2020-01-31T08:13:38Z', '716067a60a1a4034abc49a12ecafb39b', 'a1772ba2-b967-4483-b518-6fe281080e37') ('The first one of these make some predictions given that the word Doug just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('It is really useful in some surprisingly practical applications if I have text in one language, and I want to translate it to text to another language L stm is work very well, even though translation is not a word to word process.', '2020-01-31T08:13:38Z', '716067a60a1a4034abc49a12ecafb39b', 'a1772ba2-b967-4483-b518-6fe281080e37') ('The word saw is our most recent word and our most recent prediction.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n",
      "('It is a phrase to phrase or even in some cases a sentence to sentence process LS TMS are able to represent those grammar structures that are specific to each language and', '2020-01-31T08:13:38Z', '716067a60a1a4034abc49a12ecafb39b', 'a1772ba2-b967-4483-b518-6fe281080e37') ('The first one of these make some predictions given that the word Doug just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('And the task of the neural network is to put these together in the right order to make a good children is book.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', 'a8e01ed7-f7df-4351-971d-2265654b4b34') ('We just ignore the new information part and we can unwrap or unwind this Vector in time until we do have some information to base it on and then just play it forward.', '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf')\n",
      "('And the task of the neural network is to put these together in the right order to make a good children is book.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', 'a8e01ed7-f7df-4351-971d-2265654b4b34') ('So this is a nice simple example that showed recurrent neural networks now to show how they do not meet all of our needs.', '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf')\n",
      "('So for instance, if Doug was the most recent word that I saw my new information Vector would be all zeros except for a one and the dog position.', '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', 'a8e01ed7-f7df-4351-971d-2265654b4b34') ('Now we can see how if we were lacking some information.', '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf')\n",
      "('We can also have Vector for statements like, it is Tuesday in order to encode this kind of information what we do is we make a List of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true.', '2020-01-31T07:54:21Z', '716067a60a1a4034abc49a12ecafb39b', 'b9d07405-49d7-48e9-a8bf-284e87ecb9c2') ('We can make a one hot Vector for our prediction for dinner tonight.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece')\n",
      "('It seems inefficient, but for a computer, this is a lot easier way to ingest that information.', '2020-01-31T07:54:21Z', '716067a60a1a4034abc49a12ecafb39b', 'b9d07405-49d7-48e9-a8bf-284e87ecb9c2') ('The trouble is that your network does not work very well despite carefully choosing your inputs and training it thoroughly.', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('It seems inefficient, but for a computer, this is a lot easier way to ingest that information.', '2020-01-31T07:54:21Z', '716067a60a1a4034abc49a12ecafb39b', 'b9d07405-49d7-48e9-a8bf-284e87ecb9c2') ('So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday.', '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5')\n",
      "('I will take the vectors in the weights and collapse them down to that little symbol with the dots in the arrows the dots in the lines connecting them and there is one more symbol we have not talked about yet.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352') ('In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('This is a squashing function and it just helps the network to behave.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352') ('In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('When you have a loop like this where the same values get processed again and again day after day it is possible.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352') ('Because each of our predictions only looks back one time Step It has very short term memory.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('When you have a loop like this where the same values get processed again and again day after day it is possible.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352') ('In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('You can imagine if in the course of that processing say something got voted for twice.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352') ('Then it does not use the information from further back and it is subject to these types of mistakes.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('You can go through that Loop and it will not explode in a feedback loop.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352') ('Because each of our predictions only looks back one time Step It has very short term memory.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('You can go through that Loop and it will not explode in a feedback loop.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352') ('Then it does not use the information from further back and it is subject to these types of mistakes.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('So you may have noticed our neural network in its current state is subject to some mistakes.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352') ('Because each of our predictions only looks back one time Step It has very short term memory.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('So you may have noticed our neural network in its current state is subject to some mistakes.', '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352') ('In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.', '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084')\n",
      "('Now we can see how if we were lacking some information.', '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf') ('We can make a one hot Vector for our prediction for dinner tonight.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece')\n",
      "('Now we can see how if we were lacking some information.', '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf') ('We set everything equal to XnumberX except for the dinner item that we predict.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece')\n",
      "('Let is say we were out of town for two weeks.', '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf') ('We can make a one hot Vector for our prediction for dinner tonight.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece')\n",
      "('We just ignore the new information part and we can unwrap or unwind this Vector in time until we do have some information to base it on and then just play it forward.', '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf') ('We can make a one hot Vector for our prediction for dinner tonight.', '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece')\n",
      "('The first one of these make some predictions given that the word Doug just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('So it makes a positive prediction for Saul and a negative.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('So the fact that there is a vote for not Doug gets blocked here and the word saw get sent out as the prediction for the next time step.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('They get passed forward to all of these neural networks, and we get a new set of predictions because the word saw just occurred.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('We now predict that the words Doug Jane or spot might come next.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('I will pass over ignoring and attention and this example and we will take those predictions forward.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('The other thing that happened is our previous set of possibilities.', '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c') ('Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.', '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5')\n",
      "('You imagine that you have a signal and it is like a bunch of pipes and they have a certain amount of water trying to flow down them.', '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4') ('You can see this is powerful this let us hold onto things for as long as we want.', '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794')\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 390, \"module\": \"grouper_segments\", \"outlier threshold is : \": 0.4136064052581787, \"ts\": \"2020-03-05T08:27:05.437935Z\", \"msg\": \"Outlier Score\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Community Algorithm\n",
      "cluster before alteration=========>\n",
      "If you happen to be gone on a given night, let us say yesterday.\n",
      "You can still predict what is going to be for dinner tonight by thinking back two days ago think what was for dinner then so what would be predicted for you last night?\n",
      "And then you can use that prediction in turn to make a prediction for tonight.\n",
      "So we make use of not only our actual information from yesterday.\n",
      "So at this point it is helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if I want to describe the weather to you for a given day, I could say Say the high XnumberX degrees Fahrenheit the lows XnumberX wins XnumberX miles an hour.\n",
      "There is going to be a quarter inch of rain and the relative humidity is XnumberX percent.\n",
      "Let is say for a minute that you are a very lucky apartment dweller and you have a flatmate who loves to cook dinner every night.\n",
      "He cooks one of three things Sushi waffles or pizza and you would like to be able to predict what you are going to have on a Of a night so you can plan the rest of your days eating accordingly in order to predict what you are going to have for dinner.\n",
      "Let is say we were out of town for two weeks.\n",
      "We can still make a good guess about what is going to be for dinner tonight.\n",
      "We can make a one hot Vector for our prediction for dinner tonight.\n",
      "And you learn how to predict what is going to be for dinner tonight?\n",
      "It is useful to take a step back and just look at the data and when you do that, you notice a pattern your flatmate makes pizza then Sushi then waffles then pizza again in a Michael it does not depend on the day of the week or anything else.\n",
      "So if we know if we had pizza for dinner yesterday, it will be sushi tonight.\n",
      "And when it is unwrapped it looks like this and we can go back as far as we need to and see what was for dinner and then just trace it forward and play out our menu over the last two weeks until we find out what is for dinner tonight.\n",
      "We set everything equal to XnumberX except for the dinner item that we predict.\n",
      "So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday.\n",
      "You still cannot get much better than chance predictions on dinner as is often the case with complicated machine learning problems.\n",
      "We can also have Vector for statements like, it is Tuesday in order to encode this kind of information what we do is we make a List of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true.\n",
      "It seems inefficient, but for a computer, this is a lot easier way to ingest that information.\n",
      "Complete our picture we can show how the prediction for today will get recycled the dotted line there means hold onto it for a day and then reuse it tomorrow and it becomes our yesterday is predictions tomorrow.\n",
      "Now we can see how if we were lacking some information.\n",
      "The vector is the reason that it is useful is vectors list of numbers are computers native language.\n",
      "This format is called one hot and coding and it is very common to see a long Vector of zeros with just one element being one.\n",
      "Applications of machine learning have gotten a lot of traction in the last few years.\n",
      "There is a couple of big categories that have had wins one is identifying pictures the equivalent of finding cats on the internet and any problem that can be made to look like that and the other is sequence to sequence deflation is can be speech to text or one language to another most of the former are done with convolutional neural networks.\n",
      "Most of the latter are done with recurrent neural networks particularly long short term memory to give an example of how long shortterm memory works.\n",
      "You set up a neural network the inputs to this neural network are a bunch of items like the day of the week the month of the year whether or not your flatmate was in a late meeting variables that might reasonably affect what you are going to have for dinner.\n",
      "Now if you are new to neural networks, I highly recommend you take a minute and stopped to watch the how neural networks work tutorial.\n",
      "If you would rather not do that right now and you are still not familiar with neural networks, you can think of them as a voting process and so in the neural network that you set up there is a complicated voting process and all of the inputs like day of the week and month of the year go into it and then you try train it on your history of what you have had for dinner.\n",
      "And the neural network is just connections between every element in each of those input vectors to every element in the output connector.\n",
      "So this is a nice simple example that showed recurrent neural networks now to show how they do not meet all of our needs.\n",
      "The trouble is that your network does not work very well despite carefully choosing your inputs and training it thoroughly.\n",
      "It becomes a very simple voting process and and it is right all the time because your flatmate is incredibly consistent.\n",
      "We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.\n",
      "We just ignore the new information part and we can unwrap or unwind this Vector in time until we do have some information to base it on and then just play it forward.\n",
      "If you want to get something into a format that it is natural for a computer to compute to do operations on to do statistical machine learning list of numbers are the way to go everything gets reduced to a list of numbers before it goes through an algorithm.\n",
      "Itll have sentences of the format Doug saw Jane period Jane sauce.\n",
      "It is really useful in some surprisingly practical applications if I have text in one language, and I want to translate it to text to another language L stm is work very well, even though translation is not a word to word process.\n",
      "It is a phrase to phrase or even in some cases a sentence to sentence process LS TMS are able to represent those grammar structures that are specific to each language and\n",
      "cluster before alteration=========>\n",
      "So we have our New information which is the word Doug we have our recent prediction.\n",
      "We had just wrapped up a sentence with a period the new sentence can start with any name.\n",
      "Now long short term memory has a lot of pieces a lot of bits that were together and it is a little much to wrap your head around it all at once.\n",
      "Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.\n",
      "This is an intention mechanism it lets things that are not immediately relevant be set aside.\n",
      "So if they do not Cloud the predictions in memory going forward It has its own neural network and its own Logistics squashing function and its own gating activity right here.\n",
      "So what we will do is take a very simple example and step through it just to illustrate how a couple of these pieces work.\n",
      "It is admittedly an overly simplistic example, and feel free to poke holes at it later.\n",
      "When you get to that point then you know, you are ready to move on to the next level of material.\n",
      "So we are now in the process of writing our children is book.\n",
      "And for the purposes of demonstration, we will assume that this LS TM has been trained on our children is books examples that we want to mimic and all of the appropriate votes and weights in those neural networks have been learned now will show it in action.\n",
      "So so far our story so far is Jane saucepot period Doug so Doug is the most recent word that is occurred in our story and also not surprisingly for this time step.\n",
      "The name is Doug Jane and spot where all predicted as viable options.\n",
      "cluster before alteration=========>\n",
      "When we introduced all of these together what we get, we still have the combination of our previous predictions in our new information those vectors get past and we make predictions based on them.\n",
      "So now we have not just prediction for a predictions plus the memories that we have accumulated and that we have not chosen to forget.\n",
      "Youve probably noticed though when we are combining our predictions with our memories.\n",
      "So we want a little filter to keep our memories inside and let our predictions get out.\n",
      "So its own voting process so that our new information and our previous predictions.\n",
      "You can see this is powerful this let us hold onto things for as long as we want.\n",
      "Now there is an entirely separate neural network here that learns when to forget what based on what we are seeing right now.\n",
      "And that is we add another gate for that to do selection.\n",
      "So we just squash it to be careful to make sure it never gets out of control.\n",
      "And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.\n",
      "We select just a few to release as the prediction for that moment.\n",
      "We may not necessarily want to release all of those memories out as new predictions each time.\n",
      "What should be kept internal and what should be released as a prediction.\n",
      "Each of these things when to forget and when to let things out of our memory are learned by their own neural networks.\n",
      "Weve also introduced another squashing function here since we do an addition here.\n",
      "Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network.\n",
      "Some of them are remembered the ones that are remembered are added back into the prediction.\n",
      "Oceans can be used to vote on what all the gates should be.\n",
      "It is possible that things could become greater than XnumberX or smaller than XnumberX.\n",
      "cluster before alteration=========>\n",
      "And we similarly can represent our predictions and our predictions from yesterday.\n",
      "We want to be able to remember what happened many times steps ago.\n",
      "And by a similar method anytime we come across the word saw or a period we know that a name has to come after that.\n",
      "So it will learn to vote very strongly for a name Jane Doug or spot.\n",
      "For instance anytime a name comes up Jane Doug or spot.\n",
      "We would expect that to vote heavily for the word saw or for a period because those are the two words in our dictionary that can follow a name.\n",
      "So to do this, we replace our food vectors with our dictionary vectors here again, it is just a list of numbers representing each of the words.\n",
      "early, if we had predicted a name on the previous time step, we would expect those to vote also for the word saw or for a period\n",
      "So for instance, if Doug was the most recent word that I saw my new information Vector would be all zeros except for a one and the dog position.\n",
      "Period because Doug strongly votes for the word saw which in turn strongly votes for uh name any name which could be Doug similarly we could get something like Doug saw Jane saw spot saw Doug.\n",
      "Then it does not use the information from further back and it is subject to these types of mistakes.\n",
      "In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.\n",
      "The critical part that we add to the middle here is memory.\n",
      "After training this neural network and teaching it what to do.\n",
      "And the task of the neural network is to put these together in the right order to make a good children is book.\n",
      "Dictionary is small just the words Doug Jane spot saw and a period.\n",
      "Because each of our predictions only looks back one time Step It has very short term memory.\n",
      "cluster before alteration=========>\n",
      "We got x XnumberX in that case it would get twice as big every time and very soon blow up to be astronomical.\n",
      "You can imagine if in the course of that processing say something got voted for twice.\n",
      "In this form and this formulation, we have a recurrent neural network for Simplicity.\n",
      "So you may have noticed our neural network in its current state is subject to some mistakes.\n",
      "How it works is you take all of your votes coming out and you subject them to this squashing function.\n",
      "We could get a sentence for instance of the form Doug saw Doug.\n",
      "If something received a total vote of .XnumberX, you draw a vertical line up where it crosses the function you draw a horizontal line over to the y axis and there is your squash diversion out for small numbers.\n",
      "I will take the vectors in the weights and collapse them down to that little symbol with the dots in the arrows the dots in the lines connecting them and there is one more symbol we have not talked about yet.\n",
      "RXnumberX and similarly if you put in a big negative number, then what you will get out will be very close to minus XnumberX.\n",
      "No matter what you put in what comes out is between minus XnumberX and XnumberX.\n",
      "By ensuring that it is always less than XnumberX but more than minus XnumberX you can multiply it as many times as you want.\n",
      "This is a squashing function and it just helps the network to behave.\n",
      "But as your number gets larger, the number that comes out is closer and closer.\n",
      "When you have a loop like this where the same values get processed again and again day after day it is possible.\n",
      "You can go through that Loop and it will not explode in a feedback loop.\n",
      "cluster before alteration=========>\n",
      "Now the forgetting gate says hey my last word that came that occurred was the word saw based on my past experience.\n",
      "But I want to keep any predictions having to do with names.\n",
      "It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step.\n",
      "This is learned that the word saw is a great guess to make for next word, but it is also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence.\n",
      "So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out.\n",
      "The word saw is our most recent word and our most recent prediction.\n",
      "So it forgets saw holds onto the vote for not Doug.\n",
      "We have a positive vote for dog an Get a vote for Doug and so they cancel each other out.\n",
      "So the fact that there is a vote for not Doug gets blocked here and the word saw get sent out as the prediction for the next time step.\n",
      "We now predict that the words Doug Jane or spot might come next.\n",
      "What this shows is that long shortterm memory can look back two three many time steps and use that information to make good predictions about what is going to happen next now to be fair to vanilla recurrent neural networks.\n",
      "So we will skip over it for now and this prediction of Saw not Doug is passed forward and again for the purposes of Simplicity.\n",
      "I will pass over ignoring and attention and this example and we will take those predictions forward.\n",
      "The other thing that happened is our previous set of possibilities.\n",
      "Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.\n",
      "The first one of these make some predictions given that the word Doug just occurred.\n",
      "So it makes a positive prediction for Saul and a negative.\n",
      "They get passed forward to all of these neural networks, and we get a new set of predictions because the word saw just occurred.\n",
      "Then we get predictions of only Jane and spot not Doug this avoids the Dog Saw Doug period type of error and the other errors that we saw.\n",
      "The word saw and not Doug that we were maintaining internally get passed to a forgetting gate.\n",
      "They can actually look back several time steps as well, but not very many.\n",
      "LST M can look back many times steps and has shown that successfully.\n",
      "Addiction for Doug it says I do not expect to see dog in the near future.\n",
      "cluster before alteration=========>\n",
      "It takes them and uses that been to predict what text what word is being spoken and it can use the history the recent history of words to make a better guess for what is going to come next.\n",
      "It looks like is that they find the higherlevel idea and translate it from one mode of expression to another just using the bits and pieces that we just walked through.\n",
      "Also, there are a collection of really good tutorials and discussions other ways of explaining lsdm so that you may find helpful as well.\n",
      "It is inherently sequential and actions taken now can influence what is sent and what should be done many times steps down the line.\n",
      "I will not step through it, but it is encouraging that something that looks so complex expressed mathematically actually makes it fairly straightforward.\n",
      "I would also strongly encourage you to visit Andre carpet these blog posts showing examples of what L stm is can do in text.\n",
      "L spms are a great fit for any information that is embedded in time audio video on my favorite application of all forces robotics robotics is nothing more than an agent taking in information from a set of sensors and then based on that information making a decision and carrying out an action.\n",
      "If you are curious what LS ems look like in math.\n",
      "This is it this is lifted straight from the Wikipedia page.\n",
      "Picture and story and if you would like to dig into it more I encourage you to go to the Wikipedia page.\n",
      "cluster before alteration=========>\n",
      "Just a list of numbers same length, but it is the some element by element of the two.\n",
      "The way it works is you start with two vectors of equal size and you go down each one.\n",
      "And so your output Vector is the same size of each of your input vectors.\n",
      "We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.\n",
      "You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector.\n",
      "So XnumberX plus XnumberX equals XnumberX, then you go to the next element XnumberX plus XnumberX equals XnumberX.\n",
      "Youve probably guessed the X in the circle is element by element multiplication.\n",
      "It is just like Edition except instead of adding you multiply for instance.\n",
      "Three times six gives you a first element of XnumberX XnumberX times XnumberX gives you XnumberX again.\n",
      "The output Vector is the same size of each of the input vectors.\n",
      "Weve introduced here one is another squashing function this one with a flat bottom.\n",
      "One is an X in a circle and one is a cross in a circle.\n",
      "Now in order to do gating it is nice to have a value that you know is always between XnumberX and XnumberX so we introduce another squashing function.\n",
      "This will represent with a circle with a flat bottom and this is It is called The logistic function.\n",
      "It is very similar to the other squashing function the hyperbolic tangent except that it just goes between XnumberX and XnumberX instead of minus XnumberX and XnumberX.\n",
      "And this case we will just assign the number two that XnumberX.\n",
      "So in this case an open gate an open faucet would be a one and a closed faucet would be a zero and the way this works with element wise multiplication.\n",
      "And then with the gating value of XnumberX the single was passed through but it is smaller.\n",
      "So gating lets us control what passes through and what gets blocked which is really useful.\n",
      "You imagine that you have a signal and it is like a bunch of pipes and they have a certain amount of water trying to flow down them.\n",
      "Now on each of those pipes, we have a faucet and we can open it all the way close it all the way or keep it somewhere in the middle to either let that signal come through or block it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "If you happen to be gone on a given night, let us say yesterday.\n",
      "You can still predict what is going to be for dinner tonight by thinking back two days ago think what was for dinner then so what would be predicted for you last night?\n",
      "And then you can use that prediction in turn to make a prediction for tonight.\n",
      "So we make use of not only our actual information from yesterday.\n",
      "So at this point it is helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if I want to describe the weather to you for a given day, I could say Say the high XnumberX degrees Fahrenheit the lows XnumberX wins XnumberX miles an hour.\n",
      "There is going to be a quarter inch of rain and the relative humidity is XnumberX percent.\n",
      "Let is say for a minute that you are a very lucky apartment dweller and you have a flatmate who loves to cook dinner every night.\n",
      "He cooks one of three things Sushi waffles or pizza and you would like to be able to predict what you are going to have on a Of a night so you can plan the rest of your days eating accordingly in order to predict what you are going to have for dinner.\n",
      "Let is say we were out of town for two weeks.\n",
      "We can still make a good guess about what is going to be for dinner tonight.\n",
      "We can make a one hot Vector for our prediction for dinner tonight.\n",
      "And you learn how to predict what is going to be for dinner tonight?\n",
      "It is useful to take a step back and just look at the data and when you do that, you notice a pattern your flatmate makes pizza then Sushi then waffles then pizza again in a Michael it does not depend on the day of the week or anything else.\n",
      "So if we know if we had pizza for dinner yesterday, it will be sushi tonight.\n",
      "And when it is unwrapped it looks like this and we can go back as far as we need to and see what was for dinner and then just trace it forward and play out our menu over the last two weeks until we find out what is for dinner tonight.\n",
      "We set everything equal to XnumberX except for the dinner item that we predict.\n",
      "So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday.\n",
      "You still cannot get much better than chance predictions on dinner as is often the case with complicated machine learning problems.\n",
      "We can also have Vector for statements like, it is Tuesday in order to encode this kind of information what we do is we make a List of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true.\n",
      "It seems inefficient, but for a computer, this is a lot easier way to ingest that information.\n",
      "Complete our picture we can show how the prediction for today will get recycled the dotted line there means hold onto it for a day and then reuse it tomorrow and it becomes our yesterday is predictions tomorrow.\n",
      "Now we can see how if we were lacking some information.\n",
      "The vector is the reason that it is useful is vectors list of numbers are computers native language.\n",
      "This format is called one hot and coding and it is very common to see a long Vector of zeros with just one element being one.\n",
      "Applications of machine learning have gotten a lot of traction in the last few years.\n",
      "There is a couple of big categories that have had wins one is identifying pictures the equivalent of finding cats on the internet and any problem that can be made to look like that and the other is sequence to sequence deflation is can be speech to text or one language to another most of the former are done with convolutional neural networks.\n",
      "Most of the latter are done with recurrent neural networks particularly long short term memory to give an example of how long shortterm memory works.\n",
      "You set up a neural network the inputs to this neural network are a bunch of items like the day of the week the month of the year whether or not your flatmate was in a late meeting variables that might reasonably affect what you are going to have for dinner.\n",
      "Now if you are new to neural networks, I highly recommend you take a minute and stopped to watch the how neural networks work tutorial.\n",
      "If you would rather not do that right now and you are still not familiar with neural networks, you can think of them as a voting process and so in the neural network that you set up there is a complicated voting process and all of the inputs like day of the week and month of the year go into it and then you try train it on your history of what you have had for dinner.\n",
      "And the neural network is just connections between every element in each of those input vectors to every element in the output connector.\n",
      "So this is a nice simple example that showed recurrent neural networks now to show how they do not meet all of our needs.\n",
      "The trouble is that your network does not work very well despite carefully choosing your inputs and training it thoroughly.\n",
      "It becomes a very simple voting process and and it is right all the time because your flatmate is incredibly consistent.\n",
      "We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.\n",
      "We just ignore the new information part and we can unwrap or unwind this Vector in time until we do have some information to base it on and then just play it forward.\n",
      "If you want to get something into a format that it is natural for a computer to compute to do operations on to do statistical machine learning list of numbers are the way to go everything gets reduced to a list of numbers before it goes through an algorithm.\n",
      "Itll have sentences of the format Doug saw Jane period Jane sauce.\n",
      "It is really useful in some surprisingly practical applications if I have text in one language, and I want to translate it to text to another language L stm is work very well, even though translation is not a word to word process.\n",
      "It is a phrase to phrase or even in some cases a sentence to sentence process LS TMS are able to represent those grammar structures that are specific to each language and\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "So we have our New information which is the word Doug we have our recent prediction.\n",
      "We had just wrapped up a sentence with a period the new sentence can start with any name.\n",
      "Now long short term memory has a lot of pieces a lot of bits that were together and it is a little much to wrap your head around it all at once.\n",
      "Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.\n",
      "This is an intention mechanism it lets things that are not immediately relevant be set aside.\n",
      "So if they do not Cloud the predictions in memory going forward It has its own neural network and its own Logistics squashing function and its own gating activity right here.\n",
      "So what we will do is take a very simple example and step through it just to illustrate how a couple of these pieces work.\n",
      "It is admittedly an overly simplistic example, and feel free to poke holes at it later.\n",
      "When you get to that point then you know, you are ready to move on to the next level of material.\n",
      "So we are now in the process of writing our children is book.\n",
      "And for the purposes of demonstration, we will assume that this LS TM has been trained on our children is books examples that we want to mimic and all of the appropriate votes and weights in those neural networks have been learned now will show it in action.\n",
      "So so far our story so far is Jane saucepot period Doug so Doug is the most recent word that is occurred in our story and also not surprisingly for this time step.\n",
      "The name is Doug Jane and spot where all predicted as viable options.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "When we introduced all of these together what we get, we still have the combination of our previous predictions in our new information those vectors get past and we make predictions based on them.\n",
      "So now we have not just prediction for a predictions plus the memories that we have accumulated and that we have not chosen to forget.\n",
      "Youve probably noticed though when we are combining our predictions with our memories.\n",
      "So we want a little filter to keep our memories inside and let our predictions get out.\n",
      "So its own voting process so that our new information and our previous predictions.\n",
      "You can see this is powerful this let us hold onto things for as long as we want.\n",
      "Now there is an entirely separate neural network here that learns when to forget what based on what we are seeing right now.\n",
      "And that is we add another gate for that to do selection.\n",
      "So we just squash it to be careful to make sure it never gets out of control.\n",
      "And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.\n",
      "We select just a few to release as the prediction for that moment.\n",
      "We may not necessarily want to release all of those memories out as new predictions each time.\n",
      "What should be kept internal and what should be released as a prediction.\n",
      "Each of these things when to forget and when to let things out of our memory are learned by their own neural networks.\n",
      "Weve also introduced another squashing function here since we do an addition here.\n",
      "Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network.\n",
      "Some of them are remembered the ones that are remembered are added back into the prediction.\n",
      "Oceans can be used to vote on what all the gates should be.\n",
      "It is possible that things could become greater than XnumberX or smaller than XnumberX.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "And we similarly can represent our predictions and our predictions from yesterday.\n",
      "We want to be able to remember what happened many times steps ago.\n",
      "And by a similar method anytime we come across the word saw or a period we know that a name has to come after that.\n",
      "So it will learn to vote very strongly for a name Jane Doug or spot.\n",
      "For instance anytime a name comes up Jane Doug or spot.\n",
      "We would expect that to vote heavily for the word saw or for a period because those are the two words in our dictionary that can follow a name.\n",
      "So to do this, we replace our food vectors with our dictionary vectors here again, it is just a list of numbers representing each of the words.\n",
      "early, if we had predicted a name on the previous time step, we would expect those to vote also for the word saw or for a period\n",
      "So for instance, if Doug was the most recent word that I saw my new information Vector would be all zeros except for a one and the dog position.\n",
      "Period because Doug strongly votes for the word saw which in turn strongly votes for uh name any name which could be Doug similarly we could get something like Doug saw Jane saw spot saw Doug.\n",
      "Then it does not use the information from further back and it is subject to these types of mistakes.\n",
      "In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.\n",
      "The critical part that we add to the middle here is memory.\n",
      "After training this neural network and teaching it what to do.\n",
      "And the task of the neural network is to put these together in the right order to make a good children is book.\n",
      "Dictionary is small just the words Doug Jane spot saw and a period.\n",
      "Because each of our predictions only looks back one time Step It has very short term memory.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "We got x XnumberX in that case it would get twice as big every time and very soon blow up to be astronomical.\n",
      "You can imagine if in the course of that processing say something got voted for twice.\n",
      "In this form and this formulation, we have a recurrent neural network for Simplicity.\n",
      "So you may have noticed our neural network in its current state is subject to some mistakes.\n",
      "How it works is you take all of your votes coming out and you subject them to this squashing function.\n",
      "We could get a sentence for instance of the form Doug saw Doug.\n",
      "If something received a total vote of .XnumberX, you draw a vertical line up where it crosses the function you draw a horizontal line over to the y axis and there is your squash diversion out for small numbers.\n",
      "I will take the vectors in the weights and collapse them down to that little symbol with the dots in the arrows the dots in the lines connecting them and there is one more symbol we have not talked about yet.\n",
      "RXnumberX and similarly if you put in a big negative number, then what you will get out will be very close to minus XnumberX.\n",
      "No matter what you put in what comes out is between minus XnumberX and XnumberX.\n",
      "By ensuring that it is always less than XnumberX but more than minus XnumberX you can multiply it as many times as you want.\n",
      "This is a squashing function and it just helps the network to behave.\n",
      "But as your number gets larger, the number that comes out is closer and closer.\n",
      "When you have a loop like this where the same values get processed again and again day after day it is possible.\n",
      "You can go through that Loop and it will not explode in a feedback loop.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Now the forgetting gate says hey my last word that came that occurred was the word saw based on my past experience.\n",
      "But I want to keep any predictions having to do with names.\n",
      "It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step.\n",
      "This is learned that the word saw is a great guess to make for next word, but it is also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence.\n",
      "So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out.\n",
      "The word saw is our most recent word and our most recent prediction.\n",
      "So it forgets saw holds onto the vote for not Doug.\n",
      "We have a positive vote for dog an Get a vote for Doug and so they cancel each other out.\n",
      "So the fact that there is a vote for not Doug gets blocked here and the word saw get sent out as the prediction for the next time step.\n",
      "We now predict that the words Doug Jane or spot might come next.\n",
      "What this shows is that long shortterm memory can look back two three many time steps and use that information to make good predictions about what is going to happen next now to be fair to vanilla recurrent neural networks.\n",
      "So we will skip over it for now and this prediction of Saw not Doug is passed forward and again for the purposes of Simplicity.\n",
      "I will pass over ignoring and attention and this example and we will take those predictions forward.\n",
      "The other thing that happened is our previous set of possibilities.\n",
      "Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.\n",
      "The first one of these make some predictions given that the word Doug just occurred.\n",
      "So it makes a positive prediction for Saul and a negative.\n",
      "They get passed forward to all of these neural networks, and we get a new set of predictions because the word saw just occurred.\n",
      "Then we get predictions of only Jane and spot not Doug this avoids the Dog Saw Doug period type of error and the other errors that we saw.\n",
      "The word saw and not Doug that we were maintaining internally get passed to a forgetting gate.\n",
      "They can actually look back several time steps as well, but not very many.\n",
      "LST M can look back many times steps and has shown that successfully.\n",
      "Addiction for Doug it says I do not expect to see dog in the near future.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "It takes them and uses that been to predict what text what word is being spoken and it can use the history the recent history of words to make a better guess for what is going to come next.\n",
      "It looks like is that they find the higherlevel idea and translate it from one mode of expression to another just using the bits and pieces that we just walked through.\n",
      "Also, there are a collection of really good tutorials and discussions other ways of explaining lsdm so that you may find helpful as well.\n",
      "It is inherently sequential and actions taken now can influence what is sent and what should be done many times steps down the line.\n",
      "I will not step through it, but it is encouraging that something that looks so complex expressed mathematically actually makes it fairly straightforward.\n",
      "I would also strongly encourage you to visit Andre carpet these blog posts showing examples of what L stm is can do in text.\n",
      "L spms are a great fit for any information that is embedded in time audio video on my favorite application of all forces robotics robotics is nothing more than an agent taking in information from a set of sensors and then based on that information making a decision and carrying out an action.\n",
      "If you are curious what LS ems look like in math.\n",
      "This is it this is lifted straight from the Wikipedia page.\n",
      "Picture and story and if you would like to dig into it more I encourage you to go to the Wikipedia page.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Just a list of numbers same length, but it is the some element by element of the two.\n",
      "The way it works is you start with two vectors of equal size and you go down each one.\n",
      "And so your output Vector is the same size of each of your input vectors.\n",
      "We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.\n",
      "You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector.\n",
      "So XnumberX plus XnumberX equals XnumberX, then you go to the next element XnumberX plus XnumberX equals XnumberX.\n",
      "Youve probably guessed the X in the circle is element by element multiplication.\n",
      "It is just like Edition except instead of adding you multiply for instance.\n",
      "Three times six gives you a first element of XnumberX XnumberX times XnumberX gives you XnumberX again.\n",
      "The output Vector is the same size of each of the input vectors.\n",
      "Weve introduced here one is another squashing function this one with a flat bottom.\n",
      "One is an X in a circle and one is a cross in a circle.\n",
      "Now in order to do gating it is nice to have a value that you know is always between XnumberX and XnumberX so we introduce another squashing function.\n",
      "This will represent with a circle with a flat bottom and this is It is called The logistic function.\n",
      "It is very similar to the other squashing function the hyperbolic tangent except that it just goes between XnumberX and XnumberX instead of minus XnumberX and XnumberX.\n",
      "And this case we will just assign the number two that XnumberX.\n",
      "So in this case an open gate an open faucet would be a one and a closed faucet would be a zero and the way this works with element wise multiplication.\n",
      "And then with the gating value of XnumberX the single was passed through but it is smaller.\n",
      "So gating lets us control what passes through and what gets blocked which is really useful.\n",
      "You imagine that you have a signal and it is like a bunch of pipes and they have a certain amount of water trying to flow down them.\n",
      "Now on each of those pipes, we have a faucet and we can open it all the way close it all the way or keep it somewhere in the middle to either let that signal come through or block it.\n",
      "\n",
      "\n",
      "Checking Timerange --------------\n",
      "--------------\n",
      "Let is say for a minute that you are a very lucky apartment dweller and you have a flatmate who loves to cook dinner every night. 1368a07c-94f3-478d-a444-3027d5ffabb0 \n",
      "\n",
      "You set up a neural network the inputs to this neural network are a bunch of items like the day of the week the month of the year whether or not your flatmate was in a late meeting variables that might reasonably affect what you are going to have for dinner. 1368a07c-94f3-478d-a444-3027d5ffabb0 \n",
      "\n",
      "Now if you are new to neural networks, I highly recommend you take a minute and stopped to watch the how neural networks work tutorial. 1368a07c-94f3-478d-a444-3027d5ffabb0 \n",
      "\n",
      "He cooks one of three things Sushi waffles or pizza and you would like to be able to predict what you are going to have on a Of a night so you can plan the rest of your days eating accordingly in order to predict what you are going to have for dinner. 1368a07c-94f3-478d-a444-3027d5ffabb0 \n",
      "\n",
      "Most of the latter are done with recurrent neural networks particularly long short term memory to give an example of how long shortterm memory works. 1368a07c-94f3-478d-a444-3027d5ffabb0 \n",
      "\n",
      "If you would rather not do that right now and you are still not familiar with neural networks, you can think of them as a voting process and so in the neural network that you set up there is a complicated voting process and all of the inputs like day of the week and month of the year go into it and then you try train it on your history of what you have had for dinner. 1368a07c-94f3-478d-a444-3027d5ffabb0 \n",
      "\n",
      "Applications of machine learning have gotten a lot of traction in the last few years. 1368a07c-94f3-478d-a444-3027d5ffabb0 \n",
      "\n",
      "There is a couple of big categories that have had wins one is identifying pictures the equivalent of finding cats on the internet and any problem that can be made to look like that and the other is sequence to sequence deflation is can be speech to text or one language to another most of the former are done with convolutional neural networks. 1368a07c-94f3-478d-a444-3027d5ffabb0 \n",
      "\n",
      "It becomes a very simple voting process and and it is right all the time because your flatmate is incredibly consistent. d32264d8-9064-44d7-b36c-1eff8b5144f5 \n",
      "\n",
      "It is useful to take a step back and just look at the data and when you do that, you notice a pattern your flatmate makes pizza then Sushi then waffles then pizza again in a Michael it does not depend on the day of the week or anything else. d32264d8-9064-44d7-b36c-1eff8b5144f5 \n",
      "\n",
      "You still cannot get much better than chance predictions on dinner as is often the case with complicated machine learning problems. d32264d8-9064-44d7-b36c-1eff8b5144f5 \n",
      "\n",
      "The trouble is that your network does not work very well despite carefully choosing your inputs and training it thoroughly. d32264d8-9064-44d7-b36c-1eff8b5144f5 \n",
      "\n",
      "And you learn how to predict what is going to be for dinner tonight? d32264d8-9064-44d7-b36c-1eff8b5144f5 \n",
      "\n",
      "So if we know if we had pizza for dinner yesterday, it will be sushi tonight. d32264d8-9064-44d7-b36c-1eff8b5144f5 \n",
      "\n",
      "So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday. d32264d8-9064-44d7-b36c-1eff8b5144f5 \n",
      "\n",
      "If you happen to be gone on a given night, let us say yesterday. 0055bc75-2657-4694-81d2-0e4564df65ce \n",
      "\n",
      "There is going to be a quarter inch of rain and the relative humidity is XnumberX percent. 0055bc75-2657-4694-81d2-0e4564df65ce \n",
      "\n",
      "So at this point it is helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if I want to describe the weather to you for a given day, I could say Say the high XnumberX degrees Fahrenheit the lows XnumberX wins XnumberX miles an hour. 0055bc75-2657-4694-81d2-0e4564df65ce \n",
      "\n",
      "And then you can use that prediction in turn to make a prediction for tonight. 0055bc75-2657-4694-81d2-0e4564df65ce \n",
      "\n",
      "You can still predict what is going to be for dinner tonight by thinking back two days ago think what was for dinner then so what would be predicted for you last night? 0055bc75-2657-4694-81d2-0e4564df65ce \n",
      "\n",
      "So we make use of not only our actual information from yesterday. 0055bc75-2657-4694-81d2-0e4564df65ce \n",
      "\n",
      "If you want to get something into a format that it is natural for a computer to compute to do operations on to do statistical machine learning list of numbers are the way to go everything gets reduced to a list of numbers before it goes through an algorithm. b9d07405-49d7-48e9-a8bf-284e87ecb9c2 \n",
      "\n",
      "This format is called one hot and coding and it is very common to see a long Vector of zeros with just one element being one. b9d07405-49d7-48e9-a8bf-284e87ecb9c2 \n",
      "\n",
      "We can also have Vector for statements like, it is Tuesday in order to encode this kind of information what we do is we make a List of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true. b9d07405-49d7-48e9-a8bf-284e87ecb9c2 \n",
      "\n",
      "The vector is the reason that it is useful is vectors list of numbers are computers native language. b9d07405-49d7-48e9-a8bf-284e87ecb9c2 \n",
      "\n",
      "It seems inefficient, but for a computer, this is a lot easier way to ingest that information. b9d07405-49d7-48e9-a8bf-284e87ecb9c2 \n",
      "\n",
      "We set everything equal to XnumberX except for the dinner item that we predict. cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece \n",
      "\n",
      "We can make a one hot Vector for our prediction for dinner tonight. cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece \n",
      "\n",
      "And the neural network is just connections between every element in each of those input vectors to every element in the output connector. 1ce9a6a5-048d-4df5-9308-e477ec8e25a1 \n",
      "\n",
      "We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector. 1ce9a6a5-048d-4df5-9308-e477ec8e25a1 \n",
      "\n",
      "So this is a nice simple example that showed recurrent neural networks now to show how they do not meet all of our needs. c53586d5-0e6a-4799-9c8b-9aba5cbb11bf \n",
      "\n",
      "And when it is unwrapped it looks like this and we can go back as far as we need to and see what was for dinner and then just trace it forward and play out our menu over the last two weeks until we find out what is for dinner tonight. c53586d5-0e6a-4799-9c8b-9aba5cbb11bf \n",
      "\n",
      "Let is say we were out of town for two weeks. c53586d5-0e6a-4799-9c8b-9aba5cbb11bf \n",
      "\n",
      "Itll have sentences of the format Doug saw Jane period Jane sauce. c53586d5-0e6a-4799-9c8b-9aba5cbb11bf \n",
      "\n",
      "Complete our picture we can show how the prediction for today will get recycled the dotted line there means hold onto it for a day and then reuse it tomorrow and it becomes our yesterday is predictions tomorrow. c53586d5-0e6a-4799-9c8b-9aba5cbb11bf \n",
      "\n",
      "We just ignore the new information part and we can unwrap or unwind this Vector in time until we do have some information to base it on and then just play it forward. c53586d5-0e6a-4799-9c8b-9aba5cbb11bf \n",
      "\n",
      "Now we can see how if we were lacking some information. c53586d5-0e6a-4799-9c8b-9aba5cbb11bf \n",
      "\n",
      "We can still make a good guess about what is going to be for dinner tonight. c53586d5-0e6a-4799-9c8b-9aba5cbb11bf \n",
      "\n",
      "It is really useful in some surprisingly practical applications if I have text in one language, and I want to translate it to text to another language L stm is work very well, even though translation is not a word to word process. a1772ba2-b967-4483-b518-6fe281080e37 \n",
      "\n",
      "It is a phrase to phrase or even in some cases a sentence to sentence process LS TMS are able to represent those grammar structures that are specific to each language and a1772ba2-b967-4483-b518-6fe281080e37 \n",
      "\n",
      "--------------\n",
      "So what we will do is take a very simple example and step through it just to illustrate how a couple of these pieces work. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "And for the purposes of demonstration, we will assume that this LS TM has been trained on our children is books examples that we want to mimic and all of the appropriate votes and weights in those neural networks have been learned now will show it in action. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "Now long short term memory has a lot of pieces a lot of bits that were together and it is a little much to wrap your head around it all at once. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "So so far our story so far is Jane saucepot period Doug so Doug is the most recent word that is occurred in our story and also not surprisingly for this time step. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "So we have our New information which is the word Doug we have our recent prediction. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "We had just wrapped up a sentence with a period the new sentence can start with any name. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "When you get to that point then you know, you are ready to move on to the next level of material. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "So we are now in the process of writing our children is book. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "This is an intention mechanism it lets things that are not immediately relevant be set aside. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "It is admittedly an overly simplistic example, and feel free to poke holes at it later. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "So if they do not Cloud the predictions in memory going forward It has its own neural network and its own Logistics squashing function and its own gating activity right here. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "The name is Doug Jane and spot where all predicted as viable options. 1ea711a9-4901-45ea-b868-dcce8ca4b37a \n",
      "\n",
      "--------------\n",
      "So now we have not just prediction for a predictions plus the memories that we have accumulated and that we have not chosen to forget. 1eaffeca-e82f-4e22-b471-37fbd27318c1 \n",
      "\n",
      "Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network. 1eaffeca-e82f-4e22-b471-37fbd27318c1 \n",
      "\n",
      "Now there is an entirely separate neural network here that learns when to forget what based on what we are seeing right now. 1eaffeca-e82f-4e22-b471-37fbd27318c1 \n",
      "\n",
      "Some of them are remembered the ones that are remembered are added back into the prediction. 1eaffeca-e82f-4e22-b471-37fbd27318c1 \n",
      "\n",
      "When we introduced all of these together what we get, we still have the combination of our previous predictions in our new information those vectors get past and we make predictions based on them. 1eaffeca-e82f-4e22-b471-37fbd27318c1 \n",
      "\n",
      "You can see this is powerful this let us hold onto things for as long as we want. e483de91-6b4d-4058-ac7d-f157349c1794 \n",
      "\n",
      "Oceans can be used to vote on what all the gates should be. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "So we just squash it to be careful to make sure it never gets out of control. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "It is possible that things could become greater than XnumberX or smaller than XnumberX. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "We may not necessarily want to release all of those memories out as new predictions each time. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "Weve also introduced another squashing function here since we do an addition here. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "Youve probably noticed though when we are combining our predictions with our memories. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "We select just a few to release as the prediction for that moment. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "Each of these things when to forget and when to let things out of our memory are learned by their own neural networks. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "So we want a little filter to keep our memories inside and let our predictions get out. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "And that is we add another gate for that to do selection. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "What should be kept internal and what should be released as a prediction. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "So its own voting process so that our new information and our previous predictions. 94002779-6ea2-4c0b-a75b-538b2a1cda37 \n",
      "\n",
      "--------------\n",
      "So for instance, if Doug was the most recent word that I saw my new information Vector would be all zeros except for a one and the dog position. a8e01ed7-f7df-4351-971d-2265654b4b34 \n",
      "\n",
      "And the task of the neural network is to put these together in the right order to make a good children is book. a8e01ed7-f7df-4351-971d-2265654b4b34 \n",
      "\n",
      "So to do this, we replace our food vectors with our dictionary vectors here again, it is just a list of numbers representing each of the words. a8e01ed7-f7df-4351-971d-2265654b4b34 \n",
      "\n",
      "Dictionary is small just the words Doug Jane spot saw and a period. a8e01ed7-f7df-4351-971d-2265654b4b34 \n",
      "\n",
      "And we similarly can represent our predictions and our predictions from yesterday. a8e01ed7-f7df-4351-971d-2265654b4b34 \n",
      "\n",
      "For instance anytime a name comes up Jane Doug or spot. 3f6d0dea-c156-48f4-95ad-fe274d7c1490 \n",
      "\n",
      "We would expect that to vote heavily for the word saw or for a period because those are the two words in our dictionary that can follow a name. 3f6d0dea-c156-48f4-95ad-fe274d7c1490 \n",
      "\n",
      "After training this neural network and teaching it what to do. 3f6d0dea-c156-48f4-95ad-fe274d7c1490 \n",
      "\n",
      "early, if we had predicted a name on the previous time step, we would expect those to vote also for the word saw or for a period f05c0675-3618-4081-bbf3-c46f76f3e736 \n",
      "\n",
      "And by a similar method anytime we come across the word saw or a period we know that a name has to come after that. 0b0dc278-ae3e-4774-8f76-aca2c6c4fbbf \n",
      "\n",
      "So it will learn to vote very strongly for a name Jane Doug or spot. 0b0dc278-ae3e-4774-8f76-aca2c6c4fbbf \n",
      "\n",
      "The critical part that we add to the middle here is memory. f7fab60f-c02e-4756-a4d5-eeb0b878c084 \n",
      "\n",
      "Because each of our predictions only looks back one time Step It has very short term memory. f7fab60f-c02e-4756-a4d5-eeb0b878c084 \n",
      "\n",
      "Period because Doug strongly votes for the word saw which in turn strongly votes for uh name any name which could be Doug similarly we could get something like Doug saw Jane saw spot saw Doug. f7fab60f-c02e-4756-a4d5-eeb0b878c084 \n",
      "\n",
      "In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it. f7fab60f-c02e-4756-a4d5-eeb0b878c084 \n",
      "\n",
      "We want to be able to remember what happened many times steps ago. f7fab60f-c02e-4756-a4d5-eeb0b878c084 \n",
      "\n",
      "Then it does not use the information from further back and it is subject to these types of mistakes. f7fab60f-c02e-4756-a4d5-eeb0b878c084 \n",
      "\n",
      "--------------\n",
      "How it works is you take all of your votes coming out and you subject them to this squashing function. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "I will take the vectors in the weights and collapse them down to that little symbol with the dots in the arrows the dots in the lines connecting them and there is one more symbol we have not talked about yet. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "No matter what you put in what comes out is between minus XnumberX and XnumberX. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "But as your number gets larger, the number that comes out is closer and closer. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "So you may have noticed our neural network in its current state is subject to some mistakes. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "We could get a sentence for instance of the form Doug saw Doug. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "This is a squashing function and it just helps the network to behave. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "You can go through that Loop and it will not explode in a feedback loop. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "By ensuring that it is always less than XnumberX but more than minus XnumberX you can multiply it as many times as you want. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "When you have a loop like this where the same values get processed again and again day after day it is possible. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "In this form and this formulation, we have a recurrent neural network for Simplicity. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "RXnumberX and similarly if you put in a big negative number, then what you will get out will be very close to minus XnumberX. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "We got x XnumberX in that case it would get twice as big every time and very soon blow up to be astronomical. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "You can imagine if in the course of that processing say something got voted for twice. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "If something received a total vote of .XnumberX, you draw a vertical line up where it crosses the function you draw a horizontal line over to the y axis and there is your squash diversion out for small numbers. bfaa9950-9d51-4b55-80b7-fe9d3f573352 \n",
      "\n",
      "--------------\n",
      "Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection. f4510718-0d67-47aa-941c-26df17007ed5 \n",
      "\n",
      "The word saw is our most recent word and our most recent prediction. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "The other thing that happened is our previous set of possibilities. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "So the fact that there is a vote for not Doug gets blocked here and the word saw get sent out as the prediction for the next time step. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "Addiction for Doug it says I do not expect to see dog in the near future. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "So it makes a positive prediction for Saul and a negative. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "I will pass over ignoring and attention and this example and we will take those predictions forward. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "This is learned that the word saw is a great guess to make for next word, but it is also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "We now predict that the words Doug Jane or spot might come next. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "They get passed forward to all of these neural networks, and we get a new set of predictions because the word saw just occurred. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "So we will skip over it for now and this prediction of Saw not Doug is passed forward and again for the purposes of Simplicity. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "The first one of these make some predictions given that the word Doug just occurred. dafc63db-af9c-4928-ae8b-7babeb97b33c \n",
      "\n",
      "What this shows is that long shortterm memory can look back two three many time steps and use that information to make good predictions about what is going to happen next now to be fair to vanilla recurrent neural networks. 459c46c7-b5e7-4ae4-becd-26da1f772d9a \n",
      "\n",
      "So it forgets saw holds onto the vote for not Doug. 459c46c7-b5e7-4ae4-becd-26da1f772d9a \n",
      "\n",
      "Then we get predictions of only Jane and spot not Doug this avoids the Dog Saw Doug period type of error and the other errors that we saw. 459c46c7-b5e7-4ae4-becd-26da1f772d9a \n",
      "\n",
      "It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step. 459c46c7-b5e7-4ae4-becd-26da1f772d9a \n",
      "\n",
      "The word saw and not Doug that we were maintaining internally get passed to a forgetting gate. 459c46c7-b5e7-4ae4-becd-26da1f772d9a \n",
      "\n",
      "They can actually look back several time steps as well, but not very many. 459c46c7-b5e7-4ae4-becd-26da1f772d9a \n",
      "\n",
      "Now the forgetting gate says hey my last word that came that occurred was the word saw based on my past experience. 459c46c7-b5e7-4ae4-becd-26da1f772d9a \n",
      "\n",
      "But I want to keep any predictions having to do with names. 459c46c7-b5e7-4ae4-becd-26da1f772d9a \n",
      "\n",
      "LST M can look back many times steps and has shown that successfully. 459c46c7-b5e7-4ae4-becd-26da1f772d9a \n",
      "\n",
      "We have a positive vote for dog an Get a vote for Doug and so they cancel each other out. 459c46c7-b5e7-4ae4-becd-26da1f772d9a \n",
      "\n",
      "--------------\n",
      "If you are curious what LS ems look like in math. 59f1e10c-7d02-40b7-a916-2a5563bc4be3 \n",
      "\n",
      "It looks like is that they find the higherlevel idea and translate it from one mode of expression to another just using the bits and pieces that we just walked through. 59f1e10c-7d02-40b7-a916-2a5563bc4be3 \n",
      "\n",
      "I would also strongly encourage you to visit Andre carpet these blog posts showing examples of what L stm is can do in text. 59f1e10c-7d02-40b7-a916-2a5563bc4be3 \n",
      "\n",
      "L spms are a great fit for any information that is embedded in time audio video on my favorite application of all forces robotics robotics is nothing more than an agent taking in information from a set of sensors and then based on that information making a decision and carrying out an action. 59f1e10c-7d02-40b7-a916-2a5563bc4be3 \n",
      "\n",
      "It is inherently sequential and actions taken now can influence what is sent and what should be done many times steps down the line. 59f1e10c-7d02-40b7-a916-2a5563bc4be3 \n",
      "\n",
      "I will not step through it, but it is encouraging that something that looks so complex expressed mathematically actually makes it fairly straightforward. 59f1e10c-7d02-40b7-a916-2a5563bc4be3 \n",
      "\n",
      "This is it this is lifted straight from the Wikipedia page. 59f1e10c-7d02-40b7-a916-2a5563bc4be3 \n",
      "\n",
      "Also, there are a collection of really good tutorials and discussions other ways of explaining lsdm so that you may find helpful as well. 59f1e10c-7d02-40b7-a916-2a5563bc4be3 \n",
      "\n",
      "Picture and story and if you would like to dig into it more I encourage you to go to the Wikipedia page. 59f1e10c-7d02-40b7-a916-2a5563bc4be3 \n",
      "\n",
      "It takes them and uses that been to predict what text what word is being spoken and it can use the history the recent history of words to make a better guess for what is going to come next. 59f1e10c-7d02-40b7-a916-2a5563bc4be3 \n",
      "\n",
      "--------------\n",
      "Weve introduced here one is another squashing function this one with a flat bottom. 75a5f128-252c-44c8-a63e-29d15defef8e \n",
      "\n",
      "One is an X in a circle and one is a cross in a circle. 75a5f128-252c-44c8-a63e-29d15defef8e \n",
      "\n",
      "The way it works is you start with two vectors of equal size and you go down each one. 748469f5-ce1e-45d5-b075-1a0d38faccc1 \n",
      "\n",
      "Youve probably guessed the X in the circle is element by element multiplication. 748469f5-ce1e-45d5-b075-1a0d38faccc1 \n",
      "\n",
      "So XnumberX plus XnumberX equals XnumberX, then you go to the next element XnumberX plus XnumberX equals XnumberX. 748469f5-ce1e-45d5-b075-1a0d38faccc1 \n",
      "\n",
      "It is just like Edition except instead of adding you multiply for instance. 748469f5-ce1e-45d5-b075-1a0d38faccc1 \n",
      "\n",
      "You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector. 748469f5-ce1e-45d5-b075-1a0d38faccc1 \n",
      "\n",
      "The output Vector is the same size of each of the input vectors. 748469f5-ce1e-45d5-b075-1a0d38faccc1 \n",
      "\n",
      "Three times six gives you a first element of XnumberX XnumberX times XnumberX gives you XnumberX again. 748469f5-ce1e-45d5-b075-1a0d38faccc1 \n",
      "\n",
      "And so your output Vector is the same size of each of your input vectors. 748469f5-ce1e-45d5-b075-1a0d38faccc1 \n",
      "\n",
      "Just a list of numbers same length, but it is the some element by element of the two. 748469f5-ce1e-45d5-b075-1a0d38faccc1 \n",
      "\n",
      "It is very similar to the other squashing function the hyperbolic tangent except that it just goes between XnumberX and XnumberX instead of minus XnumberX and XnumberX. e1a7877d-d189-4854-a707-efc6f5c017f4 \n",
      "\n",
      "So in this case an open gate an open faucet would be a one and a closed faucet would be a zero and the way this works with element wise multiplication. e1a7877d-d189-4854-a707-efc6f5c017f4 \n",
      "\n",
      "And this case we will just assign the number two that XnumberX. e1a7877d-d189-4854-a707-efc6f5c017f4 \n",
      "\n",
      "So gating lets us control what passes through and what gets blocked which is really useful. e1a7877d-d189-4854-a707-efc6f5c017f4 \n",
      "\n",
      "Now in order to do gating it is nice to have a value that you know is always between XnumberX and XnumberX so we introduce another squashing function. e1a7877d-d189-4854-a707-efc6f5c017f4 \n",
      "\n",
      "This will represent with a circle with a flat bottom and this is It is called The logistic function. e1a7877d-d189-4854-a707-efc6f5c017f4 \n",
      "\n",
      "We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked. e1a7877d-d189-4854-a707-efc6f5c017f4 \n",
      "\n",
      "You imagine that you have a signal and it is like a bunch of pipes and they have a certain amount of water trying to flow down them. e1a7877d-d189-4854-a707-efc6f5c017f4 \n",
      "\n",
      "And then with the gating value of XnumberX the single was passed through but it is smaller. e1a7877d-d189-4854-a707-efc6f5c017f4 \n",
      "\n",
      "Now on each of those pipes, we have a faucet and we can open it all the way close it all the way or keep it somewhere in the middle to either let that signal come through or block it. e1a7877d-d189-4854-a707-efc6f5c017f4 \n",
      "\n",
      "<---------------->\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is say for a minute that you are a very lucky apartment dweller and you have a flatmate who loves to cook dinner every night.    =====    You set up a neural network the inputs to this neural network are a bunch of items like the day of the week the month of the year whether or not your flatmate was in a late meeting variables that might reasonably affect what you are going to have for dinner.\n",
      "order difference: 0\n",
      "Relevant sentence:  You set up a neural network the inputs to this neural network are a bunch of items like the day of the week the month of the year whether or not your flatmate was in a late meeting variables that might reasonably affect what you are going to have for dinner.    =====    Now if you are new to neural networks, I highly recommend you take a minute and stopped to watch the how neural networks work tutorial.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now if you are new to neural networks, I highly recommend you take a minute and stopped to watch the how neural networks work tutorial.    =====    He cooks one of three things Sushi waffles or pizza and you would like to be able to predict what you are going to have on a Of a night so you can plan the rest of your days eating accordingly in order to predict what you are going to have for dinner.\n",
      "order difference: 0\n",
      "Relevant sentence:  He cooks one of three things Sushi waffles or pizza and you would like to be able to predict what you are going to have on a Of a night so you can plan the rest of your days eating accordingly in order to predict what you are going to have for dinner.    =====    Most of the latter are done with recurrent neural networks particularly long short term memory to give an example of how long shortterm memory works.\n",
      "order difference: 0\n",
      "Relevant sentence:  Most of the latter are done with recurrent neural networks particularly long short term memory to give an example of how long shortterm memory works.    =====    If you would rather not do that right now and you are still not familiar with neural networks, you can think of them as a voting process and so in the neural network that you set up there is a complicated voting process and all of the inputs like day of the week and month of the year go into it and then you try train it on your history of what you have had for dinner.\n",
      "order difference: 0\n",
      "Relevant sentence:  If you would rather not do that right now and you are still not familiar with neural networks, you can think of them as a voting process and so in the neural network that you set up there is a complicated voting process and all of the inputs like day of the week and month of the year go into it and then you try train it on your history of what you have had for dinner.    =====    Applications of machine learning have gotten a lot of traction in the last few years.\n",
      "order difference: 0\n",
      "Relevant sentence:  Applications of machine learning have gotten a lot of traction in the last few years.    =====    There is a couple of big categories that have had wins one is identifying pictures the equivalent of finding cats on the internet and any problem that can be made to look like that and the other is sequence to sequence deflation is can be speech to text or one language to another most of the former are done with convolutional neural networks.\n",
      "order difference: 1\n",
      "Relevant sentence:  There is a couple of big categories that have had wins one is identifying pictures the equivalent of finding cats on the internet and any problem that can be made to look like that and the other is sequence to sequence deflation is can be speech to text or one language to another most of the former are done with convolutional neural networks.    =====    It becomes a very simple voting process and and it is right all the time because your flatmate is incredibly consistent.\n",
      "order difference: 0\n",
      "Relevant sentence:  It becomes a very simple voting process and and it is right all the time because your flatmate is incredibly consistent.    =====    It is useful to take a step back and just look at the data and when you do that, you notice a pattern your flatmate makes pizza then Sushi then waffles then pizza again in a Michael it does not depend on the day of the week or anything else.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is useful to take a step back and just look at the data and when you do that, you notice a pattern your flatmate makes pizza then Sushi then waffles then pizza again in a Michael it does not depend on the day of the week or anything else.    =====    You still cannot get much better than chance predictions on dinner as is often the case with complicated machine learning problems.\n",
      "order difference: 0\n",
      "Relevant sentence:  You still cannot get much better than chance predictions on dinner as is often the case with complicated machine learning problems.    =====    The trouble is that your network does not work very well despite carefully choosing your inputs and training it thoroughly.\n",
      "order difference: 0\n",
      "Relevant sentence:  The trouble is that your network does not work very well despite carefully choosing your inputs and training it thoroughly.    =====    And you learn how to predict what is going to be for dinner tonight?\n",
      "order difference: 0\n",
      "Relevant sentence:  And you learn how to predict what is going to be for dinner tonight?    =====    So if we know if we had pizza for dinner yesterday, it will be sushi tonight.\n",
      "order difference: 0\n",
      "Relevant sentence:  So if we know if we had pizza for dinner yesterday, it will be sushi tonight.    =====    So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday.\n",
      "order difference: 1\n",
      "Relevant sentence:  So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday.    =====    If you happen to be gone on a given night, let us say yesterday.\n",
      "order difference: 0\n",
      "Relevant sentence:  If you happen to be gone on a given night, let us say yesterday.    =====    There is going to be a quarter inch of rain and the relative humidity is XnumberX percent.\n",
      "order difference: 0\n",
      "Relevant sentence:  There is going to be a quarter inch of rain and the relative humidity is XnumberX percent.    =====    So at this point it is helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if I want to describe the weather to you for a given day, I could say Say the high XnumberX degrees Fahrenheit the lows XnumberX wins XnumberX miles an hour.\n",
      "order difference: 0\n",
      "Relevant sentence:  So at this point it is helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if I want to describe the weather to you for a given day, I could say Say the high XnumberX degrees Fahrenheit the lows XnumberX wins XnumberX miles an hour.    =====    And then you can use that prediction in turn to make a prediction for tonight.\n",
      "order difference: 0\n",
      "Relevant sentence:  And then you can use that prediction in turn to make a prediction for tonight.    =====    You can still predict what is going to be for dinner tonight by thinking back two days ago think what was for dinner then so what would be predicted for you last night?\n",
      "order difference: 0\n",
      "Relevant sentence:  You can still predict what is going to be for dinner tonight by thinking back two days ago think what was for dinner then so what would be predicted for you last night?    =====    So we make use of not only our actual information from yesterday.\n",
      "order difference: 1\n",
      "Relevant sentence:  So we make use of not only our actual information from yesterday.    =====    If you want to get something into a format that it is natural for a computer to compute to do operations on to do statistical machine learning list of numbers are the way to go everything gets reduced to a list of numbers before it goes through an algorithm.\n",
      "order difference: 0\n",
      "Relevant sentence:  If you want to get something into a format that it is natural for a computer to compute to do operations on to do statistical machine learning list of numbers are the way to go everything gets reduced to a list of numbers before it goes through an algorithm.    =====    This format is called one hot and coding and it is very common to see a long Vector of zeros with just one element being one.\n",
      "order difference: 0\n",
      "Relevant sentence:  This format is called one hot and coding and it is very common to see a long Vector of zeros with just one element being one.    =====    We can also have Vector for statements like, it is Tuesday in order to encode this kind of information what we do is we make a List of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true.\n",
      "order difference: 0\n",
      "Relevant sentence:  We can also have Vector for statements like, it is Tuesday in order to encode this kind of information what we do is we make a List of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true.    =====    The vector is the reason that it is useful is vectors list of numbers are computers native language.\n",
      "order difference: 0\n",
      "Relevant sentence:  The vector is the reason that it is useful is vectors list of numbers are computers native language.    =====    It seems inefficient, but for a computer, this is a lot easier way to ingest that information.\n",
      "order difference: 1\n",
      "Relevant sentence:  It seems inefficient, but for a computer, this is a lot easier way to ingest that information.    =====    We set everything equal to XnumberX except for the dinner item that we predict.\n",
      "order difference: 0\n",
      "Relevant sentence:  We set everything equal to XnumberX except for the dinner item that we predict.    =====    We can make a one hot Vector for our prediction for dinner tonight.\n",
      "order difference: 1\n",
      "Relevant sentence:  We can make a one hot Vector for our prediction for dinner tonight.    =====    And the neural network is just connections between every element in each of those input vectors to every element in the output connector.\n",
      "order difference: 0\n",
      "Relevant sentence:  And the neural network is just connections between every element in each of those input vectors to every element in the output connector.    =====    We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.\n",
      "order difference: 1\n",
      "Relevant sentence:  We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector.    =====    So this is a nice simple example that showed recurrent neural networks now to show how they do not meet all of our needs.\n",
      "order difference: 0\n",
      "Relevant sentence:  So this is a nice simple example that showed recurrent neural networks now to show how they do not meet all of our needs.    =====    And when it is unwrapped it looks like this and we can go back as far as we need to and see what was for dinner and then just trace it forward and play out our menu over the last two weeks until we find out what is for dinner tonight.\n",
      "order difference: 0\n",
      "Relevant sentence:  And when it is unwrapped it looks like this and we can go back as far as we need to and see what was for dinner and then just trace it forward and play out our menu over the last two weeks until we find out what is for dinner tonight.    =====    Let is say we were out of town for two weeks.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is say we were out of town for two weeks.    =====    Itll have sentences of the format Doug saw Jane period Jane sauce.\n",
      "order difference: 0\n",
      "Relevant sentence:  Itll have sentences of the format Doug saw Jane period Jane sauce.    =====    Complete our picture we can show how the prediction for today will get recycled the dotted line there means hold onto it for a day and then reuse it tomorrow and it becomes our yesterday is predictions tomorrow.\n",
      "order difference: 0\n",
      "Relevant sentence:  Complete our picture we can show how the prediction for today will get recycled the dotted line there means hold onto it for a day and then reuse it tomorrow and it becomes our yesterday is predictions tomorrow.    =====    We just ignore the new information part and we can unwrap or unwind this Vector in time until we do have some information to base it on and then just play it forward.\n",
      "order difference: 0\n",
      "Relevant sentence:  We just ignore the new information part and we can unwrap or unwind this Vector in time until we do have some information to base it on and then just play it forward.    =====    Now we can see how if we were lacking some information.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now we can see how if we were lacking some information.    =====    We can still make a good guess about what is going to be for dinner tonight.\n",
      "Not Relevant sentence:  We can still make a good guess about what is going to be for dinner tonight.    !=    It is really useful in some surprisingly practical applications if I have text in one language, and I want to translate it to text to another language L stm is work very well, even though translation is not a word to word process.\n",
      "order difference: 17\n",
      "order difference: 0\n",
      "Relevant sentence:  It is really useful in some surprisingly practical applications if I have text in one language, and I want to translate it to text to another language L stm is work very well, even though translation is not a word to word process.    =====    It is a phrase to phrase or even in some cases a sentence to sentence process LS TMS are able to represent those grammar structures that are specific to each language and\n",
      "order difference: 0\n",
      "Relevant sentence:  So what we will do is take a very simple example and step through it just to illustrate how a couple of these pieces work.    =====    And for the purposes of demonstration, we will assume that this LS TM has been trained on our children is books examples that we want to mimic and all of the appropriate votes and weights in those neural networks have been learned now will show it in action.\n",
      "order difference: 0\n",
      "Relevant sentence:  And for the purposes of demonstration, we will assume that this LS TM has been trained on our children is books examples that we want to mimic and all of the appropriate votes and weights in those neural networks have been learned now will show it in action.    =====    Now long short term memory has a lot of pieces a lot of bits that were together and it is a little much to wrap your head around it all at once.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now long short term memory has a lot of pieces a lot of bits that were together and it is a little much to wrap your head around it all at once.    =====    So so far our story so far is Jane saucepot period Doug so Doug is the most recent word that is occurred in our story and also not surprisingly for this time step.\n",
      "order difference: 0\n",
      "Relevant sentence:  So so far our story so far is Jane saucepot period Doug so Doug is the most recent word that is occurred in our story and also not surprisingly for this time step.    =====    Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.\n",
      "order difference: 0\n",
      "Relevant sentence:  Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in.    =====    So we have our New information which is the word Doug we have our recent prediction.\n",
      "order difference: 0\n",
      "Relevant sentence:  So we have our New information which is the word Doug we have our recent prediction.    =====    We had just wrapped up a sentence with a period the new sentence can start with any name.\n",
      "order difference: 0\n",
      "Relevant sentence:  We had just wrapped up a sentence with a period the new sentence can start with any name.    =====    When you get to that point then you know, you are ready to move on to the next level of material.\n",
      "order difference: 0\n",
      "Relevant sentence:  When you get to that point then you know, you are ready to move on to the next level of material.    =====    So we are now in the process of writing our children is book.\n",
      "order difference: 0\n",
      "Relevant sentence:  So we are now in the process of writing our children is book.    =====    This is an intention mechanism it lets things that are not immediately relevant be set aside.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is an intention mechanism it lets things that are not immediately relevant be set aside.    =====    It is admittedly an overly simplistic example, and feel free to poke holes at it later.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is admittedly an overly simplistic example, and feel free to poke holes at it later.    =====    So if they do not Cloud the predictions in memory going forward It has its own neural network and its own Logistics squashing function and its own gating activity right here.\n",
      "order difference: 0\n",
      "Relevant sentence:  So if they do not Cloud the predictions in memory going forward It has its own neural network and its own Logistics squashing function and its own gating activity right here.    =====    The name is Doug Jane and spot where all predicted as viable options.\n",
      "order difference: 0\n",
      "Relevant sentence:  So now we have not just prediction for a predictions plus the memories that we have accumulated and that we have not chosen to forget.    =====    Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network.\n",
      "order difference: 0\n",
      "Relevant sentence:  Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network.    =====    Now there is an entirely separate neural network here that learns when to forget what based on what we are seeing right now.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now there is an entirely separate neural network here that learns when to forget what based on what we are seeing right now.    =====    Some of them are remembered the ones that are remembered are added back into the prediction.\n",
      "order difference: 0\n",
      "Relevant sentence:  Some of them are remembered the ones that are remembered are added back into the prediction.    =====    When we introduced all of these together what we get, we still have the combination of our previous predictions in our new information those vectors get past and we make predictions based on them.\n",
      "order difference: 1\n",
      "Relevant sentence:  When we introduced all of these together what we get, we still have the combination of our previous predictions in our new information those vectors get past and we make predictions based on them.    =====    You can see this is powerful this let us hold onto things for as long as we want.\n",
      "order difference: 1\n",
      "Relevant sentence:  You can see this is powerful this let us hold onto things for as long as we want.    =====    Oceans can be used to vote on what all the gates should be.\n",
      "order difference: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant sentence:  Oceans can be used to vote on what all the gates should be.    =====    And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.\n",
      "order difference: 0\n",
      "Relevant sentence:  And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step.    =====    So we just squash it to be careful to make sure it never gets out of control.\n",
      "order difference: 0\n",
      "Relevant sentence:  So we just squash it to be careful to make sure it never gets out of control.    =====    It is possible that things could become greater than XnumberX or smaller than XnumberX.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is possible that things could become greater than XnumberX or smaller than XnumberX.    =====    We may not necessarily want to release all of those memories out as new predictions each time.\n",
      "order difference: 0\n",
      "Relevant sentence:  We may not necessarily want to release all of those memories out as new predictions each time.    =====    Weve also introduced another squashing function here since we do an addition here.\n",
      "order difference: 0\n",
      "Relevant sentence:  Weve also introduced another squashing function here since we do an addition here.    =====    Youve probably noticed though when we are combining our predictions with our memories.\n",
      "order difference: 0\n",
      "Relevant sentence:  Youve probably noticed though when we are combining our predictions with our memories.    =====    We select just a few to release as the prediction for that moment.\n",
      "order difference: 0\n",
      "Relevant sentence:  We select just a few to release as the prediction for that moment.    =====    Each of these things when to forget and when to let things out of our memory are learned by their own neural networks.\n",
      "order difference: 0\n",
      "Relevant sentence:  Each of these things when to forget and when to let things out of our memory are learned by their own neural networks.    =====    So we want a little filter to keep our memories inside and let our predictions get out.\n",
      "order difference: 0\n",
      "Relevant sentence:  So we want a little filter to keep our memories inside and let our predictions get out.    =====    And that is we add another gate for that to do selection.\n",
      "order difference: 0\n",
      "Relevant sentence:  And that is we add another gate for that to do selection.    =====    What should be kept internal and what should be released as a prediction.\n",
      "order difference: 0\n",
      "Relevant sentence:  What should be kept internal and what should be released as a prediction.    =====    So its own voting process so that our new information and our previous predictions.\n",
      "order difference: 0\n",
      "Relevant sentence:  So for instance, if Doug was the most recent word that I saw my new information Vector would be all zeros except for a one and the dog position.    =====    And the task of the neural network is to put these together in the right order to make a good children is book.\n",
      "order difference: 0\n",
      "Relevant sentence:  And the task of the neural network is to put these together in the right order to make a good children is book.    =====    So to do this, we replace our food vectors with our dictionary vectors here again, it is just a list of numbers representing each of the words.\n",
      "order difference: 0\n",
      "Relevant sentence:  So to do this, we replace our food vectors with our dictionary vectors here again, it is just a list of numbers representing each of the words.    =====    Dictionary is small just the words Doug Jane spot saw and a period.\n",
      "order difference: 0\n",
      "Relevant sentence:  Dictionary is small just the words Doug Jane spot saw and a period.    =====    And we similarly can represent our predictions and our predictions from yesterday.\n",
      "order difference: 1\n",
      "Relevant sentence:  And we similarly can represent our predictions and our predictions from yesterday.    =====    For instance anytime a name comes up Jane Doug or spot.\n",
      "order difference: 0\n",
      "Relevant sentence:  For instance anytime a name comes up Jane Doug or spot.    =====    We would expect that to vote heavily for the word saw or for a period because those are the two words in our dictionary that can follow a name.\n",
      "order difference: 0\n",
      "Relevant sentence:  We would expect that to vote heavily for the word saw or for a period because those are the two words in our dictionary that can follow a name.    =====    After training this neural network and teaching it what to do.\n",
      "order difference: 1\n",
      "Relevant sentence:  After training this neural network and teaching it what to do.    =====    early, if we had predicted a name on the previous time step, we would expect those to vote also for the word saw or for a period\n",
      "order difference: 1\n",
      "Relevant sentence:  early, if we had predicted a name on the previous time step, we would expect those to vote also for the word saw or for a period    =====    And by a similar method anytime we come across the word saw or a period we know that a name has to come after that.\n",
      "order difference: 0\n",
      "Relevant sentence:  And by a similar method anytime we come across the word saw or a period we know that a name has to come after that.    =====    So it will learn to vote very strongly for a name Jane Doug or spot.\n",
      "Not Relevant sentence:  So it will learn to vote very strongly for a name Jane Doug or spot.    !=    The critical part that we add to the middle here is memory.\n",
      "order difference: 2\n",
      "order difference: 0\n",
      "Relevant sentence:  The critical part that we add to the middle here is memory.    =====    Because each of our predictions only looks back one time Step It has very short term memory.\n",
      "order difference: 0\n",
      "Relevant sentence:  Because each of our predictions only looks back one time Step It has very short term memory.    =====    Period because Doug strongly votes for the word saw which in turn strongly votes for uh name any name which could be Doug similarly we could get something like Doug saw Jane saw spot saw Doug.\n",
      "order difference: 0\n",
      "Relevant sentence:  Period because Doug strongly votes for the word saw which in turn strongly votes for uh name any name which could be Doug similarly we could get something like Doug saw Jane saw spot saw Doug.    =====    In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.\n",
      "order difference: 0\n",
      "Relevant sentence:  In order to overcome this we take our recurrent neural network, and we expand it and we have add some more pieces to it.    =====    We want to be able to remember what happened many times steps ago.\n",
      "order difference: 0\n",
      "Relevant sentence:  We want to be able to remember what happened many times steps ago.    =====    Then it does not use the information from further back and it is subject to these types of mistakes.\n",
      "order difference: 0\n",
      "Relevant sentence:  How it works is you take all of your votes coming out and you subject them to this squashing function.    =====    I will take the vectors in the weights and collapse them down to that little symbol with the dots in the arrows the dots in the lines connecting them and there is one more symbol we have not talked about yet.\n",
      "order difference: 0\n",
      "Relevant sentence:  I will take the vectors in the weights and collapse them down to that little symbol with the dots in the arrows the dots in the lines connecting them and there is one more symbol we have not talked about yet.    =====    No matter what you put in what comes out is between minus XnumberX and XnumberX.\n",
      "order difference: 0\n",
      "Relevant sentence:  No matter what you put in what comes out is between minus XnumberX and XnumberX.    =====    But as your number gets larger, the number that comes out is closer and closer.\n",
      "order difference: 0\n",
      "Relevant sentence:  But as your number gets larger, the number that comes out is closer and closer.    =====    So you may have noticed our neural network in its current state is subject to some mistakes.\n",
      "order difference: 0\n",
      "Relevant sentence:  So you may have noticed our neural network in its current state is subject to some mistakes.    =====    We could get a sentence for instance of the form Doug saw Doug.\n",
      "order difference: 0\n",
      "Relevant sentence:  We could get a sentence for instance of the form Doug saw Doug.    =====    This is a squashing function and it just helps the network to behave.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is a squashing function and it just helps the network to behave.    =====    You can go through that Loop and it will not explode in a feedback loop.\n",
      "order difference: 0\n",
      "Relevant sentence:  You can go through that Loop and it will not explode in a feedback loop.    =====    By ensuring that it is always less than XnumberX but more than minus XnumberX you can multiply it as many times as you want.\n",
      "order difference: 0\n",
      "Relevant sentence:  By ensuring that it is always less than XnumberX but more than minus XnumberX you can multiply it as many times as you want.    =====    When you have a loop like this where the same values get processed again and again day after day it is possible.\n",
      "order difference: 0\n",
      "Relevant sentence:  When you have a loop like this where the same values get processed again and again day after day it is possible.    =====    In this form and this formulation, we have a recurrent neural network for Simplicity.\n",
      "order difference: 0\n",
      "Relevant sentence:  In this form and this formulation, we have a recurrent neural network for Simplicity.    =====    RXnumberX and similarly if you put in a big negative number, then what you will get out will be very close to minus XnumberX.\n",
      "order difference: 0\n",
      "Relevant sentence:  RXnumberX and similarly if you put in a big negative number, then what you will get out will be very close to minus XnumberX.    =====    We got x XnumberX in that case it would get twice as big every time and very soon blow up to be astronomical.\n",
      "order difference: 0\n",
      "Relevant sentence:  We got x XnumberX in that case it would get twice as big every time and very soon blow up to be astronomical.    =====    You can imagine if in the course of that processing say something got voted for twice.\n",
      "order difference: 0\n",
      "Relevant sentence:  You can imagine if in the course of that processing say something got voted for twice.    =====    If something received a total vote of .XnumberX, you draw a vertical line up where it crosses the function you draw a horizontal line over to the y axis and there is your squash diversion out for small numbers.\n",
      "order difference: 1\n",
      "Relevant sentence:  Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection.    =====    The word saw is our most recent word and our most recent prediction.\n",
      "order difference: 0\n",
      "Relevant sentence:  The word saw is our most recent word and our most recent prediction.    =====    The other thing that happened is our previous set of possibilities.\n",
      "order difference: 0\n",
      "Relevant sentence:  The other thing that happened is our previous set of possibilities.    =====    So the fact that there is a vote for not Doug gets blocked here and the word saw get sent out as the prediction for the next time step.\n",
      "order difference: 0\n",
      "Relevant sentence:  So the fact that there is a vote for not Doug gets blocked here and the word saw get sent out as the prediction for the next time step.    =====    Addiction for Doug it says I do not expect to see dog in the near future.\n",
      "order difference: 0\n",
      "Relevant sentence:  Addiction for Doug it says I do not expect to see dog in the near future.    =====    So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out.\n",
      "order difference: 0\n",
      "Relevant sentence:  So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out.    =====    So it makes a positive prediction for Saul and a negative.\n",
      "order difference: 0\n",
      "Relevant sentence:  So it makes a positive prediction for Saul and a negative.    =====    I will pass over ignoring and attention and this example and we will take those predictions forward.\n",
      "order difference: 0\n",
      "Relevant sentence:  I will pass over ignoring and attention and this example and we will take those predictions forward.    =====    This is learned that the word saw is a great guess to make for next word, but it is also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is learned that the word saw is a great guess to make for next word, but it is also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence.    =====    We now predict that the words Doug Jane or spot might come next.\n",
      "order difference: 0\n",
      "Relevant sentence:  We now predict that the words Doug Jane or spot might come next.    =====    They get passed forward to all of these neural networks, and we get a new set of predictions because the word saw just occurred.\n",
      "order difference: 0\n",
      "Relevant sentence:  They get passed forward to all of these neural networks, and we get a new set of predictions because the word saw just occurred.    =====    So we will skip over it for now and this prediction of Saw not Doug is passed forward and again for the purposes of Simplicity.\n",
      "order difference: 0\n",
      "Relevant sentence:  So we will skip over it for now and this prediction of Saw not Doug is passed forward and again for the purposes of Simplicity.    =====    The first one of these make some predictions given that the word Doug just occurred.\n",
      "order difference: 1\n",
      "Relevant sentence:  The first one of these make some predictions given that the word Doug just occurred.    =====    What this shows is that long shortterm memory can look back two three many time steps and use that information to make good predictions about what is going to happen next now to be fair to vanilla recurrent neural networks.\n",
      "order difference: 0\n",
      "Relevant sentence:  What this shows is that long shortterm memory can look back two three many time steps and use that information to make good predictions about what is going to happen next now to be fair to vanilla recurrent neural networks.    =====    So it forgets saw holds onto the vote for not Doug.\n",
      "order difference: 0\n",
      "Relevant sentence:  So it forgets saw holds onto the vote for not Doug.    =====    Then we get predictions of only Jane and spot not Doug this avoids the Dog Saw Doug period type of error and the other errors that we saw.\n",
      "order difference: 0\n",
      "Relevant sentence:  Then we get predictions of only Jane and spot not Doug this avoids the Dog Saw Doug period type of error and the other errors that we saw.    =====    It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step.\n",
      "order difference: 0\n",
      "Relevant sentence:  It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step.    =====    The word saw and not Doug that we were maintaining internally get passed to a forgetting gate.\n",
      "order difference: 0\n",
      "Relevant sentence:  The word saw and not Doug that we were maintaining internally get passed to a forgetting gate.    =====    They can actually look back several time steps as well, but not very many.\n",
      "order difference: 0\n",
      "Relevant sentence:  They can actually look back several time steps as well, but not very many.    =====    Now the forgetting gate says hey my last word that came that occurred was the word saw based on my past experience.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now the forgetting gate says hey my last word that came that occurred was the word saw based on my past experience.    =====    But I want to keep any predictions having to do with names.\n",
      "order difference: 0\n",
      "Relevant sentence:  But I want to keep any predictions having to do with names.    =====    LST M can look back many times steps and has shown that successfully.\n",
      "order difference: 0\n",
      "Relevant sentence:  LST M can look back many times steps and has shown that successfully.    =====    We have a positive vote for dog an Get a vote for Doug and so they cancel each other out.\n",
      "order difference: 0\n",
      "Relevant sentence:  If you are curious what LS ems look like in math.    =====    It looks like is that they find the higherlevel idea and translate it from one mode of expression to another just using the bits and pieces that we just walked through.\n",
      "order difference: 0\n",
      "Relevant sentence:  It looks like is that they find the higherlevel idea and translate it from one mode of expression to another just using the bits and pieces that we just walked through.    =====    I would also strongly encourage you to visit Andre carpet these blog posts showing examples of what L stm is can do in text.\n",
      "order difference: 0\n",
      "Relevant sentence:  I would also strongly encourage you to visit Andre carpet these blog posts showing examples of what L stm is can do in text.    =====    L spms are a great fit for any information that is embedded in time audio video on my favorite application of all forces robotics robotics is nothing more than an agent taking in information from a set of sensors and then based on that information making a decision and carrying out an action.\n",
      "order difference: 0\n",
      "Relevant sentence:  L spms are a great fit for any information that is embedded in time audio video on my favorite application of all forces robotics robotics is nothing more than an agent taking in information from a set of sensors and then based on that information making a decision and carrying out an action.    =====    It is inherently sequential and actions taken now can influence what is sent and what should be done many times steps down the line.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is inherently sequential and actions taken now can influence what is sent and what should be done many times steps down the line.    =====    I will not step through it, but it is encouraging that something that looks so complex expressed mathematically actually makes it fairly straightforward.\n",
      "order difference: 0\n",
      "Relevant sentence:  I will not step through it, but it is encouraging that something that looks so complex expressed mathematically actually makes it fairly straightforward.    =====    This is it this is lifted straight from the Wikipedia page.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is it this is lifted straight from the Wikipedia page.    =====    Also, there are a collection of really good tutorials and discussions other ways of explaining lsdm so that you may find helpful as well.\n",
      "order difference: 0\n",
      "Relevant sentence:  Also, there are a collection of really good tutorials and discussions other ways of explaining lsdm so that you may find helpful as well.    =====    Picture and story and if you would like to dig into it more I encourage you to go to the Wikipedia page.\n",
      "order difference: 0\n",
      "Relevant sentence:  Picture and story and if you would like to dig into it more I encourage you to go to the Wikipedia page.    =====    It takes them and uses that been to predict what text what word is being spoken and it can use the history the recent history of words to make a better guess for what is going to come next.\n",
      "order difference: 0\n",
      "Relevant sentence:  Weve introduced here one is another squashing function this one with a flat bottom.    =====    One is an X in a circle and one is a cross in a circle.\n",
      "order difference: 1\n",
      "Relevant sentence:  One is an X in a circle and one is a cross in a circle.    =====    The way it works is you start with two vectors of equal size and you go down each one.\n",
      "order difference: 0\n",
      "Relevant sentence:  The way it works is you start with two vectors of equal size and you go down each one.    =====    Youve probably guessed the X in the circle is element by element multiplication.\n",
      "order difference: 0\n",
      "Relevant sentence:  Youve probably guessed the X in the circle is element by element multiplication.    =====    So XnumberX plus XnumberX equals XnumberX, then you go to the next element XnumberX plus XnumberX equals XnumberX.\n",
      "order difference: 0\n",
      "Relevant sentence:  So XnumberX plus XnumberX equals XnumberX, then you go to the next element XnumberX plus XnumberX equals XnumberX.    =====    It is just like Edition except instead of adding you multiply for instance.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is just like Edition except instead of adding you multiply for instance.    =====    You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector.\n",
      "order difference: 0\n",
      "Relevant sentence:  You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector.    =====    The output Vector is the same size of each of the input vectors.\n",
      "order difference: 0\n",
      "Relevant sentence:  The output Vector is the same size of each of the input vectors.    =====    Three times six gives you a first element of XnumberX XnumberX times XnumberX gives you XnumberX again.\n",
      "order difference: 0\n",
      "Relevant sentence:  Three times six gives you a first element of XnumberX XnumberX times XnumberX gives you XnumberX again.    =====    And so your output Vector is the same size of each of your input vectors.\n",
      "order difference: 0\n",
      "Relevant sentence:  And so your output Vector is the same size of each of your input vectors.    =====    Just a list of numbers same length, but it is the some element by element of the two.\n",
      "order difference: 1\n",
      "Relevant sentence:  Just a list of numbers same length, but it is the some element by element of the two.    =====    It is very similar to the other squashing function the hyperbolic tangent except that it just goes between XnumberX and XnumberX instead of minus XnumberX and XnumberX.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is very similar to the other squashing function the hyperbolic tangent except that it just goes between XnumberX and XnumberX instead of minus XnumberX and XnumberX.    =====    So in this case an open gate an open faucet would be a one and a closed faucet would be a zero and the way this works with element wise multiplication.\n",
      "order difference: 0\n",
      "Relevant sentence:  So in this case an open gate an open faucet would be a one and a closed faucet would be a zero and the way this works with element wise multiplication.    =====    And this case we will just assign the number two that XnumberX.\n",
      "order difference: 0\n",
      "Relevant sentence:  And this case we will just assign the number two that XnumberX.    =====    So gating lets us control what passes through and what gets blocked which is really useful.\n",
      "order difference: 0\n",
      "Relevant sentence:  So gating lets us control what passes through and what gets blocked which is really useful.    =====    Now in order to do gating it is nice to have a value that you know is always between XnumberX and XnumberX so we introduce another squashing function.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now in order to do gating it is nice to have a value that you know is always between XnumberX and XnumberX so we introduce another squashing function.    =====    This will represent with a circle with a flat bottom and this is It is called The logistic function.\n",
      "order difference: 0\n",
      "Relevant sentence:  This will represent with a circle with a flat bottom and this is It is called The logistic function.    =====    We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.\n",
      "order difference: 0\n",
      "Relevant sentence:  We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times XnumberX equals XnumberX that signal the original signal was effectively blocked.    =====    You imagine that you have a signal and it is like a bunch of pipes and they have a certain amount of water trying to flow down them.\n",
      "order difference: 0\n",
      "Relevant sentence:  You imagine that you have a signal and it is like a bunch of pipes and they have a certain amount of water trying to flow down them.    =====    And then with the gating value of XnumberX the single was passed through but it is smaller.\n",
      "order difference: 0\n",
      "Relevant sentence:  And then with the gating value of XnumberX the single was passed through but it is smaller.    =====    Now on each of those pipes, we have a faucet and we can open it all the way close it all the way or keep it somewhere in the middle to either let that signal come through or block it.\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (9, 6), (7, 7), (8, 8)]\n",
      "[[[\"Applications of machine learning have gotten a lot of traction in the last few years. There's a couple of big categories that have had wins one is identifying pictures the equivalent of finding cats on the internet and any problem that can be made to look like that and the other is sequence to sequence deflation is can be speech to text or one language to another most of the former are done with convolutional neural networks. Most of the latter are done with recurrent neural networks particularly long short term memory to give an example of how long short-term memory works. We will consider the question of what's For Dinner. Let's say for a minute that you are a very lucky apartment dweller and you have a flatmate who loves to cook dinner every night. He cooks one of three things Sushi waffles or pizza and you would like to be able to predict what you're going to have on a Of a night so you can plan the rest of your days eating accordingly in order to predict what you're going to have for dinner. You set up a neural network the inputs to this neural network are a bunch of items like the day of the week the month of the year whether or not your flatmate was in a late meeting variables that might reasonably affect what you're going to have for dinner. Now if you're new to neural networks, I highly recommend you take a minute and stopped to watch the how neural networks work tutorial. There's a link down in the comment section. If you'd rather not do that right now and you're still not familiar with neural networks, you can think of them as a voting process and so in the neural network that you set up there's a complicated voting process and all of the inputs like day of the week and month of the year go into it and then you try train it on your history of what you've had for dinner. \"], '2020-01-31T07:50:13Z', '716067a60a1a4034abc49a12ecafb39b', '1368a07c-94f3-478d-a444-3027d5ffabb0'], [[\"And you learn how to predict what's going to be for dinner tonight? The trouble is that your network doesn't work very well despite carefully choosing your inputs and training it thoroughly. You still can't get much better than chance predictions on dinner as is often the case with complicated machine learning problems. It's useful to take a step back and just look at the data and when you do that, you notice a pattern your flatmate makes pizza then Sushi then waffles then pizza again in a Michael it doesn't depend on the day of the week or anything else. It's in a regular cycle. So knowing this we can make a new neural network in our new one the only inputs that matter are what we had for dinner yesterday. So if we know if we had pizza for dinner yesterday, it'll be sushi tonight. Sushi yesterday waffles tonight and Waffles yesterday pizza tonight. It becomes a very simple voting process and and it's right all the time because your flatmate is incredibly consistent. \"], '2020-01-31T07:52:13Z', '716067a60a1a4034abc49a12ecafb39b', 'd32264d8-9064-44d7-b36c-1eff8b5144f5'], [[\"If you happen to be gone on a given night, let's say yesterday. You were out. You don't know what was for dinner yesterday. You can still predict what's going to be for dinner tonight by thinking back two days ago think what was for dinner then so what would be predicted for you last night? And then you can use that prediction in turn to make a prediction for tonight. So we make use of not only our actual information from yesterday. But also what our prediction was yesterday. So at this point it's helpful to take a little detour and talk about vectors a vector is just a fancy word for a list of numbers if I want to describe the weather to you for a given day, I could say Say the high 76 degrees Fahrenheit the lows 43 wins 13 miles an hour. There's going to be a quarter inch of rain and the relative humidity is 83 percent. \"], '2020-01-31T07:53:23Z', '716067a60a1a4034abc49a12ecafb39b', '0055bc75-2657-4694-81d2-0e4564df65ce'], [[\"The vector is the reason that it's useful is vectors list of numbers are computers native language. If you want to get something into a format that it's natural for a computer to compute to do operations on to do statistical machine learning list of numbers are the way to go everything gets reduced to a list of numbers before it goes through an algorithm. We can also have Vector for statements like, it's Tuesday in order to encode this kind of information what we do is we make a List of all the possible values it could have in this case all the days of the week and we assign a number to each and then we go through and set them all equal to zero except for the one that is true. Right. Now. This format is called one hot and coding and it's very common to see a long Vector of zeros with just one element being one. It seems inefficient, but for a computer, this is a lot easier way to ingest that information. \"], '2020-01-31T07:54:21Z', '716067a60a1a4034abc49a12ecafb39b', 'b9d07405-49d7-48e9-a8bf-284e87ecb9c2'], [['We can make a one hot Vector for our prediction for dinner tonight. We set everything equal to 0 except for the dinner item that we predict. So in this case will be predicting sushi. '], '2020-01-31T07:55:21Z', '716067a60a1a4034abc49a12ecafb39b', 'cdfcfb23-5fe1-4ff2-a63e-1c31a94a2ece'], [['We can group together our we can group together our inputs and outputs into vectors separate lists of numbers and it becomes a useful shorthand for describing this neural network so we can have our dinner yesterday Vector our predictions for yesterday vector and our prediction for today vector. And the neural network is just connections between every element in each of those input vectors to every element in the output connector. '], '2020-01-31T07:55:35Z', '716067a60a1a4034abc49a12ecafb39b', '1ce9a6a5-048d-4df5-9308-e477ec8e25a1'], [[\"Complete our picture we can show how the prediction for today will get recycled the dotted line there means hold onto it for a day and then reuse it tomorrow and it becomes our yesterday's predictions tomorrow. Now we can see how if we were lacking some information. Let's say we were out of town for two weeks. We can still make a good guess about what's going to be for dinner tonight. We just ignore the new information part and we can unwrap or unwind this Vector in time until we do have some information to base it on and then just play it forward. And when it's unwrapped it looks like this and we can go back as far as we need to and see what was for dinner and then just trace it forward and play out our menu over the last two weeks until we find out what's for dinner tonight. So this is a nice simple example that showed recurrent neural networks now to show how they don't meet all of our needs. We're going to write a children's book. It'll have sentences of the format Doug saw Jane period Jane sauce. Hot period spot saw Doug period and so on. \"], '2020-01-31T07:56:07Z', '716067a60a1a4034abc49a12ecafb39b', 'c53586d5-0e6a-4799-9c8b-9aba5cbb11bf']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"It's really useful in some surprisingly practical applications if I have text in one language, and I want to translate it to text to another language L stm's work very well, even though translation is not a word to word process. It's a phrase to phrase or even in some cases a sentence to sentence process LS TMS are able to represent those grammar structures that are specific to each language and \"], '2020-01-31T08:13:38Z', '716067a60a1a4034abc49a12ecafb39b', 'a1772ba2-b967-4483-b518-6fe281080e37']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Pull the other piece we need to add to complete our picture here is yet another set of gates this lets us actually ignore possible predictions possibilities as they come in. This is an intention mechanism it lets things that aren't immediately relevant be set aside. So if they don't Cloud the predictions in memory going forward It has its own neural network and its own Logistics squashing function and its own gating activity right here. Now long short term memory has a lot of pieces a lot of bits that were together and it's a little much to wrap your head around it all at once. So what we'll do is take a very simple example and step through it just to illustrate how a couple of these pieces work. It's admittedly an overly simplistic example, and feel free to poke holes at it later. When you get to that point then you know, you're ready to move on to the next level of material. So we are now in the process of writing our children's book. And for the purposes of demonstration, we will assume that this LS TM has been trained on our children's books examples that we want to mimic and all of the appropriate votes and weights in those neural networks have been learned now will show it in action. So so far our story so far is Jane saucepot period Doug so Doug is the most recent word that's occurred in our story and also not surprisingly for this time step. The name's Doug Jane and spot where all predicted as viable options. This makes sense. We had just wrapped up a sentence with a period the new sentence can start with any name. So these are all great predictions. So we have our New information which is the word Doug we have our recent prediction. \"], '2020-01-31T08:07:33Z', '716067a60a1a4034abc49a12ecafb39b', '1ea711a9-4901-45ea-b868-dcce8ca4b37a']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"When we introduced all of these together what we get, we still have the combination of our previous predictions in our new information those vectors get past and we make predictions based on them. Those predictions get passed through but the other thing that happens is a copy of those predictions is held on to for the next time step the next pass through the network. And some of them here's a gate right here. Some of them are forgotten. Some of them are remembered the ones that are remembered are added back into the prediction. So now we have not just prediction for a predictions plus the memories that we've accumulated and that we haven't chosen to forget. Now there's an entirely separate neural network here that learns when to forget what based on what we're seeing right now. What do we want to remember? What do we want to forget? \"], '2020-01-31T08:04:58Z', '716067a60a1a4034abc49a12ecafb39b', '1eaffeca-e82f-4e22-b471-37fbd27318c1'], [['You can see this is powerful this let us hold onto things for as long as we want. '], '2020-01-31T08:06:01Z', '716067a60a1a4034abc49a12ecafb39b', 'e483de91-6b4d-4058-ac7d-f157349c1794'], [[\"You've probably noticed though when we are combining our predictions with our memories. We may not necessarily want to release all of those memories out as new predictions each time. So we want a little filter to keep our memories inside and let our predictions get out. And that's we add another gate for that to do selection. It has its own neural network. So its own voting process so that our new information and our previous predictions. Oceans can be used to vote on what all the gates should be. What should be kept internal and what should be released as a prediction. We've also introduced another squashing function here since we do an addition here. It's possible that things could become greater than 1 or smaller than -1. So we just squash it to be careful to make sure it never gets out of control. And now when we bring in new predictions, we make a lot of possibilities and then we collect those with memory over time and of all of those possible predictions at each time step. We select just a few to release as the prediction for that moment. Each of these things when to forget and when to let things out of our memory are learned by their own neural networks. \"], '2020-01-31T08:06:09Z', '716067a60a1a4034abc49a12ecafb39b', '94002779-6ea2-4c0b-a75b-538b2a1cda37']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Dictionary is small just the words Doug Jane spot saw and a period. And the task of the neural network is to put these together in the right order to make a good children's book. So to do this, we replace our food vectors with our dictionary vectors here again, it's just a list of numbers representing each of the words. So for instance, if Doug was the most recent word that I saw my new information Vector would be all zeros except for a one and the dog position. And we similarly can represent our predictions and our predictions from yesterday. \"], '2020-01-31T07:57:21Z', '716067a60a1a4034abc49a12ecafb39b', 'a8e01ed7-f7df-4351-971d-2265654b4b34'], [['After training this neural network and teaching it what to do. We would expect to see certain patterns. For instance anytime a name comes up Jane Doug or spot. We would expect that to vote heavily for the word saw or for a period because those are the two words in our dictionary that can follow a name. '], '2020-01-31T07:58:04Z', '716067a60a1a4034abc49a12ecafb39b', '3f6d0dea-c156-48f4-95ad-fe274d7c1490'], [['early, if we had predicted a name on the previous time step, we would expect those to vote also for the word saw or for a period '], '2020-01-31T07:58:29Z', '716067a60a1a4034abc49a12ecafb39b', 'f05c0675-3618-4081-bbf3-c46f76f3e736'], [['And by a similar method anytime we come across the word saw or a period we know that a name has to come after that. So it will learn to vote very strongly for a name Jane Doug or spot. '], '2020-01-31T07:58:40Z', '716067a60a1a4034abc49a12ecafb39b', '0b0dc278-ae3e-4774-8f76-aca2c6c4fbbf']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Period because Doug strongly votes for the word saw which in turn strongly votes for uh name any name which could be Doug similarly we could get something like Doug saw Jane saw spot saw Doug. Because each of our predictions only looks back one time Step It has very short term memory. Then it doesn't use the information from further back and it's subject to these types of mistakes. In order to overcome this we take our recurrent neural network, and we expand it and we've add some more pieces to it. The critical part that we add to the middle here is memory. We want to be able to remember what happened many times steps ago. \"], '2020-01-31T08:00:58Z', '716067a60a1a4034abc49a12ecafb39b', 'f7fab60f-c02e-4756-a4d5-eeb0b878c084']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"In this form and this formulation, we have a recurrent neural network for Simplicity. I'll take the vectors in the weights and collapse them down to that little symbol with the dots in the arrows the dots in the lines connecting them and there's one more symbol we haven't talked about yet. This is a squashing function and it just helps the network to behave. How it works is you take all of your votes coming out and you subject them to this squashing function. For instance. If something received a total vote of .5, you draw a vertical line up where it crosses the function you draw a horizontal line over to the y axis and there is your squash diversion out for small numbers. The squashed version is pretty close to the original version. But as your number gets larger, the number that comes out is closer and closer. R21 and similarly if you put in a big negative number, then what you'll get out will be very close to minus 1. No matter what you put in what comes out is between minus 1 and 1. So this is really helpful. When you have a loop like this where the same values get processed again and again day after day it is possible. You can imagine if in the course of that processing say something got voted for twice. We got x 2 in that case it would get twice as big every time and very soon blow up to be astronomical. By ensuring that it's always less than 1 but more than minus 1 you can multiply it as many times as you want. You can go through that Loop and it won't explode in a feedback loop. This is an example of negative feedback or attenuating feedback. So you may have noticed our neural network in its current state is subject to some mistakes. We could get a sentence for instance of the form Doug saw Doug. \"], '2020-01-31T07:58:58Z', '716067a60a1a4034abc49a12ecafb39b', 'bfaa9950-9d51-4b55-80b7-fe9d3f573352']] \n",
      "\n",
      "\n",
      "[[[\"In order to explain how this works. I'll have to describe a few new symbols. We've introduced here one is another squashing function this one with a flat bottom. One is an X in a circle and one is a cross in a circle. \"], '2020-01-31T08:01:49Z', '716067a60a1a4034abc49a12ecafb39b', '75a5f128-252c-44c8-a63e-29d15defef8e'], [[\"Cross in a circle is element by element addition. The way it works is you start with two vectors of equal size and you go down each one. You add the first element of one vector to the first element of another vector and then the total goes into the first element of the output Vector. So 3 plus 6 equals 9, then you go to the next element 4 plus 7 equals 11. And so your output Vector is the same size of each of your input vectors. Just a list of numbers same length, but it's the some element by element of the two. And very closely related to this. You've probably guessed the X in the circle is element by element multiplication. It's just like Edition except instead of adding you multiply for instance. Three times six gives you a first element of 18 4 times 7 gives you 28 again. The output Vector is the same size of each of the input vectors. \"], '2020-01-31T08:02:06Z', '716067a60a1a4034abc49a12ecafb39b', '748469f5-ce1e-45d5-b075-1a0d38faccc1'], [[\"Element wise multiplication lets you do something pretty cool. You imagine that you have a signal and it's like a bunch of pipes and they have a certain amount of water trying to flow down them. And this case we'll just assign the number two that 0.8. It's like a signal. Now on each of those pipes, we have a faucet and we can open it all the way close it all the way or keep it somewhere in the middle to either let that signal come through or block it. So in this case an open gate an open faucet would be a one and a closed faucet would be a zero and the way this works with element wise multiplication. We get point eight times one equals point eight that signal pass right through into the output Vector, but the last element point eight times 0 equals 0 that signal the original signal was effectively blocked. And then with the gating value of 0.5 the single was passed through but it's smaller. It's attenuated. So gating lets us control what passes through and what gets blocked which is really useful. Now in order to do gating it's nice to have a value that you know is always between 0 and 1 so we introduce another squashing function. This will represent with a circle with a flat bottom and this is It's called The logistic function. It's very similar to the other squashing function the hyperbolic tangent except that it just goes between 0 and 1 instead of minus 1 and 1. \"], '2020-01-31T08:03:13Z', '716067a60a1a4034abc49a12ecafb39b', 'e1a7877d-d189-4854-a707-efc6f5c017f4']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[['Which is Doug Jane and spot and we passed these two vectors together to all four of our neural networks, which are learning to make predictions to do it ignoring to do forgetting and to do selection. '], '2020-01-31T08:09:33Z', '716067a60a1a4034abc49a12ecafb39b', 'f4510718-0d67-47aa-941c-26df17007ed5'], [[\"The first one of these make some predictions given that the word Doug just occurred. This is learned that the word saw is a great guess to make for next word, but it's also learned that having seen the word Doug that it should not see the word dog again, very soon seeing the word dog at the beginning of a sentence. So it makes a positive prediction for Saul and a negative. Addiction for Doug it says I do not expect to see dog in the near future. So that's why I dog is in Black. So this example is so simple. We don't need to focus on attention or ignoring. So we'll skip over it for now and this prediction of Saw not Doug is passed forward and again for the purposes of Simplicity. Let's say there's no memory at the moment. So so and Doug get passed forward and then the selection Mechanism here has learned that when the most recent word was a name then what comes next is either going to be the word saw or a period so it blocks any other names from coming out. So the fact that there's a vote for not Doug gets blocked here and the word saw get sent out as the prediction for the next time step. So we take a step forward in time. Now. The word saw is our most recent word and our most recent prediction. They get passed forward to all of these neural networks, and we get a new set of predictions because the word saw just occurred. We now predict that the words Doug Jane or spot might come next. I will pass over ignoring and attention and this example and we'll take those predictions forward. Now. The other thing that happened is our previous set of possibilities. \"], '2020-01-31T08:09:51Z', '716067a60a1a4034abc49a12ecafb39b', 'dafc63db-af9c-4928-ae8b-7babeb97b33c'], [[\"The word saw and not Doug that we were maintaining internally get passed to a forgetting gate. Now the forgetting gate says hey my last word that came that occurred was the word saw based on my past experience. Then I freaking forget about you know, I know that it occurred I can forget that it happened. But I want to keep any predictions having to do with names. So it forgets saw holds onto the vote for not Doug. And now at this element element by element addition. We have a positive vote for dog an Get a vote for Doug and so they cancel each other out. So now we just have votes for Jane and spot. Those get passed forward our selection gate. It knows that the word saw just occurred and based on experience a name will happen next and so it passes through these predictions for names and for the next time step. Then we get predictions of only Jane and spot not Doug this avoids the Dog Saw Doug period type of error and the other errors that we saw. What this shows is that long short-term memory can look back two three many time steps and use that information to make good predictions about what's going to happen next now to be fair to vanilla recurrent neural networks. They can actually look back several time steps as well, but not very many. LST M can look back many times steps and has shown that successfully. \"], '2020-01-31T08:11:51Z', '716067a60a1a4034abc49a12ecafb39b', '459c46c7-b5e7-4ae4-becd-26da1f772d9a']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"It looks like is that they find the higher-level idea and translate it from one mode of expression to another just using the bits and pieces that we just walked through. Another thing that they do. Well is translating speech to text. Speech is just some signals that vary in time. It takes them and uses that been to predict what text what word is being spoken and it can use the history the recent history of words to make a better guess for what's going to come next. L spms are a great fit for any information that's embedded in time audio video on my favorite application of all forces robotics robotics is nothing more than an agent taking in information from a set of sensors and then based on that information making a decision and carrying out an action. It's inherently sequential and actions taken now can influence what is sent and what should be done many times steps down the line. If you're curious what LS ems look like in math. This is it this is lifted straight from the Wikipedia page. I won't step through it, but it's encouraging that something that looks so complex expressed mathematically actually makes it fairly straightforward. Picture and story and if you'd like to dig into it more I encourage you to go to the Wikipedia page. Also, there are a collection of really good tutorials and discussions other ways of explaining lsdm so that you may find helpful as well. I'd also strongly encourage you to visit Andre carpet these blog posts showing examples of what L stm's can do in text. \"], '2020-01-31T08:14:11Z', '716067a60a1a4034abc49a12ecafb39b', '59f1e10c-7d02-40b7-a916-2a5563bc4be3']] \n",
      "\n",
      "\n",
      "8\n",
      "Before Merging 10\n",
      "[]\n",
      "After Merging 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Ent map before filtering:  {'0': [3216, 3216, 3216, 3216, 2697, 3216, 3216, 2697, 3216, 3216, 3216, 3216, 3216, 3216, 3216, 4003, 3216, 3216, 3216, 3216, 56, 1818, 1818, 5941, 203, 4627, 3194, 695, 698, 440, 3524, 5249, 413, 5299, 5249, 3194, 3216, 3524, 4725, 4003, 2697, 3524, 5299, 203, 818, 5249, 2697, 5249, 201, 3194, 5249, 5249, 3216, 3524, 3216, 143, 4725, 413, 5299, 818, 2610, 4719, 5655, 5249, 103, 5140, 5249, 1037, 143, 3001], '4': [3216, 3216, 3216, 3216, 3216, 3216, 3216, 1933, 3216, 3216], '6': [3216, 3216, 3216, 3798, 3216, 3216, 3798, 3401, 3216, 3798, 1658, 3524, 897, 4943, 3524, 852, 5299, 6423, 5869, 5869, 1658, 4943, 897, 3401, 201, 27, 4930, 852, 5327, 5706], '5': [2288, 6423, 79, 3886, 5612, 3243, 5292, 5941, 1342, 5945, 4725, 631, 4814, 3216, 3216, 3216, 3216, 5292, 5292, 6496, 70, 3216, 4725, 70, 3137, 3216, 16, 183, 4814, 4865, 3216, 3216, 3216, 3216, 3216, 4725, 3216, 4814, 631, 4814, 56, 3216, 6423, 3216, 70, 3216, 3216, 2288, 3216, 70], '9': [1658, 5249, 3773, 5249, 3398, 3798, 2595, 5030, 4774, 5655, 3398, 1049, 5249, 4774, 143, 3798, 1658, 5249, 2595, 27, 5249, 3773, 3398, 1658, 5249, 27, 5030, 3798, 5655, 4896, 1658, 5249, 4774, 5249, 6582, 3773, 3398, 4459, 999, 3798], '3': [5249, 3524, 3319, 5327, 1658, 3216, 5299, 143, 4091, 3216], '2': [413, 3137, 2408, 818, 27, 4681, 3216, 6007, 3216, 4725, 2333, 3216, 3216, 3216, 4003, 3216, 3216, 44, 3216, 3216, 3137, 3424, 2404, 201, 4814, 1775, 5389, 4725, 6019, 4006], '7': [3216, 3524, 3216, 5249, 3216, 4003, 5327, 4943, 1658, 3319, 2613, 2613, 1049, 2613, 3036, 203, 398, 5140, 6055, 3378, 1658, 3524, 5048, 3524, 5941, 3668, 4943, 4737, 5941, 398, 3524, 4943, 3524, 2613, 3319, 398, 3811, 3668, 1658, 4719, 4719, 398, 1658, 3036, 3319, 3595, 2613, 5013, 3535, 5327], '1': [1049, 4865, 1951, 2697, 4185, 1366, 2489, 4429, 4429, 16, 1049, 4865, 4429, 1951, 4429, 2697, 2489, 4185, 1366, 3138], '8': [2613, 2613, 203, 3675, 541, 1233, 4081, 1818, 1818, 2288, 8, 5538, 2203, 541, 631, 2288, 1233, 1225, 1812, 3367]}\n",
      "Group Ent map after filtering:  {'0': [(3216, 20), (5249, 8), (2697, 4), (3524, 4), (3194, 3), (5299, 3)], '4': [(3216, 9)], '6': [(3216, 6), (3798, 3)], '5': [(3216, 17), (4814, 4), (70, 4), (5292, 3), (4725, 3)], '9': [(5249, 8), (1658, 4), (3398, 4), (3798, 4), (3773, 3), (4774, 3)], '3': [], '2': [(3216, 9)], '7': [(3524, 5), (2613, 5), (1658, 4), (398, 4), (3216, 3), (4943, 3), (3319, 3)], '1': [(4429, 4)], '8': []}\n",
      "Using  None  for feature extraction\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 103, \"module\": \"scorer\", \"batches count\": 1, \"number of sentences\": 171, \"ts\": \"2020-03-05T08:27:18.957581Z\", \"msg\": \"computing in batches\"}\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 106, \"module\": \"scorer\", \"iteration count:\": 0, \"ts\": \"2020-03-05T08:27:18.958740Z\", \"msg\": \"getting feature vector\"}\n",
      "('Let is dive deeper into the NLP approaches to analyze text Data.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is time to make yourself comfortable with the NLP terminologies one word boundaries.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b')\n",
      "('Let is dive deeper into the NLP approaches to analyze text Data.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('Let is dive deeper into the NLP approaches to analyze text Data.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('We get an insight into grammatical categories of the text document.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Let is dive deeper into the NLP approaches to analyze text Data.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('For example, the text features of text based on speech tag or some grammar rules.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('These approaches can be interrelated or they can be independently applied depending on the type of data to analyze.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('These approaches can be interrelated or they can be independently applied depending on the type of data to analyze.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a type of statistical model for finding abstract topics which occur in a collection of documents.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('These approaches can be interrelated or they can be independently applied depending on the type of data to analyze.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('Analyze the meaning perform quantitative analysis of the given set of data to extract the information for example find entities in the text and try to establish a relationship between them.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Basic text processing it is a way to analyze text and extract keywords that sum up the style or basic context of the text.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a technique to split words phrases idioms Etc present in a document.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b')\n",
      "('Basic text processing it is a way to analyze text and extract keywords that sum up the style or basic context of the text.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('Basic text processing it is a way to analyze text and extract keywords that sum up the style or basic context of the text.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('Wise sentence structure in this approach capture formal grammar to describe the structure of a set of sentences.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Basic text processing it is a way to analyze text and extract keywords that sum up the style or basic context of the text.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('We get an insight into grammatical categories of the text document.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Basic text processing it is a way to analyze text and extract keywords that sum up the style or basic context of the text.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('For example, the text features of text based on speech tag or some grammar rules.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('For example, if the content is religious or fictional categorizing and tagging words.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('For example, if the content is religious or fictional categorizing and tagging words.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('We get an insight into grammatical categories of the text document.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('This approach is about finding lexical categories and automatically tagging each word with this word class.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It determines where one word ends and the other begins to tokenization.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b')\n",
      "('This approach is about finding lexical categories and automatically tagging each word with this word class.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is very useful in finding synonyms and extensively used in search engines.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b')\n",
      "('This approach is about finding lexical categories and automatically tagging each word with this word class.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('This approach is about finding lexical categories and automatically tagging each word with this word class.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('We get an insight into grammatical categories of the text document.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('This approach is about finding lexical categories and automatically tagging each word with this word class.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('For example, the text features of text based on speech tag or some grammar rules.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('For example, tag a word using languages such as Chinese Spanish Etc.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It determines where one word ends and the other begins to tokenization.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b')\n",
      "('For example, tag a word using languages such as Chinese Spanish Etc.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a technique to split words phrases idioms Etc present in a document.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b')\n",
      "('For example, tag a word using languages such as Chinese Spanish Etc.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is very useful in finding synonyms and extensively used in search engines.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b')\n",
      "('For example, tag a word using languages such as Chinese Spanish Etc.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('For example, tag a word using languages such as Chinese Spanish Etc.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('We get an insight into grammatical categories of the text document.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('For example, tag a word using languages such as Chinese Spanish Etc.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('For example, the text features of text based on speech tag or some grammar rules.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Words can also be tagged as adjectives verbs nouns and so on.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It determines where one word ends and the other begins to tokenization.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b')\n",
      "('Words can also be tagged as adjectives verbs nouns and so on.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a technique to split words phrases idioms Etc present in a document.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b')\n",
      "('Words can also be tagged as adjectives verbs nouns and so on.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a process to map word to their stem or root.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b')\n",
      "('Words can also be tagged as adjectives verbs nouns and so on.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('Wise sentence structure in this approach capture formal grammar to describe the structure of a set of sentences.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Words can also be tagged as adjectives verbs nouns and so on.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('We get an insight into grammatical categories of the text document.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Words can also be tagged as adjectives verbs nouns and so on.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('For example, the text features of text based on speech tag or some grammar rules.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Classify text with this approach you can identify particular features of language and use them to classify it for example classify the text as Sports politics or technology.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('For example, the text features of text based on speech tag or some grammar rules.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Classify text with this approach you can identify particular features of language and use them to classify it for example classify the text as Sports politics or technology.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('Analyze the meaning perform quantitative analysis of the given set of data to extract the information for example find entities in the text and try to establish a relationship between them.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Extract information this approach is about identifying the entities and relationships in a text to extract information in a structured way for example date time money and a direction can be used to establish relationships with other words available in the text.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('Extract information this approach is about identifying the entities and relationships in a text to extract information in a structured way for example date time money and a direction can be used to establish relationships with other words available in the text.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('Wise sentence structure in this approach capture formal grammar to describe the structure of a set of sentences.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Extract information this approach is about identifying the entities and relationships in a text to extract information in a structured way for example date time money and a direction can be used to establish relationships with other words available in the text.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('For example, the text features of text based on speech tag or some grammar rules.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Extract information this approach is about identifying the entities and relationships in a text to extract information in a structured way for example date time money and a direction can be used to establish relationships with other words available in the text.', '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843') ('Analyze the meaning perform quantitative analysis of the given set of data to extract the information for example find entities in the text and try to establish a relationship between them.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('is to decode the input strip accents it removes accent present in the code.', '2020-01-31T08:11:40Z', '716067a60a1a4034abc49a12ecafb39b', '07d2bcc9-3ecc-4a36-81c6-96891f58a3ef') ('Let is see how it works by considering a document where we need to extract text features the steps to follow our assign a fixed integer ID to each word by splitting them into several words also known as tokenizing then count the number of occurrences of each word or token finally store it as the value feature in Matrix format.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7')\n",
      "('is to decode the input strip accents it removes accent present in the code.', '2020-01-31T08:11:40Z', '716067a60a1a4034abc49a12ecafb39b', '07d2bcc9-3ecc-4a36-81c6-96891f58a3ef') ('The goal is to convert a collection of text documents to a matrix of token counts.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7')\n",
      "('is to decode the input strip accents it removes accent present in the code.', '2020-01-31T08:11:40Z', '716067a60a1a4034abc49a12ecafb39b', '07d2bcc9-3ecc-4a36-81c6-96891f58a3ef') ('Let is take a look at the components of the signature input content.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7')\n",
      "('is to decode the input strip accents it removes accent present in the code.', '2020-01-31T08:11:40Z', '716067a60a1a4034abc49a12ecafb39b', '07d2bcc9-3ecc-4a36-81c6-96891f58a3ef') ('The input content can be a file name of the sequence of strings which needs to be vectorized.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7')\n",
      "('Tokenizer it overrides this string tokenizer method, but the default value is none.', '2020-01-31T08:11:40Z', '716067a60a1a4034abc49a12ecafb39b', '07d2bcc9-3ecc-4a36-81c6-96891f58a3ef') ('Let is see how it works by considering a document where we need to extract text features the steps to follow our assign a fixed integer ID to each word by splitting them into several words also known as tokenizing then count the number of occurrences of each word or token finally store it as the value feature in Matrix format.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7')\n",
      "('Tokenizer it overrides this string tokenizer method, but the default value is none.', '2020-01-31T08:11:40Z', '716067a60a1a4034abc49a12ecafb39b', '07d2bcc9-3ecc-4a36-81c6-96891f58a3ef') ('Let is start by importing the required Library count vectorizer class.', '2020-01-31T08:12:21Z', '716067a60a1a4034abc49a12ecafb39b', 'c434f4e4-632a-4187-97b4-f03ec380a445')\n",
      "('Tokenizer it overrides this string tokenizer method, but the default value is none.', '2020-01-31T08:11:40Z', '716067a60a1a4034abc49a12ecafb39b', '07d2bcc9-3ecc-4a36-81c6-96891f58a3ef') ('Class helps tokenize the document it converts the text into vectors by assigning numeric values to each word.', '2020-01-31T08:12:37Z', '716067a60a1a4034abc49a12ecafb39b', 'e75412de-1eff-43b9-a73e-1cdf925a5a17')\n",
      "('It indicates the terms or words that appear more than a given threshold value and should therefore be ignored.', '2020-01-31T08:11:40Z', '716067a60a1a4034abc49a12ecafb39b', '07d2bcc9-3ecc-4a36-81c6-96891f58a3ef') ('Now, let us learn about the bag of Words, which is one of the most common text feature extraction techniques.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7')\n",
      "('It indicates the terms or words that appear more than a given threshold value and should therefore be ignored.', '2020-01-31T08:11:40Z', '716067a60a1a4034abc49a12ecafb39b', '07d2bcc9-3ecc-4a36-81c6-96891f58a3ef') ('Let is see how it works by considering a document where we need to extract text features the steps to follow our assign a fixed integer ID to each word by splitting them into several words also known as tokenizing then count the number of occurrences of each word or token finally store it as the value feature in Matrix format.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7')\n",
      "('It indicates the terms or words that appear more than a given threshold value and should therefore be ignored.', '2020-01-31T08:11:40Z', '716067a60a1a4034abc49a12ecafb39b', '07d2bcc9-3ecc-4a36-81c6-96891f58a3ef') ('Class helps tokenize the document it converts the text into vectors by assigning numeric values to each word.', '2020-01-31T08:12:37Z', '716067a60a1a4034abc49a12ecafb39b', 'e75412de-1eff-43b9-a73e-1cdf925a5a17')\n",
      "('P terminology now that you have understood why NLP is so important in recent times.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('Sometimes it is also referred to as a field of computer science or artificial intelligence to extract the Linguistics information from the underlying data in LP enables machines or computers to derive meaning from human or natural language input.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374')\n",
      "('P terminology now that you have understood why NLP is so important in recent times.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('handling ambiguities while interpreting data and extracting information This is where natural language processing proves useful.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374')\n",
      "('P terminology now that you have understood why NLP is so important in recent times.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('With the help of these libraries we can also build analytical models and automate the natural language process with minimum or no human interventions.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374')\n",
      "('P terminology now that you have understood why NLP is so important in recent times.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('P terminology now that you have understood why NLP is so important in recent times.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('It is a technique to determine the meaning and a sense of words context versus intent XnumberX topic models.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('P terminology now that you have understood why NLP is so important in recent times.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('It is a type of statistical model for finding abstract topics which occur in a collection of documents.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('It is time to make yourself comfortable with the NLP terminologies one word boundaries.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('Troll language processing or NLP is an automated way to understand analyze human languages and extract information from such data by applying machine learning algorithms.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374')\n",
      "('It determines where one word ends and the other begins to tokenization.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('It is a technique to split words phrases idioms Etc present in a document.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('handling ambiguities while interpreting data and extracting information This is where natural language processing proves useful.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374')\n",
      "('It is a technique to split words phrases idioms Etc present in a document.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('It is a process to map word to their stem or root.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('Tfidf it is a numerical value which represents how important the word is to a document or Corpus.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('It is a process to map word to their stem or root.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('It is very useful in finding synonyms and extensively used in search engines.', '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('Well use string class punctuation to remove all the punctuation present in each message as they are also less waited for text analysis, then we will remove the stop words.', '2020-01-31T08:01:14Z', '716067a60a1a4034abc49a12ecafb39b', '0f850918-7639-4c1f-b058-d682b3d4c218') ('World of text analysis stop words usually have little lexical meaning some examples of stop words.', '2020-01-31T08:00:24Z', '716067a60a1a4034abc49a12ecafb39b', '9d25f0a6-3e1c-425a-9171-df4c73b4381d')\n",
      "('Well use string class punctuation to remove all the punctuation present in each message as they are also less waited for text analysis, then we will remove the stop words.', '2020-01-31T08:01:14Z', '716067a60a1a4034abc49a12ecafb39b', '0f850918-7639-4c1f-b058-d682b3d4c218') ('Now let us view the first XnumberX stop words present in the stop words Corpus of in ltk.', '2020-01-31T08:00:24Z', '716067a60a1a4034abc49a12ecafb39b', '9d25f0a6-3e1c-425a-9171-df4c73b4381d')\n",
      "('Well use string class punctuation to remove all the punctuation present in each message as they are also less waited for text analysis, then we will remove the stop words.', '2020-01-31T08:01:14Z', '716067a60a1a4034abc49a12ecafb39b', '0f850918-7639-4c1f-b058-d682b3d4c218') ('Well use basic python syntax to display the characters which do not have punctuation as you can see we make use of basic string class and its punctuation function.', '2020-01-31T08:01:43Z', '716067a60a1a4034abc49a12ecafb39b', 'e76c9b1e-00ba-4839-ba68-f032793423de')\n",
      "('Let is create its head sequence to analyze will create a variable test sentence and passed the sentence.', '2020-01-31T08:00:57Z', '716067a60a1a4034abc49a12ecafb39b', '11d761d8-f504-4219-844a-a39852210a5a') ('Then import Stop Words, which we have installed earlier as part of the environment setup.', '2020-01-31T08:00:11Z', '716067a60a1a4034abc49a12ecafb39b', '358a204b-c3b1-48fc-aea7-b86254869041')\n",
      "('Let is create its head sequence to analyze will create a variable test sentence and passed the sentence.', '2020-01-31T08:00:57Z', '716067a60a1a4034abc49a12ecafb39b', '11d761d8-f504-4219-844a-a39852210a5a') ('Now let us view the first XnumberX stop words present in the stop words Corpus of in ltk.', '2020-01-31T08:00:24Z', '716067a60a1a4034abc49a12ecafb39b', '9d25f0a6-3e1c-425a-9171-df4c73b4381d')\n",
      "('Let is create its head sequence to analyze will create a variable test sentence and passed the sentence.', '2020-01-31T08:00:57Z', '716067a60a1a4034abc49a12ecafb39b', '11d761d8-f504-4219-844a-a39852210a5a') ('Well use basic python syntax to display the characters which do not have punctuation as you can see we make use of basic string class and its punctuation function.', '2020-01-31T08:01:43Z', '716067a60a1a4034abc49a12ecafb39b', 'e76c9b1e-00ba-4839-ba68-f032793423de')\n",
      "('Let is talk about feature extraction another functionality of the scikitlearn approach.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('Let is learn about its builtin modules for loading the data set contents and categories.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff')\n",
      "('Let is talk about feature extraction another functionality of the scikitlearn approach.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('And as supervised signal label names, it does not try to extract features into a numpy array or scifi sparse Matrix.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff')\n",
      "('Let is talk about feature extraction another functionality of the scikitlearn approach.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('Let is see how a data set can be loaded using scikitlearn.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff')\n",
      "('This is a technique to convert the content into numerical vectors to perform machine learning.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('Youll need to use the sklearn feature extraction text module to build a feature extraction Transformer.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff')\n",
      "('The feature extraction technique is used mostly in machine learning while dealing with text or image data.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('Youll need to use the sklearn feature extraction text module to build a feature extraction Transformer.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff')\n",
      "('Data examples include large data sets or documents image feature extraction.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('The diagram shows you how a data set is loaded using methods and how the object is structured in terms of the container folder and it is categories to load files.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff')\n",
      "('Data examples include large data sets or documents image feature extraction.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('A data load object helps load the contents of a data set shown here are the attributes of a data load object.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff')\n",
      "('Data examples include large data sets or documents image feature extraction.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('Data, it refers to an attribute in the memory where files are loaded.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff')\n",
      "('Now, let us learn about the bag of Words, which is one of the most common text feature extraction techniques.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('Youll need to use the sklearn feature extraction text module to build a feature extraction Transformer.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff')\n",
      "('Now, let us learn about the bag of Words, which is one of the most common text feature extraction techniques.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('This demo you will learn how to process documents using bag of words.', '2020-01-31T08:12:21Z', '716067a60a1a4034abc49a12ecafb39b', 'c434f4e4-632a-4187-97b4-f03ec380a445')\n",
      "('The Matrix presentation on the screen shows how multiple documents and tokens are structured This screen shows have account vectorizer algorithm works in scikitlearn.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('This demo you will learn how to process documents using bag of words.', '2020-01-31T08:12:21Z', '716067a60a1a4034abc49a12ecafb39b', 'c434f4e4-632a-4187-97b4-f03ec380a445')\n",
      "('Let is take a look at the components of the signature input content.', '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7') ('The array is displayed here are the response data which is present in the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('Troll language processing or NLP is an automated way to understand analyze human languages and extract information from such data by applying machine learning algorithms.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('The data content can be text document image audio or video.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('Sometimes it is also referred to as a field of computer science or artificial intelligence to extract the Linguistics information from the underlying data in LP enables machines or computers to derive meaning from human or natural language input.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('Sometimes it is also referred to as a field of computer science or artificial intelligence to extract the Linguistics information from the underlying data in LP enables machines or computers to derive meaning from human or natural language input.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a technique to determine the meaning and a sense of words context versus intent XnumberX topic models.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('Sometimes it is also referred to as a field of computer science or artificial intelligence to extract the Linguistics information from the underlying data in LP enables machines or computers to derive meaning from human or natural language input.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a type of statistical model for finding abstract topics which occur in a collection of documents.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('The world is now connected globally due to the advancement of technology and devices.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('This has resulted in the high volume of Digital Data across the world and has led to a number of challenges in analyzing data including analyzing the tons of data that is generated in the form of text image audios and videos.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('Identifying approximately XnumberX,XnumberX languages and dialects followed across the globe applying quantitative analysis on huge collections of data.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('handling ambiguities while interpreting data and extracting information This is where natural language processing proves useful.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('handling ambiguities while interpreting data and extracting information This is where natural language processing proves useful.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a technique to determine the meaning and a sense of words context versus intent XnumberX topic models.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('One of the main goals of natural language processing is to understand various languages process them and extract information from them in NLP full automation can be easily achieved by modern software libraries modules and packages these software libraries and packages are aware of diverse language and culture and categorize data accordingly, which enables understanding Cinema it is better.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('With the help of these libraries we can also build analytical models and automate the natural language process with minimum or no human interventions.', '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374') ('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb')\n",
      "('Then import Stop Words, which we have installed earlier as part of the environment setup.', '2020-01-31T08:00:11Z', '716067a60a1a4034abc49a12ecafb39b', '358a204b-c3b1-48fc-aea7-b86254869041') ('Let is start by importing the required Library class string from in ltk Corpus.', '2020-01-31T07:59:54Z', '716067a60a1a4034abc49a12ecafb39b', '53949d4e-b7a0-400c-9749-6787eff62314')\n",
      "('An ism to perform an exhaustive search on the best parameters which affect the model however this approach has some constraints such as the whole grid search process is subjected to availability of the CPU cores of the system.', '2020-01-31T08:19:17Z', '716067a60a1a4034abc49a12ecafb39b', '38a75d62-8e9e-4ea6-aea7-58460c83cd6b') ('Has and response in supervised learning we make use of all three tasks predict the outcome of new observations and data sets understand which predictors affect the response or outcome and generalize the data and find the right answer generalization is also known as making predictions models can be trained to classify the documents based on the features and response.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4')\n",
      "('An ism to perform an exhaustive search on the best parameters which affect the model however this approach has some constraints such as the whole grid search process is subjected to availability of the CPU cores of the system.', '2020-01-31T08:19:17Z', '716067a60a1a4034abc49a12ecafb39b', '38a75d62-8e9e-4ea6-aea7-58460c83cd6b') ('So here we try to train the model by following a few steps such as understanding the structure of the data first and identify the pattern in the data finding predictors that behave in the same fashion or have some familiarity exploring the data set and the goal is representation of the data.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4')\n",
      "('An ism to perform an exhaustive search on the best parameters which affect the model however this approach has some constraints such as the whole grid search process is subjected to availability of the CPU cores of the system.', '2020-01-31T08:19:17Z', '716067a60a1a4034abc49a12ecafb39b', '38a75d62-8e9e-4ea6-aea7-58460c83cd6b') ('The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory.', '2020-01-31T08:19:17Z', '716067a60a1a4034abc49a12ecafb39b', '38a75d62-8e9e-4ea6-aea7-58460c83cd6b') ('Next feature of scikitlearn is model training an important task in model training is to identify the right model for the given data set scikitlearn provides a range of models to choose from and also trains them by using the extracted features from the data set.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4')\n",
      "('This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory.', '2020-01-31T08:19:17Z', '716067a60a1a4034abc49a12ecafb39b', '38a75d62-8e9e-4ea6-aea7-58460c83cd6b') ('It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory.', '2020-01-31T08:19:17Z', '716067a60a1a4034abc49a12ecafb39b', '38a75d62-8e9e-4ea6-aea7-58460c83cd6b') ('The next feature of scikitlearn will be looking into is grid search.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory.', '2020-01-31T08:19:17Z', '716067a60a1a4034abc49a12ecafb39b', '38a75d62-8e9e-4ea6-aea7-58460c83cd6b') ('A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory.', '2020-01-31T08:19:17Z', '716067a60a1a4034abc49a12ecafb39b', '38a75d62-8e9e-4ea6-aea7-58460c83cd6b') ('Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory.', '2020-01-31T08:19:17Z', '716067a60a1a4034abc49a12ecafb39b', '38a75d62-8e9e-4ea6-aea7-58460c83cd6b') ('So using scikitlearn pipeline can be created and we can train the model using a single command.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory.', '2020-01-31T08:19:17Z', '716067a60a1a4034abc49a12ecafb39b', '38a75d62-8e9e-4ea6-aea7-58460c83cd6b') ('The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('It is a powerful library with a set of modules to process and analyze natural language data such as text and images and extract information using machine learning algorithms.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3a861e7b-1736-4924-9827-63f8266e3ad7') ('It is another powerful open source python package and module XnumberX in LP.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5')\n",
      "('These are some of the essential features of scikitlearn approach builtin modules.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3a861e7b-1736-4924-9827-63f8266e3ad7') ('It is a pythonbased library for NLP and widely used in the industry to build programs to work with different human languages.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5')\n",
      "('These are some of the essential features of scikitlearn approach builtin modules.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3a861e7b-1736-4924-9827-63f8266e3ad7') ('It is another powerful open source python package and module XnumberX in LP.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5')\n",
      "('These are some of the essential features of scikitlearn approach builtin modules.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3a861e7b-1736-4924-9827-63f8266e3ad7') ('It features various algorithms and is designed for operating with other python libraries like numpy and acai pie.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5')\n",
      "('These are some of the essential features of scikitlearn approach builtin modules.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3a861e7b-1736-4924-9827-63f8266e3ad7') ('This is a technique in scikitlearn approach to streamline the NLP process in two stages.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('These are some of the essential features of scikitlearn approach builtin modules.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3a861e7b-1736-4924-9827-63f8266e3ad7') ('Let is take a look at the various stages of pipeline learning one vectorization.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('It has builtin modules to load the data sets content and categories will discuss the modules in detail in the upcoming screen.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3a861e7b-1736-4924-9827-63f8266e3ad7') ('It features various algorithms and is designed for operating with other python libraries like numpy and acai pie.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5')\n",
      "('Feature extraction it is a way to extract information from data which can be text or images scikitlearn,  is builtin functions and methods help extract features and attributes from text to data and image data for the purpose of analysis.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3a861e7b-1736-4924-9827-63f8266e3ad7') ('Text blob this library is used for processing Text data and provides simple apis for diving into NLP.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5')\n",
      "('Feature extraction it is a way to extract information from data which can be text or images scikitlearn,  is builtin functions and methods help extract features and attributes from text to data and image data for the purpose of analysis.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3a861e7b-1736-4924-9827-63f8266e3ad7') ('It is one of the most common ways to adjust the extracted features.', '2020-01-31T08:06:43Z', '716067a60a1a4034abc49a12ecafb39b', '80f384af-2e77-461e-b861-593c93a060b8')\n",
      "('Feature extraction it is a way to extract information from data which can be text or images scikitlearn,  is builtin functions and methods help extract features and attributes from text to data and image data for the purpose of analysis.', '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3a861e7b-1736-4924-9827-63f8266e3ad7') ('For example in this stage different algorithms are used to classify text documents and find the sentiment analysis.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('We can see that the list contains a lot of stop words.', '2020-01-31T08:02:11Z', '716067a60a1a4034abc49a12ecafb39b', '4ac0bcfc-09d6-4b83-96aa-fa755709e8fc') ('It is analysis and our sentences free of punctuation and stop words following are the major in alpide libraries used in Python in LT Kang.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5')\n",
      "('We can see that the list contains a lot of stop words.', '2020-01-31T08:02:11Z', '716067a60a1a4034abc49a12ecafb39b', '4ac0bcfc-09d6-4b83-96aa-fa755709e8fc') ('Well use basic python syntax to display the characters which do not have punctuation as you can see we make use of basic string class and its punctuation function.', '2020-01-31T08:01:43Z', '716067a60a1a4034abc49a12ecafb39b', 'e76c9b1e-00ba-4839-ba68-f032793423de')\n",
      "('Let is remove the stop words will use stop words Corpus dot words function with English as an argument and basic python word and split function to clean each word.', '2020-01-31T08:02:11Z', '716067a60a1a4034abc49a12ecafb39b', '4ac0bcfc-09d6-4b83-96aa-fa755709e8fc') ('It is analysis and our sentences free of punctuation and stop words following are the major in alpide libraries used in Python in LT Kang.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5')\n",
      "('Let is remove the stop words will use stop words Corpus dot words function with English as an argument and basic python word and split function to clean each word.', '2020-01-31T08:02:11Z', '716067a60a1a4034abc49a12ecafb39b', '4ac0bcfc-09d6-4b83-96aa-fa755709e8fc') ('Text blob this library is used for processing Text data and provides simple apis for diving into NLP.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5')\n",
      "('Let is remove the stop words will use stop words Corpus dot words function with English as an argument and basic python word and split function to clean each word.', '2020-01-31T08:02:11Z', '716067a60a1a4034abc49a12ecafb39b', '4ac0bcfc-09d6-4b83-96aa-fa755709e8fc') ('Well use basic python syntax to display the characters which do not have punctuation as you can see we make use of basic string class and its punctuation function.', '2020-01-31T08:01:43Z', '716067a60a1a4034abc49a12ecafb39b', 'e76c9b1e-00ba-4839-ba68-f032793423de')\n",
      "('Let is navigate through the other models and all packages ensure that stop words status is installed.', '2020-01-31T07:59:33Z', '716067a60a1a4034abc49a12ecafb39b', '4b462d15-6f4d-414c-b760-7842ef6d1a11') ('Let is start by importing the required Library class string from in ltk Corpus.', '2020-01-31T07:59:54Z', '716067a60a1a4034abc49a12ecafb39b', '53949d4e-b7a0-400c-9749-6787eff62314')\n",
      "('Let is navigate through the other models and all packages ensure that stop words status is installed.', '2020-01-31T07:59:33Z', '716067a60a1a4034abc49a12ecafb39b', '4b462d15-6f4d-414c-b760-7842ef6d1a11') ('Dont type in Python in Anaconda prompt command line type in Import in ltk here to install the Corpus Collections and models.', '2020-01-31T07:58:57Z', '716067a60a1a4034abc49a12ecafb39b', '7e939963-8230-4982-92da-54c57d8a27e5')\n",
      "('Next import grid search from the sklearn grid search class to perform the grid search.', '2020-01-31T08:23:05Z', '716067a60a1a4034abc49a12ecafb39b', '516c0d72-3ac4-4dca-82db-afe84e8d0cae') ('Create a pipeline by instantiating the pipeline class and passing vectorize transformed and model classifier has argument.', '2020-01-31T08:23:24Z', '716067a60a1a4034abc49a12ecafb39b', 'e18d7fa5-63d5-4f33-9a54-fe6c318ccfb9')\n",
      "('In this demo, you will learn how to perform sentence analysis.', '2020-01-31T07:59:54Z', '716067a60a1a4034abc49a12ecafb39b', '53949d4e-b7a0-400c-9749-6787eff62314') ('World of text analysis stop words usually have little lexical meaning some examples of stop words.', '2020-01-31T08:00:24Z', '716067a60a1a4034abc49a12ecafb39b', '9d25f0a6-3e1c-425a-9171-df4c73b4381d')\n",
      "('In this demo, you will learn how to perform sentence analysis.', '2020-01-31T07:59:54Z', '716067a60a1a4034abc49a12ecafb39b', '53949d4e-b7a0-400c-9749-6787eff62314') ('Now let us view the first XnumberX stop words present in the stop words Corpus of in ltk.', '2020-01-31T08:00:24Z', '716067a60a1a4034abc49a12ecafb39b', '9d25f0a6-3e1c-425a-9171-df4c73b4381d')\n",
      "('Let is start by importing the required Library class string from in ltk Corpus.', '2020-01-31T07:59:54Z', '716067a60a1a4034abc49a12ecafb39b', '53949d4e-b7a0-400c-9749-6787eff62314') ('You can close the window and go back to jupyter notebook environment.', '2020-01-31T07:59:48Z', '716067a60a1a4034abc49a12ecafb39b', 'e0489437-09c9-4e51-a22a-b0ecc71c9bd3')\n",
      "('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb') ('We get an insight into grammatical categories of the text document.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb') ('For example, the text features of text based on speech tag or some grammar rules.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('It is a technique to determine the meaning and a sense of words context versus intent XnumberX topic models.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb') ('Wise sentence structure in this approach capture formal grammar to describe the structure of a set of sentences.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('It is a technique to determine the meaning and a sense of words context versus intent XnumberX topic models.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb') ('We get an insight into grammatical categories of the text document.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('It is a technique to determine the meaning and a sense of words context versus intent XnumberX topic models.', '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb') ('For example, the text features of text based on speech tag or some grammar rules.', '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f')\n",
      "('Next feature of scikitlearn is model training an important task in model training is to identify the right model for the given data set scikitlearn provides a range of models to choose from and also trains them by using the extracted features from the data set.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4') ('It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('Next feature of scikitlearn is model training an important task in model training is to identify the right model for the given data set scikitlearn provides a range of models to choose from and also trains them by using the extracted features from the data set.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4') ('A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('The choice of model completely depends on the type of data set.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4') ('It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4') ('It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4') ('It is fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4') ('We use a document classifier to predict the category of a document to try predicting the outcome of a new document.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4') ('A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('So here we try to train the model by following a few steps such as understanding the structure of the data first and identify the pattern in the data finding predictors that behave in the same fashion or have some familiarity exploring the data set and the goal is representation of the data.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4') ('It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('So here we try to train the model by following a few steps such as understanding the structure of the data first and identify the pattern in the data finding predictors that behave in the same fashion or have some familiarity exploring the data set and the goal is representation of the data.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4') ('The next feature of scikitlearn will be looking into is grid search.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('Rotation is also known as extracting structure unsupervised model can also be used to group documents by applying clustering algorithms.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4') ('It is also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('Rotation is also known as extracting structure unsupervised model can also be used to group documents by applying clustering algorithms.', '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4') ('A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a')\n",
      "('Let is print the bag of words hand look at the details.', '2020-01-31T08:14:10Z', '716067a60a1a4034abc49a12ecafb39b', '5b02541f-d705-41ac-8a41-1d0236e66d01') ('Let is check for repeated words in the vocabulary that is built with the get function and then print it.', '2020-01-31T08:14:51Z', '716067a60a1a4034abc49a12ecafb39b', '7e4f326b-f583-4dd2-966c-0b383ec8629f')\n",
      "('Let is print the bag of words hand look at the details.', '2020-01-31T08:14:10Z', '716067a60a1a4034abc49a12ecafb39b', '5b02541f-d705-41ac-8a41-1d0236e66d01') ('I can see the first number is the Tuple and the second number is the frequency of words the Tuple here indicates the document number and feature indices of each word, which belongs to the document feature indices are generated from the transform method.', '2020-01-31T08:14:21Z', '716067a60a1a4034abc49a12ecafb39b', '8f7b4256-2c12-41db-9683-3099c187d7ca')\n",
      "('The backslash T indicates that data in the spam data set is tab separated or Tab delimited and set function takes it as a parameter.', '2020-01-31T08:21:49Z', '716067a60a1a4034abc49a12ecafb39b', '6289ede2-cc94-49c4-8a0b-7244eaced58c') ('Let is start by importing the required Library pandas string print and time.', '2020-01-31T08:21:11Z', '716067a60a1a4034abc49a12ecafb39b', '867aad19-34c8-4652-972f-63e59155e294')\n",
      "('The backslash T indicates that data in the spam data set is tab separated or Tab delimited and set function takes it as a parameter.', '2020-01-31T08:21:49Z', '716067a60a1a4034abc49a12ecafb39b', '6289ede2-cc94-49c4-8a0b-7244eaced58c') ('Well get the Spam data collection with the help of the pandas Library.', '2020-01-31T08:21:38Z', '716067a60a1a4034abc49a12ecafb39b', 'f95338af-3e80-472e-a164-494372b849cf')\n",
      "('Also message is a feature which refers to the text present in the data set response is the label or the category of the message.', '2020-01-31T08:21:49Z', '716067a60a1a4034abc49a12ecafb39b', '6289ede2-cc94-49c4-8a0b-7244eaced58c') ('Import counter vectorizer to tokenize the document and convert the text into numeric values Also.', '2020-01-31T08:22:33Z', '716067a60a1a4034abc49a12ecafb39b', 'a190dbad-c600-4c59-8b8d-1ef16cd7ce90')\n",
      "('Let is view the first five records of the stamp collection.', '2020-01-31T08:21:49Z', '716067a60a1a4034abc49a12ecafb39b', '6289ede2-cc94-49c4-8a0b-7244eaced58c') ('Well get the Spam data collection with the help of the pandas Library.', '2020-01-31T08:21:38Z', '716067a60a1a4034abc49a12ecafb39b', 'f95338af-3e80-472e-a164-494372b849cf')\n",
      "('The head method is used to view the topmost records in a data set.', '2020-01-31T08:21:49Z', '716067a60a1a4034abc49a12ecafb39b', '6289ede2-cc94-49c4-8a0b-7244eaced58c') ('Let is start by importing the required Library pandas string print and time.', '2020-01-31T08:21:11Z', '716067a60a1a4034abc49a12ecafb39b', '867aad19-34c8-4652-972f-63e59155e294')\n",
      "('In this demo, you learned how to use bag of words technique and transformed documents using the transform method.', '2020-01-31T08:15:20Z', '716067a60a1a4034abc49a12ecafb39b', '690e387d-0592-42ec-9046-5e6d8a02082b') ('I can see the first number is the Tuple and the second number is the frequency of words the Tuple here indicates the document number and feature indices of each word, which belongs to the document feature indices are generated from the transform method.', '2020-01-31T08:14:21Z', '716067a60a1a4034abc49a12ecafb39b', '8f7b4256-2c12-41db-9683-3099c187d7ca')\n",
      "('And in document XnumberX we can say this was an amazing experience.', '2020-01-31T08:13:05Z', '716067a60a1a4034abc49a12ecafb39b', '6c4efd28-dbcc-4cb8-86a0-2e03aaa795c9') ('Let is add some text to the documents document one can have Hi, how are you?', '2020-01-31T08:12:53Z', '716067a60a1a4034abc49a12ecafb39b', 'a3a11c95-03db-4d95-8feb-e66aaa606553')\n",
      "('It is analysis and our sentences free of punctuation and stop words following are the major in alpide libraries used in Python in LT Kang.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5') ('To transformation and transformation the approaches to extract features around the word of Interest also in this stage you find the occurrence of each word in a document.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('It is a pythonbased library for NLP and widely used in the industry to build programs to work with different human languages.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5') ('This is a technique in scikitlearn approach to streamline the NLP process in two stages.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('It is another powerful open source python package and module XnumberX in LP.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5') ('This is a technique in scikitlearn approach to streamline the NLP process in two stages.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('Text blob this library is used for processing Text data and provides simple apis for diving into NLP.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5') ('This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('Text blob this library is used for processing Text data and provides simple apis for diving into NLP.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5') ('For example in this stage different algorithms are used to classify text documents and find the sentiment analysis.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('Spacing this is another Library which provides multiple useful views of textual meaning and linguistic structure.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5') ('This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('Spacing this is another Library which provides multiple useful views of textual meaning and linguistic structure.', '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5') ('To transformation and transformation the approaches to extract features around the word of Interest also in this stage you find the occurrence of each word in a document.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('Let is create an object grid search and pass pipeline parameters jobs and verbose as arguments.', '2020-01-31T08:24:15Z', '716067a60a1a4034abc49a12ecafb39b', '740833c0-fc13-47d3-82db-c30d4bab26db') ('Print the parameters and run the grid search on the data set for both the message and response after fitting them into the grid search object using the fit method.', '2020-01-31T08:24:50Z', '716067a60a1a4034abc49a12ecafb39b', 'c97f4d0c-97b0-42ee-a8db-e8b7ddeef4e9')\n",
      "('It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('So using scikitlearn pipeline can be created and we can train the model using a single command.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('It is fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('It is fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('For example different algorithms are used to classify text documents and find the sentiment analysis.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('It is fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('So using scikitlearn pipeline can be created and we can train the model using a single command.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('It is fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('And language detection multinomial naive Bayes is used when multiple occurrences of the words matter multinomial distribution normally requires integer feature counts.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('Far we have learned that there are three major steps for analyzing text Data one vectorizer.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('And language detection multinomial naive Bayes is used when multiple occurrences of the words matter multinomial distribution normally requires integer feature counts.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('And language detection multinomial naive Bayes is used when multiple occurrences of the words matter multinomial distribution normally requires integer feature counts.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('For example different algorithms are used to classify text documents and find the sentiment analysis.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('It is also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('This is where scikitlearn becomes more productive by creating the pipeline for the data set.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('It is also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('So using scikitlearn pipeline can be created and we can train the model using a single command.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('It is also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('But if specified the prior class is adjusted according to the data.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('The next feature of scikitlearn will be looking into is grid search.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('So using scikitlearn pipeline can be created and we can train the model using a single command.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('We use a document classifier to predict the category of a document to try predicting the outcome of a new document.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('We use a document classifier to predict the category of a document to try predicting the outcome of a new document.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('The documents are split into words or tokens and each document is assigned a number.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('We use a document classifier to predict the category of a document to try predicting the outcome of a new document.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('For example different algorithms are used to classify text documents and find the sentiment analysis.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('For example different algorithms are used to classify text documents and find the sentiment analysis.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('This is where scikitlearn becomes more productive by creating the pipeline for the data set.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('So using scikitlearn pipeline can be created and we can train the model using a single command.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('It would be too tedious to run through each parameter and tweak it in such cases a grid search useful the whole data set can be divided into multiple grids like a chessboard and the search can be run on entire grids or a selected combination of Grids, so we use a grid search.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('So using scikitlearn pipeline can be created and we can train the model using a single command.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('It would be too tedious to run through each parameter and tweak it in such cases a grid search useful the whole data set can be divided into multiple grids like a chessboard and the search can be run on entire grids or a selected combination of Grids, so we use a grid search.', '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a') ('We just need to put them all together in a single class and create a pipeline to operate on a given data set.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('Let is check for repeated words in the vocabulary that is built with the get function and then print it.', '2020-01-31T08:14:51Z', '716067a60a1a4034abc49a12ecafb39b', '7e4f326b-f583-4dd2-966c-0b383ec8629f') ('I can see the first number is the Tuple and the second number is the frequency of words the Tuple here indicates the document number and feature indices of each word, which belongs to the document feature indices are generated from the transform method.', '2020-01-31T08:14:21Z', '716067a60a1a4034abc49a12ecafb39b', '8f7b4256-2c12-41db-9683-3099c187d7ca')\n",
      "('Dont type in Python in Anaconda prompt command line type in Import in ltk here to install the Corpus Collections and models.', '2020-01-31T07:58:57Z', '716067a60a1a4034abc49a12ecafb39b', '7e939963-8230-4982-92da-54c57d8a27e5') ('Open the Anaconda prompt, please ensure that your system is connected to the internet.', '2020-01-31T07:58:33Z', '716067a60a1a4034abc49a12ecafb39b', '81dc51ae-5ea4-439b-93a6-ad30b59c17ea')\n",
      "('Dont type in Python in Anaconda prompt command line type in Import in ltk here to install the Corpus Collections and models.', '2020-01-31T07:58:57Z', '716067a60a1a4034abc49a12ecafb39b', '7e939963-8230-4982-92da-54c57d8a27e5') ('Are interested in installing stop words Corpus select it and then click download button.', '2020-01-31T07:59:11Z', '716067a60a1a4034abc49a12ecafb39b', '8dca9fee-5c8a-4714-80bd-e34aa2b246e2')\n",
      "('Search a model can have multiple parameters and it is a powerful way to search parameters affecting the outcome for model training purposes.', '2020-01-31T08:06:43Z', '716067a60a1a4034abc49a12ecafb39b', '80f384af-2e77-461e-b861-593c93a060b8') ('This is a technique in scikitlearn approach to streamline the NLP process in two stages.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('Search a model can have multiple parameters and it is a powerful way to search parameters affecting the outcome for model training purposes.', '2020-01-31T08:06:43Z', '716067a60a1a4034abc49a12ecafb39b', '80f384af-2e77-461e-b861-593c93a060b8') ('Let is take a look at the various stages of pipeline learning one vectorization.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('Search a model can have multiple parameters and it is a powerful way to search parameters affecting the outcome for model training purposes.', '2020-01-31T08:06:43Z', '716067a60a1a4034abc49a12ecafb39b', '80f384af-2e77-461e-b861-593c93a060b8') ('This is required for accurate predictions a model is divided into training and test data sets to optimize the overall process.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('It is one of the most common ways to adjust the extracted features.', '2020-01-31T08:06:43Z', '716067a60a1a4034abc49a12ecafb39b', '80f384af-2e77-461e-b861-593c93a060b8') ('Let is take a look at the various stages of pipeline learning one vectorization.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('It is one of the most common ways to adjust the extracted features.', '2020-01-31T08:06:43Z', '716067a60a1a4034abc49a12ecafb39b', '80f384af-2e77-461e-b861-593c93a060b8') ('This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('Open the Anaconda prompt, please ensure that your system is connected to the internet.', '2020-01-31T07:58:33Z', '716067a60a1a4034abc49a12ecafb39b', '81dc51ae-5ea4-439b-93a6-ad30b59c17ea') ('Are interested in installing stop words Corpus select it and then click download button.', '2020-01-31T07:59:11Z', '716067a60a1a4034abc49a12ecafb39b', '8dca9fee-5c8a-4714-80bd-e34aa2b246e2')\n",
      "('Open the Anaconda prompt, please ensure that your system is connected to the internet.', '2020-01-31T07:58:33Z', '716067a60a1a4034abc49a12ecafb39b', '81dc51ae-5ea4-439b-93a6-ad30b59c17ea') ('It will install the stop words Corpus in your python environment.', '2020-01-31T07:59:11Z', '716067a60a1a4034abc49a12ecafb39b', '8dca9fee-5c8a-4714-80bd-e34aa2b246e2')\n",
      "('Let is go ahead and enter conda install scikitlearn it will install the scikitlearn package enter conda install in ltk.', '2020-01-31T07:58:33Z', '716067a60a1a4034abc49a12ecafb39b', '81dc51ae-5ea4-439b-93a6-ad30b59c17ea') ('Are interested in installing stop words Corpus select it and then click download button.', '2020-01-31T07:59:11Z', '716067a60a1a4034abc49a12ecafb39b', '8dca9fee-5c8a-4714-80bd-e34aa2b246e2')\n",
      "('It will install the in ltk package into your python environment.', '2020-01-31T07:58:33Z', '716067a60a1a4034abc49a12ecafb39b', '81dc51ae-5ea4-439b-93a6-ad30b59c17ea') ('Are interested in installing stop words Corpus select it and then click download button.', '2020-01-31T07:59:11Z', '716067a60a1a4034abc49a12ecafb39b', '8dca9fee-5c8a-4714-80bd-e34aa2b246e2')\n",
      "('Let is start by importing the required Library pandas string print and time.', '2020-01-31T08:21:11Z', '716067a60a1a4034abc49a12ecafb39b', '867aad19-34c8-4652-972f-63e59155e294') ('This is where scikitlearn becomes more productive by creating the pipeline for the data set.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('Let is start by importing the required Library pandas string print and time.', '2020-01-31T08:21:11Z', '716067a60a1a4034abc49a12ecafb39b', '867aad19-34c8-4652-972f-63e59155e294') ('So using scikitlearn pipeline can be created and we can train the model using a single command.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67')\n",
      "('Let is learn about its builtin modules for loading the data set contents and categories.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('Let is take a look at the various stages of pipeline learning one vectorization.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('The diagram shows you how a data set is loaded using methods and how the object is structured in terms of the container folder and it is categories to load files.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('Bes CR function using this function you can view all of the information which describes the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('The diagram shows you how a data set is loaded using methods and how the object is structured in terms of the container folder and it is categories to load files.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('Next let us do the data set type use this syntax type digits data set as we can see the type of the data set is bunch to view the data of the digits data set apply the function data the output displays the actual data which is bound to the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('You have to use the main method scikitlearn datasets that load files.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('Next let us do the data set type use this syntax type digits data set as we can see the type of the data set is bunch to view the data of the digits data set apply the function data the output displays the actual data which is bound to the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('Youll need to use the sklearn feature extraction text module to build a feature extraction Transformer.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.', '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8')\n",
      "('A data load object helps load the contents of a data set shown here are the attributes of a data load object.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('The array is displayed here are the response data which is present in the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('Bunch it contains fields and can be accessed as dict keys or an object Target names.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('Next let us do the data set type use this syntax type digits data set as we can see the type of the data set is bunch to view the data of the digits data set apply the function data the output displays the actual data which is bound to the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('Bunch it contains fields and can be accessed as dict keys or an object Target names.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('The array is displayed here are the response data which is present in the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('Data, it refers to an attribute in the memory where files are loaded.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('Bes CR function using this function you can view all of the information which describes the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('Let is see how a data set can be loaded using scikitlearn.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('The array is displayed here are the response data which is present in the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('Let is load the digits data set to view the data using builtin methods perform.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('Bes CR function using this function you can view all of the information which describes the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('Let is load the digits data set to view the data using builtin methods perform.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('Next let us do the data set type use this syntax type digits data set as we can see the type of the data set is bunch to view the data of the digits data set apply the function data the output displays the actual data which is bound to the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('Let is load the digits data set to view the data using builtin methods perform.', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('The array is displayed here are the response data which is present in the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('to load the data set by creating an object as we had discussed earlier a data load object is used to load contents XnumberX describe the data set using', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('Bes CR function using this function you can view all of the information which describes the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('to load the data set by creating an object as we had discussed earlier a data load object is used to load contents XnumberX describe the data set using', '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff') ('The array is displayed here are the response data which is present in the data set.', '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b')\n",
      "('Import counter vectorizer to tokenize the document and convert the text into numeric values Also.', '2020-01-31T08:22:33Z', '716067a60a1a4034abc49a12ecafb39b', 'a190dbad-c600-4c59-8b8d-1ef16cd7ce90') ('Import scikitlearn specific libraries for text processing and creating a pipeline and performing The Grid search.', '2020-01-31T08:22:22Z', '716067a60a1a4034abc49a12ecafb39b', 'ae811bf9-4271-4986-8bf6-6a63982c4616')\n",
      "('Let is add some text to the documents document one can have Hi, how are you?', '2020-01-31T08:12:53Z', '716067a60a1a4034abc49a12ecafb39b', 'a3a11c95-03db-4d95-8feb-e66aaa606553') ('This demo you will learn how to process documents using bag of words.', '2020-01-31T08:12:21Z', '716067a60a1a4034abc49a12ecafb39b', 'c434f4e4-632a-4187-97b4-f03ec380a445')\n",
      "('Let is add some text to the documents document one can have Hi, how are you?', '2020-01-31T08:12:53Z', '716067a60a1a4034abc49a12ecafb39b', 'a3a11c95-03db-4d95-8feb-e66aaa606553') ('Class helps tokenize the document it converts the text into vectors by assigning numeric values to each word.', '2020-01-31T08:12:37Z', '716067a60a1a4034abc49a12ecafb39b', 'e75412de-1eff-43b9-a73e-1cdf925a5a17')\n",
      "('Import scikitlearn specific libraries for text processing and creating a pipeline and performing The Grid search.', '2020-01-31T08:22:22Z', '716067a60a1a4034abc49a12ecafb39b', 'ae811bf9-4271-4986-8bf6-6a63982c4616') ('Well get the Spam data collection with the help of the pandas Library.', '2020-01-31T08:21:38Z', '716067a60a1a4034abc49a12ecafb39b', 'f95338af-3e80-472e-a164-494372b849cf')\n",
      "('This demo you will learn how to process documents using bag of words.', '2020-01-31T08:12:21Z', '716067a60a1a4034abc49a12ecafb39b', 'c434f4e4-632a-4187-97b4-f03ec380a445') ('Class helps tokenize the document it converts the text into vectors by assigning numeric values to each word.', '2020-01-31T08:12:37Z', '716067a60a1a4034abc49a12ecafb39b', 'e75412de-1eff-43b9-a73e-1cdf925a5a17')\n",
      "('This is where scikitlearn becomes more productive by creating the pipeline for the data set.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67') ('Well get the Spam data collection with the help of the pandas Library.', '2020-01-31T08:21:38Z', '716067a60a1a4034abc49a12ecafb39b', 'f95338af-3e80-472e-a164-494372b849cf')\n",
      "('So using scikitlearn pipeline can be created and we can train the model using a single command.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67') ('Well get the Spam data collection with the help of the pandas Library.', '2020-01-31T08:21:38Z', '716067a60a1a4034abc49a12ecafb39b', 'f95338af-3e80-472e-a164-494372b849cf')\n",
      "('We just need to put them all together in a single class and create a pipeline to operate on a given data set.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67') ('Well get the Spam data collection with the help of the pandas Library.', '2020-01-31T08:21:38Z', '716067a60a1a4034abc49a12ecafb39b', 'f95338af-3e80-472e-a164-494372b849cf')\n",
      "('The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different.', '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67') ('Well get the Spam data collection with the help of the pandas Library.', '2020-01-31T08:21:38Z', '716067a60a1a4034abc49a12ecafb39b', 'f95338af-3e80-472e-a164-494372b849cf')\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 390, \"module\": \"grouper_segments\", \"outlier threshold is : \": 0.40235504508018494, \"ts\": \"2020-03-05T08:27:24.835851Z\", \"msg\": \"Outlier Score\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Community Algorithm\n",
      "cluster before alteration=========>\n",
      "Let is dive deeper into the NLP approaches to analyze text Data.\n",
      "These approaches can be interrelated or they can be independently applied depending on the type of data to analyze.\n",
      "Basic text processing it is a way to analyze text and extract keywords that sum up the style or basic context of the text.\n",
      "For example, if the content is religious or fictional categorizing and tagging words.\n",
      "This approach is about finding lexical categories and automatically tagging each word with this word class.\n",
      "For example, tag a word using languages such as Chinese Spanish Etc.\n",
      "Words can also be tagged as adjectives verbs nouns and so on.\n",
      "Classify text with this approach you can identify particular features of language and use them to classify it for example classify the text as Sports politics or technology.\n",
      "Extract information this approach is about identifying the entities and relationships in a text to extract information in a structured way for example date time money and a direction can be used to establish relationships with other words available in the text.\n",
      "It is time to make yourself comfortable with the NLP terminologies one word boundaries.\n",
      "It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.\n",
      "We get an insight into grammatical categories of the text document.\n",
      "For example, the text features of text based on speech tag or some grammar rules.\n",
      "It is a type of statistical model for finding abstract topics which occur in a collection of documents.\n",
      "Analyze the meaning perform quantitative analysis of the given set of data to extract the information for example find entities in the text and try to establish a relationship between them.\n",
      "It is a technique to split words phrases idioms Etc present in a document.\n",
      "Wise sentence structure in this approach capture formal grammar to describe the structure of a set of sentences.\n",
      "It determines where one word ends and the other begins to tokenization.\n",
      "It is very useful in finding synonyms and extensively used in search engines.\n",
      "It is a process to map word to their stem or root.\n",
      "P terminology now that you have understood why NLP is so important in recent times.\n",
      "Sometimes it is also referred to as a field of computer science or artificial intelligence to extract the Linguistics information from the underlying data in LP enables machines or computers to derive meaning from human or natural language input.\n",
      "handling ambiguities while interpreting data and extracting information This is where natural language processing proves useful.\n",
      "With the help of these libraries we can also build analytical models and automate the natural language process with minimum or no human interventions.\n",
      "It is a technique to determine the meaning and a sense of words context versus intent XnumberX topic models.\n",
      "Troll language processing or NLP is an automated way to understand analyze human languages and extract information from such data by applying machine learning algorithms.\n",
      "Tfidf it is a numerical value which represents how important the word is to a document or Corpus.\n",
      "The data content can be text document image audio or video.\n",
      "This has resulted in the high volume of Digital Data across the world and has led to a number of challenges in analyzing data including analyzing the tons of data that is generated in the form of text image audios and videos.\n",
      "The world is now connected globally due to the advancement of technology and devices.\n",
      "Identifying approximately XnumberX,XnumberX languages and dialects followed across the globe applying quantitative analysis on huge collections of data.\n",
      "One of the main goals of natural language processing is to understand various languages process them and extract information from them in NLP full automation can be easily achieved by modern software libraries modules and packages these software libraries and packages are aware of diverse language and culture and categorize data accordingly, which enables understanding Cinema it is better.\n",
      "cluster before alteration=========>\n",
      "For example in this stage different algorithms are used to classify text documents and find the sentiment analysis.\n",
      "These are some of the essential features of scikitlearn approach builtin modules.\n",
      "Feature extraction it is a way to extract information from data which can be text or images scikitlearn,  is builtin functions and methods help extract features and attributes from text to data and image data for the purpose of analysis.\n",
      "It is one of the most common ways to adjust the extracted features.\n",
      "This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.\n",
      "It is a powerful library with a set of modules to process and analyze natural language data such as text and images and extract information using machine learning algorithms.\n",
      "It has builtin modules to load the data sets content and categories will discuss the modules in detail in the upcoming screen.\n",
      "Has and response in supervised learning we make use of all three tasks predict the outcome of new observations and data sets understand which predictors affect the response or outcome and generalize the data and find the right answer generalization is also known as making predictions models can be trained to classify the documents based on the features and response.\n",
      "So here we try to train the model by following a few steps such as understanding the structure of the data first and identify the pattern in the data finding predictors that behave in the same fashion or have some familiarity exploring the data set and the goal is representation of the data.\n",
      "Search a model can have multiple parameters and it is a powerful way to search parameters affecting the outcome for model training purposes.\n",
      "Next feature of scikitlearn is model training an important task in model training is to identify the right model for the given data set scikitlearn provides a range of models to choose from and also trains them by using the extracted features from the data set.\n",
      "model training in model training we analyze the content based on particular categories and then train them according to a specific model model training can involve supervised or unsupervised models supervised models and supervised models.\n",
      "The goal is to generate the data and find the right answer in this model training the outcome of new observation and data set is predicted.\n",
      "Unsupervised models in this unsupervised models the response outcome or the label of the data is not known the main objective is to First understand the structure of the data and then identify the pattern in the data in this type of model training.\n",
      "We have to find the predictors that behave in the same fashion or have some familiarity.\n",
      "This is a technique in scikitlearn approach to streamline the NLP process in two stages.\n",
      "Let is take a look at the various stages of pipeline learning one vectorization.\n",
      "The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features.\n",
      "Unsupervised learning model in this type of learning the response or the outcome of the label of the data is not known there is no definite or right answer in this model.\n",
      "This is required for accurate predictions a model is divided into training and test data sets to optimize the overall process.\n",
      "The choice of model completely depends on the type of data set.\n",
      "Rotation is also known as extracting structure unsupervised model can also be used to group documents by applying clustering algorithms.\n",
      "To transformation and transformation the approaches to extract features around the word of Interest also in this stage you find the occurrence of each word in a document.\n",
      "The documents are split into words or tokens and each document is assigned a number.\n",
      "cluster before alteration=========>\n",
      "For example different algorithms are used to classify text documents and find the sentiment analysis.\n",
      "But if specified the prior class is adjusted according to the data.\n",
      "So using scikitlearn pipeline can be created and we can train the model using a single command.\n",
      "This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.\n",
      "This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory.\n",
      "Search consumes a lot of memory and in jobs equals negative one instructs the process to use all CPUs while processing.\n",
      "An ism to perform an exhaustive search on the best parameters which affect the model however this approach has some constraints such as the whole grid search process is subjected to availability of the CPU cores of the system.\n",
      "It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore.\n",
      "The next feature of scikitlearn will be looking into is grid search.\n",
      "A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.\n",
      "It would be too tedious to run through each parameter and tweak it in such cases a grid search useful the whole data set can be divided into multiple grids like a chessboard and the search can be run on entire grids or a selected combination of Grids, so we use a grid search.\n",
      "The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different.\n",
      "Search is a CPU intensive process and it takes a little time since it does an exhaustive search on the entire data set on the passed parameters.\n",
      "Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance.\n",
      "This is where scikitlearn becomes more productive by creating the pipeline for the data set.\n",
      "It is fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents.\n",
      "We use a document classifier to predict the category of a document to try predicting the outcome of a new document.\n",
      "We just need to put them all together in a single class and create a pipeline to operate on a given data set.\n",
      "It is also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior.\n",
      "And language detection multinomial naive Bayes is used when multiple occurrences of the words matter multinomial distribution normally requires integer feature counts.\n",
      "Not if false a uniform prior will be used but as you notice the default is true class prior.\n",
      "Far we have learned that there are three major steps for analyzing text Data one vectorizer.\n",
      "The documents are split into words or tokens and each document is assigned a number.\n",
      "Two Transformer during transformation you find the occurrence of each word in a document.\n",
      "cluster before alteration=========>\n",
      "is to decode the input strip accents it removes accent present in the code.\n",
      "Tokenizer it overrides this string tokenizer method, but the default value is none.\n",
      "It indicates the terms or words that appear more than a given threshold value and should therefore be ignored.\n",
      "Men threshold, it indicates the terms or words that appear less than a given threshold value and should therefore be ignored.\n",
      "Let is start by importing the required Library count vectorizer class.\n",
      "Class helps tokenize the document it converts the text into vectors by assigning numeric values to each word.\n",
      "This demo you will learn how to process documents using bag of words.\n",
      "Next import grid search from the sklearn grid search class to perform the grid search.\n",
      "Let is create an object grid search and pass pipeline parameters jobs and verbose as arguments.\n",
      "Print the parameters and run the grid search on the data set for both the message and response after fitting them into the grid search object using the fit method.\n",
      "Create a pipeline by instantiating the pipeline class and passing vectorize transformed and model classifier has argument.\n",
      "In this demo, you learned how to use bag of words technique and transformed documents using the transform method.\n",
      "Document to we can add today is a very very very pleasant day and we can have some fun fun fun.\n",
      "And in document XnumberX we can say this was an amazing experience.\n",
      "Let is add some text to the documents document one can have Hi, how are you?\n",
      "Let is create an object vectorizer by instantiating the count vectorizer class.\n",
      "cluster before alteration=========>\n",
      "Let is see how it works by considering a document where we need to extract text features the steps to follow our assign a fixed integer ID to each word by splitting them into several words also known as tokenizing then count the number of occurrences of each word or token finally store it as the value feature in Matrix format.\n",
      "The goal is to convert a collection of text documents to a matrix of token counts.\n",
      "Let is take a look at the components of the signature input content.\n",
      "The input content can be a file name of the sequence of strings which needs to be vectorized.\n",
      "Now, let us learn about the bag of Words, which is one of the most common text feature extraction techniques.\n",
      "Let is talk about feature extraction another functionality of the scikitlearn approach.\n",
      "This is a technique to convert the content into numerical vectors to perform machine learning.\n",
      "The feature extraction technique is used mostly in machine learning while dealing with text or image data.\n",
      "The text feature extraction it is used to extract features from text.\n",
      "Data examples include large data sets or documents image feature extraction.\n",
      "It is used to extract image features examples include patch extraction or connectivity graph of an image forming contiguous patches also known as hierarchical clustering bag of words.\n",
      "It is used to convert text Data into numerical feature vectors with a fixed size.\n",
      "The Matrix presentation on the screen shows how multiple documents and tokens are structured This screen shows have account vectorizer algorithm works in scikitlearn.\n",
      "cluster before alteration=========>\n",
      "Well use basic python syntax to display the characters which do not have punctuation as you can see we make use of basic string class and its punctuation function.\n",
      "Well use string class punctuation to remove all the punctuation present in each message as they are also less waited for text analysis, then we will remove the stop words.\n",
      "World of text analysis stop words usually have little lexical meaning some examples of stop words.\n",
      "Now let us view the first XnumberX stop words present in the stop words Corpus of in ltk.\n",
      "It is a pythonbased library for NLP and widely used in the industry to build programs to work with different human languages.\n",
      "Let is remove the stop words will use stop words Corpus dot words function with English as an argument and basic python word and split function to clean each word.\n",
      "It is another powerful open source python package and module XnumberX in LP.\n",
      "It features various algorithms and is designed for operating with other python libraries like numpy and acai pie.\n",
      "Text blob this library is used for processing Text data and provides simple apis for diving into NLP.\n",
      "We can see that the list contains a lot of stop words.\n",
      "It is analysis and our sentences free of punctuation and stop words following are the major in alpide libraries used in Python in LT Kang.\n",
      "In this demo, you will learn how to perform sentence analysis.\n",
      "Spacing this is another Library which provides multiple useful views of textual meaning and linguistic structure.\n",
      "Are I me myself we our hours you yours and so on.\n",
      "This is done by passing XnumberX to XnumberX index position in the parentheses.\n",
      "cluster before alteration=========>\n",
      "Let is create its head sequence to analyze will create a variable test sentence and passed the sentence.\n",
      "Let is check for repeated words in the vocabulary that is built with the get function and then print it.\n",
      "This is a feature extraction process and to extract the features.\n",
      "Let is print the bag of words hand look at the details.\n",
      "Next comes the interesting part which is creating the bag of words for the list of documents.\n",
      "I can see the first number is the Tuple and the second number is the frequency of words the Tuple here indicates the document number and feature indices of each word, which belongs to the document feature indices are generated from the transform method.\n",
      "This is done by accessing the fit method and fitting the documents into the vectorized object.\n",
      "You have to refer to the indexes which is assigned to any particular feature.\n",
      "cluster before alteration=========>\n",
      "Then import Stop Words, which we have installed earlier as part of the environment setup.\n",
      "Open the Anaconda prompt, please ensure that your system is connected to the internet.\n",
      "Let is navigate through the other models and all packages ensure that stop words status is installed.\n",
      "Let is start by importing the required Library class string from in ltk Corpus.\n",
      "Are interested in installing stop words Corpus select it and then click download button.\n",
      "It will install the stop words Corpus in your python environment.\n",
      "Dont type in Python in Anaconda prompt command line type in Import in ltk here to install the Corpus Collections and models.\n",
      "Import scikitlearn specific libraries for text processing and creating a pipeline and performing The Grid search.\n",
      "It will install the in ltk package into your python environment.\n",
      "Let is start by importing the required Library pandas string print and time.\n",
      "You can close the window and go back to jupyter notebook environment.\n",
      "Also message is a feature which refers to the text present in the data set response is the label or the category of the message.\n",
      "The backslash T indicates that data in the spam data set is tab separated or Tab delimited and set function takes it as a parameter.\n",
      "Let is view the first five records of the stamp collection.\n",
      "Well get the Spam data collection with the help of the pandas Library.\n",
      "Import counter vectorizer to tokenize the document and convert the text into numeric values Also.\n",
      "Let is go ahead and enter conda install scikitlearn it will install the scikitlearn package enter conda install in ltk.\n",
      "We just completed the first step of the Senate is analysis process.\n",
      "cluster before alteration=========>\n",
      "Let is learn about its builtin modules for loading the data set contents and categories.\n",
      "And as supervised signal label names, it does not try to extract features into a numpy array or scifi sparse Matrix.\n",
      "Youll need to use the sklearn feature extraction text module to build a feature extraction Transformer.\n",
      "Let is see how a data set can be loaded using scikitlearn.\n",
      "The diagram shows you how a data set is loaded using methods and how the object is structured in terms of the container folder and it is categories to load files.\n",
      "A data load object helps load the contents of a data set shown here are the attributes of a data load object.\n",
      "Data, it refers to an attribute in the memory where files are loaded.\n",
      "The array is displayed here are the response data which is present in the data set.\n",
      "It does not try to load the files in memory to use text files in a scikitlearn classification or clustering algorithm.\n",
      "The head method is used to view the topmost records in a data set.\n",
      "Let is load the digits data set to view the data using builtin methods perform.\n",
      "Bes CR function using this function you can view all of the information which describes the data set.\n",
      "We can load txt files with categories as sub folder names.\n",
      "to load the data set by creating an object as we had discussed earlier a data load object is used to load contents XnumberX describe the data set using\n",
      "The following steps One Import the desired data set use the syntax from sklearn datasets import load digits.\n",
      "You have to use the main method scikitlearn datasets that load files.\n",
      "Bunch it contains fields and can be accessed as dict keys or an object Target names.\n",
      "Next let us do the data set type use this syntax type digits data set as we can see the type of the data set is bunch to view the data of the digits data set apply the function data the output displays the actual data which is bound to the data set.\n",
      "cluster before alteration=========>\n",
      "That you see here helps create the pipeline for the entire text processing.\n",
      "cluster before alteration=========>\n",
      "Bond to accessing the transform method and transform the list of documents.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Let is dive deeper into the NLP approaches to analyze text Data.\n",
      "These approaches can be interrelated or they can be independently applied depending on the type of data to analyze.\n",
      "Basic text processing it is a way to analyze text and extract keywords that sum up the style or basic context of the text.\n",
      "For example, if the content is religious or fictional categorizing and tagging words.\n",
      "This approach is about finding lexical categories and automatically tagging each word with this word class.\n",
      "For example, tag a word using languages such as Chinese Spanish Etc.\n",
      "Words can also be tagged as adjectives verbs nouns and so on.\n",
      "Classify text with this approach you can identify particular features of language and use them to classify it for example classify the text as Sports politics or technology.\n",
      "Extract information this approach is about identifying the entities and relationships in a text to extract information in a structured way for example date time money and a direction can be used to establish relationships with other words available in the text.\n",
      "It is time to make yourself comfortable with the NLP terminologies one word boundaries.\n",
      "It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.\n",
      "We get an insight into grammatical categories of the text document.\n",
      "For example, the text features of text based on speech tag or some grammar rules.\n",
      "It is a type of statistical model for finding abstract topics which occur in a collection of documents.\n",
      "Analyze the meaning perform quantitative analysis of the given set of data to extract the information for example find entities in the text and try to establish a relationship between them.\n",
      "It is a technique to split words phrases idioms Etc present in a document.\n",
      "Wise sentence structure in this approach capture formal grammar to describe the structure of a set of sentences.\n",
      "It determines where one word ends and the other begins to tokenization.\n",
      "It is very useful in finding synonyms and extensively used in search engines.\n",
      "It is a process to map word to their stem or root.\n",
      "P terminology now that you have understood why NLP is so important in recent times.\n",
      "Sometimes it is also referred to as a field of computer science or artificial intelligence to extract the Linguistics information from the underlying data in LP enables machines or computers to derive meaning from human or natural language input.\n",
      "handling ambiguities while interpreting data and extracting information This is where natural language processing proves useful.\n",
      "With the help of these libraries we can also build analytical models and automate the natural language process with minimum or no human interventions.\n",
      "It is a technique to determine the meaning and a sense of words context versus intent XnumberX topic models.\n",
      "Troll language processing or NLP is an automated way to understand analyze human languages and extract information from such data by applying machine learning algorithms.\n",
      "Tfidf it is a numerical value which represents how important the word is to a document or Corpus.\n",
      "The data content can be text document image audio or video.\n",
      "This has resulted in the high volume of Digital Data across the world and has led to a number of challenges in analyzing data including analyzing the tons of data that is generated in the form of text image audios and videos.\n",
      "The world is now connected globally due to the advancement of technology and devices.\n",
      "Identifying approximately XnumberX,XnumberX languages and dialects followed across the globe applying quantitative analysis on huge collections of data.\n",
      "One of the main goals of natural language processing is to understand various languages process them and extract information from them in NLP full automation can be easily achieved by modern software libraries modules and packages these software libraries and packages are aware of diverse language and culture and categorize data accordingly, which enables understanding Cinema it is better.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "For example in this stage different algorithms are used to classify text documents and find the sentiment analysis.\n",
      "These are some of the essential features of scikitlearn approach builtin modules.\n",
      "Feature extraction it is a way to extract information from data which can be text or images scikitlearn,  is builtin functions and methods help extract features and attributes from text to data and image data for the purpose of analysis.\n",
      "It is one of the most common ways to adjust the extracted features.\n",
      "This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.\n",
      "It is a powerful library with a set of modules to process and analyze natural language data such as text and images and extract information using machine learning algorithms.\n",
      "It has builtin modules to load the data sets content and categories will discuss the modules in detail in the upcoming screen.\n",
      "Has and response in supervised learning we make use of all three tasks predict the outcome of new observations and data sets understand which predictors affect the response or outcome and generalize the data and find the right answer generalization is also known as making predictions models can be trained to classify the documents based on the features and response.\n",
      "So here we try to train the model by following a few steps such as understanding the structure of the data first and identify the pattern in the data finding predictors that behave in the same fashion or have some familiarity exploring the data set and the goal is representation of the data.\n",
      "Search a model can have multiple parameters and it is a powerful way to search parameters affecting the outcome for model training purposes.\n",
      "Next feature of scikitlearn is model training an important task in model training is to identify the right model for the given data set scikitlearn provides a range of models to choose from and also trains them by using the extracted features from the data set.\n",
      "model training in model training we analyze the content based on particular categories and then train them according to a specific model model training can involve supervised or unsupervised models supervised models and supervised models.\n",
      "The goal is to generate the data and find the right answer in this model training the outcome of new observation and data set is predicted.\n",
      "Unsupervised models in this unsupervised models the response outcome or the label of the data is not known the main objective is to First understand the structure of the data and then identify the pattern in the data in this type of model training.\n",
      "We have to find the predictors that behave in the same fashion or have some familiarity.\n",
      "This is a technique in scikitlearn approach to streamline the NLP process in two stages.\n",
      "Let is take a look at the various stages of pipeline learning one vectorization.\n",
      "The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features.\n",
      "Unsupervised learning model in this type of learning the response or the outcome of the label of the data is not known there is no definite or right answer in this model.\n",
      "This is required for accurate predictions a model is divided into training and test data sets to optimize the overall process.\n",
      "The choice of model completely depends on the type of data set.\n",
      "Rotation is also known as extracting structure unsupervised model can also be used to group documents by applying clustering algorithms.\n",
      "To transformation and transformation the approaches to extract features around the word of Interest also in this stage you find the occurrence of each word in a document.\n",
      "The documents are split into words or tokens and each document is assigned a number.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "For example different algorithms are used to classify text documents and find the sentiment analysis.\n",
      "But if specified the prior class is adjusted according to the data.\n",
      "So using scikitlearn pipeline can be created and we can train the model using a single command.\n",
      "This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.\n",
      "This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory.\n",
      "Search consumes a lot of memory and in jobs equals negative one instructs the process to use all CPUs while processing.\n",
      "An ism to perform an exhaustive search on the best parameters which affect the model however this approach has some constraints such as the whole grid search process is subjected to availability of the CPU cores of the system.\n",
      "It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore.\n",
      "The next feature of scikitlearn will be looking into is grid search.\n",
      "A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.\n",
      "It would be too tedious to run through each parameter and tweak it in such cases a grid search useful the whole data set can be divided into multiple grids like a chessboard and the search can be run on entire grids or a selected combination of Grids, so we use a grid search.\n",
      "The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different.\n",
      "Search is a CPU intensive process and it takes a little time since it does an exhaustive search on the entire data set on the passed parameters.\n",
      "Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance.\n",
      "This is where scikitlearn becomes more productive by creating the pipeline for the data set.\n",
      "It is fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents.\n",
      "We use a document classifier to predict the category of a document to try predicting the outcome of a new document.\n",
      "We just need to put them all together in a single class and create a pipeline to operate on a given data set.\n",
      "It is also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior.\n",
      "And language detection multinomial naive Bayes is used when multiple occurrences of the words matter multinomial distribution normally requires integer feature counts.\n",
      "Not if false a uniform prior will be used but as you notice the default is true class prior.\n",
      "Far we have learned that there are three major steps for analyzing text Data one vectorizer.\n",
      "The documents are split into words or tokens and each document is assigned a number.\n",
      "Two Transformer during transformation you find the occurrence of each word in a document.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "is to decode the input strip accents it removes accent present in the code.\n",
      "Tokenizer it overrides this string tokenizer method, but the default value is none.\n",
      "It indicates the terms or words that appear more than a given threshold value and should therefore be ignored.\n",
      "Men threshold, it indicates the terms or words that appear less than a given threshold value and should therefore be ignored.\n",
      "Let is start by importing the required Library count vectorizer class.\n",
      "Class helps tokenize the document it converts the text into vectors by assigning numeric values to each word.\n",
      "This demo you will learn how to process documents using bag of words.\n",
      "Next import grid search from the sklearn grid search class to perform the grid search.\n",
      "Let is create an object grid search and pass pipeline parameters jobs and verbose as arguments.\n",
      "Print the parameters and run the grid search on the data set for both the message and response after fitting them into the grid search object using the fit method.\n",
      "Create a pipeline by instantiating the pipeline class and passing vectorize transformed and model classifier has argument.\n",
      "In this demo, you learned how to use bag of words technique and transformed documents using the transform method.\n",
      "Document to we can add today is a very very very pleasant day and we can have some fun fun fun.\n",
      "And in document XnumberX we can say this was an amazing experience.\n",
      "Let is add some text to the documents document one can have Hi, how are you?\n",
      "Let is create an object vectorizer by instantiating the count vectorizer class.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Let is see how it works by considering a document where we need to extract text features the steps to follow our assign a fixed integer ID to each word by splitting them into several words also known as tokenizing then count the number of occurrences of each word or token finally store it as the value feature in Matrix format.\n",
      "The goal is to convert a collection of text documents to a matrix of token counts.\n",
      "Let is take a look at the components of the signature input content.\n",
      "The input content can be a file name of the sequence of strings which needs to be vectorized.\n",
      "Now, let us learn about the bag of Words, which is one of the most common text feature extraction techniques.\n",
      "Let is talk about feature extraction another functionality of the scikitlearn approach.\n",
      "This is a technique to convert the content into numerical vectors to perform machine learning.\n",
      "The feature extraction technique is used mostly in machine learning while dealing with text or image data.\n",
      "The text feature extraction it is used to extract features from text.\n",
      "Data examples include large data sets or documents image feature extraction.\n",
      "It is used to extract image features examples include patch extraction or connectivity graph of an image forming contiguous patches also known as hierarchical clustering bag of words.\n",
      "It is used to convert text Data into numerical feature vectors with a fixed size.\n",
      "The Matrix presentation on the screen shows how multiple documents and tokens are structured This screen shows have account vectorizer algorithm works in scikitlearn.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Well use basic python syntax to display the characters which do not have punctuation as you can see we make use of basic string class and its punctuation function.\n",
      "Well use string class punctuation to remove all the punctuation present in each message as they are also less waited for text analysis, then we will remove the stop words.\n",
      "World of text analysis stop words usually have little lexical meaning some examples of stop words.\n",
      "Now let us view the first XnumberX stop words present in the stop words Corpus of in ltk.\n",
      "It is a pythonbased library for NLP and widely used in the industry to build programs to work with different human languages.\n",
      "Let is remove the stop words will use stop words Corpus dot words function with English as an argument and basic python word and split function to clean each word.\n",
      "It is another powerful open source python package and module XnumberX in LP.\n",
      "It features various algorithms and is designed for operating with other python libraries like numpy and acai pie.\n",
      "Text blob this library is used for processing Text data and provides simple apis for diving into NLP.\n",
      "We can see that the list contains a lot of stop words.\n",
      "It is analysis and our sentences free of punctuation and stop words following are the major in alpide libraries used in Python in LT Kang.\n",
      "Spacing this is another Library which provides multiple useful views of textual meaning and linguistic structure.\n",
      "Are I me myself we our hours you yours and so on.\n",
      "This is done by passing XnumberX to XnumberX index position in the parentheses.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Let is create its head sequence to analyze will create a variable test sentence and passed the sentence.\n",
      "Let is check for repeated words in the vocabulary that is built with the get function and then print it.\n",
      "This is a feature extraction process and to extract the features.\n",
      "Let is print the bag of words hand look at the details.\n",
      "Next comes the interesting part which is creating the bag of words for the list of documents.\n",
      "I can see the first number is the Tuple and the second number is the frequency of words the Tuple here indicates the document number and feature indices of each word, which belongs to the document feature indices are generated from the transform method.\n",
      "This is done by accessing the fit method and fitting the documents into the vectorized object.\n",
      "You have to refer to the indexes which is assigned to any particular feature.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Then import Stop Words, which we have installed earlier as part of the environment setup.\n",
      "Open the Anaconda prompt, please ensure that your system is connected to the internet.\n",
      "Let is navigate through the other models and all packages ensure that stop words status is installed.\n",
      "Let is start by importing the required Library class string from in ltk Corpus.\n",
      "Are interested in installing stop words Corpus select it and then click download button.\n",
      "It will install the stop words Corpus in your python environment.\n",
      "Dont type in Python in Anaconda prompt command line type in Import in ltk here to install the Corpus Collections and models.\n",
      "Import scikitlearn specific libraries for text processing and creating a pipeline and performing The Grid search.\n",
      "It will install the in ltk package into your python environment.\n",
      "Let is start by importing the required Library pandas string print and time.\n",
      "You can close the window and go back to jupyter notebook environment.\n",
      "Also message is a feature which refers to the text present in the data set response is the label or the category of the message.\n",
      "The backslash T indicates that data in the spam data set is tab separated or Tab delimited and set function takes it as a parameter.\n",
      "Let is view the first five records of the stamp collection.\n",
      "Well get the Spam data collection with the help of the pandas Library.\n",
      "Import counter vectorizer to tokenize the document and convert the text into numeric values Also.\n",
      "Let is go ahead and enter conda install scikitlearn it will install the scikitlearn package enter conda install in ltk.\n",
      "We just completed the first step of the Senate is analysis process.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Let is learn about its builtin modules for loading the data set contents and categories.\n",
      "And as supervised signal label names, it does not try to extract features into a numpy array or scifi sparse Matrix.\n",
      "Youll need to use the sklearn feature extraction text module to build a feature extraction Transformer.\n",
      "Let is see how a data set can be loaded using scikitlearn.\n",
      "The diagram shows you how a data set is loaded using methods and how the object is structured in terms of the container folder and it is categories to load files.\n",
      "A data load object helps load the contents of a data set shown here are the attributes of a data load object.\n",
      "Data, it refers to an attribute in the memory where files are loaded.\n",
      "The array is displayed here are the response data which is present in the data set.\n",
      "It does not try to load the files in memory to use text files in a scikitlearn classification or clustering algorithm.\n",
      "Let is load the digits data set to view the data using builtin methods perform.\n",
      "Bes CR function using this function you can view all of the information which describes the data set.\n",
      "We can load txt files with categories as sub folder names.\n",
      "to load the data set by creating an object as we had discussed earlier a data load object is used to load contents XnumberX describe the data set using\n",
      "The following steps One Import the desired data set use the syntax from sklearn datasets import load digits.\n",
      "You have to use the main method scikitlearn datasets that load files.\n",
      "Bunch it contains fields and can be accessed as dict keys or an object Target names.\n",
      "Next let us do the data set type use this syntax type digits data set as we can see the type of the data set is bunch to view the data of the digits data set apply the function data the output displays the actual data which is bound to the data set.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "That you see here helps create the pipeline for the entire text processing.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Bond to accessing the transform method and transform the list of documents.\n",
      "\n",
      "\n",
      "Checking Timerange --------------\n",
      "--------------\n",
      "The data content can be text document image audio or video. 281d85ad-92c3-48e8-8b06-43e8dc623374 \n",
      "\n",
      "With the help of these libraries we can also build analytical models and automate the natural language process with minimum or no human interventions. 281d85ad-92c3-48e8-8b06-43e8dc623374 \n",
      "\n",
      "Troll language processing or NLP is an automated way to understand analyze human languages and extract information from such data by applying machine learning algorithms. 281d85ad-92c3-48e8-8b06-43e8dc623374 \n",
      "\n",
      "This has resulted in the high volume of Digital Data across the world and has led to a number of challenges in analyzing data including analyzing the tons of data that is generated in the form of text image audios and videos. 281d85ad-92c3-48e8-8b06-43e8dc623374 \n",
      "\n",
      "Identifying approximately XnumberX,XnumberX languages and dialects followed across the globe applying quantitative analysis on huge collections of data. 281d85ad-92c3-48e8-8b06-43e8dc623374 \n",
      "\n",
      "Sometimes it is also referred to as a field of computer science or artificial intelligence to extract the Linguistics information from the underlying data in LP enables machines or computers to derive meaning from human or natural language input. 281d85ad-92c3-48e8-8b06-43e8dc623374 \n",
      "\n",
      "handling ambiguities while interpreting data and extracting information This is where natural language processing proves useful. 281d85ad-92c3-48e8-8b06-43e8dc623374 \n",
      "\n",
      "One of the main goals of natural language processing is to understand various languages process them and extract information from them in NLP full automation can be easily achieved by modern software libraries modules and packages these software libraries and packages are aware of diverse language and culture and categorize data accordingly, which enables understanding Cinema it is better. 281d85ad-92c3-48e8-8b06-43e8dc623374 \n",
      "\n",
      "The world is now connected globally due to the advancement of technology and devices. 281d85ad-92c3-48e8-8b06-43e8dc623374 \n",
      "\n",
      "It is a technique to split words phrases idioms Etc present in a document. 0ca53093-564d-47fa-8874-336d7c86ef9b \n",
      "\n",
      "It is very useful in finding synonyms and extensively used in search engines. 0ca53093-564d-47fa-8874-336d7c86ef9b \n",
      "\n",
      "P terminology now that you have understood why NLP is so important in recent times. 0ca53093-564d-47fa-8874-336d7c86ef9b \n",
      "\n",
      "It is a process to map word to their stem or root. 0ca53093-564d-47fa-8874-336d7c86ef9b \n",
      "\n",
      "It is time to make yourself comfortable with the NLP terminologies one word boundaries. 0ca53093-564d-47fa-8874-336d7c86ef9b \n",
      "\n",
      "It determines where one word ends and the other begins to tokenization. 0ca53093-564d-47fa-8874-336d7c86ef9b \n",
      "\n",
      "Tfidf it is a numerical value which represents how important the word is to a document or Corpus. 54736d9f-7cb6-4251-912a-632a480c81bb \n",
      "\n",
      "It is a type of statistical model for finding abstract topics which occur in a collection of documents. 54736d9f-7cb6-4251-912a-632a480c81bb \n",
      "\n",
      "It is a technique to determine the meaning and a sense of words context versus intent XnumberX topic models. 54736d9f-7cb6-4251-912a-632a480c81bb \n",
      "\n",
      "It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms. 54736d9f-7cb6-4251-912a-632a480c81bb \n",
      "\n",
      "These approaches can be interrelated or they can be independently applied depending on the type of data to analyze. 04e2080b-f2ab-4201-b4be-7bbd34173843 \n",
      "\n",
      "Extract information this approach is about identifying the entities and relationships in a text to extract information in a structured way for example date time money and a direction can be used to establish relationships with other words available in the text. 04e2080b-f2ab-4201-b4be-7bbd34173843 \n",
      "\n",
      "For example, tag a word using languages such as Chinese Spanish Etc. 04e2080b-f2ab-4201-b4be-7bbd34173843 \n",
      "\n",
      "Let is dive deeper into the NLP approaches to analyze text Data. 04e2080b-f2ab-4201-b4be-7bbd34173843 \n",
      "\n",
      "For example, if the content is religious or fictional categorizing and tagging words. 04e2080b-f2ab-4201-b4be-7bbd34173843 \n",
      "\n",
      "This approach is about finding lexical categories and automatically tagging each word with this word class. 04e2080b-f2ab-4201-b4be-7bbd34173843 \n",
      "\n",
      "Words can also be tagged as adjectives verbs nouns and so on. 04e2080b-f2ab-4201-b4be-7bbd34173843 \n",
      "\n",
      "Classify text with this approach you can identify particular features of language and use them to classify it for example classify the text as Sports politics or technology. 04e2080b-f2ab-4201-b4be-7bbd34173843 \n",
      "\n",
      "Basic text processing it is a way to analyze text and extract keywords that sum up the style or basic context of the text. 04e2080b-f2ab-4201-b4be-7bbd34173843 \n",
      "\n",
      "Wise sentence structure in this approach capture formal grammar to describe the structure of a set of sentences. c5a346ba-86c0-4c6c-8fcc-29a627310e3f \n",
      "\n",
      "We get an insight into grammatical categories of the text document. c5a346ba-86c0-4c6c-8fcc-29a627310e3f \n",
      "\n",
      "For example, the text features of text based on speech tag or some grammar rules. c5a346ba-86c0-4c6c-8fcc-29a627310e3f \n",
      "\n",
      "Analyze the meaning perform quantitative analysis of the given set of data to extract the information for example find entities in the text and try to establish a relationship between them. c5a346ba-86c0-4c6c-8fcc-29a627310e3f \n",
      "\n",
      "--------------\n",
      "It is a powerful library with a set of modules to process and analyze natural language data such as text and images and extract information using machine learning algorithms. 3a861e7b-1736-4924-9827-63f8266e3ad7 \n",
      "\n",
      "It has builtin modules to load the data sets content and categories will discuss the modules in detail in the upcoming screen. 3a861e7b-1736-4924-9827-63f8266e3ad7 \n",
      "\n",
      "model training in model training we analyze the content based on particular categories and then train them according to a specific model model training can involve supervised or unsupervised models supervised models and supervised models. 3a861e7b-1736-4924-9827-63f8266e3ad7 \n",
      "\n",
      "Feature extraction it is a way to extract information from data which can be text or images scikitlearn,  is builtin functions and methods help extract features and attributes from text to data and image data for the purpose of analysis. 3a861e7b-1736-4924-9827-63f8266e3ad7 \n",
      "\n",
      "Unsupervised models in this unsupervised models the response outcome or the label of the data is not known the main objective is to First understand the structure of the data and then identify the pattern in the data in this type of model training. 3a861e7b-1736-4924-9827-63f8266e3ad7 \n",
      "\n",
      "We have to find the predictors that behave in the same fashion or have some familiarity. 3a861e7b-1736-4924-9827-63f8266e3ad7 \n",
      "\n",
      "These are some of the essential features of scikitlearn approach builtin modules. 3a861e7b-1736-4924-9827-63f8266e3ad7 \n",
      "\n",
      "The goal is to generate the data and find the right answer in this model training the outcome of new observation and data set is predicted. 3a861e7b-1736-4924-9827-63f8266e3ad7 \n",
      "\n",
      "The documents are split into words or tokens and each document is assigned a number. aa217665-d5e0-487d-9259-540258ba91f8 \n",
      "\n",
      "To transformation and transformation the approaches to extract features around the word of Interest also in this stage you find the occurrence of each word in a document. aa217665-d5e0-487d-9259-540258ba91f8 \n",
      "\n",
      "For example in this stage different algorithms are used to classify text documents and find the sentiment analysis. aa217665-d5e0-487d-9259-540258ba91f8 \n",
      "\n",
      "This is a technique in scikitlearn approach to streamline the NLP process in two stages. aa217665-d5e0-487d-9259-540258ba91f8 \n",
      "\n",
      "This is required for accurate predictions a model is divided into training and test data sets to optimize the overall process. aa217665-d5e0-487d-9259-540258ba91f8 \n",
      "\n",
      "Let is take a look at the various stages of pipeline learning one vectorization. aa217665-d5e0-487d-9259-540258ba91f8 \n",
      "\n",
      "This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization. aa217665-d5e0-487d-9259-540258ba91f8 \n",
      "\n",
      "It is one of the most common ways to adjust the extracted features. 80f384af-2e77-461e-b861-593c93a060b8 \n",
      "\n",
      "Search a model can have multiple parameters and it is a powerful way to search parameters affecting the outcome for model training purposes. 80f384af-2e77-461e-b861-593c93a060b8 \n",
      "\n",
      "Has and response in supervised learning we make use of all three tasks predict the outcome of new observations and data sets understand which predictors affect the response or outcome and generalize the data and find the right answer generalization is also known as making predictions models can be trained to classify the documents based on the features and response. 5494a3cb-1575-4bee-a553-062406bc6ea4 \n",
      "\n",
      "So here we try to train the model by following a few steps such as understanding the structure of the data first and identify the pattern in the data finding predictors that behave in the same fashion or have some familiarity exploring the data set and the goal is representation of the data. 5494a3cb-1575-4bee-a553-062406bc6ea4 \n",
      "\n",
      "The choice of model completely depends on the type of data set. 5494a3cb-1575-4bee-a553-062406bc6ea4 \n",
      "\n",
      "Next feature of scikitlearn is model training an important task in model training is to identify the right model for the given data set scikitlearn provides a range of models to choose from and also trains them by using the extracted features from the data set. 5494a3cb-1575-4bee-a553-062406bc6ea4 \n",
      "\n",
      "Unsupervised learning model in this type of learning the response or the outcome of the label of the data is not known there is no definite or right answer in this model. 5494a3cb-1575-4bee-a553-062406bc6ea4 \n",
      "\n",
      "The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features. 5494a3cb-1575-4bee-a553-062406bc6ea4 \n",
      "\n",
      "Rotation is also known as extracting structure unsupervised model can also be used to group documents by applying clustering algorithms. 5494a3cb-1575-4bee-a553-062406bc6ea4 \n",
      "\n",
      "--------------\n",
      "It is fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents. 7999f002-6896-4e88-9c1c-96fa3d150b6a \n",
      "\n",
      "Not if false a uniform prior will be used but as you notice the default is true class prior. 7999f002-6896-4e88-9c1c-96fa3d150b6a \n",
      "\n",
      "We use a document classifier to predict the category of a document to try predicting the outcome of a new document. 7999f002-6896-4e88-9c1c-96fa3d150b6a \n",
      "\n",
      "And language detection multinomial naive Bayes is used when multiple occurrences of the words matter multinomial distribution normally requires integer feature counts. 7999f002-6896-4e88-9c1c-96fa3d150b6a \n",
      "\n",
      "It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore. 7999f002-6896-4e88-9c1c-96fa3d150b6a \n",
      "\n",
      "But if specified the prior class is adjusted according to the data. 7999f002-6896-4e88-9c1c-96fa3d150b6a \n",
      "\n",
      "It would be too tedious to run through each parameter and tweak it in such cases a grid search useful the whole data set can be divided into multiple grids like a chessboard and the search can be run on entire grids or a selected combination of Grids, so we use a grid search. 7999f002-6896-4e88-9c1c-96fa3d150b6a \n",
      "\n",
      "The next feature of scikitlearn will be looking into is grid search. 7999f002-6896-4e88-9c1c-96fa3d150b6a \n",
      "\n",
      "A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters. 7999f002-6896-4e88-9c1c-96fa3d150b6a \n",
      "\n",
      "It is also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior. 7999f002-6896-4e88-9c1c-96fa3d150b6a \n",
      "\n",
      "This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory. 38a75d62-8e9e-4ea6-aea7-58460c83cd6b \n",
      "\n",
      "An ism to perform an exhaustive search on the best parameters which affect the model however this approach has some constraints such as the whole grid search process is subjected to availability of the CPU cores of the system. 38a75d62-8e9e-4ea6-aea7-58460c83cd6b \n",
      "\n",
      "This is where scikitlearn becomes more productive by creating the pipeline for the data set. c920660d-4b47-42cb-891a-8685f93b1d67 \n",
      "\n",
      "This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization. c920660d-4b47-42cb-891a-8685f93b1d67 \n",
      "\n",
      "The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different. c920660d-4b47-42cb-891a-8685f93b1d67 \n",
      "\n",
      "Far we have learned that there are three major steps for analyzing text Data one vectorizer. c920660d-4b47-42cb-891a-8685f93b1d67 \n",
      "\n",
      "We just need to put them all together in a single class and create a pipeline to operate on a given data set. c920660d-4b47-42cb-891a-8685f93b1d67 \n",
      "\n",
      "Two Transformer during transformation you find the occurrence of each word in a document. c920660d-4b47-42cb-891a-8685f93b1d67 \n",
      "\n",
      "The documents are split into words or tokens and each document is assigned a number. c920660d-4b47-42cb-891a-8685f93b1d67 \n",
      "\n",
      "For example different algorithms are used to classify text documents and find the sentiment analysis. c920660d-4b47-42cb-891a-8685f93b1d67 \n",
      "\n",
      "Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance. c920660d-4b47-42cb-891a-8685f93b1d67 \n",
      "\n",
      "So using scikitlearn pipeline can be created and we can train the model using a single command. c920660d-4b47-42cb-891a-8685f93b1d67 \n",
      "\n",
      "Search consumes a lot of memory and in jobs equals negative one instructs the process to use all CPUs while processing. 5a3bb0e1-3b96-4e7e-a4e9-45eee2c24343 \n",
      "\n",
      "Search is a CPU intensive process and it takes a little time since it does an exhaustive search on the entire data set on the passed parameters. e05cb905-601b-471d-ba02-512a4f07c074 \n",
      "\n",
      "--------------\n",
      "is to decode the input strip accents it removes accent present in the code. 07d2bcc9-3ecc-4a36-81c6-96891f58a3ef \n",
      "\n",
      "It indicates the terms or words that appear more than a given threshold value and should therefore be ignored. 07d2bcc9-3ecc-4a36-81c6-96891f58a3ef \n",
      "\n",
      "Tokenizer it overrides this string tokenizer method, but the default value is none. 07d2bcc9-3ecc-4a36-81c6-96891f58a3ef \n",
      "\n",
      "Men threshold, it indicates the terms or words that appear less than a given threshold value and should therefore be ignored. 07d2bcc9-3ecc-4a36-81c6-96891f58a3ef \n",
      "\n",
      "Let is start by importing the required Library count vectorizer class. c434f4e4-632a-4187-97b4-f03ec380a445 \n",
      "\n",
      "This demo you will learn how to process documents using bag of words. c434f4e4-632a-4187-97b4-f03ec380a445 \n",
      "\n",
      "Let is create an object vectorizer by instantiating the count vectorizer class. e75412de-1eff-43b9-a73e-1cdf925a5a17 \n",
      "\n",
      "Class helps tokenize the document it converts the text into vectors by assigning numeric values to each word. e75412de-1eff-43b9-a73e-1cdf925a5a17 \n",
      "\n",
      "Let is add some text to the documents document one can have Hi, how are you? a3a11c95-03db-4d95-8feb-e66aaa606553 \n",
      "\n",
      "And in document XnumberX we can say this was an amazing experience. 6c4efd28-dbcc-4cb8-86a0-2e03aaa795c9 \n",
      "\n",
      "Document to we can add today is a very very very pleasant day and we can have some fun fun fun. 6c4efd28-dbcc-4cb8-86a0-2e03aaa795c9 \n",
      "\n",
      "In this demo, you learned how to use bag of words technique and transformed documents using the transform method. 690e387d-0592-42ec-9046-5e6d8a02082b \n",
      "\n",
      "Next import grid search from the sklearn grid search class to perform the grid search. 516c0d72-3ac4-4dca-82db-afe84e8d0cae \n",
      "\n",
      "Create a pipeline by instantiating the pipeline class and passing vectorize transformed and model classifier has argument. e18d7fa5-63d5-4f33-9a54-fe6c318ccfb9 \n",
      "\n",
      "Let is create an object grid search and pass pipeline parameters jobs and verbose as arguments. 740833c0-fc13-47d3-82db-c30d4bab26db \n",
      "\n",
      "Print the parameters and run the grid search on the data set for both the message and response after fitting them into the grid search object using the fit method. c97f4d0c-97b0-42ee-a8db-e8b7ddeef4e9 \n",
      "\n",
      "--------------\n",
      "The goal is to convert a collection of text documents to a matrix of token counts. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "Let is take a look at the components of the signature input content. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "The input content can be a file name of the sequence of strings which needs to be vectorized. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "Let is talk about feature extraction another functionality of the scikitlearn approach. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "Let is see how it works by considering a document where we need to extract text features the steps to follow our assign a fixed integer ID to each word by splitting them into several words also known as tokenizing then count the number of occurrences of each word or token finally store it as the value feature in Matrix format. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "This is a technique to convert the content into numerical vectors to perform machine learning. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "The text feature extraction it is used to extract features from text. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "Data examples include large data sets or documents image feature extraction. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "It is used to convert text Data into numerical feature vectors with a fixed size. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "The feature extraction technique is used mostly in machine learning while dealing with text or image data. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "Now, let us learn about the bag of Words, which is one of the most common text feature extraction techniques. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "It is used to extract image features examples include patch extraction or connectivity graph of an image forming contiguous patches also known as hierarchical clustering bag of words. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "The Matrix presentation on the screen shows how multiple documents and tokens are structured This screen shows have account vectorizer algorithm works in scikitlearn. 1603562c-c2fd-491c-9658-ab40f91a24e7 \n",
      "\n",
      "--------------\n",
      "World of text analysis stop words usually have little lexical meaning some examples of stop words. 9d25f0a6-3e1c-425a-9171-df4c73b4381d \n",
      "\n",
      "Now let us view the first XnumberX stop words present in the stop words Corpus of in ltk. 9d25f0a6-3e1c-425a-9171-df4c73b4381d \n",
      "\n",
      "This is done by passing XnumberX to XnumberX index position in the parentheses. 9d25f0a6-3e1c-425a-9171-df4c73b4381d \n",
      "\n",
      "Are I me myself we our hours you yours and so on. 9d25f0a6-3e1c-425a-9171-df4c73b4381d \n",
      "\n",
      "Well use string class punctuation to remove all the punctuation present in each message as they are also less waited for text analysis, then we will remove the stop words. 0f850918-7639-4c1f-b058-d682b3d4c218 \n",
      "\n",
      "Well use basic python syntax to display the characters which do not have punctuation as you can see we make use of basic string class and its punctuation function. e76c9b1e-00ba-4839-ba68-f032793423de \n",
      "\n",
      "Let is remove the stop words will use stop words Corpus dot words function with English as an argument and basic python word and split function to clean each word. 4ac0bcfc-09d6-4b83-96aa-fa755709e8fc \n",
      "\n",
      "We can see that the list contains a lot of stop words. 4ac0bcfc-09d6-4b83-96aa-fa755709e8fc \n",
      "\n",
      "Spacing this is another Library which provides multiple useful views of textual meaning and linguistic structure. 70ef739b-df5c-4d1b-8805-e5613d7e9ce5 \n",
      "\n",
      "It is a pythonbased library for NLP and widely used in the industry to build programs to work with different human languages. 70ef739b-df5c-4d1b-8805-e5613d7e9ce5 \n",
      "\n",
      "It is another powerful open source python package and module XnumberX in LP. 70ef739b-df5c-4d1b-8805-e5613d7e9ce5 \n",
      "\n",
      "Text blob this library is used for processing Text data and provides simple apis for diving into NLP. 70ef739b-df5c-4d1b-8805-e5613d7e9ce5 \n",
      "\n",
      "It is analysis and our sentences free of punctuation and stop words following are the major in alpide libraries used in Python in LT Kang. 70ef739b-df5c-4d1b-8805-e5613d7e9ce5 \n",
      "\n",
      "It features various algorithms and is designed for operating with other python libraries like numpy and acai pie. 70ef739b-df5c-4d1b-8805-e5613d7e9ce5 \n",
      "\n",
      "--------------\n",
      "Let is create its head sequence to analyze will create a variable test sentence and passed the sentence. 11d761d8-f504-4219-844a-a39852210a5a \n",
      "\n",
      "This is done by accessing the fit method and fitting the documents into the vectorized object. 5dc11789-193f-4ca4-a731-056e6f9e4174 \n",
      "\n",
      "Next comes the interesting part which is creating the bag of words for the list of documents. 5dc11789-193f-4ca4-a731-056e6f9e4174 \n",
      "\n",
      "Let is print the bag of words hand look at the details. 5b02541f-d705-41ac-8a41-1d0236e66d01 \n",
      "\n",
      "You have to refer to the indexes which is assigned to any particular feature. 8f7b4256-2c12-41db-9683-3099c187d7ca \n",
      "\n",
      "I can see the first number is the Tuple and the second number is the frequency of words the Tuple here indicates the document number and feature indices of each word, which belongs to the document feature indices are generated from the transform method. 8f7b4256-2c12-41db-9683-3099c187d7ca \n",
      "\n",
      "This is a feature extraction process and to extract the features. 8f7b4256-2c12-41db-9683-3099c187d7ca \n",
      "\n",
      "Let is check for repeated words in the vocabulary that is built with the get function and then print it. 7e4f326b-f583-4dd2-966c-0b383ec8629f \n",
      "\n",
      "--------------\n",
      "Open the Anaconda prompt, please ensure that your system is connected to the internet. 81dc51ae-5ea4-439b-93a6-ad30b59c17ea \n",
      "\n",
      "Let is go ahead and enter conda install scikitlearn it will install the scikitlearn package enter conda install in ltk. 81dc51ae-5ea4-439b-93a6-ad30b59c17ea \n",
      "\n",
      "It will install the in ltk package into your python environment. 81dc51ae-5ea4-439b-93a6-ad30b59c17ea \n",
      "\n",
      "Dont type in Python in Anaconda prompt command line type in Import in ltk here to install the Corpus Collections and models. 7e939963-8230-4982-92da-54c57d8a27e5 \n",
      "\n",
      "Are interested in installing stop words Corpus select it and then click download button. 8dca9fee-5c8a-4714-80bd-e34aa2b246e2 \n",
      "\n",
      "It will install the stop words Corpus in your python environment. 8dca9fee-5c8a-4714-80bd-e34aa2b246e2 \n",
      "\n",
      "Let is navigate through the other models and all packages ensure that stop words status is installed. 4b462d15-6f4d-414c-b760-7842ef6d1a11 \n",
      "\n",
      "You can close the window and go back to jupyter notebook environment. e0489437-09c9-4e51-a22a-b0ecc71c9bd3 \n",
      "\n",
      "Let is start by importing the required Library class string from in ltk Corpus. 53949d4e-b7a0-400c-9749-6787eff62314 \n",
      "\n",
      "Then import Stop Words, which we have installed earlier as part of the environment setup. 358a204b-c3b1-48fc-aea7-b86254869041 \n",
      "\n",
      "We just completed the first step of the Senate is analysis process. f020de9f-3646-469b-add2-69d7a918e155 \n",
      "\n",
      "Let is start by importing the required Library pandas string print and time. 867aad19-34c8-4652-972f-63e59155e294 \n",
      "\n",
      "Well get the Spam data collection with the help of the pandas Library. f95338af-3e80-472e-a164-494372b849cf \n",
      "\n",
      "The backslash T indicates that data in the spam data set is tab separated or Tab delimited and set function takes it as a parameter. 6289ede2-cc94-49c4-8a0b-7244eaced58c \n",
      "\n",
      "Let is view the first five records of the stamp collection. 6289ede2-cc94-49c4-8a0b-7244eaced58c \n",
      "\n",
      "Also message is a feature which refers to the text present in the data set response is the label or the category of the message. 6289ede2-cc94-49c4-8a0b-7244eaced58c \n",
      "\n",
      "Import scikitlearn specific libraries for text processing and creating a pipeline and performing The Grid search. ae811bf9-4271-4986-8bf6-6a63982c4616 \n",
      "\n",
      "Import counter vectorizer to tokenize the document and convert the text into numeric values Also. a190dbad-c600-4c59-8b8d-1ef16cd7ce90 \n",
      "\n",
      "--------------\n",
      "to load the data set by creating an object as we had discussed earlier a data load object is used to load contents XnumberX describe the data set using 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "The diagram shows you how a data set is loaded using methods and how the object is structured in terms of the container folder and it is categories to load files. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "The following steps One Import the desired data set use the syntax from sklearn datasets import load digits. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "We can load txt files with categories as sub folder names. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "It does not try to load the files in memory to use text files in a scikitlearn classification or clustering algorithm. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "Let is load the digits data set to view the data using builtin methods perform. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "Bunch it contains fields and can be accessed as dict keys or an object Target names. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "A data load object helps load the contents of a data set shown here are the attributes of a data load object. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "Let is see how a data set can be loaded using scikitlearn. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "Data, it refers to an attribute in the memory where files are loaded. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "And as supervised signal label names, it does not try to extract features into a numpy array or scifi sparse Matrix. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "You have to use the main method scikitlearn datasets that load files. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "Youll need to use the sklearn feature extraction text module to build a feature extraction Transformer. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "Let is learn about its builtin modules for loading the data set contents and categories. 9a2e1a1c-863a-4f84-a3dd-847741a45fff \n",
      "\n",
      "Bes CR function using this function you can view all of the information which describes the data set. 9a5f97b6-42be-4c0f-9f8a-17b40d97165b \n",
      "\n",
      "The array is displayed here are the response data which is present in the data set. 9a5f97b6-42be-4c0f-9f8a-17b40d97165b \n",
      "\n",
      "Next let us do the data set type use this syntax type digits data set as we can see the type of the data set is bunch to view the data of the digits data set apply the function data the output displays the actual data which is bound to the data set. 9a5f97b6-42be-4c0f-9f8a-17b40d97165b \n",
      "\n",
      "--------------\n",
      "That you see here helps create the pipeline for the entire text processing. 1854052f-b6d8-4ec1-af28-0a7497b716cb \n",
      "\n",
      "--------------\n",
      "Bond to accessing the transform method and transform the list of documents. e871a7f7-e161-46f0-9308-0e2277d2f16a \n",
      "\n",
      "<---------------->\n",
      "order difference: 0\n",
      "Relevant sentence:  The data content can be text document image audio or video.    =====    With the help of these libraries we can also build analytical models and automate the natural language process with minimum or no human interventions.\n",
      "order difference: 0\n",
      "Relevant sentence:  With the help of these libraries we can also build analytical models and automate the natural language process with minimum or no human interventions.    =====    Troll language processing or NLP is an automated way to understand analyze human languages and extract information from such data by applying machine learning algorithms.\n",
      "order difference: 0\n",
      "Relevant sentence:  Troll language processing or NLP is an automated way to understand analyze human languages and extract information from such data by applying machine learning algorithms.    =====    This has resulted in the high volume of Digital Data across the world and has led to a number of challenges in analyzing data including analyzing the tons of data that is generated in the form of text image audios and videos.\n",
      "order difference: 0\n",
      "Relevant sentence:  This has resulted in the high volume of Digital Data across the world and has led to a number of challenges in analyzing data including analyzing the tons of data that is generated in the form of text image audios and videos.    =====    Identifying approximately XnumberX,XnumberX languages and dialects followed across the globe applying quantitative analysis on huge collections of data.\n",
      "order difference: 0\n",
      "Relevant sentence:  Identifying approximately XnumberX,XnumberX languages and dialects followed across the globe applying quantitative analysis on huge collections of data.    =====    Sometimes it is also referred to as a field of computer science or artificial intelligence to extract the Linguistics information from the underlying data in LP enables machines or computers to derive meaning from human or natural language input.\n",
      "order difference: 0\n",
      "Relevant sentence:  Sometimes it is also referred to as a field of computer science or artificial intelligence to extract the Linguistics information from the underlying data in LP enables machines or computers to derive meaning from human or natural language input.    =====    handling ambiguities while interpreting data and extracting information This is where natural language processing proves useful.\n",
      "order difference: 0\n",
      "Relevant sentence:  handling ambiguities while interpreting data and extracting information This is where natural language processing proves useful.    =====    One of the main goals of natural language processing is to understand various languages process them and extract information from them in NLP full automation can be easily achieved by modern software libraries modules and packages these software libraries and packages are aware of diverse language and culture and categorize data accordingly, which enables understanding Cinema it is better.\n",
      "order difference: 0\n",
      "Relevant sentence:  One of the main goals of natural language processing is to understand various languages process them and extract information from them in NLP full automation can be easily achieved by modern software libraries modules and packages these software libraries and packages are aware of diverse language and culture and categorize data accordingly, which enables understanding Cinema it is better.    =====    The world is now connected globally due to the advancement of technology and devices.\n",
      "order difference: 1\n",
      "Relevant sentence:  The world is now connected globally due to the advancement of technology and devices.    =====    It is a technique to split words phrases idioms Etc present in a document.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is a technique to split words phrases idioms Etc present in a document.    =====    It is very useful in finding synonyms and extensively used in search engines.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is very useful in finding synonyms and extensively used in search engines.    =====    P terminology now that you have understood why NLP is so important in recent times.\n",
      "order difference: 0\n",
      "Relevant sentence:  P terminology now that you have understood why NLP is so important in recent times.    =====    It is a process to map word to their stem or root.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is a process to map word to their stem or root.    =====    It is time to make yourself comfortable with the NLP terminologies one word boundaries.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is time to make yourself comfortable with the NLP terminologies one word boundaries.    =====    It determines where one word ends and the other begins to tokenization.\n",
      "order difference: 1\n",
      "Relevant sentence:  It determines where one word ends and the other begins to tokenization.    =====    Tfidf it is a numerical value which represents how important the word is to a document or Corpus.\n",
      "order difference: 0\n",
      "Relevant sentence:  Tfidf it is a numerical value which represents how important the word is to a document or Corpus.    =====    It is a type of statistical model for finding abstract topics which occur in a collection of documents.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is a type of statistical model for finding abstract topics which occur in a collection of documents.    =====    It is a technique to determine the meaning and a sense of words context versus intent XnumberX topic models.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is a technique to determine the meaning and a sense of words context versus intent XnumberX topic models.    =====    It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.\n",
      "order difference: 1\n",
      "Relevant sentence:  It is a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms.    =====    These approaches can be interrelated or they can be independently applied depending on the type of data to analyze.\n",
      "order difference: 0\n",
      "Relevant sentence:  These approaches can be interrelated or they can be independently applied depending on the type of data to analyze.    =====    Extract information this approach is about identifying the entities and relationships in a text to extract information in a structured way for example date time money and a direction can be used to establish relationships with other words available in the text.\n",
      "order difference: 0\n",
      "Relevant sentence:  Extract information this approach is about identifying the entities and relationships in a text to extract information in a structured way for example date time money and a direction can be used to establish relationships with other words available in the text.    =====    For example, tag a word using languages such as Chinese Spanish Etc.\n",
      "order difference: 0\n",
      "Relevant sentence:  For example, tag a word using languages such as Chinese Spanish Etc.    =====    Let is dive deeper into the NLP approaches to analyze text Data.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is dive deeper into the NLP approaches to analyze text Data.    =====    For example, if the content is religious or fictional categorizing and tagging words.\n",
      "order difference: 0\n",
      "Relevant sentence:  For example, if the content is religious or fictional categorizing and tagging words.    =====    This approach is about finding lexical categories and automatically tagging each word with this word class.\n",
      "order difference: 0\n",
      "Relevant sentence:  This approach is about finding lexical categories and automatically tagging each word with this word class.    =====    Words can also be tagged as adjectives verbs nouns and so on.\n",
      "order difference: 0\n",
      "Relevant sentence:  Words can also be tagged as adjectives verbs nouns and so on.    =====    Classify text with this approach you can identify particular features of language and use them to classify it for example classify the text as Sports politics or technology.\n",
      "order difference: 0\n",
      "Relevant sentence:  Classify text with this approach you can identify particular features of language and use them to classify it for example classify the text as Sports politics or technology.    =====    Basic text processing it is a way to analyze text and extract keywords that sum up the style or basic context of the text.\n",
      "order difference: 1\n",
      "Relevant sentence:  Basic text processing it is a way to analyze text and extract keywords that sum up the style or basic context of the text.    =====    Wise sentence structure in this approach capture formal grammar to describe the structure of a set of sentences.\n",
      "order difference: 0\n",
      "Relevant sentence:  Wise sentence structure in this approach capture formal grammar to describe the structure of a set of sentences.    =====    We get an insight into grammatical categories of the text document.\n",
      "order difference: 0\n",
      "Relevant sentence:  We get an insight into grammatical categories of the text document.    =====    For example, the text features of text based on speech tag or some grammar rules.\n",
      "order difference: 0\n",
      "Relevant sentence:  For example, the text features of text based on speech tag or some grammar rules.    =====    Analyze the meaning perform quantitative analysis of the given set of data to extract the information for example find entities in the text and try to establish a relationship between them.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is a powerful library with a set of modules to process and analyze natural language data such as text and images and extract information using machine learning algorithms.    =====    It has builtin modules to load the data sets content and categories will discuss the modules in detail in the upcoming screen.\n",
      "order difference: 0\n",
      "Relevant sentence:  It has builtin modules to load the data sets content and categories will discuss the modules in detail in the upcoming screen.    =====    model training in model training we analyze the content based on particular categories and then train them according to a specific model model training can involve supervised or unsupervised models supervised models and supervised models.\n",
      "order difference: 0\n",
      "Relevant sentence:  model training in model training we analyze the content based on particular categories and then train them according to a specific model model training can involve supervised or unsupervised models supervised models and supervised models.    =====    Feature extraction it is a way to extract information from data which can be text or images scikitlearn,  is builtin functions and methods help extract features and attributes from text to data and image data for the purpose of analysis.\n",
      "order difference: 0\n",
      "Relevant sentence:  Feature extraction it is a way to extract information from data which can be text or images scikitlearn,  is builtin functions and methods help extract features and attributes from text to data and image data for the purpose of analysis.    =====    Unsupervised models in this unsupervised models the response outcome or the label of the data is not known the main objective is to First understand the structure of the data and then identify the pattern in the data in this type of model training.\n",
      "order difference: 0\n",
      "Relevant sentence:  Unsupervised models in this unsupervised models the response outcome or the label of the data is not known the main objective is to First understand the structure of the data and then identify the pattern in the data in this type of model training.    =====    We have to find the predictors that behave in the same fashion or have some familiarity.\n",
      "order difference: 0\n",
      "Relevant sentence:  We have to find the predictors that behave in the same fashion or have some familiarity.    =====    These are some of the essential features of scikitlearn approach builtin modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order difference: 0\n",
      "Relevant sentence:  These are some of the essential features of scikitlearn approach builtin modules.    =====    The goal is to generate the data and find the right answer in this model training the outcome of new observation and data set is predicted.\n",
      "order difference: 1\n",
      "Relevant sentence:  The goal is to generate the data and find the right answer in this model training the outcome of new observation and data set is predicted.    =====    The documents are split into words or tokens and each document is assigned a number.\n",
      "order difference: 0\n",
      "Relevant sentence:  The documents are split into words or tokens and each document is assigned a number.    =====    To transformation and transformation the approaches to extract features around the word of Interest also in this stage you find the occurrence of each word in a document.\n",
      "order difference: 0\n",
      "Relevant sentence:  To transformation and transformation the approaches to extract features around the word of Interest also in this stage you find the occurrence of each word in a document.    =====    For example in this stage different algorithms are used to classify text documents and find the sentiment analysis.\n",
      "order difference: 0\n",
      "Relevant sentence:  For example in this stage different algorithms are used to classify text documents and find the sentiment analysis.    =====    This is a technique in scikitlearn approach to streamline the NLP process in two stages.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is a technique in scikitlearn approach to streamline the NLP process in two stages.    =====    This is required for accurate predictions a model is divided into training and test data sets to optimize the overall process.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is required for accurate predictions a model is divided into training and test data sets to optimize the overall process.    =====    Let is take a look at the various stages of pipeline learning one vectorization.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is take a look at the various stages of pipeline learning one vectorization.    =====    This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.\n",
      "order difference: 1\n",
      "Relevant sentence:  This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.    =====    It is one of the most common ways to adjust the extracted features.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is one of the most common ways to adjust the extracted features.    =====    Search a model can have multiple parameters and it is a powerful way to search parameters affecting the outcome for model training purposes.\n",
      "Not Relevant sentence:  Search a model can have multiple parameters and it is a powerful way to search parameters affecting the outcome for model training purposes.    !=    Has and response in supervised learning we make use of all three tasks predict the outcome of new observations and data sets understand which predictors affect the response or outcome and generalize the data and find the right answer generalization is also known as making predictions models can be trained to classify the documents based on the features and response.\n",
      "order difference: 15\n",
      "order difference: 0\n",
      "Relevant sentence:  Has and response in supervised learning we make use of all three tasks predict the outcome of new observations and data sets understand which predictors affect the response or outcome and generalize the data and find the right answer generalization is also known as making predictions models can be trained to classify the documents based on the features and response.    =====    So here we try to train the model by following a few steps such as understanding the structure of the data first and identify the pattern in the data finding predictors that behave in the same fashion or have some familiarity exploring the data set and the goal is representation of the data.\n",
      "order difference: 0\n",
      "Relevant sentence:  So here we try to train the model by following a few steps such as understanding the structure of the data first and identify the pattern in the data finding predictors that behave in the same fashion or have some familiarity exploring the data set and the goal is representation of the data.    =====    The choice of model completely depends on the type of data set.\n",
      "order difference: 0\n",
      "Relevant sentence:  The choice of model completely depends on the type of data set.    =====    Next feature of scikitlearn is model training an important task in model training is to identify the right model for the given data set scikitlearn provides a range of models to choose from and also trains them by using the extracted features from the data set.\n",
      "order difference: 0\n",
      "Relevant sentence:  Next feature of scikitlearn is model training an important task in model training is to identify the right model for the given data set scikitlearn provides a range of models to choose from and also trains them by using the extracted features from the data set.    =====    Unsupervised learning model in this type of learning the response or the outcome of the label of the data is not known there is no definite or right answer in this model.\n",
      "order difference: 0\n",
      "Relevant sentence:  Unsupervised learning model in this type of learning the response or the outcome of the label of the data is not known there is no definite or right answer in this model.    =====    The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features.\n",
      "order difference: 0\n",
      "Relevant sentence:  The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features.    =====    Rotation is also known as extracting structure unsupervised model can also be used to group documents by applying clustering algorithms.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents.    =====    Not if false a uniform prior will be used but as you notice the default is true class prior.\n",
      "order difference: 0\n",
      "Relevant sentence:  Not if false a uniform prior will be used but as you notice the default is true class prior.    =====    We use a document classifier to predict the category of a document to try predicting the outcome of a new document.\n",
      "order difference: 0\n",
      "Relevant sentence:  We use a document classifier to predict the category of a document to try predicting the outcome of a new document.    =====    And language detection multinomial naive Bayes is used when multiple occurrences of the words matter multinomial distribution normally requires integer feature counts.\n",
      "order difference: 0\n",
      "Relevant sentence:  And language detection multinomial naive Bayes is used when multiple occurrences of the words matter multinomial distribution normally requires integer feature counts.    =====    It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore.\n",
      "order difference: 0\n",
      "Relevant sentence:  It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore.    =====    But if specified the prior class is adjusted according to the data.\n",
      "order difference: 0\n",
      "Relevant sentence:  But if specified the prior class is adjusted according to the data.    =====    It would be too tedious to run through each parameter and tweak it in such cases a grid search useful the whole data set can be divided into multiple grids like a chessboard and the search can be run on entire grids or a selected combination of Grids, so we use a grid search.\n",
      "order difference: 0\n",
      "Relevant sentence:  It would be too tedious to run through each parameter and tweak it in such cases a grid search useful the whole data set can be divided into multiple grids like a chessboard and the search can be run on entire grids or a selected combination of Grids, so we use a grid search.    =====    The next feature of scikitlearn will be looking into is grid search.\n",
      "order difference: 0\n",
      "Relevant sentence:  The next feature of scikitlearn will be looking into is grid search.    =====    A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.\n",
      "order difference: 0\n",
      "Relevant sentence:  A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters.    =====    It is also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior.\n",
      "order difference: 1\n",
      "Relevant sentence:  It is also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior.    =====    This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory.    =====    An ism to perform an exhaustive search on the best parameters which affect the model however this approach has some constraints such as the whole grid search process is subjected to availability of the CPU cores of the system.\n",
      "order difference: 1\n",
      "Relevant sentence:  An ism to perform an exhaustive search on the best parameters which affect the model however this approach has some constraints such as the whole grid search process is subjected to availability of the CPU cores of the system.    =====    This is where scikitlearn becomes more productive by creating the pipeline for the data set.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is where scikitlearn becomes more productive by creating the pipeline for the data set.    =====    This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization.    =====    The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different.\n",
      "order difference: 0\n",
      "Relevant sentence:  The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different.    =====    Far we have learned that there are three major steps for analyzing text Data one vectorizer.\n",
      "order difference: 0\n",
      "Relevant sentence:  Far we have learned that there are three major steps for analyzing text Data one vectorizer.    =====    We just need to put them all together in a single class and create a pipeline to operate on a given data set.\n",
      "order difference: 0\n",
      "Relevant sentence:  We just need to put them all together in a single class and create a pipeline to operate on a given data set.    =====    Two Transformer during transformation you find the occurrence of each word in a document.\n",
      "order difference: 0\n",
      "Relevant sentence:  Two Transformer during transformation you find the occurrence of each word in a document.    =====    The documents are split into words or tokens and each document is assigned a number.\n",
      "order difference: 0\n",
      "Relevant sentence:  The documents are split into words or tokens and each document is assigned a number.    =====    For example different algorithms are used to classify text documents and find the sentiment analysis.\n",
      "order difference: 0\n",
      "Relevant sentence:  For example different algorithms are used to classify text documents and find the sentiment analysis.    =====    Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance.\n",
      "order difference: 0\n",
      "Relevant sentence:  Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance.    =====    So using scikitlearn pipeline can be created and we can train the model using a single command.\n",
      "Not Relevant sentence:  So using scikitlearn pipeline can be created and we can train the model using a single command.    !=    Search consumes a lot of memory and in jobs equals negative one instructs the process to use all CPUs while processing.\n",
      "order difference: 11\n",
      "Not Relevant sentence:  Search consumes a lot of memory and in jobs equals negative one instructs the process to use all CPUs while processing.    !=    Search is a CPU intensive process and it takes a little time since it does an exhaustive search on the entire data set on the passed parameters.\n",
      "order difference: 2\n",
      "order difference: 0\n",
      "Relevant sentence:  is to decode the input strip accents it removes accent present in the code.    =====    It indicates the terms or words that appear more than a given threshold value and should therefore be ignored.\n",
      "order difference: 0\n",
      "Relevant sentence:  It indicates the terms or words that appear more than a given threshold value and should therefore be ignored.    =====    Tokenizer it overrides this string tokenizer method, but the default value is none.\n",
      "order difference: 0\n",
      "Relevant sentence:  Tokenizer it overrides this string tokenizer method, but the default value is none.    =====    Men threshold, it indicates the terms or words that appear less than a given threshold value and should therefore be ignored.\n",
      "order difference: 1\n",
      "Relevant sentence:  Men threshold, it indicates the terms or words that appear less than a given threshold value and should therefore be ignored.    =====    Let is start by importing the required Library count vectorizer class.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is start by importing the required Library count vectorizer class.    =====    This demo you will learn how to process documents using bag of words.\n",
      "order difference: 1\n",
      "Relevant sentence:  This demo you will learn how to process documents using bag of words.    =====    Let is create an object vectorizer by instantiating the count vectorizer class.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is create an object vectorizer by instantiating the count vectorizer class.    =====    Class helps tokenize the document it converts the text into vectors by assigning numeric values to each word.\n",
      "order difference: 1\n",
      "Relevant sentence:  Class helps tokenize the document it converts the text into vectors by assigning numeric values to each word.    =====    Let is add some text to the documents document one can have Hi, how are you?\n",
      "order difference: 1\n",
      "Relevant sentence:  Let is add some text to the documents document one can have Hi, how are you?    =====    And in document XnumberX we can say this was an amazing experience.\n",
      "order difference: 0\n",
      "Relevant sentence:  And in document XnumberX we can say this was an amazing experience.    =====    Document to we can add today is a very very very pleasant day and we can have some fun fun fun.\n",
      "Not Relevant sentence:  Document to we can add today is a very very very pleasant day and we can have some fun fun fun.    !=    In this demo, you learned how to use bag of words technique and transformed documents using the transform method.\n",
      "order difference: 6\n",
      "Not Relevant sentence:  In this demo, you learned how to use bag of words technique and transformed documents using the transform method.    !=    Next import grid search from the sklearn grid search class to perform the grid search.\n",
      "order difference: 10\n",
      "order difference: 1\n",
      "Relevant sentence:  Next import grid search from the sklearn grid search class to perform the grid search.    =====    Create a pipeline by instantiating the pipeline class and passing vectorize transformed and model classifier has argument.\n",
      "Not Relevant sentence:  Create a pipeline by instantiating the pipeline class and passing vectorize transformed and model classifier has argument.    !=    Let is create an object grid search and pass pipeline parameters jobs and verbose as arguments.\n",
      "order difference: 3\n",
      "Not Relevant sentence:  Let is create an object grid search and pass pipeline parameters jobs and verbose as arguments.    !=    Print the parameters and run the grid search on the data set for both the message and response after fitting them into the grid search object using the fit method.\n",
      "order difference: 2\n",
      "order difference: 0\n",
      "Relevant sentence:  The goal is to convert a collection of text documents to a matrix of token counts.    =====    Let is take a look at the components of the signature input content.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is take a look at the components of the signature input content.    =====    The input content can be a file name of the sequence of strings which needs to be vectorized.\n",
      "order difference: 0\n",
      "Relevant sentence:  The input content can be a file name of the sequence of strings which needs to be vectorized.    =====    Let is talk about feature extraction another functionality of the scikitlearn approach.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is talk about feature extraction another functionality of the scikitlearn approach.    =====    Let is see how it works by considering a document where we need to extract text features the steps to follow our assign a fixed integer ID to each word by splitting them into several words also known as tokenizing then count the number of occurrences of each word or token finally store it as the value feature in Matrix format.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is see how it works by considering a document where we need to extract text features the steps to follow our assign a fixed integer ID to each word by splitting them into several words also known as tokenizing then count the number of occurrences of each word or token finally store it as the value feature in Matrix format.    =====    This is a technique to convert the content into numerical vectors to perform machine learning.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is a technique to convert the content into numerical vectors to perform machine learning.    =====    The text feature extraction it is used to extract features from text.\n",
      "order difference: 0\n",
      "Relevant sentence:  The text feature extraction it is used to extract features from text.    =====    Data examples include large data sets or documents image feature extraction.\n",
      "order difference: 0\n",
      "Relevant sentence:  Data examples include large data sets or documents image feature extraction.    =====    It is used to convert text Data into numerical feature vectors with a fixed size.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is used to convert text Data into numerical feature vectors with a fixed size.    =====    The feature extraction technique is used mostly in machine learning while dealing with text or image data.\n",
      "order difference: 0\n",
      "Relevant sentence:  The feature extraction technique is used mostly in machine learning while dealing with text or image data.    =====    Now, let us learn about the bag of Words, which is one of the most common text feature extraction techniques.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now, let us learn about the bag of Words, which is one of the most common text feature extraction techniques.    =====    It is used to extract image features examples include patch extraction or connectivity graph of an image forming contiguous patches also known as hierarchical clustering bag of words.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is used to extract image features examples include patch extraction or connectivity graph of an image forming contiguous patches also known as hierarchical clustering bag of words.    =====    The Matrix presentation on the screen shows how multiple documents and tokens are structured This screen shows have account vectorizer algorithm works in scikitlearn.\n",
      "order difference: 0\n",
      "Relevant sentence:  World of text analysis stop words usually have little lexical meaning some examples of stop words.    =====    Now let us view the first XnumberX stop words present in the stop words Corpus of in ltk.\n",
      "order difference: 0\n",
      "Relevant sentence:  Now let us view the first XnumberX stop words present in the stop words Corpus of in ltk.    =====    This is done by passing XnumberX to XnumberX index position in the parentheses.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is done by passing XnumberX to XnumberX index position in the parentheses.    =====    Are I me myself we our hours you yours and so on.\n",
      "Not Relevant sentence:  Are I me myself we our hours you yours and so on.    !=    Well use string class punctuation to remove all the punctuation present in each message as they are also less waited for text analysis, then we will remove the stop words.\n",
      "order difference: 2\n",
      "order difference: 1\n",
      "Relevant sentence:  Well use string class punctuation to remove all the punctuation present in each message as they are also less waited for text analysis, then we will remove the stop words.    =====    Well use basic python syntax to display the characters which do not have punctuation as you can see we make use of basic string class and its punctuation function.\n",
      "Not Relevant sentence:  Well use basic python syntax to display the characters which do not have punctuation as you can see we make use of basic string class and its punctuation function.    !=    Let is remove the stop words will use stop words Corpus dot words function with English as an argument and basic python word and split function to clean each word.\n",
      "order difference: 2\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is remove the stop words will use stop words Corpus dot words function with English as an argument and basic python word and split function to clean each word.    =====    We can see that the list contains a lot of stop words.\n",
      "order difference: 1\n",
      "Relevant sentence:  We can see that the list contains a lot of stop words.    =====    Spacing this is another Library which provides multiple useful views of textual meaning and linguistic structure.\n",
      "order difference: 0\n",
      "Relevant sentence:  Spacing this is another Library which provides multiple useful views of textual meaning and linguistic structure.    =====    It is a pythonbased library for NLP and widely used in the industry to build programs to work with different human languages.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is a pythonbased library for NLP and widely used in the industry to build programs to work with different human languages.    =====    It is another powerful open source python package and module XnumberX in LP.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is another powerful open source python package and module XnumberX in LP.    =====    Text blob this library is used for processing Text data and provides simple apis for diving into NLP.\n",
      "order difference: 0\n",
      "Relevant sentence:  Text blob this library is used for processing Text data and provides simple apis for diving into NLP.    =====    It is analysis and our sentences free of punctuation and stop words following are the major in alpide libraries used in Python in LT Kang.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is analysis and our sentences free of punctuation and stop words following are the major in alpide libraries used in Python in LT Kang.    =====    It features various algorithms and is designed for operating with other python libraries like numpy and acai pie.\n",
      "Not Relevant sentence:  Let is create its head sequence to analyze will create a variable test sentence and passed the sentence.    !=    This is done by accessing the fit method and fitting the documents into the vectorized object.\n",
      "order difference: 17\n",
      "order difference: 0\n",
      "Relevant sentence:  This is done by accessing the fit method and fitting the documents into the vectorized object.    =====    Next comes the interesting part which is creating the bag of words for the list of documents.\n",
      "Not Relevant sentence:  Next comes the interesting part which is creating the bag of words for the list of documents.    !=    Let is print the bag of words hand look at the details.\n",
      "order difference: 2\n",
      "order difference: 1\n",
      "Relevant sentence:  Let is print the bag of words hand look at the details.    =====    You have to refer to the indexes which is assigned to any particular feature.\n",
      "order difference: 0\n",
      "Relevant sentence:  You have to refer to the indexes which is assigned to any particular feature.    =====    I can see the first number is the Tuple and the second number is the frequency of words the Tuple here indicates the document number and feature indices of each word, which belongs to the document feature indices are generated from the transform method.\n",
      "order difference: 0\n",
      "Relevant sentence:  I can see the first number is the Tuple and the second number is the frequency of words the Tuple here indicates the document number and feature indices of each word, which belongs to the document feature indices are generated from the transform method.    =====    This is a feature extraction process and to extract the features.\n",
      "order difference: 1\n",
      "Relevant sentence:  This is a feature extraction process and to extract the features.    =====    Let is check for repeated words in the vocabulary that is built with the get function and then print it.\n",
      "order difference: 0\n",
      "Relevant sentence:  Open the Anaconda prompt, please ensure that your system is connected to the internet.    =====    Let is go ahead and enter conda install scikitlearn it will install the scikitlearn package enter conda install in ltk.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is go ahead and enter conda install scikitlearn it will install the scikitlearn package enter conda install in ltk.    =====    It will install the in ltk package into your python environment.\n",
      "order difference: 1\n",
      "Relevant sentence:  It will install the in ltk package into your python environment.    =====    Dont type in Python in Anaconda prompt command line type in Import in ltk here to install the Corpus Collections and models.\n",
      "order difference: 1\n",
      "Relevant sentence:  Dont type in Python in Anaconda prompt command line type in Import in ltk here to install the Corpus Collections and models.    =====    Are interested in installing stop words Corpus select it and then click download button.\n",
      "order difference: 0\n",
      "Relevant sentence:  Are interested in installing stop words Corpus select it and then click download button.    =====    It will install the stop words Corpus in your python environment.\n",
      "order difference: 1\n",
      "Relevant sentence:  It will install the stop words Corpus in your python environment.    =====    Let is navigate through the other models and all packages ensure that stop words status is installed.\n",
      "order difference: 1\n",
      "Relevant sentence:  Let is navigate through the other models and all packages ensure that stop words status is installed.    =====    You can close the window and go back to jupyter notebook environment.\n",
      "order difference: 1\n",
      "Relevant sentence:  You can close the window and go back to jupyter notebook environment.    =====    Let is start by importing the required Library class string from in ltk Corpus.\n",
      "order difference: 1\n",
      "Relevant sentence:  Let is start by importing the required Library class string from in ltk Corpus.    =====    Then import Stop Words, which we have installed earlier as part of the environment setup.\n",
      "Not Relevant sentence:  Then import Stop Words, which we have installed earlier as part of the environment setup.    !=    We just completed the first step of the Senate is analysis process.\n",
      "order difference: 5\n",
      "Not Relevant sentence:  We just completed the first step of the Senate is analysis process.    !=    Let is start by importing the required Library pandas string print and time.\n",
      "order difference: 24\n",
      "order difference: 1\n",
      "Relevant sentence:  Let is start by importing the required Library pandas string print and time.    =====    Well get the Spam data collection with the help of the pandas Library.\n",
      "order difference: 1\n",
      "Relevant sentence:  Well get the Spam data collection with the help of the pandas Library.    =====    The backslash T indicates that data in the spam data set is tab separated or Tab delimited and set function takes it as a parameter.\n",
      "order difference: 0\n",
      "Relevant sentence:  The backslash T indicates that data in the spam data set is tab separated or Tab delimited and set function takes it as a parameter.    =====    Let is view the first five records of the stamp collection.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is view the first five records of the stamp collection.    =====    Also message is a feature which refers to the text present in the data set response is the label or the category of the message.\n",
      "order difference: 1\n",
      "Relevant sentence:  Also message is a feature which refers to the text present in the data set response is the label or the category of the message.    =====    Import scikitlearn specific libraries for text processing and creating a pipeline and performing The Grid search.\n",
      "order difference: 1\n",
      "Relevant sentence:  Import scikitlearn specific libraries for text processing and creating a pipeline and performing The Grid search.    =====    Import counter vectorizer to tokenize the document and convert the text into numeric values Also.\n",
      "order difference: 0\n",
      "Relevant sentence:  to load the data set by creating an object as we had discussed earlier a data load object is used to load contents XnumberX describe the data set using    =====    The diagram shows you how a data set is loaded using methods and how the object is structured in terms of the container folder and it is categories to load files.\n",
      "order difference: 0\n",
      "Relevant sentence:  The diagram shows you how a data set is loaded using methods and how the object is structured in terms of the container folder and it is categories to load files.    =====    The following steps One Import the desired data set use the syntax from sklearn datasets import load digits.\n",
      "order difference: 0\n",
      "Relevant sentence:  The following steps One Import the desired data set use the syntax from sklearn datasets import load digits.    =====    We can load txt files with categories as sub folder names.\n",
      "order difference: 0\n",
      "Relevant sentence:  We can load txt files with categories as sub folder names.    =====    It does not try to load the files in memory to use text files in a scikitlearn classification or clustering algorithm.\n",
      "order difference: 0\n",
      "Relevant sentence:  It does not try to load the files in memory to use text files in a scikitlearn classification or clustering algorithm.    =====    Let is load the digits data set to view the data using builtin methods perform.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is load the digits data set to view the data using builtin methods perform.    =====    Bunch it contains fields and can be accessed as dict keys or an object Target names.\n",
      "order difference: 0\n",
      "Relevant sentence:  Bunch it contains fields and can be accessed as dict keys or an object Target names.    =====    A data load object helps load the contents of a data set shown here are the attributes of a data load object.\n",
      "order difference: 0\n",
      "Relevant sentence:  A data load object helps load the contents of a data set shown here are the attributes of a data load object.    =====    Let is see how a data set can be loaded using scikitlearn.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is see how a data set can be loaded using scikitlearn.    =====    Data, it refers to an attribute in the memory where files are loaded.\n",
      "order difference: 0\n",
      "Relevant sentence:  Data, it refers to an attribute in the memory where files are loaded.    =====    And as supervised signal label names, it does not try to extract features into a numpy array or scifi sparse Matrix.\n",
      "order difference: 0\n",
      "Relevant sentence:  And as supervised signal label names, it does not try to extract features into a numpy array or scifi sparse Matrix.    =====    You have to use the main method scikitlearn datasets that load files.\n",
      "order difference: 0\n",
      "Relevant sentence:  You have to use the main method scikitlearn datasets that load files.    =====    Youll need to use the sklearn feature extraction text module to build a feature extraction Transformer.\n",
      "order difference: 0\n",
      "Relevant sentence:  Youll need to use the sklearn feature extraction text module to build a feature extraction Transformer.    =====    Let is learn about its builtin modules for loading the data set contents and categories.\n",
      "order difference: 1\n",
      "Relevant sentence:  Let is learn about its builtin modules for loading the data set contents and categories.    =====    Bes CR function using this function you can view all of the information which describes the data set.\n",
      "order difference: 0\n",
      "Relevant sentence:  Bes CR function using this function you can view all of the information which describes the data set.    =====    The array is displayed here are the response data which is present in the data set.\n",
      "order difference: 0\n",
      "Relevant sentence:  The array is displayed here are the response data which is present in the data set.    =====    Next let us do the data set type use this syntax type digits data set as we can see the type of the data set is bunch to view the data of the digits data set apply the function data the output displays the actual data which is bound to the data set.\n",
      "[(0, 0), (1, 1), (2, 1), (3, 1), (4, 2), (5, 3), (11, 3), (6, 4), (7, 5), (8, 5), (9, 6), (10, 7), (12, 8), (13, 8)]\n",
      "[[[\"Troll language processing or NLP is an automated way to understand analyze human languages and extract information from such data by applying machine learning algorithms. The data content can be text document image audio or video. Sometimes it's also referred to as a field of computer science or artificial intelligence to extract the Linguistics information from the underlying data in LP enables machines or computers to derive meaning from human or natural language input. The world is now connected globally due to the advancement of technology and devices. This has resulted in the high volume of Digital Data across the world and has led to a number of challenges in analyzing data including analyzing the tons of data that is generated in the form of text image audios and videos. Identifying approximately 6,500 languages and dialects followed across the globe applying quantitative analysis on huge collections of data. handling ambiguities while interpreting data and extracting information This is where natural language processing proves useful. One of the main goals of natural language processing is to understand various languages process them and extract information from them in NLP full automation can be easily achieved by modern software libraries modules and packages these software libraries and packages are aware of diverse language and culture and categorize data accordingly, which enables understanding Cinema it's better. With the help of these libraries we can also build analytical models and automate the natural language process with minimum or no human interventions. \"], '2020-01-31T07:53:01Z', '716067a60a1a4034abc49a12ecafb39b', '281d85ad-92c3-48e8-8b06-43e8dc623374'], [[\"P terminology now that you have understood why NLP is so important in recent times. It's time to make yourself comfortable with the NLP terminologies one word boundaries. It determines where one word ends and the other begins to tokenization. It's a technique to split words phrases idioms Etc present in a document. Three stemming. It's a process to map word to their stem or root. It's very useful in finding synonyms and extensively used in search engines. \"], '2020-01-31T07:54:54Z', '716067a60a1a4034abc49a12ecafb39b', '0ca53093-564d-47fa-8874-336d7c86ef9b'], [[\"Tf-idf it's a numerical value which represents how important the word is to a document or Corpus. Five semantic analytics. It's a technique in vectorial semantics of analyzing relationships between a set of documents and the terms it contains a producing a set of concepts related to the documents and terms. Six disambiguation. It's a technique to determine the meaning and a sense of words context versus intent 7 topic models. It's a type of statistical model for finding abstract topics which occur in a collection of documents. \"], '2020-01-31T07:55:33Z', '716067a60a1a4034abc49a12ecafb39b', '54736d9f-7cb6-4251-912a-632a480c81bb'], [[\"That you are familiar with the various terminologies. Let's dive deeper into the NLP approaches to analyze text Data. These approaches can be interrelated or they can be independently applied depending on the type of data to analyze. Basic text processing it's a way to analyze text and extract keywords that sum up the style or basic context of the text. For example, if the content is religious or fictional categorizing and tagging words. This approach is about finding lexical categories and automatically tagging each word with this word class. For example, tag a word using languages such as Chinese Spanish Etc. Words can also be tagged as adjectives verbs nouns and so on. Classify text with this approach you can identify particular features of language and use them to classify it for example classify the text as Sports politics or technology. Extract information this approach is about identifying the entities and relationships in a text to extract information in a structured way for example date time money and a direction can be used to establish relationships with other words available in the text. \"], '2020-01-31T07:56:19Z', '716067a60a1a4034abc49a12ecafb39b', '04e2080b-f2ab-4201-b4be-7bbd34173843'], [['Wise sentence structure in this approach capture formal grammar to describe the structure of a set of sentences. For example find a well formed or ill-formed sentence structure. Build feature based structure through this approach. We get an insight into grammatical categories of the text document. For example, the text features of text based on speech tag or some grammar rules. Analyze the meaning perform quantitative analysis of the given set of data to extract the information for example find entities in the text and try to establish a relationship between them. '], '2020-01-31T07:57:42Z', '716067a60a1a4034abc49a12ecafb39b', 'c5a346ba-86c0-4c6c-8fcc-29a627310e3f']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Let's look at the scikit-learn approach and its features. It's a powerful library with a set of modules to process and analyze natural language data such as text and images and extract information using machine learning algorithms. These are some of the essential features of scikit-learn approach built-in modules. It has built-in modules to load the data sets content and categories will discuss the modules in detail in the upcoming screen. Feature extraction it's a way to extract information from data which can be text or images scikit-learn, 's built-in functions and methods help extract features and attributes from text to data and image data for the purpose of analysis. model training in model training we analyze the content based on particular categories and then train them according to a specific model model training can involve supervised or unsupervised models supervised models and supervised models. The goal is to generate the data and find the right answer in this model training the outcome of new observation and data set is predicted. Unsupervised models in this unsupervised models the response outcome or the label of the data is not known the main objective is to First understand the structure of the data and then identify the pattern in the data in this type of model training. We have to find the predictors that behave in the same fashion or have some familiarity. \"], '2020-01-31T08:03:53Z', '716067a60a1a4034abc49a12ecafb39b', '3a861e7b-1736-4924-9827-63f8266e3ad7'], [[\"I'm building. This is a technique in scikit-learn approach to streamline the NLP process in two stages. Let's take a look at the various stages of pipeline learning one vectorization. This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization. The documents are split into words or tokens and each document is assigned a number. To transformation and transformation the approaches to extract features around the word of Interest also in this stage you find the occurrence of each word in a document. Three model training and application. This is required for accurate predictions a model is divided into training and test data sets to optimize the overall process. For example in this stage different algorithms are used to classify text documents and find the sentiment analysis. Performance optimization in this stage. We trained the models to optimize the overall process. \"], '2020-01-31T08:05:31Z', '716067a60a1a4034abc49a12ecafb39b', 'aa217665-d5e0-487d-9259-540258ba91f8'], [[\"Search a model can have multiple parameters and it's a powerful way to search parameters affecting the outcome for model training purposes. It's one of the most common ways to adjust the extracted features. \"], '2020-01-31T08:06:43Z', '716067a60a1a4034abc49a12ecafb39b', '80f384af-2e77-461e-b861-593c93a060b8']] \n",
      "\n",
      "\n",
      "[[['Next feature of scikit-learn is model training an important task in model training is to identify the right model for the given data set scikit-learn provides a range of models to choose from and also trains them by using the extracted features from the data set. The choice of model completely depends on the type of data set. The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features. Has and response in supervised learning we make use of all three tasks predict the outcome of new observations and data sets understand which predictors affect the response or outcome and generalize the data and find the right answer generalization is also known as making predictions models can be trained to classify the documents based on the features and response. For example, naive Bayes svm linear regression and KNN Neighbors. Unsupervised learning model in this type of learning the response or the outcome of the label of the data is not known there is no definite or right answer in this model. So here we try to train the model by following a few steps such as understanding the structure of the data first and identify the pattern in the data finding predictors that behave in the same fashion or have some familiarity exploring the data set and the goal is representation of the data. Rotation is also known as extracting structure unsupervised model can also be used to group documents by applying clustering algorithms. For example clustering text documents using k-means. '], '2020-01-31T08:15:33Z', '716067a60a1a4034abc49a12ecafb39b', '5494a3cb-1575-4bee-a553-062406bc6ea4']] \n",
      "\n",
      "\n",
      "[[[\"Bayes is a basic technique for classification of text. It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore. It's very efficient. It's fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents. And language detection multinomial naive Bayes is used when multiple occurrences of the words matter multinomial distribution normally requires integer feature counts. Let's take a quick look at the psyche. It multi nominal in be model class. There are three major parameters in this class Alpha. It's also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior. It indicates whether to learn class prior probabilities. Not if false a uniform prior will be used but as you notice the default is true class prior. The default is none. But if specified the prior class is adjusted according to the data. The next feature of scikit-learn will be looking into is grid search. We use a document classifier to predict the category of a document to try predicting the outcome of a new document. We need to extract the features. A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters. It would be too tedious to run through each parameter and tweak it in such cases a grid search useful the whole data set can be divided into multiple grids like a chessboard and the search can be run on entire grids or a selected combination of Grids, so we use a grid search. \"], '2020-01-31T08:17:17Z', '716067a60a1a4034abc49a12ecafb39b', '7999f002-6896-4e88-9c1c-96fa3d150b6a'], [['An ism to perform an exhaustive search on the best parameters which affect the model however this approach has some constraints such as the whole grid search process is subjected to availability of the CPU cores of the system. This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory. '], '2020-01-31T08:19:17Z', '716067a60a1a4034abc49a12ecafb39b', '38a75d62-8e9e-4ea6-aea7-58460c83cd6b'], [['Far we have learned that there are three major steps for analyzing text Data one vectorizer. This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization. The documents are split into words or tokens and each document is assigned a number. Two Transformer during transformation you find the occurrence of each word in a document. This process is primarily used for feature extraction. Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance. For example different algorithms are used to classify text documents and find the sentiment analysis. This is where scikit-learn becomes more productive by creating the pipeline for the data set. A pipeline is essentially a combination of these three steps. So using scikit-learn pipeline can be created and we can train the model using a single command. We just need to put them all together in a single class and create a pipeline to operate on a given data set. The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different. printers '], '2020-01-31T08:19:47Z', '716067a60a1a4034abc49a12ecafb39b', 'c920660d-4b47-42cb-891a-8685f93b1d67']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"is to decode the input strip accents it removes accent present in the code. Tokenizer it overrides this string tokenizer method, but the default value is none. Stop words. It's a built-in stop words list that is used. Max threshold. It indicates the terms or words that appear more than a given threshold value and should therefore be ignored. Men threshold, it indicates the terms or words that appear less than a given threshold value and should therefore be ignored. \"], '2020-01-31T08:11:40Z', '716067a60a1a4034abc49a12ecafb39b', '07d2bcc9-3ecc-4a36-81c6-96891f58a3ef'], [[\"This demo you'll learn how to process documents using bag of words. Let's start by importing the required Library count vectorizer class. \"], '2020-01-31T08:12:21Z', '716067a60a1a4034abc49a12ecafb39b', 'c434f4e4-632a-4187-97b4-f03ec380a445'], [[\"Class helps tokenize the document it converts the text into vectors by assigning numeric values to each word. Let's create an object vectorizer by instantiating the count vectorizer class. \"], '2020-01-31T08:12:37Z', '716067a60a1a4034abc49a12ecafb39b', 'e75412de-1eff-43b9-a73e-1cdf925a5a17'], [[\"Will create three random documents. Let's add some text to the documents document one can have Hi, how are you? \"], '2020-01-31T08:12:53Z', '716067a60a1a4034abc49a12ecafb39b', 'a3a11c95-03db-4d95-8feb-e66aaa606553'], [['Document to we can add today is a very very very pleasant day and we can have some fun fun fun. And in document 3 we can say this was an amazing experience. '], '2020-01-31T08:13:05Z', '716067a60a1a4034abc49a12ecafb39b', '6c4efd28-dbcc-4cb8-86a0-2e03aaa795c9']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[['Next import grid search from the sklearn grid search class to perform the grid search. '], '2020-01-31T08:23:05Z', '716067a60a1a4034abc49a12ecafb39b', '516c0d72-3ac4-4dca-82db-afe84e8d0cae'], [['Create a pipeline by instantiating the pipeline class and passing vectorize transformed and model classifier has argument. '], '2020-01-31T08:23:24Z', '716067a60a1a4034abc49a12ecafb39b', 'e18d7fa5-63d5-4f33-9a54-fe6c318ccfb9']] \n",
      "\n",
      "\n",
      "[[[\"Open the Anaconda prompt, please ensure that your system is connected to the internet. Let's go ahead and enter conda install scikit-learn it will install the scikit-learn package enter conda install in ltk. It will install the in ltk package into your python environment. \"], '2020-01-31T07:58:33Z', '716067a60a1a4034abc49a12ecafb39b', '81dc51ae-5ea4-439b-93a6-ad30b59c17ea'], [[\"Don't type in Python in Anaconda prompt command line type in Import in ltk here to install the Corpus Collections and models. \"], '2020-01-31T07:58:57Z', '716067a60a1a4034abc49a12ecafb39b', '7e939963-8230-4982-92da-54c57d8a27e5'], [['Are interested in installing stop words Corpus select it and then click download button. It will install the stop words Corpus in your python environment. '], '2020-01-31T07:59:11Z', '716067a60a1a4034abc49a12ecafb39b', '8dca9fee-5c8a-4714-80bd-e34aa2b246e2'], [[\"Let's navigate through the other models and all packages ensure that stop words status is installed. \"], '2020-01-31T07:59:33Z', '716067a60a1a4034abc49a12ecafb39b', '4b462d15-6f4d-414c-b760-7842ef6d1a11'], [['You can close the window and go back to jupyter notebook environment. '], '2020-01-31T07:59:48Z', '716067a60a1a4034abc49a12ecafb39b', 'e0489437-09c9-4e51-a22a-b0ecc71c9bd3'], [[\"Demo, you learned how to install the NLP environment. In this demo, you'll learn how to perform sentence analysis. Let's start by importing the required Library class string from in ltk Corpus. \"], '2020-01-31T07:59:54Z', '716067a60a1a4034abc49a12ecafb39b', '53949d4e-b7a0-400c-9749-6787eff62314'], [[\"Then import Stop Words, which we've installed earlier as part of the environment setup. \"], '2020-01-31T08:00:11Z', '716067a60a1a4034abc49a12ecafb39b', '358a204b-c3b1-48fc-aea7-b86254869041']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Let's talk about feature extraction another functionality of the scikit-learn approach. This is a technique to convert the content into numerical vectors to perform machine learning. The feature extraction technique is used mostly in machine learning while dealing with text or image data. There are two major types of feature extraction techniques. The text feature extraction it's used to extract features from text. Data examples include large data sets or documents image feature extraction. It's used to extract image features examples include patch extraction or connectivity graph of an image forming contiguous patches also known as hierarchical clustering bag of words. Now, let's learn about the bag of Words, which is one of the most common text feature extraction techniques. It's used to convert text Data into numerical feature vectors with a fixed size. Let's see how it works by considering a document where we need to extract text features the steps to follow our assign a fixed integer ID to each word by splitting them into several words also known as tokenizing then count the number of occurrences of each word or token finally store it as the value feature in Matrix format. The Matrix presentation on the screen shows how multiple documents and tokens are structured This screen shows have account vectorizer algorithm works in scikit-learn. The goal is to convert a collection of text documents to a matrix of token counts. There are several parameters, which affect the vectorization process. Let's take a look at the components of the signature input content. The input content can be a file name of the sequence of strings which needs to be vectorized. encoding the default encoding is utf-8 the \"], '2020-01-31T08:09:40Z', '716067a60a1a4034abc49a12ecafb39b', '1603562c-c2fd-491c-9658-ab40f91a24e7']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"World of text analysis stop words usually have little lexical meaning some examples of stop words. Are I me myself we our hours you yours and so on. Now let's view the first 10 stop words present in the stop words Corpus of in ltk. Library. This is done by passing 0 to 10 index position in the parentheses. \"], '2020-01-31T08:00:24Z', '716067a60a1a4034abc49a12ecafb39b', '9d25f0a6-3e1c-425a-9171-df4c73b4381d']] \n",
      "\n",
      "\n",
      "[[[\"We'll use string class punctuation to remove all the punctuation present in each message as they are also less waited for text analysis, then we'll remove the stop words. \"], '2020-01-31T08:01:14Z', '716067a60a1a4034abc49a12ecafb39b', '0f850918-7639-4c1f-b058-d682b3d4c218'], [[\"We'll use basic python syntax to display the characters which don't have punctuation as you can see we make use of basic string class and its punctuation function. \"], '2020-01-31T08:01:43Z', '716067a60a1a4034abc49a12ecafb39b', 'e76c9b1e-00ba-4839-ba68-f032793423de']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Let's split the sentence into words here. We can see that the list contains a lot of stop words. Let's remove the stop words will use stop words Corpus dot words function with English as an argument and basic python word and split function to clean each word. \"], '2020-01-31T08:02:11Z', '716067a60a1a4034abc49a12ecafb39b', '4ac0bcfc-09d6-4b83-96aa-fa755709e8fc'], [[\"This we are done with sin. It's analysis and our sentences free of punctuation and stop words following are the major in alpide libraries used in Python in LT Kang. It's a python-based library for NLP and widely used in the industry to build programs to work with different human languages. Sty kid Lon. It's another powerful open source python package and module 4 in LP. It features various algorithms and is designed for operating with other python libraries like numpy and acai pie. Text blob this library is used for processing Text data and provides simple apis for diving into NLP. Spacing this is another Library which provides multiple useful views of textual meaning and linguistic structure. \"], '2020-01-31T08:02:57Z', '716067a60a1a4034abc49a12ecafb39b', '70ef739b-df5c-4d1b-8805-e5613d7e9ce5']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Let's print the bag of words hand look at the details. You can see the properties of vectorized object. \"], '2020-01-31T08:14:10Z', '716067a60a1a4034abc49a12ecafb39b', '5b02541f-d705-41ac-8a41-1d0236e66d01'], [['I can see the first number is the Tuple and the second number is the frequency of words the Tuple here indicates the document number and feature indices of each word, which belongs to the document feature indices are generated from the transform method. This is a feature extraction process and to extract the features. You have to refer to the indexes which is assigned to any particular feature. '], '2020-01-31T08:14:21Z', '716067a60a1a4034abc49a12ecafb39b', '8f7b4256-2c12-41db-9683-3099c187d7ca'], [[\"Let's check for repeated words in the vocabulary that is built with the get function and then print it. \"], '2020-01-31T08:14:51Z', '716067a60a1a4034abc49a12ecafb39b', '7e4f326b-f583-4dd2-966c-0b383ec8629f']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Demo will learn how to apply Pipeline and grid search. Let's start by importing the required Library pandas string print and time. \"], '2020-01-31T08:21:11Z', '716067a60a1a4034abc49a12ecafb39b', '867aad19-34c8-4652-972f-63e59155e294'], [[\"We'll get the Spam data collection with the help of the pandas Library. \"], '2020-01-31T08:21:38Z', '716067a60a1a4034abc49a12ecafb39b', 'f95338af-3e80-472e-a164-494372b849cf'], [[\"The backslash T indicates that data in the spam data set is tab separated or Tab delimited and set function takes it as a parameter. Also message is a feature which refers to the text present in the data set response is the label or the category of the message. Let's view the first five records of the stamp collection. The head method is used to view the topmost records in a data set. \"], '2020-01-31T08:21:49Z', '716067a60a1a4034abc49a12ecafb39b', '6289ede2-cc94-49c4-8a0b-7244eaced58c'], [['Import scikit-learn specific libraries for text processing and creating a pipeline and performing The Grid search. '], '2020-01-31T08:22:22Z', '716067a60a1a4034abc49a12ecafb39b', 'ae811bf9-4271-4986-8bf6-6a63982c4616'], [[\"Import counter vectorizer to tokenize the document and convert the text into numeric values Also. Let's import tf-idf transformer for tf-idf transformation. \"], '2020-01-31T08:22:33Z', '716067a60a1a4034abc49a12ecafb39b', 'a190dbad-c600-4c59-8b8d-1ef16cd7ce90']] \n",
      "\n",
      "\n",
      "[[[\"That you have gone through the scikit-learn approach. Let's learn about its built-in modules for loading the data set contents and categories. The diagram shows you how a data set is loaded using methods and how the object is structured in terms of the container folder and it's categories to load files. You have to use the main method scikit-learn datasets that load files. We can load txt files with categories as sub folder names. The folder names are used. And as supervised signal label names, it doesn't try to extract features into a numpy array or sci-fi sparse Matrix. In addition if load content parameter is false. It doesn't try to load the files in memory to use text files in a scikit-learn classification or clustering algorithm. You'll need to use the sklearn feature extraction text module to build a feature extraction Transformer. That suits your problem. A data load object helps load the contents of a data set shown here are the attributes of a data load object. Bunch it contains fields and can be accessed as dict keys or an object Target names. It's a list of requested categories. Data, it refers to an attribute in the memory where files are loaded. Let's see how a data set can be loaded using scikit-learn. There are several data sets that can be loaded. Let's load the digits data set to view the data using built-in methods perform. The following steps One Import the desired data set use the syntax from sklearn datasets import load digits. to load the data set by creating an object as we had discussed earlier a data load object is used to load contents 3 describe the data set using \"], '2020-01-31T08:06:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a2e1a1c-863a-4f84-a3dd-847741a45fff'], [[\"Bes CR function using this function you can view all of the information which describes the data set. Next let's do the data set type use this syntax type digits data set as we can see the type of the data set is bunch to view the data of the digits data set apply the function data the output displays the actual data which is bound to the data set. Let's view the target with the function dot Target. The array is displayed here are the response data which is present in the data set. \"], '2020-01-31T08:08:59Z', '716067a60a1a4034abc49a12ecafb39b', '9a5f97b6-42be-4c0f-9f8a-17b40d97165b']] \n",
      "\n",
      "\n",
      "8\n",
      "Before Merging 14\n",
      "[(10, 11), (2, 3)]\n",
      "\n",
      "\n",
      "World of text analysis stop words usually have little lexical meaning some examples of stop words. Are I me myself we our hours you yours and so on. Now let's view the first 10 stop words present in the stop words Corpus of in ltk. Library. This is done by passing 0 to 10 index position in the parentheses. \n",
      "\n",
      "\n",
      "We'll use string class punctuation to remove all the punctuation present in each message as they are also less waited for text analysis, then we'll remove the stop words. \n",
      "We'll use basic python syntax to display the characters which don't have punctuation as you can see we make use of basic string class and its punctuation function. \n",
      "\n",
      "\n",
      "Next feature of scikit-learn is model training an important task in model training is to identify the right model for the given data set scikit-learn provides a range of models to choose from and also trains them by using the extracted features from the data set. The choice of model completely depends on the type of data set. The models can be trained as a supervised learning model or unsupervised learning model supervised learning model for a given data set will have observations features. Has and response in supervised learning we make use of all three tasks predict the outcome of new observations and data sets understand which predictors affect the response or outcome and generalize the data and find the right answer generalization is also known as making predictions models can be trained to classify the documents based on the features and response. For example, naive Bayes svm linear regression and KNN Neighbors. Unsupervised learning model in this type of learning the response or the outcome of the label of the data is not known there is no definite or right answer in this model. So here we try to train the model by following a few steps such as understanding the structure of the data first and identify the pattern in the data finding predictors that behave in the same fashion or have some familiarity exploring the data set and the goal is representation of the data. Rotation is also known as extracting structure unsupervised model can also be used to group documents by applying clustering algorithms. For example clustering text documents using k-means. \n",
      "\n",
      "\n",
      "Bayes is a basic technique for classification of text. It assumes that the probability of each attribute belongs to a given class value and is independent of all other attributes the advantages of naive Bayes classifier it performs the task with limited CPU and memory therefore. It's very efficient. It's fast as the model training time is very less naive Bayes is heavily used in cinnamon analysis email spam detection categorization of documents. And language detection multinomial naive Bayes is used when multiple occurrences of the words matter multinomial distribution normally requires integer feature counts. Let's take a quick look at the psyche. It multi nominal in be model class. There are three major parameters in this class Alpha. It's also known as a lap Place transform or smoothing parameter and is mainly used for categorical data fit prior. It indicates whether to learn class prior probabilities. Not if false a uniform prior will be used but as you notice the default is true class prior. The default is none. But if specified the prior class is adjusted according to the data. The next feature of scikit-learn will be looking into is grid search. We use a document classifier to predict the category of a document to try predicting the outcome of a new document. We need to extract the features. A document classifier can have many parameters and the grid approach helps to search the best parameters for model training and predicting the outcome accurately when we create a model it ends up with multiple parameters. It would be too tedious to run through each parameter and tweak it in such cases a grid search useful the whole data set can be divided into multiple grids like a chessboard and the search can be run on entire grids or a selected combination of Grids, so we use a grid search. \n",
      "An ism to perform an exhaustive search on the best parameters which affect the model however this approach has some constraints such as the whole grid search process is subjected to availability of the CPU cores of the system. This is owing to the fact that there can be several parameters and running an exhaustive search can impact the system performance by taking more of the CPU memory. \n",
      "Far we have learned that there are three major steps for analyzing text Data one vectorizer. This is a general process of converting a collection of text documents into a numerical feature Vector in vectorization. The documents are split into words or tokens and each document is assigned a number. Two Transformer during transformation you find the occurrence of each word in a document. This process is primarily used for feature extraction. Three classifier or model training this is required for accurate predictions a model uses training and testing data sets to optimize its overall performance. For example different algorithms are used to classify text documents and find the sentiment analysis. This is where scikit-learn becomes more productive by creating the pipeline for the data set. A pipeline is essentially a combination of these three steps. So using scikit-learn pipeline can be created and we can train the model using a single command. We just need to put them all together in a single class and create a pipeline to operate on a given data set. The main purpose of the pipeline is to assemble several steps that can be cross validated while setting different. printers \n",
      "After Merging 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Ent map before filtering:  {'2': [1706, 4865, 4452, 4865, 4351, 4221, 4598, 2613, 1049, 239, 4865, 239, 4865, 3675, 1049, 6487, 1706, 4602, 2923, 4452, 4865, 4452, 4865, 1706, 1593, 1049, 2613, 4221, 4351, 2613, 1049, 2613, 2613, 4865, 5140, 4865, 4452, 398, 1706, 2613, 1049, 5140, 4865, 4865, 1875, 1049, 1366, 2613, 631, 1706, 4865, 1706, 2613, 1049, 5140, 4452, 4865, 2613, 2613, 2613], '9': [6118, 4455, 2404, 656, 5956, 3507, 2046, 5096, 3238, 4889], '0': [2613, 2613, 4976, 5363, 2613, 1366, 1081, 2404, 1658, 6420, 2613, 4976, 2613, 5363, 1081, 1658, 2404, 2613, 1366, 6420, 1081, 1658, 5363, 2404, 6420, 2613, 4976, 2613, 1366, 2613, 2613, 2613, 4976, 5363, 1081, 1658, 2404, 2613, 1366, 6420, 1081, 2613, 2404, 5363, 2613, 1658, 4976, 6420, 2613, 1366], '7': [2613, 5140, 4151, 5140, 4447, 2610, 2613, 5140, 1049, 4719], '3': [4598, 2404, 1775, 4040, 5956, 1494, 2888, 299, 239, 4352, 818, 3157, 1292, 3157, 2272, 3811, 818, 3216, 143, 4822, 2404, 748, 6113, 4598, 714, 2888, 4814, 299, 1044, 748, 1049, 5140, 5140, 4447, 2613, 1049, 2613, 5140, 4865, 2613], '11': [1016, 5132, 1021, 3282, 4247, 5564, 2404, 1017, 6647, 5488, 5132, 1016, 1021, 3996, 5501, 4921, 6329, 5501, 5824, 6515, 1016, 5132, 4247, 3282, 2404, 1021, 5824, 5564, 6515, 6647, 3282, 4247, 2404, 5132, 4921, 1016, 4247, 4814, 1494, 6647], '6': [4447, 2613, 5140, 5140, 2613, 2613, 2613, 5140, 4150, 5140, 4006, 1879, 4006, 3216, 2426, 44, 4822, 5063, 4006, 1292, 2613, 2613, 5140, 4447, 5140, 5140, 4150, 5140, 2613, 2613], '4': [5140, 4150, 5140, 5299, 2613, 2613, 5184, 5363, 5299, 3319, 2613, 2613, 5140, 5140, 4150, 2613, 3604, 1292, 6159, 3319], '8': [2613, 2613, 1658, 4086, 5059, 4602, 3363, 2669, 2613, 2613, 4602, 1658, 5059, 3363, 4086, 2669, 5059, 1658, 2613, 2613, 4602, 4086, 3363, 2669, 1658, 5059, 4086, 2613, 2613, 4602, 2669, 3363, 2613, 2613, 1658, 4086, 5059, 4602, 3363, 2669], '1': [3157, 2989, 818, 818, 3157, 1292, 3157, 4822, 5299, 4271, 818, 2299, 3103, 6709, 4814, 3157, 4081, 1292, 4271, 3426, 5299, 3861, 1292, 818, 4814, 3157, 4150, 4814, 143, 3157, 1508, 1875, 143, 3712, 6432, 5132, 3997, 852, 4774, 631, 6778, 5249, 818, 6124, 4081, 4510, 5013, 5299, 5249, 5299, 536, 6709, 2333, 2333, 3131, 4271, 6204, 3027, 3426, 2267, 1049, 5140, 5140, 4447, 5140, 4150, 2613, 2613, 1049, 3312, 5871, 2299, 2404, 992, 5070, 6098, 4456, 2046, 6709, 4081], '10': [4456, 1016, 5005, 6429, 6329, 4574, 4057, 4246, 5184, 1775, 1016, 5005, 2937, 6329, 4574, 5184, 6159, 3319, 852, 6429], '5': [3282, 3131, 818, 818, 4129, 5363, 4446, 372, 3282, 818, 818, 5363, 3131, 4446, 372, 4129]}\n",
      "Group Ent map after filtering:  {'2': [(4865, 12), (2613, 11), (1049, 7), (1706, 6), (4452, 5), (5140, 3)], '9': [], '0': [(2613, 15), (4976, 5), (5363, 5), (1366, 5), (1081, 5), (2404, 5), (1658, 5), (6420, 5)], '7': [(5140, 3)], '3': [(5140, 3), (2613, 3)], '11': [(1016, 4), (5132, 4), (4247, 4), (1021, 3), (3282, 3), (2404, 3), (6647, 3)], '6': [(2613, 8), (5140, 8), (4006, 3)], '4': [(2613, 5), (5140, 4)], '8': [(2613, 10), (1658, 5), (4086, 5), (5059, 5), (4602, 5), (3363, 5), (2669, 5)], '1': [(3157, 6), (818, 5), (5299, 4), (1292, 3), (4271, 3), (6709, 3), (4814, 3), (4081, 3), (5140, 3)], '10': [], '5': [(818, 4)]}\n",
      "Using  None  for feature extraction\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 103, \"module\": \"scorer\", \"batches count\": 1, \"number of sentences\": 45, \"ts\": \"2020-03-05T08:27:42.009927Z\", \"msg\": \"computing in batches\"}\n",
      "{\"level\": \"info\", \"filename\": \"scorer.py\", \"lineno\": 106, \"module\": \"scorer\", \"iteration count:\": 0, \"ts\": \"2020-03-05T08:27:42.010855Z\", \"msg\": \"getting feature vector\"}\n",
      "('Are two types of hierarchical clustering agglomerative or bottomup and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('Are two types of hierarchical clustering agglomerative or bottomup and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('Then the two closest clusters are combined then another XnumberX and so on until we are left with a single cluster note that all cluster Solutions are nested inside the dendrogram.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('Are two types of hierarchical clustering agglomerative or bottomup and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('We will have a short lecture about clustering of clustering originally cluster analysis was developed by anthropologists aiming to explain more engine of human beings later.', '2020-01-31T07:50:50Z', '716067a60a1a4034abc49a12ecafb39b', 'f9fccf2c-d252-4988-90f9-115e0dd5acf9')\n",
      "('Are two types of hierarchical clustering agglomerative or bottomup and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('Nowadays, there are two broad types of clustering flat and hierarchical kmeans is a flat method in the sense that there is no hierarchy but rather we choose the number of clusters and the magic happens.', '2020-01-31T07:50:50Z', '716067a60a1a4034abc49a12ecafb39b', 'f9fccf2c-d252-4988-90f9-115e0dd5acf9')\n",
      "('Are two types of hierarchical clustering agglomerative or bottomup and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('So it makes sense to get acquainted with it an example of clustering with hierarchy is taxonomy of the animal kingdom.', '2020-01-31T07:50:50Z', '716067a60a1a4034abc49a12ecafb39b', 'f9fccf2c-d252-4988-90f9-115e0dd5acf9')\n",
      "('Then we continue with XnumberX XnumberX XnumberX and so on until each observation.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('The name for this type of graph is a dendrogram a line starts from each observation.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('However, in order to find the best split we Must explore all possibilities at each step.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('However, in order to find the best split we Must explore all possibilities at each step.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('With K means we can simulate this divisive technique when it comes to agglomerative clustering.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('With K means we can simulate this divisive technique when it comes to agglomerative clustering.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('With K means we can simulate this divisive technique when it comes to agglomerative clustering.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('We will have a short lecture about clustering of clustering originally cluster analysis was developed by anthropologists aiming to explain more engine of human beings later.', '2020-01-31T07:50:50Z', '716067a60a1a4034abc49a12ecafb39b', 'f9fccf2c-d252-4988-90f9-115e0dd5acf9')\n",
      "('With K means we can simulate this divisive technique when it comes to agglomerative clustering.', '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48') ('Nowadays, there are two broad types of clustering flat and hierarchical kmeans is a flat method in the sense that there is no hierarchy but rather we choose the number of clusters and the magic happens.', '2020-01-31T07:50:50Z', '716067a60a1a4034abc49a12ecafb39b', 'f9fccf2c-d252-4988-90f9-115e0dd5acf9')\n",
      "('Two lines that merge are those of Germany and France according to the dendrogram these two countries of the closest in terms of the features considered at this point.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('If we Pierce them here we will get three Clusters North America, Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('There are five clusters Germany and France are XnumberX and each country has its own cluster from this point on going up Germany, and France will be considered one cluster.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('If we Pierce them here we will get three Clusters North America, Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('There are five clusters Germany and France are XnumberX and each country has its own cluster from this point on going up Germany, and France will be considered one cluster.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('This is the show that at the start each country is a separate cluster.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('The next two lines that emerge are those of the Germany and France cluster and the UK at this point.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('If we Pierce them here we will get three Clusters North America, Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('There are four clusters Germany France and the UK are one and the rest are single observation clusters at the next stage of the hierarchy Canada and the u.s.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('If we Pierce them here we will get three Clusters North America, Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('There are four clusters Germany France and the UK are one and the rest are single observation clusters at the next stage of the hierarchy Canada and the u.s.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('I would draw the line at three clusters and remain with North America Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('The next step is to unite the Germany France UK cluster with the Canada US XnumberX Australia is still alone.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('I would draw the line at three clusters and remain with North America Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('Finally all countries become one big cluster representing the whole sample.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('If we Pierce them here we will get three Clusters North America, Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('Finally all countries become one big cluster representing the whole sample.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('I would draw the line at three clusters and remain with North America Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('Finally all countries become one big cluster representing the whole sample.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('This is the show that at the start each country is a separate cluster.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('This shows that they are very similar in terms of longitude and latitude moreover Germany and France are closer than Germany and UK or France and UK the USA and Canada came together not long after.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('If we Pierce them here we will get three Clusters North America, Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('However, it took half of the dendrogram to join these five countries together.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('I would draw the line at three clusters and remain with North America Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('This indicates the Europe cluster and the North America cluster are not so alike.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('If we Pierce them here we will get three Clusters North America, Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('This indicates the Europe cluster and the North America cluster are not so alike.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('I would draw the line at three clusters and remain with North America Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('The other five countries was the other half of the dendrogram meaning it is extremely different from them.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('If we Pierce them here we will get three Clusters North America, Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('The other five countries was the other half of the dendrogram meaning it is extremely different from them.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('I would draw the line at three clusters and remain with North America Europe and Australia.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4')\n",
      "('To sum up the distance between the links show similarity or better dissimilarity between features.', '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76') ('There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('Next on our list is the choice of number of clusters.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4') ('There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('Next on our list is the choice of number of clusters.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4') ('Then the two closest clusters are combined then another XnumberX and so on until we are left with a single cluster note that all cluster Solutions are nested inside the dendrogram.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('Let is see some pros and cons the biggest Pro is that hierarchical clustering shows all the possible linkages between clusters.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4') ('This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('Let is see some pros and cons the biggest Pro is that hierarchical clustering shows all the possible linkages between clusters.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4') ('There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('Let is see some pros and cons the biggest Pro is that hierarchical clustering shows all the possible linkages between clusters.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4') ('Then the two closest clusters are combined then another XnumberX and so on until we are left with a single cluster note that all cluster Solutions are nested inside the dendrogram.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('So it is a nice option to be able to choose the method that works better for you kmeans is a one size fits it all method so you do not have that luxury.', '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4') ('Alright, let us explore a dendrogram and see how it works.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4')\n",
      "('Bomber Ativan device of clustering should reach similar results, but agglomerative is much easier to solve mathematically.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4') ('We will have a short lecture about clustering of clustering originally cluster analysis was developed by anthropologists aiming to explain more engine of human beings later.', '2020-01-31T07:50:50Z', '716067a60a1a4034abc49a12ecafb39b', 'f9fccf2c-d252-4988-90f9-115e0dd5acf9')\n",
      "('This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4') ('We will have a short lecture about clustering of clustering originally cluster analysis was developed by anthropologists aiming to explain more engine of human beings later.', '2020-01-31T07:50:50Z', '716067a60a1a4034abc49a12ecafb39b', 'f9fccf2c-d252-4988-90f9-115e0dd5acf9')\n",
      "('This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4') ('Nowadays, there are two broad types of clustering flat and hierarchical kmeans is a flat method in the sense that there is no hierarchy but rather we choose the number of clusters and the magic happens.', '2020-01-31T07:50:50Z', '716067a60a1a4034abc49a12ecafb39b', 'f9fccf2c-d252-4988-90f9-115e0dd5acf9')\n",
      "('This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4') ('So it makes sense to get acquainted with it an example of clustering with hierarchy is taxonomy of the animal kingdom.', '2020-01-31T07:50:50Z', '716067a60a1a4034abc49a12ecafb39b', 'f9fccf2c-d252-4988-90f9-115e0dd5acf9')\n",
      "('There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster.', '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4') ('Nowadays, there are two broad types of clustering flat and hierarchical kmeans is a flat method in the sense that there is no hierarchy but rather we choose the number of clusters and the magic happens.', '2020-01-31T07:50:50Z', '716067a60a1a4034abc49a12ecafb39b', 'f9fccf2c-d252-4988-90f9-115e0dd5acf9')\n",
      "{\"level\": \"info\", \"filename\": \"grouper_segments.py\", \"lineno\": 390, \"module\": \"grouper_segments\", \"outlier threshold is : \": 0.5998990535736084, \"ts\": \"2020-03-05T08:27:43.014424Z\", \"msg\": \"Outlier Score\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Community Algorithm\n",
      "cluster before alteration=========>\n",
      "Are two types of hierarchical clustering agglomerative or bottomup and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones.\n",
      "Then we continue with XnumberX XnumberX XnumberX and so on until each observation.\n",
      "However, in order to find the best split we Must explore all possibilities at each step.\n",
      "With K means we can simulate this divisive technique when it comes to agglomerative clustering.\n",
      "We start from different dog and cat breeds cluster them into dogs and cats respectively and then we continue pairing up Species until we reach the animal cluster.\n",
      "There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster.\n",
      "Then the two closest clusters are combined then another XnumberX and so on until we are left with a single cluster note that all cluster Solutions are nested inside the dendrogram.\n",
      "The name for this type of graph is a dendrogram a line starts from each observation.\n",
      "Alright, let us explore a dendrogram and see how it works.\n",
      "This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering.\n",
      "This is the show that at the start each country is a separate cluster.\n",
      "Bomber Ativan device of clustering should reach similar results, but agglomerative is much easier to solve mathematically.\n",
      "Okay so each Line starts from a cluster you can see the names of the countries at the beginning of those lines.\n",
      "cluster before alteration=========>\n",
      "We will have a short lecture about clustering of clustering originally cluster analysis was developed by anthropologists aiming to explain more engine of human beings later.\n",
      "Nowadays, there are two broad types of clustering flat and hierarchical kmeans is a flat method in the sense that there is no hierarchy but rather we choose the number of clusters and the magic happens.\n",
      "So it makes sense to get acquainted with it an example of clustering with hierarchy is taxonomy of the animal kingdom.\n",
      "There are birds which can fly and those that cannot we can continue in this way until we reach dogs and cats even then we can divide dogs and cats into different breeds moreover.\n",
      "The other type is hierarchical and that is what we are going to discuss in this lecture historically hierarchical clustering was developed first.\n",
      "Hi and welcome, we at three six five data science specializing in data science trainings.\n",
      "We post videos weekly so you can Master indispensable skills for free.\n",
      "There is the general term animal subclusters are fish mammals and birds for instance.\n",
      "cluster before alteration=========>\n",
      "Two lines that merge are those of Germany and France according to the dendrogram these two countries of the closest in terms of the features considered at this point.\n",
      "There are five clusters Germany and France are XnumberX and each country has its own cluster from this point on going up Germany, and France will be considered one cluster.\n",
      "The next two lines that emerge are those of the Germany and France cluster and the UK at this point.\n",
      "There are four clusters Germany France and the UK are one and the rest are single observation clusters at the next stage of the hierarchy Canada and the u.s.\n",
      "The next step is to unite the Germany France UK cluster with the Canada US XnumberX Australia is still alone.\n",
      "Finally all countries become one big cluster representing the whole sample.\n",
      "Well, the bigger the distance between two lengths the bigger the difference in terms of the chosen features as you can see Germany France and the UK merged into one cluster very quickly.\n",
      "This shows that they are very similar in terms of longitude and latitude moreover Germany and France are closer than Germany and UK or France and UK the USA and Canada came together not long after.\n",
      "However, it took half of the dendrogram to join these five countries together.\n",
      "This indicates the Europe cluster and the North America cluster are not so alike.\n",
      "The other five countries was the other half of the dendrogram meaning it is extremely different from them.\n",
      "To sum up the distance between the links show similarity or better dissimilarity between features.\n",
      "cluster before alteration=========>\n",
      "If we Pierce them here we will get three Clusters North America, Europe and Australia.\n",
      "I would draw the line at three clusters and remain with North America Europe and Australia.\n",
      "Next on our list is the choice of number of clusters.\n",
      "We will be left with two clusters, right Australia in one and all the rest in the other instead.\n",
      "So we will be left with three clusters because the links were coming out of those three clusters.\n",
      "Well, there is no specific rule, but after solving several problems, you kind of develop an intuition when the distance between two stages is too big.\n",
      "It is probably a good idea to stop there for our case.\n",
      "Okay, when most people get acquainted with dendrograms, they like them a lot and I presume that is the case.\n",
      "Let is see some pros and cons the biggest Pro is that hierarchical clustering shows all the possible linkages between clusters.\n",
      "Another Pro is that there are many different methods to perform hierarchical clustering the most famous of which is the ward method different data behaves in different ways.\n",
      "So it is a nice option to be able to choose the method that works better for you kmeans is a one size fits it all method so you do not have that luxury.\n",
      "How about a con the biggest con which is also one of the reasons why hierarchical clustering is far from a\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Are two types of hierarchical clustering agglomerative or bottomup and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones.\n",
      "Then we continue with XnumberX XnumberX XnumberX and so on until each observation.\n",
      "However, in order to find the best split we Must explore all possibilities at each step.\n",
      "With K means we can simulate this divisive technique when it comes to agglomerative clustering.\n",
      "We start from different dog and cat breeds cluster them into dogs and cats respectively and then we continue pairing up Species until we reach the animal cluster.\n",
      "There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster.\n",
      "Then the two closest clusters are combined then another XnumberX and so on until we are left with a single cluster note that all cluster Solutions are nested inside the dendrogram.\n",
      "The name for this type of graph is a dendrogram a line starts from each observation.\n",
      "Alright, let us explore a dendrogram and see how it works.\n",
      "This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering.\n",
      "This is the show that at the start each country is a separate cluster.\n",
      "Bomber Ativan device of clustering should reach similar results, but agglomerative is much easier to solve mathematically.\n",
      "Okay so each Line starts from a cluster you can see the names of the countries at the beginning of those lines.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "We will have a short lecture about clustering of clustering originally cluster analysis was developed by anthropologists aiming to explain more engine of human beings later.\n",
      "Nowadays, there are two broad types of clustering flat and hierarchical kmeans is a flat method in the sense that there is no hierarchy but rather we choose the number of clusters and the magic happens.\n",
      "So it makes sense to get acquainted with it an example of clustering with hierarchy is taxonomy of the animal kingdom.\n",
      "There are birds which can fly and those that cannot we can continue in this way until we reach dogs and cats even then we can divide dogs and cats into different breeds moreover.\n",
      "The other type is hierarchical and that is what we are going to discuss in this lecture historically hierarchical clustering was developed first.\n",
      "Hi and welcome, we at three six five data science specializing in data science trainings.\n",
      "We post videos weekly so you can Master indispensable skills for free.\n",
      "There is the general term animal subclusters are fish mammals and birds for instance.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "Two lines that merge are those of Germany and France according to the dendrogram these two countries of the closest in terms of the features considered at this point.\n",
      "There are five clusters Germany and France are XnumberX and each country has its own cluster from this point on going up Germany, and France will be considered one cluster.\n",
      "The next two lines that emerge are those of the Germany and France cluster and the UK at this point.\n",
      "There are four clusters Germany France and the UK are one and the rest are single observation clusters at the next stage of the hierarchy Canada and the u.s.\n",
      "The next step is to unite the Germany France UK cluster with the Canada US XnumberX Australia is still alone.\n",
      "Finally all countries become one big cluster representing the whole sample.\n",
      "Well, the bigger the distance between two lengths the bigger the difference in terms of the chosen features as you can see Germany France and the UK merged into one cluster very quickly.\n",
      "This shows that they are very similar in terms of longitude and latitude moreover Germany and France are closer than Germany and UK or France and UK the USA and Canada came together not long after.\n",
      "However, it took half of the dendrogram to join these five countries together.\n",
      "This indicates the Europe cluster and the North America cluster are not so alike.\n",
      "The other five countries was the other half of the dendrogram meaning it is extremely different from them.\n",
      "To sum up the distance between the links show similarity or better dissimilarity between features.\n",
      "\n",
      "After removing overlapping groups\n",
      "cluster =========>\n",
      "\n",
      "If we Pierce them here we will get three Clusters North America, Europe and Australia.\n",
      "I would draw the line at three clusters and remain with North America Europe and Australia.\n",
      "Next on our list is the choice of number of clusters.\n",
      "We will be left with two clusters, right Australia in one and all the rest in the other instead.\n",
      "So we will be left with three clusters because the links were coming out of those three clusters.\n",
      "Well, there is no specific rule, but after solving several problems, you kind of develop an intuition when the distance between two stages is too big.\n",
      "It is probably a good idea to stop there for our case.\n",
      "Okay, when most people get acquainted with dendrograms, they like them a lot and I presume that is the case.\n",
      "Let is see some pros and cons the biggest Pro is that hierarchical clustering shows all the possible linkages between clusters.\n",
      "Another Pro is that there are many different methods to perform hierarchical clustering the most famous of which is the ward method different data behaves in different ways.\n",
      "So it is a nice option to be able to choose the method that works better for you kmeans is a one size fits it all method so you do not have that luxury.\n",
      "How about a con the biggest con which is also one of the reasons why hierarchical clustering is far from a\n",
      "\n",
      "\n",
      "Checking Timerange --------------\n",
      "--------------\n",
      "However, in order to find the best split we Must explore all possibilities at each step. 17f84449-d736-4e5d-a8a2-ccd6bf258e48 \n",
      "\n",
      "Are two types of hierarchical clustering agglomerative or bottomup and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones. 17f84449-d736-4e5d-a8a2-ccd6bf258e48 \n",
      "\n",
      "Then we continue with XnumberX XnumberX XnumberX and so on until each observation. 17f84449-d736-4e5d-a8a2-ccd6bf258e48 \n",
      "\n",
      "We start from different dog and cat breeds cluster them into dogs and cats respectively and then we continue pairing up Species until we reach the animal cluster. 17f84449-d736-4e5d-a8a2-ccd6bf258e48 \n",
      "\n",
      "With K means we can simulate this divisive technique when it comes to agglomerative clustering. 17f84449-d736-4e5d-a8a2-ccd6bf258e48 \n",
      "\n",
      "There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster. ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4 \n",
      "\n",
      "This is the show that at the start each country is a separate cluster. ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4 \n",
      "\n",
      "Bomber Ativan device of clustering should reach similar results, but agglomerative is much easier to solve mathematically. ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4 \n",
      "\n",
      "Alright, let us explore a dendrogram and see how it works. ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4 \n",
      "\n",
      "This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering. ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4 \n",
      "\n",
      "Okay so each Line starts from a cluster you can see the names of the countries at the beginning of those lines. ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4 \n",
      "\n",
      "The name for this type of graph is a dendrogram a line starts from each observation. ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4 \n",
      "\n",
      "Then the two closest clusters are combined then another XnumberX and so on until we are left with a single cluster note that all cluster Solutions are nested inside the dendrogram. ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4 \n",
      "\n",
      "--------------\n",
      "There are birds which can fly and those that cannot we can continue in this way until we reach dogs and cats even then we can divide dogs and cats into different breeds moreover. f9fccf2c-d252-4988-90f9-115e0dd5acf9 \n",
      "\n",
      "So it makes sense to get acquainted with it an example of clustering with hierarchy is taxonomy of the animal kingdom. f9fccf2c-d252-4988-90f9-115e0dd5acf9 \n",
      "\n",
      "Hi and welcome, we at three six five data science specializing in data science trainings. f9fccf2c-d252-4988-90f9-115e0dd5acf9 \n",
      "\n",
      "The other type is hierarchical and that is what we are going to discuss in this lecture historically hierarchical clustering was developed first. f9fccf2c-d252-4988-90f9-115e0dd5acf9 \n",
      "\n",
      "There is the general term animal subclusters are fish mammals and birds for instance. f9fccf2c-d252-4988-90f9-115e0dd5acf9 \n",
      "\n",
      "We will have a short lecture about clustering of clustering originally cluster analysis was developed by anthropologists aiming to explain more engine of human beings later. f9fccf2c-d252-4988-90f9-115e0dd5acf9 \n",
      "\n",
      "We post videos weekly so you can Master indispensable skills for free. f9fccf2c-d252-4988-90f9-115e0dd5acf9 \n",
      "\n",
      "Nowadays, there are two broad types of clustering flat and hierarchical kmeans is a flat method in the sense that there is no hierarchy but rather we choose the number of clusters and the magic happens. f9fccf2c-d252-4988-90f9-115e0dd5acf9 \n",
      "\n",
      "--------------\n",
      "The next two lines that emerge are those of the Germany and France cluster and the UK at this point. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "The other five countries was the other half of the dendrogram meaning it is extremely different from them. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "Two lines that merge are those of Germany and France according to the dendrogram these two countries of the closest in terms of the features considered at this point. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "The next step is to unite the Germany France UK cluster with the Canada US XnumberX Australia is still alone. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "Finally all countries become one big cluster representing the whole sample. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "Well, the bigger the distance between two lengths the bigger the difference in terms of the chosen features as you can see Germany France and the UK merged into one cluster very quickly. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "This shows that they are very similar in terms of longitude and latitude moreover Germany and France are closer than Germany and UK or France and UK the USA and Canada came together not long after. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "There are four clusters Germany France and the UK are one and the rest are single observation clusters at the next stage of the hierarchy Canada and the u.s. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "However, it took half of the dendrogram to join these five countries together. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "There are five clusters Germany and France are XnumberX and each country has its own cluster from this point on going up Germany, and France will be considered one cluster. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "To sum up the distance between the links show similarity or better dissimilarity between features. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "This indicates the Europe cluster and the North America cluster are not so alike. 7c954dac-81dd-40a1-b705-7aafe7181f76 \n",
      "\n",
      "--------------\n",
      "Okay, when most people get acquainted with dendrograms, they like them a lot and I presume that is the case. 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "If we Pierce them here we will get three Clusters North America, Europe and Australia. 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "It is probably a good idea to stop there for our case. 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "So we will be left with three clusters because the links were coming out of those three clusters. 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "How about a con the biggest con which is also one of the reasons why hierarchical clustering is far from a 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "Another Pro is that there are many different methods to perform hierarchical clustering the most famous of which is the ward method different data behaves in different ways. 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "We will be left with two clusters, right Australia in one and all the rest in the other instead. 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "Let is see some pros and cons the biggest Pro is that hierarchical clustering shows all the possible linkages between clusters. 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "Well, there is no specific rule, but after solving several problems, you kind of develop an intuition when the distance between two stages is too big. 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "Next on our list is the choice of number of clusters. 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "I would draw the line at three clusters and remain with North America Europe and Australia. 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "So it is a nice option to be able to choose the method that works better for you kmeans is a one size fits it all method so you do not have that luxury. 7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4 \n",
      "\n",
      "<---------------->\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order difference: 0\n",
      "Relevant sentence:  However, in order to find the best split we Must explore all possibilities at each step.    =====    Are two types of hierarchical clustering agglomerative or bottomup and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones.\n",
      "order difference: 0\n",
      "Relevant sentence:  Are two types of hierarchical clustering agglomerative or bottomup and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones.    =====    Then we continue with XnumberX XnumberX XnumberX and so on until each observation.\n",
      "order difference: 0\n",
      "Relevant sentence:  Then we continue with XnumberX XnumberX XnumberX and so on until each observation.    =====    We start from different dog and cat breeds cluster them into dogs and cats respectively and then we continue pairing up Species until we reach the animal cluster.\n",
      "order difference: 0\n",
      "Relevant sentence:  We start from different dog and cat breeds cluster them into dogs and cats respectively and then we continue pairing up Species until we reach the animal cluster.    =====    With K means we can simulate this divisive technique when it comes to agglomerative clustering.\n",
      "order difference: 1\n",
      "Relevant sentence:  With K means we can simulate this divisive technique when it comes to agglomerative clustering.    =====    There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster.\n",
      "order difference: 0\n",
      "Relevant sentence:  There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n XnumberX cluster solution Then we repeat this procedure until all observations are in a single cluster.    =====    This is the show that at the start each country is a separate cluster.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is the show that at the start each country is a separate cluster.    =====    Bomber Ativan device of clustering should reach similar results, but agglomerative is much easier to solve mathematically.\n",
      "order difference: 0\n",
      "Relevant sentence:  Bomber Ativan device of clustering should reach similar results, but agglomerative is much easier to solve mathematically.    =====    Alright, let us explore a dendrogram and see how it works.\n",
      "order difference: 0\n",
      "Relevant sentence:  Alright, let us explore a dendrogram and see how it works.    =====    This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering.\n",
      "order difference: 0\n",
      "Relevant sentence:  This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering.    =====    Okay so each Line starts from a cluster you can see the names of the countries at the beginning of those lines.\n",
      "order difference: 0\n",
      "Relevant sentence:  Okay so each Line starts from a cluster you can see the names of the countries at the beginning of those lines.    =====    The name for this type of graph is a dendrogram a line starts from each observation.\n",
      "order difference: 0\n",
      "Relevant sentence:  The name for this type of graph is a dendrogram a line starts from each observation.    =====    Then the two closest clusters are combined then another XnumberX and so on until we are left with a single cluster note that all cluster Solutions are nested inside the dendrogram.\n",
      "order difference: 0\n",
      "Relevant sentence:  There are birds which can fly and those that cannot we can continue in this way until we reach dogs and cats even then we can divide dogs and cats into different breeds moreover.    =====    So it makes sense to get acquainted with it an example of clustering with hierarchy is taxonomy of the animal kingdom.\n",
      "order difference: 0\n",
      "Relevant sentence:  So it makes sense to get acquainted with it an example of clustering with hierarchy is taxonomy of the animal kingdom.    =====    Hi and welcome, we at three six five data science specializing in data science trainings.\n",
      "order difference: 0\n",
      "Relevant sentence:  Hi and welcome, we at three six five data science specializing in data science trainings.    =====    The other type is hierarchical and that is what we are going to discuss in this lecture historically hierarchical clustering was developed first.\n",
      "order difference: 0\n",
      "Relevant sentence:  The other type is hierarchical and that is what we are going to discuss in this lecture historically hierarchical clustering was developed first.    =====    There is the general term animal subclusters are fish mammals and birds for instance.\n",
      "order difference: 0\n",
      "Relevant sentence:  There is the general term animal subclusters are fish mammals and birds for instance.    =====    We will have a short lecture about clustering of clustering originally cluster analysis was developed by anthropologists aiming to explain more engine of human beings later.\n",
      "order difference: 0\n",
      "Relevant sentence:  We will have a short lecture about clustering of clustering originally cluster analysis was developed by anthropologists aiming to explain more engine of human beings later.    =====    We post videos weekly so you can Master indispensable skills for free.\n",
      "order difference: 0\n",
      "Relevant sentence:  We post videos weekly so you can Master indispensable skills for free.    =====    Nowadays, there are two broad types of clustering flat and hierarchical kmeans is a flat method in the sense that there is no hierarchy but rather we choose the number of clusters and the magic happens.\n",
      "order difference: 0\n",
      "Relevant sentence:  The next two lines that emerge are those of the Germany and France cluster and the UK at this point.    =====    The other five countries was the other half of the dendrogram meaning it is extremely different from them.\n",
      "order difference: 0\n",
      "Relevant sentence:  The other five countries was the other half of the dendrogram meaning it is extremely different from them.    =====    Two lines that merge are those of Germany and France according to the dendrogram these two countries of the closest in terms of the features considered at this point.\n",
      "order difference: 0\n",
      "Relevant sentence:  Two lines that merge are those of Germany and France according to the dendrogram these two countries of the closest in terms of the features considered at this point.    =====    The next step is to unite the Germany France UK cluster with the Canada US XnumberX Australia is still alone.\n",
      "order difference: 0\n",
      "Relevant sentence:  The next step is to unite the Germany France UK cluster with the Canada US XnumberX Australia is still alone.    =====    Finally all countries become one big cluster representing the whole sample.\n",
      "order difference: 0\n",
      "Relevant sentence:  Finally all countries become one big cluster representing the whole sample.    =====    Well, the bigger the distance between two lengths the bigger the difference in terms of the chosen features as you can see Germany France and the UK merged into one cluster very quickly.\n",
      "order difference: 0\n",
      "Relevant sentence:  Well, the bigger the distance between two lengths the bigger the difference in terms of the chosen features as you can see Germany France and the UK merged into one cluster very quickly.    =====    This shows that they are very similar in terms of longitude and latitude moreover Germany and France are closer than Germany and UK or France and UK the USA and Canada came together not long after.\n",
      "order difference: 0\n",
      "Relevant sentence:  This shows that they are very similar in terms of longitude and latitude moreover Germany and France are closer than Germany and UK or France and UK the USA and Canada came together not long after.    =====    There are four clusters Germany France and the UK are one and the rest are single observation clusters at the next stage of the hierarchy Canada and the u.s.\n",
      "order difference: 0\n",
      "Relevant sentence:  There are four clusters Germany France and the UK are one and the rest are single observation clusters at the next stage of the hierarchy Canada and the u.s.    =====    However, it took half of the dendrogram to join these five countries together.\n",
      "order difference: 0\n",
      "Relevant sentence:  However, it took half of the dendrogram to join these five countries together.    =====    There are five clusters Germany and France are XnumberX and each country has its own cluster from this point on going up Germany, and France will be considered one cluster.\n",
      "order difference: 0\n",
      "Relevant sentence:  There are five clusters Germany and France are XnumberX and each country has its own cluster from this point on going up Germany, and France will be considered one cluster.    =====    To sum up the distance between the links show similarity or better dissimilarity between features.\n",
      "order difference: 0\n",
      "Relevant sentence:  To sum up the distance between the links show similarity or better dissimilarity between features.    =====    This indicates the Europe cluster and the North America cluster are not so alike.\n",
      "order difference: 0\n",
      "Relevant sentence:  Okay, when most people get acquainted with dendrograms, they like them a lot and I presume that is the case.    =====    If we Pierce them here we will get three Clusters North America, Europe and Australia.\n",
      "order difference: 0\n",
      "Relevant sentence:  If we Pierce them here we will get three Clusters North America, Europe and Australia.    =====    It is probably a good idea to stop there for our case.\n",
      "order difference: 0\n",
      "Relevant sentence:  It is probably a good idea to stop there for our case.    =====    So we will be left with three clusters because the links were coming out of those three clusters.\n",
      "order difference: 0\n",
      "Relevant sentence:  So we will be left with three clusters because the links were coming out of those three clusters.    =====    How about a con the biggest con which is also one of the reasons why hierarchical clustering is far from a\n",
      "order difference: 0\n",
      "Relevant sentence:  How about a con the biggest con which is also one of the reasons why hierarchical clustering is far from a    =====    Another Pro is that there are many different methods to perform hierarchical clustering the most famous of which is the ward method different data behaves in different ways.\n",
      "order difference: 0\n",
      "Relevant sentence:  Another Pro is that there are many different methods to perform hierarchical clustering the most famous of which is the ward method different data behaves in different ways.    =====    We will be left with two clusters, right Australia in one and all the rest in the other instead.\n",
      "order difference: 0\n",
      "Relevant sentence:  We will be left with two clusters, right Australia in one and all the rest in the other instead.    =====    Let is see some pros and cons the biggest Pro is that hierarchical clustering shows all the possible linkages between clusters.\n",
      "order difference: 0\n",
      "Relevant sentence:  Let is see some pros and cons the biggest Pro is that hierarchical clustering shows all the possible linkages between clusters.    =====    Well, there is no specific rule, but after solving several problems, you kind of develop an intuition when the distance between two stages is too big.\n",
      "order difference: 0\n",
      "Relevant sentence:  Well, there is no specific rule, but after solving several problems, you kind of develop an intuition when the distance between two stages is too big.    =====    Next on our list is the choice of number of clusters.\n",
      "order difference: 0\n",
      "Relevant sentence:  Next on our list is the choice of number of clusters.    =====    I would draw the line at three clusters and remain with North America Europe and Australia.\n",
      "order difference: 0\n",
      "Relevant sentence:  I would draw the line at three clusters and remain with North America Europe and Australia.    =====    So it is a nice option to be able to choose the method that works better for you kmeans is a one size fits it all method so you do not have that luxury.\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3)]\n",
      "[[['Are two types of hierarchical clustering agglomerative or bottom-up and divisive or top down with Device of clustering we start from a situation where all observations are in the same cluster like the dinosaurs then we split this big cluster into two smaller ones. Then we continue with 3 4 5 and so on until each observation. Is it separate cluster? However, in order to find the best split we Must explore all possibilities at each step. Therefore faster methods have been developed such as K means. With K means we can simulate this divisive technique when it comes to agglomerative clustering. The approach is bottom up. We start from different dog and cat breeds cluster them into dogs and cats respectively and then we continue pairing up Species until we reach the animal cluster. '], '2020-01-31T07:52:20Z', '716067a60a1a4034abc49a12ecafb39b', '17f84449-d736-4e5d-a8a2-ccd6bf258e48'], [[\"Bomber Ativan device of clustering should reach similar results, but agglomerative is much easier to solve mathematically. This is also the other clustering method, we will explore agglomerative hierarchical clustering in order to perform agglomerative hierarchical clustering. We start with each case being its own cluster. There is a total of n clusters second using some similarity measure like euclidean distance we group the two closest clusters together reaching and n- 1 cluster solution Then we repeat this procedure until all observations are in a single cluster. The end result looks like this Animal Kingdom representation. The name for this type of graph is a dendrogram a line starts from each observation. Then the two closest clusters are combined then another 2 and so on until we are left with a single cluster note that all cluster Solutions are nested inside the dendrogram. Alright, let's explore a dendrogram and see how it works. Here is the dendrogram created on our country cluster data. Okay so each Line starts from a cluster you can see the names of the countries at the beginning of those lines. This is the show that at the start each country is a separate cluster. \"], '2020-01-31T07:53:17Z', '716067a60a1a4034abc49a12ecafb39b', 'ae85ee1b-512b-4ee6-ac7b-bf66ee4bc7b4']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Hi and welcome, we at three six five data science specializing in data science trainings. We post videos weekly so you can Master indispensable skills for free. All right, let's get started. We will have a short lecture about clustering of clustering originally cluster analysis was developed by anthropologists aiming to explain more engine of human beings later. It was adopted by psychology intelligence and other areas. Nowadays, there are two broad types of clustering flat and hierarchical k-means is a flat method in the sense that there is no hierarchy but rather we choose the number of clusters and the magic happens. The other type is hierarchical and that's what we are going to discuss in this lecture historically hierarchical clustering was developed first. So it makes sense to get acquainted with it an example of clustering with hierarchy is taxonomy of the animal kingdom. For instance. There is the general term animal subclusters are fish mammals and birds for instance. There are birds which can fly and those that can't we can continue in this way until we reach dogs and cats even then we can divide dogs and cats into different breeds moreover. Some breeds have sub breeds. \"], '2020-01-31T07:50:50Z', '716067a60a1a4034abc49a12ecafb39b', 'f9fccf2c-d252-4988-90f9-115e0dd5acf9']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"Two lines that merge are those of Germany and France according to the dendrogram these two countries of the closest in terms of the features considered at this point. There are five clusters Germany and France are 1 and each country has its own cluster from this point on going up Germany, and France will be considered one cluster. Now here's where it becomes interesting. The next two lines that emerge are those of the Germany and France cluster and the UK at this point. There are four clusters Germany France and the UK are one and the rest are single observation clusters at the next stage of the hierarchy Canada and the u.s. Joined forces. The next step is to unite the Germany France UK cluster with the Canada US 1 Australia is still alone. Finally all countries become one big cluster representing the whole sample. Okay, cool. What other information can we get from the dendrogram? Well, the bigger the distance between two lengths the bigger the difference in terms of the chosen features as you can see Germany France and the UK merged into one cluster very quickly. This shows that they are very similar in terms of longitude and latitude moreover Germany and France are closer than Germany and UK or France and UK the USA and Canada came together not long after. However, it took half of the dendrogram to join these five countries together. This indicates the Europe cluster and the North America cluster are not so alike. Finally the distance needed for Australia to join. The other five countries was the other half of the dendrogram meaning it is extremely different from them. To sum up the distance between the links show similarity or better dissimilarity between features. \"], '2020-01-31T07:54:42Z', '716067a60a1a4034abc49a12ecafb39b', '7c954dac-81dd-40a1-b705-7aafe7181f76']] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================Group Cluster=========================\n",
      "[[[\"All right. Next on our list is the choice of number of clusters. If we draw a straight line piercing these two links. We will be left with two clusters, right Australia in one and all the rest in the other instead. If we Pierce them here we will get three Clusters North America, Europe and Australia. The general rule is when you draw a straight line. You should count. The number of links that have been broken. In this case. We have broken Three Links. So we will be left with three clusters because the links were coming out of those three clusters. Should we break the links here? There will be four clusters and so on great. Finally, how should we decide where to draw the line? Well, there is no specific rule, but after solving several problems, you kind of develop an intuition when the distance between two stages is too big. It is probably a good idea to stop there for our case. I would draw the line at three clusters and remain with North America Europe and Australia. Okay, when most people get acquainted with dendrograms, they like them a lot and I presume that is the case. With you too. Let's see some pros and cons the biggest Pro is that hierarchical clustering shows all the possible linkages between clusters. This helps us understand the data much much better moreover. We don't need to preset the number of clusters. We just observe the dendrogram and take a decision. Another Pro is that there are many different methods to perform hierarchical clustering the most famous of which is the ward method different data behaves in different ways. So it is a nice option to be able to choose the method that works better for you k-means is a one size fits it all method so you don't have that luxury. How about a con the biggest con which is also one of the reasons why hierarchical clustering is far from a \"], '2020-01-31T07:56:42Z', '716067a60a1a4034abc49a12ecafb39b', '7ee8531d-fe3f-46e2-8fb7-ef8c2877d2a4']] \n",
      "\n",
      "\n",
      "3\n",
      "Before Merging 4\n",
      "[]\n",
      "After Merging 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Ent map before filtering:  {'1': [891, 139, 1470, 2162, 2445, 3886, 2697, 449, 113, 16, 1508, 3216, 4774, 4814, 4814, 4725, 3216, 6204, 999, 1762], '0': [1508, 5704, 3715, 748, 3216, 103, 5299, 3712, 6119, 2267], '2': [1342, 1342, 3712, 398, 5016, 1342, 3524, 1342, 4719, 852, 1342, 3712, 1342, 5016, 1342, 3524, 1342, 1342, 852, 5704, 1342, 1342, 1342, 1342, 1342, 1342, 1342, 5016, 3524, 79], '3': [1342, 1342, 32, 3524, 3524, 1342, 1342, 1342, 1342, 5941]}\n",
      "Group Ent map after filtering:  {'1': [], '0': [], '2': [(1342, 16), (5016, 3), (3524, 3)], '3': [(1342, 6)]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset = os.listdir(\"../golden_dataset/ai/\")\n",
    "\n",
    "ranked = generate_gs([\"../golden_dataset/ai/\"+i for i in dataset if i!=\".ipynb_checkpoints\"])\n",
    "\n",
    "#dataset = pickle.load(open(\"../notebooks/last_10_meeting_dump.json\",\"rb\"))\n",
    "#ranked = generate_gs(dataset, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:27:46.147285Z",
     "start_time": "2020-03-05T08:27:46.060338Z"
    }
   },
   "outputs": [],
   "source": [
    "ranked_sorted = sorted(ranked.items(), key=lambda kv:kv[1], reverse=True)\n",
    "\n",
    "clusters = {}\n",
    "temp = []\n",
    "prev_com = 0\n",
    "for index,(word,cluster) in enumerate(sorted(com_map.items(), key=lambda kv:kv[1])):\n",
    "    if prev_com==cluster:\n",
    "        temp.append(word)\n",
    "        if index==len(com_map.items())-1:\n",
    "            clusters[cluster] = temp\n",
    "    else:\n",
    "        clusters[prev_com] = temp\n",
    "        temp = []\n",
    "        prev_com = cluster\n",
    "        temp.append(word)\n",
    "        if index==len(com_map.items())-1:\n",
    "            clusters[cluster] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:27:46.486248Z",
     "start_time": "2020-03-05T08:27:46.150254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Community:  3216   Freq:  19 \n",
      "\n",
      "['Transformers', 'CNN', 'LSTMs', 'NNs', 'Siamese Network', 'Hidden Layer 1', 'FCN', 'LSTM', 'Autoencoders', 'ANN', 'FFNs', 'Neural Networks', 'Neural Network Architecture', 'Networks', 'AAE', 'Recurrent Neural Network', 'Deep Neural Networks', 'Convolutional Neural Network', 'Artificial Neural Networks', 'Neural Net', 'FCNs', 'Convolutional', 'Deep', 'FC', 'Convolutional Neural Networks', 'Long', 'Neural', 'Softmax', 'LSTM Layer', 'Siamese', 'FFN', 'Fast', 'Neural Architecture', 'Layer', 'GRU', 'Neural Network', 'Neural Nets', 'LeNet', 'Recurrent Neural Networks', 'Transformer', 'Deep Neural Network', 'DCIGN', 'CNNs', 'RNNs', 'ConvNet', 'DBN', 'Artificial Neural Network', 'Network', 'Memory', 'Deep Neural', 'Perceptron', 'ANNs', 'Fc', 'CNN Architecture', 'Autoencoder', 'DNN', 'U-Net', 'RNN', 'DNNs']\n",
      "\n",
      "\n",
      "Community:  2613   Freq:  12 \n",
      "\n",
      "['', 'Speech', 'Stem', 'Unknown', 'Great', 'Norwegian', 'NLTKs', 'Hapax Legomena', 'BoW', 'On', 'This', 'Zrich', 'How', 'FastText', 'Good Morning', 'Lemmatization', 'Document-term', 'Arabic', 'Umm', 'She', 'All', 'Word Embedding', 'What', 'Bag-of-words', 'Queen', 'Word2vec', 'Ok', 'Feel', 'None', 'Word2Vec', 'Is', 'Bad', 'Review', 'World', 'English', 'A', 'Fasttext', 'Text', 'You', 'In', 'Lemmas', 'It', 'Not', 'Banyan', 'non-English', 'Thanks', 'Wow', 'Lexicon', 'And', 'Play', 'Grove', 'WordNet', 'Red Fox', 'Parse', 'Meh', 'See', 'Hebrew', 'Be', 'Me', 'Better', 'Like', 'The', 'Father', 'Thai', 'Wuthering Heights', 'He', 'Word Embeddings', 'Wordnet', 'Term Frequency-Inverse Document Frequency', 'Mandarin', 'Brother', 'Running', 'Bag', 'Love', 'Subword', 'Its', 'I', 'Ran', 'Words', 'They']\n",
      "\n",
      "\n",
      "Community:  6496   Freq:  6 \n",
      "\n",
      "['MM', 'Agent', 'PG', 'RL', 'Pst', '-greedy', 'Q-learning', 'Reinforcement', 'Reinforcement Learning RL', 'Qs', 'Learning RL', 'Q', 'MCST', 'A3C', 'Reinforcement Learning', 'Monte-Carlo', 'KL-divergence', 'A2C', 'Q-value', 'Deep Q', 'Minimax', 'KL-', 'SARSA Algorithm', 'MC', 'Qhat', 'Monte Carlo', 'MDP', 'Policy Gradient', 'DPG', 'Monte Carlo Tree Search', 'Q Learning', 'Q-Learning', 'Entropy', 'PPO', 'MDPs', 'DP', 'Trust Region', 'SARSA', 'Critic Network', 'Cart-pole', 'Reinforce', 'Wasserstein', 'Metric', 'Action-value', 'Bellmans Equation', 'SAC-X', 'Epsilon-greedy', 'Dynamic', 'Deep RL', 'Deep Reinforcement Learning', 'Lipschitz', 'Equation', 'DDPG', 'Actor', 'iLQR', 'Dynamic Programming', 'DQN', 'TRPO', 'Value', 'Kullback-Leibler', 'Epsilon', 'Epsilon-Greedy', 'MCTS']\n",
      "\n",
      "\n",
      "Community:  5140   Freq:  6 \n",
      "\n",
      "['TF', 'DTM', 'Tf-idf', 'TF-IDF', 'TDM', 'IDF']\n",
      "\n",
      "\n",
      "Community:  5249   Freq:  5 \n",
      "\n",
      "['Advantage', 'Fx', 'X2', 'Cv', 'F', 'Ymx+c', 'Wi', 'X', 'Hypothesis', 'Mx', 'Logistic Function', 'Gt', 'Al+2', 'S1', 'Horizontal', 'wL', 'Px', 'C0', 'Pc', 'Cx', 'Ith', 'Z', 'Zx', 'Cw', 'C1', 'fA', 'zL', 'fX', 'Lx', 'Xi', 'Ci', 'S0', 'PYX', 'Ft', 'Hx', 'Conditional', 'S', 'Nk', 'Xt']\n",
      "\n",
      "\n",
      "Community:  1049   Freq:  4 \n",
      "\n",
      "['Yi', 'Si', 'NMT', 'Context', 'Self-attention', 'Embeddings', 'POS', 'Word', 'Attention', 'Hi', 'Si-1', 'Vec', 'E_ij', 'Q_i', 'Context Vector', 'Decoder', 'C_i', 'Are', 'Positional', 'Hm', 'Emb_dim', 'Big', 'BERT', 'V_i', 'Pos', 'K_i']\n",
      "\n",
      "\n",
      "Community:  4627   Freq:  4 \n",
      "\n",
      "['OpenAI Fives', 'Atari', 'Lee Sedol', 'TD-Gammon', 'Atari Games', 'BAIR', 'Starcraft II', 'Checkers', 'DeepMinds AlphaGo', 'Dota2', 'Pokemon GO', 'Gary Kasparov', 'Games', 'Alpha Zero', 'Space Invaders', 'Overmind', 'TStarBot2', 'Dendi', 'Dota', 'Libratus', 'Mario', 'Attractor', 'AlphaGo', 'Goomba', 'Atari 2600', 'Jeopardy', 'AIIDE', 'Shogi', 'GO', 'StarCraft AI', 'Roshan', 'Zerg', 'Googles DeepMind', 'DOTA 2', 'Starcraft', 'DeepBlue', 'UAlbertaBot', 'Zero', 'Big Blue', 'Deep Blue', 'Ken Jennings', 'AlphaStar', 'William Blitz Lee', 'AlphaGo Zero', 'Game', 'Dota 2', 'Wolfenstein 3D', 'Go', 'Garry Kasparov', 'Poker', 'OpenAI Five', 'APM', 'Pong', 'TLO', 'Google DeepMind', 'MOBA', 'Breakout', 'Protoss', 'Tic-tac-toe', 'AlphaGos', 'AlphaZero', 'Alpha Go', 'FSM', 'PGN', 'RTS', 'StarCraft 2', 'Game AI', 'Mana', 'Montezumas Revenge', 'Alphastar', 'Vancouver', 'MaNa', 'DeepMinds', 'StarCraft', 'TStarBot1', 'Backgammon', 'Aiur', 'Raiman', 'Arthur Samuel', 'Googles AlphaGo', 'Chess', 'AlphaStars', 'IBMs Deep Blue', 'StarCraft II']\n",
      "\n",
      "\n",
      "Community:  3798   Freq:  3 \n",
      "\n",
      "['Sigmoid Functions', 'BN', 'Sigmoid', 'ReLU Function', 'Activation', 'ReLU', 'Tanh Function', 'Tanh', 'Vanishing Gradient', 'ELU', 'Sigmoid Function', 'Logistic Sigmoid', 'RELU', 'Forward Function', 'Sigmoid Neuron', 'Wij', 'Relu']\n",
      "\n",
      "\n",
      "Community:  3524   Freq:  3 \n",
      "\n",
      "['Color', 'Blue-green', 'Green', 'White', 'Blue Circle', 'Purple', 'Red Line', 'Pink', 'CxO', 'Blue', 'Black', 'Red', 'Yellow', 'Orange']\n",
      "\n",
      "\n",
      "Community:  4006   Freq:  2 \n",
      "\n",
      "['GoogLeNet', 'Vgg', 'ResNet-50', 'VGGNet', 'VGG-16', 'ImageNet', 'R-CNN', 'Resnet50', 'Vgg-16', 'CNN-based', 'CIFAR-10 Dataset', 'LocalizerIQ-Net', 'ResNet50', 'VGG19', 'Image', 'VGG', 'ILSVRC', 'InceptionNet', 'ResNet', 'AlexNet', 'Resnet101', 'ImageNet Dataset', 'Inception', 'CIFAR-100', 'CIFAR-10', 'Image Classification', 'RCNN', 'DenseNet', 'Resnet', 'VGG16']\n",
      "\n",
      "\n",
      "Community:  2697   Freq:  2 \n",
      "\n",
      "['Computer', 'Machine Learning', 'Learning', 'ML', 'Deep Learning', 'ML-based']\n",
      "\n",
      "\n",
      "Community:  70   Freq:  2 \n",
      "\n",
      "['AI', 'AIs']\n",
      "\n",
      "\n",
      "Community:  5299   Freq:  2 \n",
      "\n",
      "['Hypergeometric', 'T-distribution', 'Central Limit Theorem', 'AKA', 'Bernoulli', 'Pearsons', 'Non-parametric', 'Nonparametric', 'Least', 'Naive Bayes Model', 'Bayes', 'Ordinary Least Squares', 'Total Probability', 'Poisson', 'Poisson Distribution', 'PX', 'Error', 'FMD', 'Chi-square', 'LASSO', 'Statistical Mechanics', 'R-Squared', 'Laplace Smoothing', 'Binomial', 'Bell Curve', 'Naive Bayes Algorithm', 'Regularization', 'Mean', 'Conditional Entropy', 'Bayesian', 'Quantile', 'Logistic', 'KullbackLeibler', 'Naive Bayes', 'F-', 'Lasso', 'Exponential', 'F-statistic', 'Gini Coefficient', 'Bayesian Inference', 'T-test', 'Linear Regression', 'Gutenberg-Richter Law', 'Ordinary Least', 'Linear', 'Normal', 'Bayes Theorem', 'Uniform', 'Multicollinearity', 'Ridge', 'Chi-squared', 'KL', 'Power Law', 'IID', 'Laplace', 'Gaussian', 'Coefficient', 'Regression', 'Bernoulli Distribution', 'Covariance', 'Bayesian Statistics', 'Gaussian Process', 'Sx', 'Conditional Probability', 'Gaussian Distribution', 'Probabilistic', 'OLS', 'R-squared', 'Additive', 'Parametric', 'Dynamics', 'Naive Bayes Classification', 'Squares', 'Gamma', 'Boltzmann Distribution', 'Ridge Regression', 'GMM', 'L2 Regularization', 'Differential', 'Lasso Regression', 'Gaussians', 'Information-adjusted']\n",
      "\n",
      "\n",
      "Community:  4814   Freq:  2 \n",
      "\n",
      "['Support Vector Machines', 'Support Vector Machine', 'SVMs', 'SVM', 'Vector Machine', 'SVRs', 'SVR', 'Support Vector']\n",
      "\n",
      "\n",
      "Community:  2404   Freq:  2 \n",
      "\n",
      "['Python', 'SciPy']\n",
      "\n",
      "\n",
      "Community:  818   Freq:  2 \n",
      "\n",
      "['Model', 'Classifier', 'AIC']\n",
      "\n",
      "\n",
      "Community:  1342   Freq:  2 \n",
      "\n",
      "['South', 'UK', 'Singapore', 'Iran', 'Nigeria', 'Israel', 'North', 'ISIS', 'Denmark', 'USA', 'Turkey', 'Taiwan', 'Kenya', 'Russia', 'Syria', 'West', 'Korea', 'Western Europe', 'Japan', 'Iceland', 'Egypt', 'Asia-Pacific', 'Venezuela', 'Netherlands', 'United States', 'South Korea', 'Canada', 'Democracies', 'Italy', 'Yemen', 'Vietnam', 'Portugal', 'Spain', 'Iraq', 'France', 'African', 'North Korea', 'Europe', 'European', 'Africa', 'Malaysia', 'Hungary', 'United Kingdom', 'Paris Agreements', 'Mexico', 'Brazil', 'U', 'Western Democracies', 'Asia', 'Germany', 'World Bank', 'Sweden', 'NATO', 'India', 'Thailand', 'Indonesia', 'Britain', 'Bangkok', 'Finland', 'Kazakhstan', 'Norway', 'Asia Pacific', 'South Africa', 'Haiti', 'Ireland', 'Bangladesh', 'Australia', 'Pakistan', 'North America']\n",
      "\n",
      "\n",
      "Community:  56   Freq:  1 \n",
      "\n",
      "['Humans', 'Computers', 'Nature', 'Human']\n",
      "\n",
      "\n",
      "Community:  3378   Freq:  1 \n",
      "\n",
      "['Cats', 'Dog', 'Cat', 'Dogs']\n",
      "\n",
      "\n",
      "Community:  1951   Freq:  1 \n",
      "\n",
      "['Mathematics', 'Introduction', 'Probability', 'Information Theory', 'Statistical', 'Statistics', 'Statistical Learning', 'Linear Algebra', 'Probability Theory', 'Algebra', 'Graph Theory', 'Theorem', 'Social Science', 'Calculus', 'Equations']\n",
      "\n",
      "\n",
      "Community:  27   Freq:  1 \n",
      "\n",
      "['C', '3', '2', '1']\n",
      "\n",
      "\n",
      "Community:  103   Freq:  1 \n",
      "\n",
      "['Origin', 'Real', 'Space', 'Matrix']\n",
      "\n",
      "\n",
      "Community:  449   Freq:  1 \n",
      "\n",
      "['Action', 'Master']\n",
      "\n",
      "\n",
      "Community:  2267   Freq:  1 \n",
      "\n",
      "['RMSProp', 'Descent', 'ADAM', 'Objective', 'Adaptive Moment Estimation', 'Adam Optimizer', 'Gradient Descent', 'Backpropagation', 'Stochastic', 'Stochastic Gradient Descent', 'Rumelhart', 'Gradient', 'Momentum', 'SGD', 'Adam', 'Convex', 'ADAgrad', 'Vanishing Gradient Problem', 'GD', 'Hessian', 'Adam Optimization', 'RMSprop']\n",
      "\n",
      "\n",
      "Community:  5292   Freq:  1 \n",
      "\n",
      "['SMA', 'USD', 'MACD', 'BTC']\n",
      "\n",
      "\n",
      "Community:  4725   Freq:  1 \n",
      "\n",
      "['MLP', 'MLPs']\n",
      "\n",
      "\n",
      "Community:  3398   Freq:  1 \n",
      "\n",
      "['Self', 'Identity', 'Cartesian', 'Square Matrix', 'NxN']\n",
      "\n",
      "\n",
      "Community:  4774   Freq:  1 \n",
      "\n",
      "['Radial Basis Function', 'Kernel']\n",
      "\n",
      "\n",
      "Community:  4943   Freq:  1 \n",
      "\n",
      "['X-axis', 'Axis']\n",
      "\n",
      "\n",
      "Community:  4429   Freq:  1 \n",
      "\n",
      "['Visualization', 'Grammar', 'Data Visualization']\n",
      "\n",
      "\n",
      "Community:  4865   Freq:  1 \n",
      "\n",
      "['NLU', 'NLC', 'Language', 'NLP']\n",
      "\n",
      "\n",
      "Community:  1706   Freq:  1 \n",
      "\n",
      "['IntelligenceAI', 'Natural Language Processing', 'Natural Language Understanding', 'Natural Language Generation']\n",
      "\n",
      "\n",
      "Community:  4452   Freq:  1 \n",
      "\n",
      "['Natural Language', 'NL']\n",
      "\n",
      "\n",
      "Community:  1366   Freq:  1 \n",
      "\n",
      "['Finance', 'Analysis']\n",
      "\n",
      "\n",
      "Community:  4247   Freq:  1 \n",
      "\n",
      "['Numpy', 'NumPy']\n",
      "\n",
      "\n",
      "Community:  1021   Freq:  1 \n",
      "\n",
      "['Txt', 'Xml']\n",
      "\n",
      "\n",
      "Community:  2669   Freq:  1 \n",
      "\n",
      "['First', 'Theory']\n",
      "\n",
      "\n",
      "Community:  3157   Freq:  1 \n",
      "\n",
      "['Gaussian Naive Bayes', 'Bagging', 'Gradient Boosting Classifier', 'DecisionTree', 'Random Forest Algorithm', 'Decision Trees', 'Tree', 'Random', 'CART', 'AdaBoost', 'Random Forest Classifier', 'Random Forests', 'Random Forest', 'Forest', 'Logistic Regression', 'Forest-based', 'Trees', 'CART Algorithm', 'Decision Tree', 'ID3', 'RandomForestClassifier']\n",
      "\n",
      "\n",
      "Community:  4271   Freq:  1 \n",
      "\n",
      "['Hyper', 'Hyperparameter']\n"
     ]
    }
   ],
   "source": [
    "for com, freq in ranked_sorted:\n",
    "    if len(clusters[com])!=1:\n",
    "        print (\"\\n\\nCommunity: \", com, \"  Freq: \", freq ,\"\\n\")\n",
    "        print (clusters[com])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:29:32.263062Z",
     "start_time": "2020-03-05T08:29:32.193299Z"
    }
   },
   "outputs": [],
   "source": [
    "com_map_selected = {}\n",
    "for index, (com, freq) in enumerate(ranked_sorted):\n",
    "    com_map_selected[com] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:29:40.448232Z",
     "start_time": "2020-03-05T08:29:40.398967Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(ranked, open(\"/home/ray__/ssd/minds/ai/cw/gc.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:29:43.962906Z",
     "start_time": "2020-03-05T08:29:43.903755Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(com_map_selected, open(\"/home/ray__/ssd/minds/ai/cw/ranked_com.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:29:46.237686Z",
     "start_time": "2020-03-05T08:29:46.188991Z"
    }
   },
   "outputs": [],
   "source": [
    "lc = {}\n",
    "for cls, freq in ranked.items():\n",
    "    lc[cls] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T08:29:58.062922Z",
     "start_time": "2020-03-05T08:29:58.012910Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(lc, open(\"/home/ray__/ssd/minds/ai/cw/lc.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T11:35:15.690822Z",
     "start_time": "2020-01-28T11:35:15.640309Z"
    }
   },
   "outputs": [],
   "source": [
    "\"Spreadsheet\" in clusters[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:41:48.208310Z",
     "start_time": "2020-01-16T12:41:48.203264Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "com_map = pickle.load(open(\"/home/ray__/ssd/minds/ether/com_map.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:43:00.156492Z",
     "start_time": "2020-01-16T12:43:00.134884Z"
    }
   },
   "outputs": [],
   "source": [
    "ranked_sorted = sorted(com_map.items(), key=lambda kv:kv[1], reverse=True)\n",
    "\n",
    "clusters = []\n",
    "temp = []\n",
    "prev_com = 0\n",
    "for index,(word,cluster) in enumerate(sorted(com_map.items(), key=lambda kv:kv[1])):\n",
    "    if prev_com==cluster:\n",
    "        temp.append(word)\n",
    "        if index==len(com_map.items())-1:\n",
    "            clusters.append(temp)\n",
    "    else:\n",
    "        clusters.append(temp)\n",
    "        temp = []\n",
    "        prev_com = cluster\n",
    "        temp.append(word)\n",
    "        if index==len(com_map.items())-1:\n",
    "            clusters.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:44:29.472744Z",
     "start_time": "2020-01-16T12:44:29.464267Z"
    }
   },
   "outputs": [],
   "source": [
    "com_map_refined = {}\n",
    "for ent,cls in com_map.items():\n",
    "    if len(clusters[cls])<=2:\n",
    "        continue\n",
    "    else:\n",
    "        com_map_refined[ent] = cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T12:44:45.676671Z",
     "start_time": "2020-01-16T12:44:45.669775Z"
    }
   },
   "outputs": [],
   "source": [
    "len(com_map_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T14:22:15.174248Z",
     "start_time": "2020-01-08T14:22:14.504920Z"
    }
   },
   "outputs": [],
   "source": [
    "ent_check = \"Harvard Business Review\"\n",
    "ck = []\n",
    "for ent in ent_fv.keys():\n",
    "    ck.append((ent, 1-cosine(ent_fv[ent], ent_fv[ent_check])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T14:24:03.115899Z",
     "start_time": "2020-01-08T14:24:03.039837Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for ent, score in sorted(ck, key=lambda kv:kv[1], reverse=True)[:30]:\n",
    "    print (ent, \" -> \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:14:15.568122Z",
     "start_time": "2020-01-08T18:14:14.602193Z"
    }
   },
   "source": [
    "# testing sentence to entity similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T19:07:26.071775Z",
     "start_time": "2020-01-08T19:07:25.933420Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from extra_preprocess import preprocess_text\n",
    "\n",
    "#text = \"Everybody wants to create applications using the micro service architecture in this video. But rather than reading out a list of architectural Concepts, we will approach that a completely different place by me telling you a story that you really need to know. Let is go back in time to the old days when we did not have anything called micro services. Theyll go to a time when a computer was big enough to fill the room. Mr. Seen pictures of one of these things you would walk up to that computer. Fast forward a little bit to desktop machines again programs recite it on the desktop machine the programmer application contains all the instructions that are needed to execute. So when people would write applications for it their code would be compiled down in the whole thing would be installed on the computer in one go and it will be installed on the same computer think of purely desktop only apps. Okay, something like Microsoft Word or a text editor that you install on your I mean this has historically influence how we write code when we need to write an application. We would start a new code project and we would add functionality into that project need more functionality add more code. And so the size of the code base for any given application keeps increasing over time. So what starts as a small code base might end up turning into a large complex code base over time and people realize this.\"\n",
    "text = \"Nothing too fancy, but overtime web applications have started becoming better bigger quicker more complicated bigger scale bigger user base bigger everything and today we have web applications that can you know find something from the whole internet in a matter of milliseconds are fine. What cabs are available around you all over the world in milliseconds. These are incredible Feats if you think about it, and they need incredibly complicated code to be Developed and applied and this complexity becomes harder and harder to maintain. We have nice modular architecture on the code side of things is not that enough to handle the complexity during development time? Well with the type of applications we are talking about the complexity needs to be handled not just at the coding side of things. They also need to be handled at the runtime or execution side of things having a single thing that you would apply. Didnt work anymore this way of having a single application is called the monolithic application or monolithic architecture mono means single lithic means Stone single Stone monolithic. This is the Smosh basically What are some of the disadvantages of this monolithic model first the bigger the deployment the more challenging the deployment Let me Give an example. Let is say you want to push a new feature to your big monolithic application. So among all the code comments that you want to deploy is the single quote comment by this new guy. The company has just hired me not so sure about him. He probably does not know much and he still learning but his first code come it ever is still sitting there. Well, you need to test the whole thing before you would apply the whole application you never know which part of the application that come it might have broken. Well, I am exaggerating here, of course, but the fact remains.\"\n",
    "text_p = \" \".join(preprocess_text(text))\n",
    "\n",
    "text_pp = \" \".join(tp.preprocess(text, stop_words=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T19:07:28.262925Z",
     "start_time": "2020-01-08T19:07:27.046008Z"
    }
   },
   "outputs": [],
   "source": [
    "text_fv = gpt_model.get_text_feats(text)\n",
    "ck = []\n",
    "for ent in ent_fv.keys():\n",
    "    ck.append((ent, 1-cosine(ent_fv[ent], text_fv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T19:07:29.157499Z",
     "start_time": "2020-01-08T19:07:29.071270Z"
    }
   },
   "outputs": [],
   "source": [
    "[i for i,j in sorted(ck, key=lambda kv:kv[1], reverse=True)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T19:07:32.769075Z",
     "start_time": "2020-01-08T19:07:31.538365Z"
    }
   },
   "outputs": [],
   "source": [
    "text_fv = gpt_model.get_text_feats(text_p)\n",
    "ck = []\n",
    "for ent in ent_fv.keys():\n",
    "    ck.append((ent, 1-cosine(ent_fv[ent], text_fv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T19:07:32.839574Z",
     "start_time": "2020-01-08T19:07:32.771931Z"
    }
   },
   "outputs": [],
   "source": [
    "[i for i,j in sorted(ck, key=lambda kv:kv[1], reverse=True)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:31:05.951403Z",
     "start_time": "2020-01-08T18:31:05.022510Z"
    }
   },
   "outputs": [],
   "source": [
    "text_fv = gpt_model.get_text_feats(text_pp)\n",
    "ck = []\n",
    "for ent in ent_fv.keys():\n",
    "    ck.append((ent, 1-cosine(ent_fv[ent], text_fv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:31:09.098325Z",
     "start_time": "2020-01-08T18:31:09.038932Z"
    }
   },
   "outputs": [],
   "source": [
    "[i for i,j in sorted(ck, key=lambda kv:kv[1], reverse=True)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:48:50.370912Z",
     "start_time": "2020-01-08T18:48:50.319486Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_dict['Perfect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
