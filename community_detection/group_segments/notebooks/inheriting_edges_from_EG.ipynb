{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "import itertools\n",
    "import json\n",
    "from community import best_partition, modularity\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../../ai-engine/pkg/\")\n",
    "\n",
    "from text_preprocessing import preprocess as tp\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "def st_get_candidate_phrases(text, pos_search_pattern_list=[r\"\"\"base: {(<JJ.*>*<NN.*>+<IN>)?<JJ>*<NN.*>+}\"\"\"]):\n",
    "        punct = set(string.punctuation)\n",
    "        all_chunks = []\n",
    "\n",
    "        for pattern in pos_search_pattern_list:\n",
    "            all_chunks+=st_getregexChunks(text,pattern)\n",
    "\n",
    "        candidates_tokens = [' '.join(word for word, pos, \n",
    "                    chunk in group).lower() \n",
    "                    for key, group in itertools.groupby(all_chunks, \n",
    "                    lambda_unpack(lambda word, pos, chunk: chunk != 'O')) if key]\n",
    "\n",
    "        candidate_phrases = [cand for cand in candidates_tokens if cand not in stop_words and not all(char in punct for char in cand)]\n",
    "\n",
    "        return candidate_phrases\n",
    "    \n",
    "def st_getregexChunks(text,grammar):\n",
    "\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
    "                                                    for tagged_sent in tagged_sents))\n",
    "    #print(grammar)\n",
    "    #print(all_chunks)\n",
    "    #print()\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "def lambda_unpack(f):\n",
    "    return lambda args: f(*args)\n",
    "\n",
    "\n",
    "def get_filtered_pos(filtered, pos_list=['NN', 'JJ']):\n",
    "    filtered_list_temp = []\n",
    "    filtered_list = []\n",
    "    flag = False\n",
    "    flag_JJ = False\n",
    "    for word, pos in filtered:\n",
    "        if pos == 'NN' or pos == 'JJ':\n",
    "            flag=True\n",
    "            if pos == 'JJ':\n",
    "                flag_JJ = True\n",
    "            else:\n",
    "                flag_JJ = False\n",
    "            filtered_list_temp.append((word, pos))\n",
    "            continue\n",
    "        if flag:\n",
    "            if 'NN' in list(map(lambda x: x[1], filtered_list_temp)):\n",
    "                if not flag_JJ:\n",
    "                    filtered_list.append(list(map(lambda x:x[0], filtered_list_temp)))\n",
    "                else:\n",
    "                    filtered_list.append(list(map(lambda x:x[0], filtered_list_temp))[:-1])\n",
    "                    #print (filtered_list_temp)\n",
    "                    #print (filtered_list[-1])\n",
    "                    flag_JJ = False\n",
    "            filtered_list_temp = []\n",
    "            flag=False\n",
    "            \n",
    "    return filtered_list\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from boto3 import client as boto3_client\n",
    "import json\n",
    "import logging\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "config = Config(connect_timeout=240, read_timeout=240, retries={'max_attempts': 0} )\n",
    "lambda_client = boto3_client('lambda', config=config,     aws_access_key_id=\"AKIA5SUS6MWO4MP7KDEJ\",\n",
    "    aws_secret_access_key=\"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\"\n",
    ")\n",
    "\n",
    "def cosine(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "\n",
    "def get_embeddings(input_list, req_data=None):\n",
    "    if req_data is None:\n",
    "\n",
    "        #lambda_payload = {\"body\": {\"text_input\": input_list}}\n",
    "        lambda_payload = {\"body\": {\"text\": input_list}}\n",
    "    else:\n",
    "        lambda_payload = {\"body\": {\"request\": req_data, \"text_input\": input_list}}\n",
    "\n",
    "    #logger.info(\"Invoking lambda function\")\n",
    "    invoke_response = lambda_client.invoke(\n",
    "        #FunctionName=\"arn:aws:lambda:us-east-1:933389821341:function:keyphrase_ranker\",\n",
    "        FunctionName=\"arn:aws:lambda:us-east-1:933389821341:function:mind-01daaqy88qzb19jqz5prjfr76y\",\n",
    "        InvocationType=\"RequestResponse\",\n",
    "        Payload=json.dumps(lambda_payload)\n",
    "    )\n",
    "    \n",
    "    lambda_output = (\n",
    "        invoke_response[\"Payload\"].read().decode(\"utf8\").replace(\"'\", '\"')\n",
    "    )\n",
    "    response = json.loads(lambda_output)\n",
    "    try:\n",
    "        status_code = response[\"statusCode\"]\n",
    "    except Exception as e:\n",
    "        print (response)\n",
    "    response_body = response[\"body\"]\n",
    "\n",
    "    if status_code == 200:\n",
    "        #embedding_vector = np.asarray(json.loads(response_body)[\"embeddings\"])\n",
    "        embedding_vector = np.asarray(json.loads(response_body)[\"sent_feats\"])\n",
    "    else:\n",
    "        raise Exception\n",
    "        embedding_vector = np.asarray(json.loads(response_body)[\"embeddings\"])\n",
    "    return embedding_vector\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000000)\n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_kp_graph = pickle.load(open('/mnt/hdd/Venkat/knowledge_graphs/entity_graph_builder/graph_dumps/pruned_entity_kp_graph.pkl','rb'))\n",
    "ent_fv = pickle.load(open(\"/mnt/hdd/Venkat/knowledge_graphs/entity_graph_builder/graph_dumps/entity_features.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = json.load(open(\"../topic_testing/se_demo_test.txt\", \"r\"))[\"body\"]\n",
    "req[\"segments\"] = sorted(req['segments'], key=lambda kv:kv['startTime']) \n",
    "\n",
    "text = list(map(lambda seg: (seg[\"originalText\"], seg[\"id\"]), req['segments']))\n",
    "seg_list = [sent for sent, id  in text]\n",
    "segid_list = [id for sent, id in text]\n",
    "sent_list = list(map(lambda seg:[sent for sent in seg.split(\". \")], seg_list))\n",
    "sent_list = [sent for seg in sent_list for sent in seg if len(sent.split(\" \"))>6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list_from_graph_exact = []\n",
    "entities_in_graph = [ent.lower() for ent in entity_kp_graph if entity_kp_graph.nodes[ent][\"node_type\"]==\"entity\"]\n",
    "entities_in_graph_cased = [ent for ent in entity_kp_graph if entity_kp_graph.nodes[ent][\"node_type\"]==\"entity\"]\n",
    "entities_in_graph_org = {}\n",
    "for index in range(len(entities_in_graph_cased)):\n",
    "    entities_in_graph_org[entities_in_graph[index]] = entities_in_graph_cased[index]\n",
    "    \n",
    "for index, sent in enumerate(sent_list):\n",
    "    entities_list_from_graph_temp = []\n",
    "    for word in tp.preprocess(sent, word_tokenize=True)[0]:\n",
    "        if word.lower() in entities_in_graph:\n",
    "            entities_list_from_graph_temp.extend([word.lower()])\n",
    "        #entities_list_from_graph_temp.extend([sub_word for sub_word in entities_in_graph if word.lower() in sub_word])\n",
    "    entities_list_from_graph_exact.append(entities_list_from_graph_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_ent = [entities_in_graph_org[i] for j in entities_list_from_graph_exact for i in j]\n",
    "sub_graph = entity_kp_graph.subgraph(flattened_ent)\n",
    "sub_graph_copy = deepcopy(sub_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_graph = nx.Graph()\n",
    "for nodea, nodeb in sub_graph_copy.edges():\n",
    "    #if entity_kp_graph.nodes[nodea]['node_freq'] != 1 and entity_kp_graph.nodes[nodeb]['node_freq'] !=1:\n",
    "    weighted_graph.add_edge(nodea, nodeb, weight = entity_kp_graph[nodea][nodeb]['edge_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import pagerank\n",
    "\n",
    "pg = pagerank(weighted_graph, weight=\"weight\")\n",
    "\n",
    "pg_sorted = sorted(pg.items(), key=lambda kv:kv[1], reverse=True)\n",
    "pg_map = {}\n",
    "for ent, score in pg_sorted:\n",
    "    pg_map[ent] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = []\n",
    "for index in range(len(sent_list)):\n",
    "    graph_list.append((sent_list[index], [entities_in_graph_org[i] for i in entities_list_from_graph_exact[index] if entities_in_graph_org[i] in pg_map.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_graph = nx.Graph()\n",
    "\n",
    "for index, (sentence, ent) in enumerate(graph_list):\n",
    "    ent_list_t = list(map(lambda k: (k, pg_map[k]), ent))\n",
    "    if ent_list_t!=[]:\n",
    "        ent_list = max(ent_list_t, key=lambda kv:kv[1])[0]\n",
    "    else:\n",
    "        continue\n",
    "    if ent_list in ent_fv.keys():\n",
    "        meeting_graph.add_node(index, entities=ent_list)\n",
    "\n",
    "# mean of entities\n",
    "\n",
    "# meeting_graph = nx.Graph()\n",
    "\n",
    "# for index, (sentence, ent) in enumerate(graph_list):\n",
    "#     ent_list_t = list(map(lambda k: (k, pg_map[k]), ent))\n",
    "#     if ent_list_t!=[]:\n",
    "#         temp_list = []\n",
    "#         for ent in ent_list_t:\n",
    "#             if ent[0] in ent_fv.keys():\n",
    "#                 temp_list.append(ent[0])\n",
    "#         if temp_list!=[]:\n",
    "#             meeting_graph.add_node(index, entities=temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nodea in meeting_graph.nodes():\n",
    "    for nodeb in meeting_graph.nodes():\n",
    "        if nodea != nodeb:\n",
    "            nodea_ent = meeting_graph.nodes[nodea][\"entities\"]\n",
    "            nodeb_ent = meeting_graph.nodes[nodeb][\"entities\"]\n",
    "            \n",
    "            if nodeb_ent in dict(sub_graph_copy[nodea_ent]).keys():\n",
    "                meeting_graph.add_edge(nodea, nodeb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 0.41406940841391726\n",
      "75 0.41406940841391726\n"
     ]
    }
   ],
   "source": [
    "meeting_graph_rescaled = deepcopy(meeting_graph)\n",
    "# X = nx.to_numpy_array(meeting_graph)\n",
    "\n",
    "# for i in range(len(X)):\n",
    "#     X[i][i] = X[i].mean()\n",
    "\n",
    "# norm_mat = (X - X.min(axis=1)) / (X.max(axis=1) - X.min(axis=1))\n",
    "# norm_mat = (np.transpose(np.tril(norm_mat)) + np.triu(norm_mat)) / 2\n",
    "# norm_mat = norm_mat + np.transpose(norm_mat)\n",
    "\n",
    "# meeting_graph_rescaled = nx.from_numpy_array(norm_mat)\n",
    "# meeting_graph_rescaled.remove_edges_from(\n",
    "#     list(map(lambda x: (x, x), range(meeting_graph.number_of_nodes())))\n",
    "# )\n",
    "# norm_nodes = deepcopy(meeting_graph_rescaled.nodes())\n",
    "# for node in norm_nodes:\n",
    "#     if node not in meeting_graph.nodes():\n",
    "#         meeting_graph_rescaled.remove_node(node)\n",
    "            \n",
    "def prune(meeting_graph, v):\n",
    "    meeting_graph_norm = deepcopy(meeting_graph)\n",
    "    pg_scores = list(map(lambda kv:kv[-1][\"weight\"], meeting_graph_norm.edges.data()))\n",
    "    #pg_scores = [score[1] for score in pg.items()]\n",
    "    q3 = np.percentile(pg_scores, v)\n",
    "    edge_data = deepcopy(meeting_graph_norm.edges.data())\n",
    "    for node1, node2, weights in edge_data:\n",
    "        if weights[\"weight\"]<q3:\n",
    "            meeting_graph_norm.remove_edge(node1, node2)\n",
    "\n",
    "\n",
    "    return meeting_graph_norm\n",
    "\n",
    "v = 75\n",
    "meeting_graph_norm = prune(meeting_graph_rescaled, v)\n",
    "meeting_graph_com = best_partition(meeting_graph_norm)\n",
    "\n",
    "print (v, modularity(meeting_graph_com, meeting_graph_norm) )\n",
    "\n",
    "while modularity(meeting_graph_com, meeting_graph_norm) < 0.35:\n",
    "    v +=1\n",
    "    print (v, modularity(meeting_graph_com, meeting_graph_norm) )\n",
    "    meeting_graph_norm = prune(meeting_graph_rescaled, v)\n",
    "    meeting_graph_com = best_partition(meeting_graph_norm)\n",
    "print (v, modularity(meeting_graph_com, meeting_graph_norm) )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11051478703503811"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not pruned\n",
    "meeting_graph_com = best_partition(meeting_graph)\n",
    "modularity(meeting_graph_com, meeting_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- slice --------- \n",
      "\n",
      "\n",
      "Services from this 23 terabyte server in an upcoming video before we get started  [  Server ] \n",
      "\n",
      "So your software say is a system agnostic making software simpler to use less work to develop and easier to maintain these containers running on your computer or server act like little micro meters each with very specific jobs each with their own operating system their own isolated CPU processes memory and network resources and because of this they can be easily added removed stopped and started again without affecting each other or those machines containers  [  Server ] \n",
      "\n",
      "There are of course many options available when running your containers such as running a batch mode to catch D or assigning ports for web services  [  Web ] \n",
      "\n",
      "If you are going to have a secure Home Server, you have to get yourself a domain name for SSL  [  Server ] \n",
      "\n",
      "So you can access that server anywhere in the world  [  Server ] \n",
      "\n",
      "And since we're building a home server, let's just type that in and see what we get when we put some domains on the screen is currently available that are Related to Home Server get him now while they're available and make sure you get that 10% off and support the show again  [  Server ] \n",
      "\n",
      "Infrastructure for performance and availability wouldn't it be nice? If you could just focus on building great applications without having to spend lots of time managing servers introducing AWS Lambda AWS  [  Infrastructure ] \n",
      "\n",
      "And this whiteboard session is all about giving kubernetes to you in five minutes, and it's going to be a little bit of a challenge  [  Kubernetes ] \n",
      "\n",
      "So I'm going to have to brush over a lot of detailed Concepts and kubernetes, but I want to get a rough idea of what is it? And what does it actually do and what is responsible for four? So there's a few different Architectural Components to it, which I'm going to talk about the first of which is the kubernetes cluster  [  Kubernetes ] \n",
      "\n",
      "The fundamental premise behind kubernetes is that we can enforce what's called desired state  [  Kubernetes ] \n",
      "\n",
      "Really what that means is that I'm going to feed the cluster Services a specific configuration and it will be up to the cluster services to go out and run that configuration in my infrastructure  [  Cluster ] \n",
      "\n",
      "The one thing unique about a worker or the container host in a kubernetes environment is that it does have this couplet process that runs which is responsible for communicating with yes  [  Kubernetes ] \n",
      "\n",
      "You guessed it the kubernetes cluster services  [  Kubernetes ] \n",
      "\n",
      "And so this whole thing the cluster Services the workers themselves, that's what makes up this quote-unquote kubernetes cluster  [  Kubernetes ] \n",
      "\n",
      "A pod configuration in a pot is like the smallest unit of deployment in kubernetes in terms of the kubernetes object model and what it means is that in a pod  [  Kubernetes ] \n",
      "\n",
      "But the other additional thing is that I'm going to specify how many of these pods need to be running here  [  Pods ] \n",
      "\n",
      "So I'm going to have pod one  [  Pod ] \n",
      "\n",
      "Replica one pod one replica to pod one  [  Pod ] \n",
      "\n",
      "Pod to replica one and in this case  [  Pod ] \n",
      "\n",
      "So let's just do pod one replica to here  [  Pod ] \n",
      "\n",
      "And so you can see at any point in time  [  Time ] \n",
      "\n",
      "I lose a worker now my running configuration because that pot is now dead by running configuration does not match this and it's up to the kubernetes cluster Services through the cumulative process that's running to notify me a bad hand to understand what's actually running  [  Kubernetes ] \n",
      "\n",
      "So what I've done is I actually drop this pod one replica to so therefore this killer has to make a scheduling decision on where I   [  Pod ] \n",
      "\n",
      "So, this is kind of the basics of kubernetes five minutes  [  Kubernetes ] \n",
      "\n",
      "Stay tuned for more like word sessions on the subject of kubernetes with Photon platform  [  Kubernetes ] \n",
      "\n",
      "it looks less and less this inevitable Divergence can Zoom challenges that can introduce bugs create bottlenecks or even bring development to Italy Testing using a dedicated server or CI service whenever developer adds you work to a branch the server will automatically build and test the code to determine whether it works and integrated with the code on the main development Branch  [  Server ] \n",
      "\n",
      "The CI server will produce output contain the results of the build and an indication of whether or not the ranch houses all the requirements for integration into the main development Branch by exposing build and test information for every comment on every Branch CI paves the way for what's known as control  [  Server ] \n",
      "\n",
      "It's a process allows you to actually deploy really develop features into Action with confidence and experience with any jail time  [  Time ] \n",
      "\n",
      "That's the CI server so it can determine whether or not they will integrate with the currently development Branch  [  Server ] \n",
      "\n",
      "We can subscribe our CI server to receive a message anytime  [  Server ] \n",
      "\n",
      "Continuous deployment Works in a similar way you can often configure your CI server to avoid crashes as part of its processes in a simple set up anytime  [  Server ] \n",
      "\n",
      "--------- slice  1 --------- \n",
      "\n",
      "\n",
      "Tucker is mainly a software development platform and kind of a virtualization technology that makes it easy for us to develop and deploy apps inside of neatly packaged virtual containerized in arms meaning apps from the same no matter where they are or what machine they're running South Africa  [  Platform ] \n",
      "\n",
      "Let me just get experienced doctor as well using containers for popular apps like Plex Media Server X cloud and many other open-source apps and tools many of which we will be installing in upcoming videos  [  Cloud ] \n",
      "\n",
      "The dockerfile is a surprisingly Simple Text document that instructs how the docker image will be built like a blueprint you first select a base image to start with using the from keyword, which you can find a container to use from the docker Hub  [  Docker ] \n",
      "\n",
      "It's downloading installing and running your software  [  Software ] \n",
      "\n",
      "We can build it using Docker build followed by the T Flex  [  Docker ] \n",
      "\n",
      "We can name our image and pass our commands the location of the dockerfile once complete  [  Dockerfile ] \n",
      "\n",
      "Now your build image can run a container of that image or you can push it to the cloud to share with others  [  Cloud ] \n",
      "\n",
      "Tanner's with Docker container LS and as you add more envelope, you're here running a single container is fun, but it's annoying to enter all of these commands to get a container running and we may want to control several containers as part of a single application such as running an app and a database together something you might want to do  [  Docker ] \n",
      "\n",
      "Lambda is a compute service that runs your back-end code in response to events such as object uploads to Amazon S3 buckets updates to Amazon dynamodb tables data in Amazon Kinesis streams or in app activity  [  Lambda ] \n",
      "\n",
      "Once you upload your code to Lambda the service handles all the capacity scaling patching and administration of the infrastructure to run your code and provides visibility into performance by publishing real-time metrics and logs to Amazon cloudwatch  [  Lambda ] \n",
      "\n",
      "There are no new languages tools or Frameworks to learn you can use any third-party Library even native ones  [  Library ] \n",
      "\n",
      "Lambda will be ready to trigger your function automatically when an event occurs  [  Lambda ] \n",
      "\n",
      "With Lambda any event can trigger your function  [  Lambda ] \n",
      "\n",
      "But before we learn about database, let's understand  [  Database ] \n",
      "\n",
      "Now, what is a database? We already know what data is but this data could be random a database is the systematic collection of Since the data in a database is organized  [  Database ] \n",
      "\n",
      "What is a database management system dbms database management system or dbms collection of programs which enables its users to access a database manipulate data and help in the representation of data  [  Database ] \n",
      "\n",
      "It also helps control access to the database by various users  [  Database ] \n",
      "\n",
      "Apples and online telephone directory would definitely use database management system to store data pertaining to people phone numbers and other contact details  [  Database ] \n",
      "\n",
      "Charles Bachman's integrated data store or IDs is said to be the first dbms in history with time database Technologies evolved a lot while usage and expected functionalities of databases have been increased immensely types of dbms  [  Database ] \n",
      "\n",
      "This type of dbms supports many to many relationships this usually results in complex database structures RDM server is an example of a database management system that implements the network model relational dbms  [  Database ] \n",
      "\n",
      "This type of dbms defines database relationships in forms of tables also known as relations unlike the network dbms  [  Database ] \n",
      "\n",
      "In fact, it helps in optimizing and maintenance of databases and much more relational database  [  Database ] \n",
      "\n",
      "dbms stands for database management system We have four major types of dbms is called hierarchical Network relational and object-oriented  [  Database ] \n",
      "\n",
      "--------- slice  2 --------- \n",
      "\n",
      "\n",
      "Usually run one specific tasks such as a a mySQL database or a node.js application and then their Network together and potentially scaled the developer will usually start by accessing the docker Hub and online Cloud repository of Docker containers and put one containing a pre-configured environment for their specific programming languages such as Ruby or node js with all the files  [  Docker ] \n",
      "\n",
      "This allows you to run many Docker containers where you may only be able to run a few virtually you see a virtual machine has to quarantine office set amount of resources hard drive space memory and processing power emulate hardware and then Bhutan entire operating system Mmm, then the VM communicates with the host computer via a translator application running on the host operating system called a hybrid  [  Docker ] \n",
      "\n",
      "Docker communicates natively with the system Peril by passing the middleman on Linux machines and even Windows 10 and Windows Server 2016 and above this means you can run any version of Linux in the container and it will run natively  [  Docker ] \n",
      "\n",
      "If you have multiple Docker images using the same beats image for instance Docker will only keep a single copy of the files needed and share them with each container  [  Docker ] \n",
      "\n",
      "So how do we use Docker install Docker on your machine and will provide links in the description begin with Machine built into a Docker image which can be run as a Docker container  [  Docker ] \n",
      "\n",
      "Like we mentioned before a bun to an Alpine Linux are popular choices from there  [  Linux ] \n",
      "\n",
      "Of course pulling the docks below once their Docker file is complete  [  Docker ] \n",
      "\n",
      "You can verify your images existence with Docker images  [  Docker ] \n",
      "\n",
      "If you don't create your own Docker image and you just want to use a premade one component from the docker hub using Docker bowl and the image name  [  Docker ] \n",
      "\n",
      "If you don't specify a tag the latest version will be what snatched to run a container pulled down from the docker Hub rebuild the image and then enter Docker run followed by the image name  [  Docker ] \n",
      "\n",
      "It's run WordPress for example, and we're going to be accomplishing that with Docker compose in our next video where we build a 23 terabyte Home Server  [  Docker ] \n",
      "\n",
      "Let me know if you'd be interested in a part two we go deeper into Docker will go over some of the specifics of Docker layers and so on and stick around coming up we're building at twenty three terabyte server using Docker compose  [  Docker ] \n",
      "\n",
      "to make this magic happen your application needs back-end code that runs in response to events, like image uploads in app activity website clicks or sensor outputs, but managing the infrastructure to host and execute back-end code requires you to size provision and scale a bunch of servers managed operating system updates apply security patches and then monitor all   [  App ] \n",
      "\n",
      "All you need to do is write the code  [  Code ] \n",
      "\n",
      "Once your function is loaded you select the Event Source to monitor such as an Amazon S3 bucket or Amazon dynamodb table and within a few seconds  [  Amazon ] \n",
      "\n",
      "The Windows registry used in Windows XP is an example of a hierarchical database  [  Windows ] \n",
      "\n",
      "Configuration settings are stored as tree structures with nodes Network dbms  [  Network ] \n",
      "\n",
      "Well, the worker is really just a container host  [  Container ] \n",
      "\n",
      "So in order to run that pot I need to specify some sort of container  [  Container ] \n",
      "\n",
      "The cluster service is a responsible to make sure that configuration is running across all of my container host short or my workers  [  Service ] \n",
      "\n",
      "So what's the difference between continuous delivery and continuous deployment continuous delivery is the practice of developing software in such a way that you could release it at any time when coupled with CI continuous delivery lets you develop features with modular code and more manageable increments continuous deployment is an extension of continuous delivery  [  Code ] \n",
      "\n",
      "Yeah, let's take a look at how GitHub fits into this process will take it one step at a time starting with C  [  Github ] \n",
      "\n",
      "I get how it is like a Clearinghouse for your code developers make changes locally and push those changes to get up and they want to share them with others with CI  [  Code ] \n",
      "\n",
      "Someone pushes code to a branch or opens all requests on GitHub this CI server will parse the message from GitHub rather current copy of the project built in the branch and run the tests when the CI server finishes its processes for the current commit  [  Github ] \n",
      "\n",
      "It sends a message to get Hub status API containing status information about commit GitHub uses that message to display information about the committee and can even link back to more detailed information on the sky server  [  Github ] \n",
      "\n",
      "The master Branch receives a new commit to see i provider grab some current object project and deploys the master Branch production step for this type of deployment will vary depending on your provider if your project requires Portland Billy you have also exposes that It lets you create custom deployments from ranches tags or comments  [  Master ] \n",
      "\n",
      "You can use the deployments API in conjunction webhooks automatically notified third-party systems, which can then retrieve a copy of the code from GitHub and deploy the version you request the environment you specify so let's review all about one more time continuous integration is a workflow strategy in Lyon to help you pay shorter  [  Github ] \n",
      "\n",
      "You code will integrate into the current version of the software continuous delivery is developing some where that could be released at any time get up puts your code in the center of your development ecosystem by Sir  [  Code ] \n",
      "\n",
      "to get started setting up support integrating GitHub with the tools you love there's a github.com business to learn more  [  Github ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_list = []\n",
    "meeting_graph_com_sorted = sorted(meeting_graph_com.items(), key = lambda kv: kv[1])\n",
    "prev = 0\n",
    "tag = 0\n",
    "com_list = []\n",
    "G2 = nx.Graph()\n",
    "print (\"--------- slice ---------\", \"\\n\\n\")\n",
    "for ent, cluster in meeting_graph_com_sorted:\n",
    "    if prev!=cluster:\n",
    "        print (\"--------- slice \", cluster, \"---------\", \"\\n\\n\")\n",
    "        prev = cluster\n",
    "    if cluster==0:\n",
    "        G2.add_node(ent)\n",
    "    #print (graph_list[ent][0], \" [ \", meeting_graph.nodes[ent][\"entities\"] ,\"]\" ,\"sentence representation: \", meeting_graph.nodes[ent][\"entities\"] in sentence_fv_ent,\"\\n\")\n",
    "    print (graph_list[ent][0], \" [ \", meeting_graph.nodes[ent][\"entities\"] ,\"]\", \"\\n\")\n",
    "    temp_list.append(meeting_graph.nodes[ent][\"entities\"])\n",
    "    #print (ent, entity_kp_graph.nodes[ent]['node_freq'])\n",
    "    \n",
    "# comm_map = {\n",
    "# }\n",
    "# for word, cluster in comm_sorted:\n",
    "#     comm_map[word] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Linux App\n",
      "Linux Github\n",
      "Linux Github\n",
      "Linux Github\n",
      "Linux Github\n",
      "Linux Github\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Dockerfile App\n",
      "Dockerfile Github\n",
      "Dockerfile Github\n",
      "Dockerfile Github\n",
      "Dockerfile Github\n",
      "Dockerfile Github\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Cloud Docker\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web Docker\n",
      "Web App\n",
      "Web Windows\n",
      "Web Github\n",
      "Web Github\n",
      "Web Github\n",
      "Web Github\n",
      "Web Github\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Cloud\n",
      "Docker Cloud\n",
      "Docker Web\n",
      "Docker App\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "Docker Kubernetes\n",
      "App Docker\n",
      "App Docker\n",
      "App Docker\n",
      "App Docker\n",
      "App Docker\n",
      "App Docker\n",
      "App Linux\n",
      "App Docker\n",
      "App Docker\n",
      "App Dockerfile\n",
      "App Docker\n",
      "App Docker\n",
      "App Docker\n",
      "App Web\n",
      "App Docker\n",
      "App Docker\n",
      "App Docker\n",
      "App Kubernetes\n",
      "App Kubernetes\n",
      "App Kubernetes\n",
      "App Kubernetes\n",
      "App Kubernetes\n",
      "App Kubernetes\n",
      "App Kubernetes\n",
      "App Kubernetes\n",
      "App Kubernetes\n",
      "App Kubernetes\n",
      "Windows Web\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes App\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes App\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes App\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes App\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes App\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes App\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes App\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes App\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes App\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes Docker\n",
      "Kubernetes App\n",
      "Github Linux\n",
      "Github Dockerfile\n",
      "Github Web\n",
      "Github Linux\n",
      "Github Dockerfile\n",
      "Github Web\n",
      "Github Linux\n",
      "Github Dockerfile\n",
      "Github Web\n",
      "Github Linux\n",
      "Github Dockerfile\n",
      "Github Web\n",
      "Github Linux\n",
      "Github Dockerfile\n",
      "Github Web\n"
     ]
    }
   ],
   "source": [
    "for nodea in meeting_graph.nodes():\n",
    "    for nodeb in meeting_graph.nodes():\n",
    "        if nodea != nodeb:\n",
    "            nodea_ent = meeting_graph.nodes[nodea][\"entities\"]\n",
    "            nodeb_ent = meeting_graph.nodes[nodeb][\"entities\"]\n",
    "            \n",
    "            if nodeb_ent in dict(sub_graph_copy[nodea_ent]).keys():\n",
    "                if sub_graph_copy[nodea_ent][nodeb_ent][\"edge_ctr\"] > 10 and nodea_ent!=nodeb_ent:\n",
    "                    print (nodea_ent, nodeb_ent)\n",
    "                #meeting_graph.add_edge(nodea, nodeb, weight=sub_graph_copy[nodea_ent][nodeb_ent][\"edge_ctr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lambda': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Dockerfile': {'edge_freq': 29, 'edge_ctr': 160, 'edge_type': 'ent_to_ent'},\n",
       " 'Repository': {'edge_freq': 3, 'edge_ctr': 5, 'edge_type': 'ent_to_ent'},\n",
       " 'Docker': {'edge_freq': 2, 'edge_ctr': 2, 'edge_type': 'ent_to_ent'},\n",
       " 'Directory': {'edge_freq': 1, 'edge_ctr': 4, 'edge_type': 'ent_to_ent'},\n",
       " 'Project': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Node': {'edge_freq': 7, 'edge_ctr': 14, 'edge_type': 'ent_to_ent'},\n",
       " 'Container': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Data': {'edge_freq': 2, 'edge_ctr': 3, 'edge_type': 'ent_to_ent'},\n",
       " 'Image': {'edge_freq': 3, 'edge_ctr': 3, 'edge_type': 'ent_to_ent'},\n",
       " 'Linux': {'edge_freq': 6, 'edge_ctr': 9, 'edge_type': 'ent_to_ent'},\n",
       " 'Building': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'App': {'edge_freq': 6, 'edge_ctr': 26, 'edge_type': 'ent_to_ent'},\n",
       " 'Sql': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Runs': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Languages': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'User': {'edge_freq': 1, 'edge_ctr': 4, 'edge_type': 'ent_to_ent'},\n",
       " 'Api': {'edge_freq': 3, 'edge_ctr': 4, 'edge_type': 'ent_to_ent'},\n",
       " 'Kubernetes': {'edge_freq': 4, 'edge_ctr': 6, 'edge_type': 'ent_to_ent'},\n",
       " 'Web': {'edge_freq': 2, 'edge_ctr': 3, 'edge_type': 'ent_to_ent'},\n",
       " 'Photos': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Branch': {'edge_freq': 1, 'edge_ctr': 2, 'edge_type': 'ent_to_ent'},\n",
       " 'Mysql': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Version': {'edge_freq': 3, 'edge_ctr': 4, 'edge_type': 'ent_to_ent'},\n",
       " 'Github': {'edge_freq': 8, 'edge_ctr': 12, 'edge_type': 'ent_to_ent'},\n",
       " 'Model': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Scale': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Compose': {'edge_freq': 1, 'edge_ctr': 2, 'edge_type': 'ent_to_ent'},\n",
       " 'Common': {'edge_freq': 1, 'edge_ctr': 2, 'edge_type': 'ent_to_ent'},\n",
       " 'Aws': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Alpine': {'edge_freq': 4, 'edge_ctr': 9, 'edge_type': 'ent_to_ent'},\n",
       " 'Client': {'edge_freq': 1, 'edge_ctr': 2, 'edge_type': 'ent_to_ent'},\n",
       " 'Application': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Server': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Environment': {'edge_freq': 2, 'edge_ctr': 4, 'edge_type': 'ent_to_ent'},\n",
       " 'Microsoft': {'edge_freq': 1, 'edge_ctr': 3, 'edge_type': 'ent_to_ent'},\n",
       " 'Registry': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'},\n",
       " 'Database': {'edge_freq': 1, 'edge_ctr': 2, 'edge_type': 'ent_to_ent'},\n",
       " 'Ruby': {'edge_freq': 1, 'edge_ctr': 2, 'edge_type': 'ent_to_ent'},\n",
       " 'Tag': {'edge_freq': 1, 'edge_ctr': 4, 'edge_type': 'ent_to_ent'},\n",
       " 'Sdk': {'edge_freq': 1, 'edge_ctr': 2, 'edge_type': 'ent_to_ent'},\n",
       " 'Machine': {'edge_freq': 1, 'edge_ctr': 1, 'edge_type': 'ent_to_ent'}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sub_graph_copy[\"Dockerfile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_type': 'entity', 'node_freq': 15, 'x': 1}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_graph.nodes[\"Machine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph.add_node"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sri_gpt",
   "language": "python3",
   "name": "sri_gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
