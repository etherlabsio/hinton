{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "from bert_ner_utils import BERT_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing models\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ner_model = BERT_NER('/home/arjun/BERT_Similarity_experiments/models/bert-ner/',cased=False)\n",
    "ner_model_u = BERT_NER('/home/arjun/BERT_Similarity_experiments/models/bert-ner-uncased-2/',cased=False,labels=[\"O\",\"E\"])\n",
    "# ner_model_c = BERT_NER('/home/arjun/BERT_Similarity_experiments/models/bert-ner-cased/',cased=True)\n",
    "ner_model_c = BERT_NER('/home/arjun/BERT_Similarity_experiments/models/bert-ner-cased-2/',cased=True,labels=[\"O\",\"E\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_segment = \"'gotham city needs a #Wash', said batman in his low, grumbly voice as he waved his wand to cast the spell aguamenti on the city.\"\n",
    "model = ner_model_u\n",
    "\n",
    "ents,sc,nents,nsc = model.get_entities(test_segment,get_non_entities=True)\n",
    "print(\">\",test_segment,end=\"\\n\\n\")\n",
    "print(\"Entities Detected:\",ner_model.wordize(ents, capitalize=True),end=\"\\n\\n\")\n",
    "print(\"Non Entity tokens, sorted by score:\",end=\"\\n\\n\")\n",
    "\n",
    "for ne,ns in sorted(zip(nents,nsc),key=lambda x: x[1]):\n",
    "    print(ne,ns,sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models based on segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "segment = \"most of you. How many of you actually used one? No, that's that's good. I gave this talk a similar talk. Clashing there was, too. So now we're up to about 10 or 15. How many of you actually put something into production with one? We're down about four. How many of you actually put it into production successfully? We're down to a couple. So that's sort of what the whole premise of this talk is to kind of walk through the process of what it actually takes to make a raft database implementation. Successful. Ah, so little bit about myself before we get started. So, uh, I've been doing this. I've been doing software development for about 20 years. Every time I say that, it starts to make me feel older and older a little more than 20 years now, But I don't want to admit how long. Ah, and I've done everything. I've been a little bit of everything. I started out. My actual first job was to build embedded, see programs on top of dos on. Other than that, everything they're through building out high through building out high performance websites. Most recently, I've been focused on big data problems specifically around building. Ah, high performance, low latent See data platforms. A lot of times, at least part of the Zeta platform has been leveraging crap leveraging a draft database. Currently, I am, ah, solutions, architect. Data s*x in the global graft practice. If you're not familiar with the data stacks is we are the Apache Ca*sandra people. Ah, we have our own distribution of Ca*sandra that integrates search a**lytics and then Graff on top of it, which is, uh, why I'm part of the global graph practice there. Most of the co author of an upcoming book by Manny called Graph Databases in Action On And if you are, if you want my that's my Twitter handle up in the corner. If you tweet out during this this presentation I have five free copies like I'll give away at the end. Doesn't have to be nice, but I would prefer it be nice. \"\n",
    "print(segment,end=\"\\n\\n\")\n",
    "entities=[]\n",
    "entities_c = []\n",
    "entities_u = []\n",
    "non_entities_u=[]\n",
    "conf = []\n",
    "conf_c = []\n",
    "conf_u=[]\n",
    "non_conf_u=[]\n",
    "for text in sent_tokenize(segment):\n",
    "#     print(\"Entities Uncased Mul\")\n",
    "    ent, con, ne,nc = ner_model.get_entities(text,get_non_entities=True)\n",
    "#     print(\"Entities Cased Bin\")\n",
    "    ent_c, con_c,ne_c, nc_c = ner_model_c.get_entities(text,get_non_entities=True)\n",
    "#     print(\"Entities Uncased Bin\")\n",
    "    ent_u, con_u,ne_u,nc_u = ner_model_u.get_entities(text,get_non_entities=True)\n",
    "#     print()\n",
    "    if len(ent)==0 and len(ent_c)==0 and len(ent_u)==0:\n",
    "        continue\n",
    "    entities.extend(ent)\n",
    "    entities_c.extend(ent_c)\n",
    "    entities_u.extend(ent_u)\n",
    "    non_entities_u.extend(ne_u)\n",
    "    non_conf_u.extend(nc_u)\n",
    "    conf.extend(con)\n",
    "    conf_c.extend(con_c)\n",
    "    conf_u.extend(con_u)\n",
    "\n",
    "print(\"UNCASED BERT-NER\",end=\"\\n\\n\")\n",
    "[print(e,c) for e,c in zip(entities,conf)]\n",
    "print(len(entities),len(conf))\n",
    "entities_words = ner_model.wordize(entities)\n",
    "print(entities_words,end=\"\\n\\n\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(\"UNCASED BERT-NER-BINARY\",end=\"\\n\\n\")\n",
    "[print(e,c) for e,c in zip(entities_u,conf_u)]\n",
    "print(\"#\"*100)\n",
    "[print(e,c) for e,c in sorted(zip(non_entities_u,non_conf_u),key=lambda x: x[1])]\n",
    "entities_words_u = ner_model_u.wordize(entities_u)\n",
    "print(entities_words_u,end=\"\\n\\n\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(\"CASED BERT-NER-BINARY\",end=\"\\n\\n\")\n",
    "# [print(e,c) for e,c in zip(entities_c,conf_c)]\n",
    "entities_words_c = ner_model_c.wordize(entities_c)\n",
    "print(entities_words_c,end=\"\\n\\n\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(\"SPACY\")\n",
    "doc=nlp(segment)\n",
    "entities_spacy=[]\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ not in [\"CARDINAL\",\"ORDINAL\"]:\n",
    "        entities_spacy.append(ent.text)\n",
    "print(\", \".join(list(set(entities_spacy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on entity-sentences set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/home/venkat/knowledge_graphs/entity_graph_builder/graph_dumps/ppn_sentences.pkl\",\"rb\") as f:\n",
    "    sent_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "num_of_entities=10\n",
    "for gold_entity in sent_dict:\n",
    "    entities = []\n",
    "    entities_c = []\n",
    "    entities_u = []\n",
    "    entities_spacy_u = []\n",
    "    entities_spacy_c = []\n",
    "    # Selecting entities based on starting letter\n",
    "#     if gold_entity!=\"Fix\":\n",
    "#         continue\n",
    "    gold_sentences = sent_dict[gold_entity][:500]\n",
    "    gold_entity = gold_entity.replace(\"the \",\"\")\n",
    "    if len(gold_sentences)<4:\n",
    "        continue\n",
    "    \n",
    "    for text in gold_sentences:\n",
    "        ent, con= ner_model.get_entities(text)\n",
    "        ent_c, con_c= ner_model_c.get_entities(text)\n",
    "        ent_u, con_u= ner_model_u.get_entities(text)\n",
    "        \n",
    "        doc_c = nlp(text)\n",
    "        entities_spacy_c.extend(list(set([x for e in doc_c.ents for x in e.text.upper().split() ])))\n",
    "        doc_u = nlp(text.lower())\n",
    "        entities_spacy_u.extend(list(set([x for e in doc_u.ents for x in e.text.upper().split() ])))\n",
    "        \n",
    "        if len(ent)==0 and len(ent_c)==0 and len(ent_u)==0:\n",
    "            continue\n",
    "        entities.extend(list(set(ner_model.wordize(ent))))\n",
    "        entities_c.extend(list(set(ner_model_c.wordize(ent_c))))\n",
    "        entities_u.extend(list(set(ner_model_u.wordize(ent_u))))\n",
    "    \n",
    "    # Consider n-gram entities. Take mean score of all entity detections.\n",
    "    gold_entity_list = gold_entity.upper().split()\n",
    "    counts_1 = np.mean([entities.count(gold_ent) for gold_ent in gold_entity_list])\n",
    "    counts_2 = np.mean([entities_c.count(gold_ent) for gold_ent in gold_entity_list])\n",
    "    counts_3 = np.mean([entities_u.count(gold_ent) for gold_ent in gold_entity_list])\n",
    "    counts_ner_spacy_cased = entities_spacy_c.count(gold_entity.upper())\n",
    "    counts_ner_spacy_uncased = entities_spacy_u.count(gold_entity.upper())\n",
    "    \n",
    "    print(\"#\"*100,\"\\n\",gold_entity,\"| #Sentences:\",len(gold_sentences),\"\\n\")\n",
    "    print(\"BERT NER UNCASED MUL: \", counts_1/len(gold_sentences))\n",
    "    print(\"BERT NER CASED BIN: \", counts_2/len(gold_sentences))\n",
    "    print(\"BERT NER UNCASED BIN: \", counts_3/len(gold_sentences))\n",
    "    print(\"SPACY NER CASED: \", counts_ner_spacy_cased/len(gold_sentences))\n",
    "    print(\"SPACY NER UNCASED: \", counts_ner_spacy_uncased/len(gold_sentences))\n",
    "    \n",
    "    ctr+=1\n",
    "    if ctr==num_of_entities:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ner_model_u\n",
    "for text in sent_dict['Atlassian'][:10]:\n",
    "    print(text)\n",
    "    ents = model.get_entities(text,get_non_entities=True)[0]\n",
    "    print(model.wordize(ents,capitalize=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on Meeting Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/arjun/BERT_Similarity_experiments/data/entity_val_pl1.json\",\"r\") as f:\n",
    "    entity_val_set = json.loads(f.read().replace(\"][\",\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg_chunk in entity_val_set:\n",
    "    gold_entity_list = list(seg_chunk['entities'].keys())\n",
    "    entities = []\n",
    "    entities_c = []\n",
    "    entities_u = []\n",
    "    entities_spacy_u = []\n",
    "    entities_spacy_c = []\n",
    "    conf=[]\n",
    "    conf_c=[]\n",
    "    conf_u=[]\n",
    "    print(seg_chunk['segments'],end=\"\\n\\n\")\n",
    "    gold_sentences = sent_tokenize(seg_chunk['segments'])\n",
    "    for text in gold_sentences:\n",
    "#         text=text.lower() # ABLATION STUDY FOR BERT CASED MODEL\n",
    "        ent, con= ner_model.get_entities(text)\n",
    "        ent_c, con_c= ner_model_c.get_entities(text)\n",
    "        ent_u, con_u= ner_model_u.get_entities(text)\n",
    "        entities.extend(ent)\n",
    "        entities_c.extend(ent_c)\n",
    "        entities_u.extend(ent_u)\n",
    "        conf.extend(con)\n",
    "        conf_c.extend(con_c)\n",
    "        conf_u.extend(con_u)\n",
    "        \n",
    "        doc_c = nlp(text)\n",
    "        entities_spacy_c.extend(list(set([e.text for e in doc_c.ents])))\n",
    "        doc_u = nlp(text.lower())\n",
    "        entities_spacy_u.extend(list(set([e.text for e in doc_u.ents])))\n",
    "    entities = ner_model.wordize(entities, capitalize=True)\n",
    "    entities_c = ner_model_c.wordize(entities_c, capitalize=True)\n",
    "    entities_u = ner_model_u.wordize(entities_u, capitalize=True)\n",
    "    \n",
    "    print(\"\\n\",gold_entity_list,\"No. of Sentences:\",len(gold_sentences),\"\\n\")\n",
    "    print(\"BERT NER UNCASED MUL: \",entities,conf,sep=\"\\n\",end=\"\\n\\n\")\n",
    "    print(\"BERT NER CASED BIN: \",entities_c,conf_c,sep=\"\\n\",end=\"\\n\\n\")\n",
    "    print(\"BERT NER UNCASED BIN: \",entities_u,conf_u,sep=\"\\n\",end=\"\\n\\n\")\n",
    "    print(\"SPACY NER CASED\",set(entities_spacy_c))\n",
    "    print(\"SPACY NER UNCASED\",set(entities_spacy_u))\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Arjun",
   "language": "python",
   "name": "arjun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
