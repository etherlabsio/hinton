"{\"body\": {\"segments\": [{\"id\": \"6a0a928643ab4788aa51b94b2ea8952e\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:27:49Z\", \"duration\": 58, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:27:49.689414673Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:26:51Z\", \"updatedAt\": \"2019-10-31T13:28:10.991100613Z\", \"confidence\": 0.85336706, \"recordingId\": \"3bc1c9198e69456cb8e31b17de092d30\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"That makes it so this we're doing this way. What we have done is we have had to hack the data hunger of the more of the neural network language models. I because we get the data in a very small increments for the machine learning model. So but we have their own to to keep the to accumulate the data as much as possible and then after the language model which is which is what you said, it's a static component. And then in the meantime, we don't want to lose the information that we have. Have gathered which is recency and other aspects of it. So we have introduced this channel men that takes care of this the more recent aspects under the changing dynamics that we're giving a giving the AI engine an additional boost of recency without actually changing the static component very frequently. So that's kind of the engineering that we did for the channel Minds got it. Yeah, we can move to the next slide. Okay? Yeah. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"72abdc13b8734ebfb15db8ca1605d93f\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:27:54Z\", \"duration\": 6, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:27:54.26303131Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:27:48Z\", \"updatedAt\": \"2019-10-31T13:28:15.418164223Z\", \"confidence\": 0.9128387, \"recordingId\": \"28cbcec8484d4c0ba5cb0d73ead0b879\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"What do you want to talk about the validation real quick about how we do the validation? \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"628ff421a42f4b3fb25d44a675466163\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:29:39Z\", \"duration\": 3, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:29:39.61518563Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:29:36Z\", \"updatedAt\": \"2019-10-31T13:30:00.576147114Z\", \"confidence\": 0.9106261, \"recordingId\": \"e1423865deb74a77ba4ec9cd18bea660\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Got it, and that's done based on feedback. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"d2c0d63048af47c0808ed53e761c9f7b\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:30:03Z\", \"duration\": 20, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:30:04.359626818Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:29:43Z\", \"updatedAt\": \"2019-10-31T13:30:25.306446937Z\", \"confidence\": 0.89225197, \"recordingId\": \"deae9914effc48dcb09ead3e22d0f2bd\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"So I guess this is kind of a very it's a bit of an eye chart, but I guess it breaks down the whole Channel mind life cycle into our domain mind gets generated. How is that attach to the with a slack Channel and then how it how the channel Minds self-generated right? Yeah. That's right. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"e55cb15e24f34c1090a70d0c8bc8f562\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:29:36Z\", \"duration\": 102, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:29:38.01461616Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:27:54Z\", \"updatedAt\": \"2019-10-31T13:30:39.864096404Z\", \"confidence\": 0.8304946524999999, \"recordingId\": \"3351dc159c8e44d0a97a7e144460bf95\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Oh, yeah, I'm sorry. I I think I think we can on the validation component. What we do is actually there are there is a two-stage validation one is one is the language model that we that we have two other people which is well, which is actually fine-tune various tasks. When I say a task it is it is like asking asking the language model to predict whether whether a sentence is at whether a sentence whether two sentences are continuous. Or not that gives the ability for the language wanted to you know, learn to understand the language and also the nuances of the conversations. So so we have we have fairly statistical validation approaches to validate this auxiliary tasks. So we follow them but whereas for the downstream tasks we had to formulate our own ways of validation, which we are going to talk a little bit later and then coming to validating the mind which is not which is the graph representation. So what we do for validating the graphics, Action of the mind is we have a pre annotated human annotated, you know topics of the graph communities that are formed of line and then we compare them with the the new channel Minds that get generated every time there is a new data and then and then we keep updating this Benchmark get also so that it keeps up with the with updates on the in the as an offline tasks. So that way we just we just semi-automated this validation process so that the human invention so good so that the ml engineer not really look into all the mines that are getting generated except with some exceptions where they see a lot difference in the validation numbers that they are. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"79d8bd4f528d42128311b753ac48139d\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:32:03Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:32:04.759329901Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:30:03Z\", \"updatedAt\": \"2019-10-31T13:33:06.421702671Z\", \"confidence\": 0.8027778250000001, \"recordingId\": \"77236e3690844c888c5a868a6bb7f79e\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"So a quick heads up on this on this slide because this puts everything I can with my cousin in one single view. So so this this just shows how how the chat the domain Minds, you know shape-shifts into the channel and by learning all the information that it gets from the conversations. So let me get just started my mind Generations on the top. So we we we use open data to to you know, generate generate a fix. A library of domain Minds as as we mentioned some of the samples here some this is ever growing we keep on adding so to this list. So these are some of the domain Minds that we actively use and then and then what we do is for each of the domain mind betrayal specific domain language model that means when we say software engineering language model, it is well-versed with the English in general obviously, and then also it knows it knows what what the software I can be better than the rest of the language model. It's a simple example would be a bug a bug in nature may be different from the bug in a software engineering model. I mean just a very very intuitive example on that. So so that way it learns it gets the ability to you know, differentiate the conversations the even if there is a conflict of the context so it should be it the domain specific language model should be able to clearly identify that that we are talking about software engineering. Nearing a not about some biology right? So that gives us the ability for The Ether AI engine in the downstream task when we talk about these topics and important moment generation. So this domain mind gets it domain language for who gets attached to the to the channel when the user says invites you to the channel. So from once we have this domain language model right as we were talking about the domain minder. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"c0378db3c78e4aae90c1b891b5ccafaf\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:44:21Z\", \"duration\": 31, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:44:21.517862295Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:43:50Z\", \"updatedAt\": \"2019-10-31T13:44:42.863378873Z\", \"confidence\": 0.649518215, \"recordingId\": \"e48eafe64abd4983bec7af691f1a01ae\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Something like, you know, if I'm part of a channel that's a very direct relationship which doesn't require a lot of intelligence which is get it right out of the station. Whereas when I say, hey, you know Karthik follow up on this production issue, you know the idea that Karthik is the person who I'm assigning this action to is something that is not so intuitive and needs to be gleaned right by first applying NLP in these types of rules then later on also being getting represented in the laugh \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"d664f7f953984352a85c1b8b06a309b8\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:47:39Z\", \"duration\": 10, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:47:39.744259388Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:47:29Z\", \"updatedAt\": \"2019-10-31T13:48:00.947351101Z\", \"confidence\": 0.75053823, \"recordingId\": \"920f9214dc7d40a28108efeddea64865\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Certain topics poker when when this group is talking about these topics, but not the other group so that kind of insights that would be captured in the computation. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"ca7d7bdff13246d89898f335ec0ab4ac\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:48:01Z\", \"duration\": 18, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:48:01.810180137Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:47:43Z\", \"updatedAt\": \"2019-10-31T13:48:22.819149656Z\", \"confidence\": 0.9112488, \"recordingId\": \"69770f899b7a4203a16ead4e85085bf2\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Yes, I eat a graph is more of a traditional representation of God. Yes, it's teams and peoples and topics and actions in these types of things become nodes and then they just become from the relationships between them. Yeah, that's like not. Okay and then I guess once you have these types of graphs. What can you do with them? \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"42c18ef5d2f543f98e10b9c46d7e1d3a\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:50:21Z\", \"duration\": 20, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:50:22.226302424Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:50:01Z\", \"updatedAt\": \"2019-10-31T13:50:43.363619487Z\", \"confidence\": 0.72698486, \"recordingId\": \"27c7f730ee99431ba5d56fec90a94348\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Just think of integrating these insights into you know, a ticket management tool like jira, right? So when when when there is a new ticket, so so this is the graph using this unintuitive relationships can actually recommend for the set of uses that this ticket can be a said because they talk about it because they are relieved from this. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"abceb463b6d442dd88e9607b7c09c7b9\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:50:35Z\", \"duration\": 12, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:50:35.96882916Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:50:23Z\", \"updatedAt\": \"2019-10-31T13:50:56.827612227Z\", \"confidence\": 0.8160915, \"recordingId\": \"019d8438e2fb4f9a81cab29089095e41\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"That's kind of a that's what this this computation graph in a enables us when we do all these kind of note a graph based algorithms built graph based Solutions around it. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"d30c6bebcc7e476cb9e87bf4f4f3f958\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:50:38Z\", \"duration\": 3, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:50:38.546365488Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:50:35Z\", \"updatedAt\": \"2019-10-31T13:50:59.655547044Z\", \"confidence\": 0.7286456, \"recordingId\": \"7419196ff52b43c286b056b36e195a5d\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Product. Okay, cool. Seems like a cool stuff. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"e1e4c629ba3549f09e95805e87b19902\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:33:00Z\", \"duration\": 57, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:33:01.474840428Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:32:03Z\", \"updatedAt\": \"2019-10-31T13:33:22.564725126Z\", \"confidence\": 0.7900469, \"recordingId\": \"a64f18055ac04b41aec6c5ee322f6c2b\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"That we generate this domain mind which is nothing but the graph representation of the of the text that is of what we think are the important topics for the or topics of the domain would be organized such that the topic hierarchy within and then and then we use the domain language model to establish various unintuitive relations between these notes and notes in the in this graph. So that means if someone is talking about deployment aunt and then if in a certain channel is if they always associate deployment with let's say AWS Lambda. So this relationship is learned that when there is an aid of a deployment. This is likelihood that with the talk about a double s Lambda also, it means that they seem to be talking about a similar as similar aspect for that certain Tim. So this is kind of relationship that this combination of language model undermined men captures. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"a0b2e39566144458bce00c786b7bcd4b\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:39:02Z\", \"duration\": 4, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:39:02.920400892Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:38:58Z\", \"updatedAt\": \"2019-10-31T13:39:24.565875452Z\", \"confidence\": 0.814725, \"recordingId\": \"fdf2cf35c981474897b7865e9085b90d\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Exactly. They're called three different somebody's based on context, right? \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"676708b576584e38acaa6d634427fa8c\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:35:23Z\", \"duration\": 22, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:35:23.542560163Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:35:01Z\", \"updatedAt\": \"2019-10-31T13:35:44.682874479Z\", \"confidence\": 0.84163785, \"recordingId\": \"98cc0dfbd4084bf292f2a51b2883ef56\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Come sit. So excellent. Yeah. Okay as we mentioned so domain Channel mine is fairly Dynamic. That means that that it can it can read all the information that it gives every every every minute or so, whereas somewhat relatively static is the channel language model which we update once we have enough information for a neural network to be fine, too. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"97f2b973beea4fad9394feb35dde9990\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:53:13Z\", \"duration\": 32, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:53:14.204696427Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:52:41Z\", \"updatedAt\": \"2019-10-31T13:53:35.52687808Z\", \"confidence\": 0.8720049750000001, \"recordingId\": \"90f129cb5c7c4dc588e956ea40f6ba4c\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Lemons search recommendation smart alerts and so on. That's right. Yeah. Okay. That's great. So let's move on and maybe just give a quick example not going to spend a whole lot of time on this but just to give a representation of here is a real world example. Let me see if I can actually pull up the graph itself. Yeah. Yeah. Actually I have it here little bit here. Let me stop the content here and show the real graph. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"ed40868559d24c61b4a8957bb698a4e4\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:54:28Z\", \"duration\": 5, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:54:28.679703223Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:54:23Z\", \"updatedAt\": \"2019-10-31T13:54:49.51553535Z\", \"confidence\": 0.87979764, \"recordingId\": \"c78bd42d3fb34150a71fd4c35dc27f1e\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Okay, so let's move on back to the discussion here. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"434f467038184a43972715f61c42213f\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:09:36Z\", \"duration\": 10, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:09:36.971364822Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:09:26Z\", \"updatedAt\": \"2019-10-31T13:09:58.259241653Z\", \"confidence\": 0.8599725, \"recordingId\": \"acae3c1418da42b3aa1c7c2260745ab8\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"It let me share the presentation on OVI stack and then we get started you're okay. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"f091fdbca2c645d5876c2c90d46e73f6\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:35:01Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:35:03.502864415Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:33:01Z\", \"updatedAt\": \"2019-10-31T13:36:05.840378253Z\", \"confidence\": 0.8961185325000001, \"recordingId\": \"97bc68061c934193a8b87a7dddf4e663\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"That's on the domain minds and then in the process of selection, right when the user invoice The Ether to the to the to the channel so we get to choose as we show in the in the second the second horizontal one, very marketing software engineering HR once it shows the domain language model and the domain my gets attached to that channel. So every time every time there's a new conversation that comes in the comes in the eater. Calls there would be there would be scored or they would be compared against this domain mind till we have the transformation to the channel Minds. So so this is this is fairly simple, you know, simple. What do you call it? Attaching the channel mind is fairly simple domain decidedly and then move to the channel mind. We're in this is where the actual actual, you know, a trick comes in where in How do we how do we change the domain mind such that it retains all the nuances that it knows about the domain and also tries to incrementally learn from the in size that it has all the text that it has seen over a period of time in the in the channel That is invited to so how we do that is as we discussed earlier. We continually gain the data from the data sources that we got access to and also the ether conversations and then we we actually at a high level we Score all the topics in the domain mind against the conversations and then and then we say that the the transform domain mind to the channel Tyler May to this channel as a chat Channel Minds. So we're in along with the let's say software engineering it'll also, you know, look at the priorities of the team from the recency perspective or even what what goes as it has the most important thing for the team. So that's this is where you know, you have the real learning component of Egypt \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"b5e0038d0a064f619167d6d0c34e6f82\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:36:02Z\", \"duration\": 39, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:36:02.812559621Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:35:23Z\", \"updatedAt\": \"2019-10-31T13:36:24.133810309Z\", \"confidence\": 0.75591935, \"recordingId\": \"746aa6354c1a4a2bb4e200908d5869fb\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Got it. Excellent. Just want to call out that when we say Channel. It is just in the context of a team, right like essentially Channel equals a team. So for whatever reason let a ether is, you know taken out of the slack context the team could actually be something else right? Like for example, that could be a virtual team. Let's say on either some other channel at home or some other let's say Google Hangouts for example, right if there is a concept of a Group in Google Hangout and that prisons attain the same level of attachment of mines and ending can also happen there. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"c0811e5112d64a2283f7245cad8a2f9c\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:11:52Z\", \"duration\": 121, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:11:53.033606231Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:09:51Z\", \"updatedAt\": \"2019-10-31T13:12:54.804267581Z\", \"confidence\": 0.8924218349999999, \"recordingId\": \"d951cbcff3174581bd40d9bc9ec130d0\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"You're able to see this, right? Yes. Okay, excellent. Okay, so when could I thought it'll be good for us to do a little bit have a dis little bit of a discussion on just the I stack with ether. I did a separate overview of our platform. So I'm assuming that a lot of the folks who are seeing. This will be gone through that that call and got up. Bit of an understanding about what ether is over. All right, but just to reiterate if you look at ethers a I stack there are a few things that that we want to kind of highlight, right? The first is its concept of Channel or team mines where whereby we are representing the team context both the static aspects of it. In other words. Let's say there's a team that is working on software engineering or databases or Our devops in general, right? So there is a static idea that they're working on these types of car idea of domains or subjects for example, and then there is a dynamic issue aspects of it, which is what conversations are they having recently. Is that a production issue that is being addressed right now it is that a particular type of database are talking about or you know, a particular aspect of their software engineering stack that they're talking about. So there's a dime static and dynamic aspects. So the first time Idea is idea about Channel or team Minds. We're in ether. We try to clean the idea or the context of what the team is all about and is able to represent that as a model and use that model to extract important information provide context to the interactions and the analytics that we provide. So that's what we call as Chandler team wins. The second is \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"879767f019d74aabbba1b22bc86ff0cb\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:38:35Z\", \"duration\": 118, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:38:36.602887048Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:36:37Z\", \"updatedAt\": \"2019-10-31T13:39:18.082442731Z\", \"confidence\": 0.84089589, \"recordingId\": \"2404ee576e964b18b950d4102ee92c55\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Or the real test. Yeah, so this is actually example where for example there are three channels here software engineering marketing and HR right in and what we did essentially was take the same discussion or meeting through three different channels. Right? So basically at have attached three different mines to the same conversation and see what type of summaries are produced, right? So for example, in this case, We to use a standardized data data set I guess where which talks about a bunch of different topics about HR related topics marketing related topics and software related topics. And then when you pass it through when you have that meeting in the context of a software engineering mind, the discussion summarizes The Ether summarizes the discussion pulls out topics related to software for example things about database continuously ICD kubernetes Etc. Our infrastructure that Kafka whereas when you talk about when you run the same meeting through the marketing channel, you know, it pulls out things about, you know, digital marketing. You know, what platforms how do you do outbound reaching so on and so forth, right and similarly when you do it on top of HR, it talks about HR related topics, which could be about employment about investment in a chart budgets a gas is a little thing of the talks about Out HR leaders and productivity employee benefits and so on and so forth, right? So this is a great example of how you know the same exact same meeting can produce three different summaries based on the channel mind that has been associated with it or the context that's associated with it. Right. So again, let me go back switch to my presentation here. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"3e7f63d9f4b446a18a0b25758e6020ca\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:38:58Z\", \"duration\": 9, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:38:58.468531904Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:38:49Z\", \"updatedAt\": \"2019-10-31T13:39:19.337989919Z\", \"confidence\": 0.7891593, \"recordingId\": \"02400940a8514c40b74e36c1daa9b20a\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"it's fair play shows how how the channel man is able to differentiate based on the Mind attached to it during the \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"b60bd6a19af74c8babd823c334cabe41\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:13:52Z\", \"duration\": 121, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:13:53.41548786Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:11:51Z\", \"updatedAt\": \"2019-10-31T13:14:55.187382684Z\", \"confidence\": 0.8939604125, \"recordingId\": \"a240a5fc90224536ba9e345c8e136751\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Alveta graph where all you know, every Enterprise has a lot of interactions that happen and in general there is this idea that in ether we try to capture all these interactions and and and take a lot of these unstructured interactions and per system is structured format so that we can gain insights and understanding from them. So one quick way of kind of talking about the the graph would be that it's a graph of who said what when and in what context and then we use it both in both computationally and then also for insights, right? So that's either graph the second. I think the the next one is this idea of you know, how once a call is over. Let's say you're in a meeting and you're and the team is having a discussion and in an hour's call you're talking about a bunch of different topics. How do you kind of I understand and and extract what is important in this column not and you know it all flooded with so much information. And so how do you extract what is Meaningful and what's not so that is another aspect of how either ether zai gets applied to just the meetings their interactions, right? The next is the idea of topic detection using communities, which is a you know, when you there's a there's a meeting that's happening. Being and you're having a discussion, maybe it's a group meeting and you're talking about five different topics in the call or you're talking about, you know, you're doing a two people are talking about one particular subject. But then in that subject are touching on five or six different things how ether automatically extracts these chapters are these topics and makes them available either on the time line around the summaries, right? So that's the other aspect that \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"49a9c7427412404abce4ff763aad210b\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:39:48Z\", \"duration\": 45, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:39:49.217519197Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:39:03Z\", \"updatedAt\": \"2019-10-31T13:40:10.695598523Z\", \"confidence\": 0.85640697, \"recordingId\": \"2e2544d9e4014caf82e5f4abbaf10ac0\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"All right, so it's a great segue into you know, so when we talked about Channel Minds you brought up this notion of the dynamic portion. That is the chat that happens within the context is all captured in a graph and we talked about ether graph earlier as well as something that's a very unique way of representing the intelligence of the Insight that's happening in the context of teams and and the organization in a graph format and using this in a variety of different ways. So I guess you know We put this kind of block diagram together to kind of outline what the pipeline looks like. So half of this kind of leads into how the graph is actually formed and then the other half talks about how it is. Maybe you can just walk us through this a little bit. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"dc366d7c14994362a3401100b8ba0651\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:41:27Z\", \"duration\": 3, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:41:27.364822829Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:41:24Z\", \"updatedAt\": \"2019-10-31T13:41:48.236019493Z\", \"confidence\": 0.8498999, \"recordingId\": \"5ad2c3d8c938475fb4ed4bd877a682a0\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"And then all of this goes into our NLP and yeah. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"b052f73988884a48894804b2d1113e77\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:15:52Z\", \"duration\": 121, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:15:53.288500559Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:13:51Z\", \"updatedAt\": \"2019-10-31T13:16:34.935805139Z\", \"confidence\": 0.830554115, \"recordingId\": \"867d958521364898921c0311f427ca17\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Should talk about next is the keyphrase extraction, which is sometimes it's hard to blame the entire discussion. So a lot of products in the market today just do a blind transcription of the conversations and sometimes these transcript tration, you know, nobody reads them. First of all, the weight is very hard to read and understand so sometimes it's important to just be able to pull out. What are the key phrases right in a conversational setting. Segments so that you can quickly in a snare at-a-glance come to know what is what is being discussed and so on and so forth. So, how do we do that? So that's a very interesting aspect of ethers a sec as well and last but not the least. We also automatically extract key meeting Primitives right action items and decisions who conversations and be able to you know offer them as such Stood markers in addition to we do this manually anyway, right? So in any tell me call you can kind of click on the plus sign and create a lot of these manually as a call is going on, but we also have a way by which we are automatically detecting these and how we present them and you know how we use them is another aspect of ethers a a stack that I talk will discuss. So, of course, there's also a couple of other things that we are working on. On the background and we'll touch upon those as well. So my intent in this call is is to kind of have this discussion with you where we can maybe dig deep dive a little bit on each of these aspects. Sure. Okay, so, all right. So before we get started a lot of times question comes up when we talk about ether how we do our speech-to-text, right? We just to kind of lay this out is we don't \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"cc053b7c0140422da427a2de89414448\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:17:52Z\", \"duration\": 121, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:17:52.952034126Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:15:51Z\", \"updatedAt\": \"2019-10-31T13:18:34.673504414Z\", \"confidence\": 0.8319212650000001, \"recordingId\": \"a4b5cae25582439d980e84ebd8af8c10\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Build a speech to text technology ourselves because a lot of it one one reason for it is that it there's a lot of vendors in the market place that do this well, and we also believe that this is going to be like a table Stakes thing that will be driven down costs will go down over a period of time. So we actually focus on we assume that that is actually a speech-to-text engine that's in the background either with our with our partners lot of times. When we go to market with our partners, they actually like to use their own speech-to-text technology for integrating. And so the way we built ether was to have a lot of flexibility in being able to associate any speech-to-text engine to with our for our purposes. So the way we do this is, you know showcases app, which is ether meet. We integrate with the close partner called Deep Graham to provide. Custom models, which are caused optimized and continually trained and we use that and then we of course use a couple of other providers as well. We've integrated with AWS transcribe. We also have the ability to integrate with other models. We use Google Cloud speech with their video model for somebody processing because it provides the best kind of quality. It's also the most expensive. So in inside ethers engine, we have this ability where we do a first pass shall we say with the with the with the with a different provider? Let's say deepground as a first pass. And then once we determine that these are the key segments in the call that really needs to be, you know presented to the user and we want to be able to you know, use the best quality possible-- So we \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"a7d5164db7a84332b5973d95477f6c2c\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:18:46Z\", \"duration\": 55, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:18:46.952260465Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:17:51Z\", \"updatedAt\": \"2019-10-31T13:19:08.10996151Z\", \"confidence\": 0.8568464250000001, \"recordingId\": \"d963daf4b24945638d8d4a013803194d\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Can pause with Google's speeches video model. So that's the other kind of unique aspect about how we've built it inside our architecture. So the pipeline is very flexible. We have the ability to associate speech-to-text Provider by workspace by and we have the ability to do it in two passes one passes. So on and so forth, so it's very very flexible. Right? So anyway, that is a little quick note on. How we do speech to text so yeah, okay. So now we're getting to the interesting portion. So let's talk about Channel Minds right little deeper into Channel Minds. Maybe you can just give us a quick overview of what channel Minds is. I mean, I guess we've already discussed what channel minds are but more about how we build it and how we use it and so on. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"48da3759d2eb45f49c1ba344eec381ce\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:18:55Z\", \"duration\": 9, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:18:56.011444021Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:18:46Z\", \"updatedAt\": \"2019-10-31T13:19:17.200705213Z\", \"confidence\": 0.7909735, \"recordingId\": \"b827d3c573964e9e81424f26d5b749ec\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"I think before before we get started with its II think I think I'll just give you a heads up on a thin configuration by it. So how we are placed as a team. Mlh. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"642438db75064a729d29f91ac8463dcb\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:19:10Z\", \"duration\": 15, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:19:10.765331715Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:18:55Z\", \"updatedAt\": \"2019-10-31T13:19:31.928027252Z\", \"confidence\": 0.78798467, \"recordingId\": \"7e3da5e9ac8e4a3e9f24ee5c45a2a8da\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Great great portion. Maybe you know, I sorry for not doing this, but maybe you can just give a quick introduction about yourself. And also maybe give a little bit of background about Ari, I think sure. Yeah, so I'm Vanka. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"e79cf121424c4717afd9092c89923582\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:20:49Z\", \"duration\": 18, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:20:49.727029659Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:20:31Z\", \"updatedAt\": \"2019-10-31T13:21:10.730454107Z\", \"confidence\": 0.8249978, \"recordingId\": \"24f58e09d0aa47f387e1ae072d32bbc9\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"That's good thinking. Yeah, maybe we can now just dive deeper. Yeah. Sure. What I'll do is I'll ask you speak. I'll also create markers here. So you can kind of get a feel for it as we go on but wine to get started and let's dive in and talk a little bit about Channel Minds. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"547cd6b3d9a64d828e6c3eb3417149bc\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:40:52Z\", \"duration\": 64, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:40:53.518120519Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:39:48Z\", \"updatedAt\": \"2019-10-31T13:41:34.905985262Z\", \"confidence\": 0.7920440733333333, \"recordingId\": \"76423c5341944e9ca8113a62db28bb92\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Check just like so eat a graph serves multiple purposes one one being as we talked about it. It just enables the the whole whole concept of Channel Minds with the with properly, you know, organizing the data that is available as a factual information and also and also being able to as a whole ether graph is actually having two components that that that that I'll come but where I'll start is, you know, it's about its ability. T to be able to glean the data from multiple data sources that reminds same like like how we if we get if we on the left we can see that there are there multiple sources from where we can actually get the data from so one on the interaction. So one thing that that we that we have intermediate is this pixelization of the actual conversations that happens in any of this audio audio video providers and then everything else would just go as is every other data. A source would just go as is to the general feeling. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"4e970b30bd224610abf23d1fe9121aad\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:41:22Z\", \"duration\": 30, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:41:22.727928393Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:40:52Z\", \"updatedAt\": \"2019-10-31T13:41:44.097689179Z\", \"confidence\": 0.8154758, \"recordingId\": \"e8adee431c0148ccbb0f1969b39583b3\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Yeah, I guess it really we're operating in the text domain. So if it is a speech data we converted to text first if there is Vision data, let's say the slides that need to be processed. I guess we can extract textual information from slides and make them available in the text domain but things like messaging or let's say interactions between people and tools such as jira Google Docs or whatever which are already available in text can be used as \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"88bc2ce1eeb44c5e8215109c21594d90\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:41:24Z\", \"duration\": 2, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:41:24.677449546Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:41:22Z\", \"updatedAt\": \"2019-10-31T13:41:45.74111726Z\", \"confidence\": 0.8037341, \"recordingId\": \"73986f29b74843979b801647e3ae428f\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Yep, that's okay. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"0028af3fa1cd42fb916efc5ea85f9abe\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:43:50Z\", \"duration\": 23, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:43:50.931781215Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:43:27Z\", \"updatedAt\": \"2019-10-31T13:44:11.99352591Z\", \"confidence\": 0.8717734, \"recordingId\": \"26064510a9a94a8d82b6cef93d5ca983\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Crew captured non intuitive insights like you know who spoke about certain topic or who I mean on whom is the action air protection because each of these Services itself isn't are a model or an AI Pipeline on its own that will enable The Ether graph to capture very counter intuitive insights of or non-obvious inside sort of the god Apollo. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"95a41af139e448c19878b2703b380e0f\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:54:58Z\", \"duration\": 25, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:54:59.148589806Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:54:33Z\", \"updatedAt\": \"2019-10-31T13:55:20.200596692Z\", \"confidence\": 0.8751134, \"recordingId\": \"5bd26032055942009af6361a4d8552b9\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"So moving on from the knowledge graph. Let's move on to keyphrase extraction, right? Yeah, Sookie physics action again just to reiterate lot of time raw transcripts are pretty useless because it's too long and too much has discussed you want to be able to quickly glance, you know glance at at at a moment in a call and say what was discussed right in this is where key phrases come into the \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"9b39c578138346dab4bd145d4f91466f\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:58:15Z\", \"duration\": 5, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:58:15.506457667Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:58:10Z\", \"updatedAt\": \"2019-10-31T13:58:36.482896015Z\", \"confidence\": 0.84046817, \"recordingId\": \"b5f8facfcb3746ee8fc830f74c564c51\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Yes, Channel Minds comes in even in the in the in the key phrase \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"2a6e41e2ddda495d82a94089f247211b\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:02:09Z\", \"duration\": 51, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:02:10.239858483Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:01:18Z\", \"updatedAt\": \"2019-10-31T14:02:31.60780085Z\", \"confidence\": 0.80193565, \"recordingId\": \"c59a60061e8a4a53b6196e1833557a81\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"So I guess it's a very good very quick kind of a real world example of this extracted from real call, right? So where we have a particular discussion about an HR topic and then the red I guess litems are the key phrases that we extract the right in well transcript you see the full segment and then to the left in our timeline the markers you see the keywords that are extracted for that particular segment, right? That's right. for example pull-out top employer work requires substantial investment top technical universities HR talent management, and these are all things that are representative of the discussion that happened and a lot easier to look at and understand than having to read through the entire segment, I guess so \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"2fb1f86ea66b4d2d940c92e0bc34a0fa\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:02:14Z\", \"duration\": 5, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:02:14.450407635Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:02:09Z\", \"updatedAt\": \"2019-10-31T14:02:35.455736687Z\", \"confidence\": 0.7710156, \"recordingId\": \"e037955b30c84c4daaba0d7dd148497f\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"somebody apart what this this entire file line segment is talking about \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"958c5e5250b846b1b1e79045b696a018\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:02:48Z\", \"duration\": 34, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:02:48.710213957Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:02:14Z\", \"updatedAt\": \"2019-10-31T14:03:09.834553548Z\", \"confidence\": 0.782045165, \"recordingId\": \"5f3898b9b9ab4b0abf4a0569c5737db4\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Got it yet. So moving on I guess the next very I guess very important thing one way. One of the things that I always like to say is ether is the world's best meeting somebody engine right there. So we do a phenomenally good job of understanding not just pulling out random things based on frequency of words are freaking of topics, but on really utilizing how we apply Channel Minds to an overall discussion meeting of an hour hour and a half. How do you Pull out what's important and what's not so maybe you can just quickly run through. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"ccaf53d160fa453e9e591b4920256a16\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:07:26Z\", \"duration\": 19, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:07:26.745083508Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:07:07Z\", \"updatedAt\": \"2019-10-31T14:07:47.91748523Z\", \"confidence\": 0.76836765, \"recordingId\": \"cda3cd99d008495d91347825a16a4dde\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"That's the whole point of the channel when so so so the inference that I said, it just scores against channel channel Pine and then and then what comes out is the bunch of segments that that post that mostly covers the whole conversation at the same time sticking to what the channel is related to ensure of a deviating from that. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"b411b227696341bba3ef7cc36371c806\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:08:19Z\", \"duration\": 22, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:08:20.141480969Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:07:57Z\", \"updatedAt\": \"2019-10-31T14:08:41.088730257Z\", \"confidence\": 0.75648767, \"recordingId\": \"36bc86fd653543b58420bc6081f3bfa6\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"So here's a good example. I mean, I think we all had the solution as well. Yeah where you know when you pass meeting the taxes on a variety of different subjects and once we take it through our meeting summary engine is somebody that is extracted pulls out all the relevant topics related to that theme in this case. This is the engineering team, right? \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"1d3467ec094e40c49dbceda7dce7183b\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:08:25Z\", \"duration\": 6, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:08:25.871101311Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:08:19Z\", \"updatedAt\": \"2019-10-31T14:08:46.945200658Z\", \"confidence\": 0.7306538, \"recordingId\": \"43bc5bd3857a4b55a6767cf2a736c2de\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Okay, so I have a few phrases instead of the whole sector Somalia exact. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"15c91e41708949e6b6661aa7213b5f70\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:09:03Z\", \"duration\": 32, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:09:03.550196102Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:08:31Z\", \"updatedAt\": \"2019-10-31T14:09:24.86768587Z\", \"confidence\": 0.60135641, \"recordingId\": \"56c616ba022541af8c37a1b1152f5d1d\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Okay. No, this is a new ones as well. Right thus often times, you know, people are talking like, you know, in a team of say five six people one a call teams tend to get into discussions, you know, we're going back and forth particular subject and then you move on to some other subject in some other topic and so on right where so how do we kind of use this? How do we pull these chapters out of pull these topics out and and show them? Yeah. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"aaaa272fedd34dc482ce4005349d1bb0\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:20:32Z\", \"duration\": 82, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:20:33.05980272Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:19:10Z\", \"updatedAt\": \"2019-10-31T13:21:14.400129751Z\", \"confidence\": 0.8235309866666666, \"recordingId\": \"c030977f5531476b8ab698f56b35895c\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Then and I leave The A Team we are we are a team of 5. Ml Engineers who are who primarily work on machine learning deployments and also building the state-of-the-art machine learning models. So it's a fairly diverse team with people from really good colleges within India and outside so and from their experience perspective there were going to very poor. I mean as a team they worked on very broader set of are use cases ranging. Not just not just in the NLP. I also also like the Imaging and then the video processing and on the text, I mean the speech-to-text and and and the whole whole spectrum of the AI ecosystem. So that gives us a fairly good configuration of the team and then we have your pretty flexible in playing around with the state-of-the-art models in the Deep learning or any other machine learning space coming out then so that's where you see, you know. Big being able to quickly adapt to the industry Trends and then and then the best practices that are within within the rest of the best machine learning organizations across the globe. So having said that I think I think that should be good enough with the team and then maybe it's good time to get here. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"dbc27c69e0cb400792109ef93ffa4b9a\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:15:30Z\", \"duration\": 13, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:15:30.979655525Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:15:17Z\", \"updatedAt\": \"2019-10-31T14:15:52.1207181Z\", \"confidence\": 0.81728965, \"recordingId\": \"33886a1561164137aa9928e0fc82d2a7\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"That's something very unique as well. Right the fact that when a user is interacting with our tool. Yep at the manual tasks that they do very naturally helps reinforce our a models. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"69109483e398424f96b987d904227cf1\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:17:55Z\", \"duration\": 42, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:17:56.357031117Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:17:13Z\", \"updatedAt\": \"2019-10-31T14:18:17.491395559Z\", \"confidence\": 0.8153171699999999, \"recordingId\": \"2079a4a83d89443abe94840b63514ede\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Okay, I guess so it kind of going into the details right the other I guess very important aspect is how we pull out meeting Primitives, I guess. So for example one very important thing that happens when teams discuss in projects and so on so forth is action items right A lot of times these things just get created and unless there's a project manager in the call was actually writing down the action item or something many times these Message right. So one of the things that we try to do with either is to try to pull out these meeting mating Primitives right action items decisions and so on. So maybe you can quickly run through how we pull out action items. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"1f23ce5a7c6f43e4b66322a373145f7a\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:24:42Z\", \"duration\": 3, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:24:42.578451535Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:24:39Z\", \"updatedAt\": \"2019-10-31T14:25:03.51311742Z\", \"confidence\": 0.7770457, \"recordingId\": \"c0211ab864f24118bd60d6977c7d6a87\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Got it. Yeah. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"ecf667ce7d2341bc9de3acb7140eeba7\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:24:29Z\", \"duration\": 59, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:24:29.652532401Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:23:30Z\", \"updatedAt\": \"2019-10-31T14:25:11.319477456Z\", \"confidence\": 0.7787625, \"recordingId\": \"c3b1a8dee89546ceab3db1ffcc6576cd\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Okay, cool, so I guess here's a taking a quick example, so here are three different segments, right? Yeah where we pull out our additional segments is run through real calls where we are not only able to pull out, you know subtle follow-up type of action items like Doom or regression tests before we deploy let's send migration of these are not like typical traditional action items where someone says amen I can go, you know take an action to do this or something very specific right? You are very nuanced and these types of actions are actually pulled out automatically from the conversations. That is one. I guess the second is by intelligently applying our graph. We are also able to extract who it is assigned to write based on who's present in the call. And you know who's speaking and who's the recipient and so on? Yep. Yeah. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"a939ecf68bc24d3895eef49731912d5c\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:24:54Z\", \"duration\": 12, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:24:54.634677565Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:24:42Z\", \"updatedAt\": \"2019-10-31T14:25:15.70132313Z\", \"confidence\": 0.891685, \"recordingId\": \"df2de96f130a478ab21b21ea122e7e74\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"I guess once this these things are detected. They can always be, you know made into permanent, you know actions are moved to a different action item tracking problem solved. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"6a3e920f287c437b85d4a4ff698236ee\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:25:02Z\", \"duration\": 8, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:25:02.778978696Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:24:54Z\", \"updatedAt\": \"2019-10-31T14:25:23.928042904Z\", \"confidence\": 0.823251, \"recordingId\": \"c349bc7066df4fd0a2d5d6e48e21a6db\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Needless to say this just enables The Ether compute clap on the factual graphite and then it just okay. Just I think \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"acf261aee5bf4ea6adfcb987687da4d5\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:22:49Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:22:50.501544357Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:20:49Z\", \"updatedAt\": \"2019-10-31T13:23:52.231793318Z\", \"confidence\": 0.89514616, \"recordingId\": \"ee942ba4801042f6acaf61b02bad0df3\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Yeah, sure son. So I think with the headset that you gave about the channel Minds I meant that that's like a thousand feet free of what channel mine does. I mean to keep it simple. It's just captures the Ever Changing dynamics of the team fixing to the static part as he is like if a team talks about software engineering so it know the The Ether AI knows that it talks about software engineering and at the same time, it just gets it up, you know get adapted to what what is happening within the team over a period. Out of time. So I'll just walk you through I'll give you a tip. I'll give you what goes behind the scenes for the for the channel Minds technically and then we'll come to how it works across all the a downstream applications. So Channel Minds as we as we have architecture. These is a combination of a language model plus plus a representation of the the whole text that we glean out of The Ether. Ether calls or any other data sources that we get access to and though that takes being represented as a graph or a bar or in certain cases a different data structure that will give us access in a in a priority of the hierarchical fashion. So coming to coming to the language model aspect of it which is which is like the which is like the base that acts as a trans. I mean the feature extractor for the whole process wherein we train we train the neural net. Work language model on on the on the source of data that we choose to so having now that we are coming to the data source what we how we started with this channel Minds is to we have a pre-loaded, you know library of domain mines be call. So we stopped with such which talks about each domain when talks about certain certain. You know, what you call the horizontal. It's a wonder my mind is software engineering the other one could be markers. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"a710f5a0577544f5b83550f11a02fa2f\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:27:24Z\", \"duration\": 22, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:27:24.543903108Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:27:02Z\", \"updatedAt\": \"2019-10-31T14:27:45.486465249Z\", \"confidence\": 0.80908126, \"recordingId\": \"d7f0a6dbd6754852aa7013c4a9de5f31\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"We actually identify and tag that tag that speaker and then later on we plan to expose this in other ways as well where if there's an unknown speaker. Maybe someone can manually tag it and so on using our user interface, right? So we don't need to go into two sub detail around this, but I just want to kind of quickly call out that these are a couple of things that were also \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"b623465fc8024dee92d363bfd236281f\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:27:57Z\", \"duration\": 32, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:27:57.985966229Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:27:25Z\", \"updatedAt\": \"2019-10-31T14:28:19.264488068Z\", \"confidence\": 0.76822822, \"recordingId\": \"3132d9be4df140f1a61f4bb338bbba2c\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"So I think that that kind of concludes this discussion right thanks to incur that we've covered a lot of things. Hopefully this gives you a pretty good idea to the viewer about the different things types of things that we use in either for AI and I guess in a different discussion, I guess maybe we can go into how actually a lot of this is operationalized inside our deployments, but that's a discussion for another day. All right. Thanks. Guys, appreciate it. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"eac06a96244c481ab1e2e4445eddd5bd\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:24:49Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:24:51.454980926Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:22:49Z\", \"updatedAt\": \"2019-10-31T13:25:53.205396642Z\", \"confidence\": 0.8974705574999999, \"recordingId\": \"e93e8fafead3431a9be206b1397147b4\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"And I mean it could be sale. So that's ever-growing. That means we keep on adding the the new domains to our domain Library. So when when a user's invites ether to the channel, we give the option to choose one of these domain Minds so that so that as soon as the user chooses determine what we do is we attach the associated language model to that channel. So which means I mean, Coming to the language model aspect of it. We use the neural neural network based language models like birth GPT which are which have recently proven to be the state of the art and have beaten most of the benchmarks in the nice language processing tasks and even and also that and also they have worked really well for us or in last one year wherein we have seen significant gains compared to what you're doing earlier, right? So that's on the language model. Coming to coming to fine-tune that means how do we adapt that language model to the channel in which it is chosen is when when we choose when the user chooses a certain domain mind what it does is it gives a baseline, you know information to The Ether about the channel on what it is talking about. I'm going to show some examples in the next slides, but but to give you what it means, let's say if the user chooses software engineering and then it Gives an idea of what this team would be working on and talking about so that the just handles the cold start problem effectively and then as the conversations built up in the channel and as the number of you know ether calls that happen in the channel what what it does is it gives the software engineering intact and it just updates its it just updates the priorities based on what is being talked. Let's say the if the engineering teams talks about production. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"3a204f1bbd3148a09474a475c92d3ae5\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:36:31Z\", \"duration\": 28, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:36:31.599927298Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:36:03Z\", \"updatedAt\": \"2019-10-31T13:36:53.873323126Z\", \"confidence\": 0.88342446, \"recordingId\": \"9178a612f1664a45bb565380dad62ae9\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Cool. Alright, so that is actually a good segue to actually before we do that. I wanted to kind of call out Channel minds and action. All right, so there is there is here is a kind of an example. Let me actually take this down and show it in a real slack conversation. So let's give me a second here. Let me pull up Slack. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"07aa7bee0c1d4d5b8fb24bb453411b15\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:26:24Z\", \"duration\": 95, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:26:25.624200594Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:24:49Z\", \"updatedAt\": \"2019-10-31T13:27:07.061085741Z\", \"confidence\": 0.8218142425, \"recordingId\": \"794cc8e89aca4238a6860f6f138ff0d8\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"In past two weeks or three weeks. What it does is ETA automatically understands that any discussion related relating to production deployment should be prioritized and the mind for that comes and that decision capabilities for the AI engine comes from this channel Minds because it continuously sees what what is happening in the in the channel and then organelles the data coming to organizing the data. I'll talk about the Mind generation in the next slide, but How do we fine-tune I'll just continue on that. So so along with fine tuning this language model wherein you know, software engineering language model would be fine-tuned for the to adapt to the changes in the team that team conversations and parities and then that gives that gives The Ether ability to you know, flexibly understand what what is being talked about along with that. Once we once we have the language model, that's fine tune. We also adjust Oddities of the channel of based on the data structure that I just briefly mentioned which is the graph so that the the graph would be re readjusted with the priorities on the topics that are being discussed will be given a lot more importance than the than the than the others that are not mentioned yet. So so that's where the channel mind comes into play when we say mine. The model is the actual neural network model and the mind is is the graph data structure that organizes the information. Action that is coming, you know to The Ether AI engine if you're it. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"0004d9a74fe448f5af8ab09915ca2eb6\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:26:51Z\", \"duration\": 27, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:26:51.714400654Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:26:24Z\", \"updatedAt\": \"2019-10-31T13:27:12.900819135Z\", \"confidence\": 0.8786565, \"recordingId\": \"dc995915d64b4cbbafbac158b8f6df47\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"I guess one way to talk about that would be there's a static component. Then there's a dynamic component. Right? Like the static component is the model which is pre of trained and you know publicly available data and then there is the dynamic component which is the constant interactions and new updates that is happening between the teams and all of this gets mapped into a graph and brings it gives it the element of what is recency and so on. Yep. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"ba1e58eebf634454876be91308dd126d\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:43:27Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:43:28.763433874Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:41:27Z\", \"updatedAt\": \"2019-10-31T13:44:30.44300895Z\", \"confidence\": 0.856192665, \"recordingId\": \"f00a432d9a9e44dc8114f3dbcf018927\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"So in that help engine, it's like a it's like a pre fabricating the data such so that it can go into the graph structure. So these are two different components coming together. So we're in in the NIT engine what if primarily do if I have to do one, you know, if I have to put it in one sentence, we actually feature is it that means that we use all this language models that we use and then under the and all the graph based neural network architectures and then we make the text it to into a feature. So presentation so that we can be can you know use it for all sorts of computations. So we again use the associated domain language model of the custom custom Channel language model for for feature is Amal the text that picket and then and then we also bring in the notion of Miles here. That means these two are like in Hindi interdependent. So the mines enables the graphs at the same time ether graphs in Britain enables The Ether Minds to get to get you know, you know in Superior. In terms of its relevance relevance and other aspects caps in other aspects. So we also use them how we use them is you know to when we have a mind attached we use that mind and the associated model to feature is the text that we get out of the sources that we have. And also these these these NLP components would in turn enable us to do this, you know down sin tasks like keep Extraction team generation topic extraction on the insights from those Services would actually be fed into the up swim. You know, Knowledge Graph like the factual graph on The Ether graph. I'll talk about the difference just just in a minute, but but so what we are trying to say here is along with the actual factual information like who spoke what or or what are the you know who attended the meeting so these Services enable The Ether \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"9bc06ede99df459890697edb5ca18a38\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:45:28Z\", \"duration\": 67, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:45:29.423549179Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:44:21Z\", \"updatedAt\": \"2019-10-31T13:46:11.228706242Z\", \"confidence\": 0.8524303433333333, \"recordingId\": \"659b1bf408824006ba0f5e75d2bf996c\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"That's right. Yeah. So as you said the example that you gave write a card they can you follow the on this can come as part of the conversation in the free text, you know free language. So what this what this action detection service does is it just captures that and then and then preserves it once we once we try to update the computation graph, it just goes as an inside the Karthik assigned to the action item zones action item so that gets built without which is not really a factual information that we get. So having said that I'll just Deep dive into the computation graph and then The Ether graph that on the the knowledge cups component. So we're in the in the computation graph as we as mentioned earlier. It catches attaches the non factual information that can make the that is coming out of all this NLP base Downstream tasks. So say for example, if I have to give you one example of how the how the body is non-intuitive relations could be is you know, let's say let's say let's say Karthik talked about kubernetes in one of the in couple of calls \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"f144cefb8c3240909750cb9940444be9\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:47:29Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:47:30.958744945Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:45:29Z\", \"updatedAt\": \"2019-10-31T13:48:32.840082388Z\", \"confidence\": 0.77356691, \"recordingId\": \"c1b36d4b6a5c4fe19712c2c74cdd12cc\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Yeah, okay. So so what why did that what what is there a engine does is if I have to take you through the flow that the topic extraction service Maps identifies that there is a topical kubernetes and then and then we know that it is spoken by Karthik. So so so what it does is it also forms a relation between Karthik as anode which is the user and then the kubernetes as a topic and then says you know here Initially then as as as as more and more conversations around kubernetes are happening. The talked about can get interest into is a is an expert in tuber natives even say that is very cool. Yeah. So how in fact we can even take consensus just because Karthik is talking we need not say that he could be an expert. But if I orthotic that means every time Karthik talks about kubernetes if you are a watch it so which which makes in which makes him kind of we approve his Don't open it is and that we captured that inside to and then reinforce that that relationship as a Karthik is an expert in kubernetes. So that's how the evaluation of you know, relationships happened within the that's one of the example that I just gave program give you an idea of how computation graph works. So along that that just on topic so you can extend the same thing two action items which itself is an AI in AI module that we're going to talk about a little so so so and also other top XO from the from the from the computation graph perspective. I think I just Brave briefly highlighted. What bought the nodes could be along with the the factual notes like the users and favorite pics. So we we just organize the topics key phrases captured as the notes on that. They just forms the contextual relationship between these nodes and also, you know using some of the conventional techniques which is form some of the ideas whether they whether certain topics for occur within the context of a meeting. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"2c322826f6084509a38f3ea7be1705f2\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:50:01Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:50:03.505323616Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:48:01Z\", \"updatedAt\": \"2019-10-31T13:51:05.398742213Z\", \"confidence\": 0.8648083925000001, \"recordingId\": \"f0d7533ead7a4039aa4244ce7f6c81d4\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"So once we have this computation graph right along so as I said, it actually captures very non, you know non trivial aspects of the conversations of The Ether conversations. In fact, we can even do better at this point. If you don't talk about anything else if direct enables us to form the you know, form the form this non-inductive relationships like talked about a certain topic or action item assigned to so now let's take forward and then use This graph that is actually being formed after all the conversations. And then what we what we do is we just put it through a graph neural networks kind or the different graph algorithms. And then what comes out is actually a lot more, you know a lot more insights that will enable us to do like very non-intuitive recommendations. So as I talked about that Karthik being expert in a in kubernetes, right so that can be formed from the from some of this. You know, I'd rather than that we use like the node embeddings what I mean by no damn wedding is when there is Karthik so we don't just we don't just see Karthik as you know a person so we also input all the information that he that actually this ether captures about about that. That means all the relationships about the topics whom he follows or whom who I mean, what are the topics that he usually discusses and then we just call that whole component and then we encode everything into into the cottage. It snowed embedded like that. All the users will have their own representation that not just captures what who they are. But but what they come as a combination represents, so once they have this as a mathematical object, like like a like a embedding what we can do is we can do lot of you know algorithms on because we are free to do all the math around it and then we can you can get for the similar users to kartik from the context of you know, the topics that that he discusses or from the context of the action items. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"e7c40d097f274f80bad1505be523a451\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:50:45Z\", \"duration\": 7, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:50:46.109875014Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:50:38Z\", \"updatedAt\": \"2019-10-31T13:51:06.97822326Z\", \"confidence\": 0.80139107, \"recordingId\": \"3f051ae45b014224844407bcc71ea04d\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Into, you know, two steps deep into the artificial intelligence of the body of mission, I guess, you know. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"2e6829cebf67481d95955e3afdf08c5b\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:52:13Z\", \"duration\": 88, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:52:14.396511719Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:50:45Z\", \"updatedAt\": \"2019-10-31T13:52:55.759993809Z\", \"confidence\": 0.8621219866666666, \"recordingId\": \"7e061b33323e41af87dd5451fef0ced2\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"What are the use cases that we always talk about is, you know a lot of times when you have a meeting, let's say you and I are having this call and then we produce a summary it will be always good to know who should you know, what people who are would actually likely to watch this, right? Yeah, and so kind of generating this idea of who are the recommended Watchers. You're a discussion right are more importantly, you know, as we see in e-commerce lot of times right you buy this product you also You know you get this common thing that saves users who bought this are also interested in these other types of products, right? So those types of things can be applied to Enterprise intelligence as well because a lot of times T teams keep discussing the same subjects again and again, right so it's very common that in a team for example in an engineering team a particular type of databases that you know is being considered for their architecture, you know, the engineer leaves the company goes away voila six months later. A new person comes and then he comes and talks about the same subject right ether can very easily extract the idea that this discussion has happened before and actually say hey now that you're discussing this subject. You may also want to follow up on this other topic right? Are you watch this other discussion that happened a few months ago and that makes teams just work a lot efficiently and save a lot of time in going back and forth, right? So that's the beauty of this type of relationships. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"d5a60e4f0a5a4ce48d929b81dd6260d6\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:52:42Z\", \"duration\": 29, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:52:42.298909171Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:52:13Z\", \"updatedAt\": \"2019-10-31T13:53:03.691354059Z\", \"confidence\": 0.8635678, \"recordingId\": \"423804a338ec4274871b3165589e5c1d\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Yeah, so so it actually brings it to one more notion. Right? We can actually capture the person of the user within the graph of depending based on the interactions, you know conversations interactions and other aspects of the person and then we have a person a match that can go into this apps and insights part on the on the extreme right? We're in we can do recommendations Martellus. That's what you just saw. It's one of the you know extraction. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"2413039a385f4b5e96464f3064d6142b\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:54:22Z\", \"duration\": 65, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T13:54:23.242770111Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:53:17Z\", \"updatedAt\": \"2019-10-31T13:55:04.517225107Z\", \"confidence\": 0.7493102, \"recordingId\": \"e52233cd037f4f7782c0ee64e33575a9\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"I don't know if you're able to see this but here is an example of a real graph right where you know, there is a there is a particular insight about the bunch of different things. Right? So one is we're able to we took essentially one of the workspaces and map the interactions into a into a graph here and let's say we want to find out who the goo. Will experts are in a workspace in this can be very simple as a who's the person who's interested in a particular technology. Right? So those types of relationships can be gleaned very easily from the knowledge graph, right like so for example in this particular case when we run this query about who's who's a personal talked about Google these three people who come up myself Cullen and Karthik right where we talk a lot about Google. So these types of relationships get pulled out very easily from the knowledge. Graphite. Yep. That's just a quick example at a sample of what we do. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"fefe76dee147480e853c0df3db683dbf\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:56:58Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:56:59.813305929Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:54:58Z\", \"updatedAt\": \"2019-10-31T13:58:01.475551511Z\", \"confidence\": 0.85403549, \"recordingId\": \"6daf076889d64c0e9c166068e6573bed\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Yeah, this is this is like Channel Minds. This is also I am in we can call this as a fairly, you know, secondary generic service because we don't use keyphrase extraction stand alone. But what we do is when we say topic identification or be identified the potential important moments in the call. So what we do is instead of study showing the whole lot of text that you can anybody in the transcripts what we do is we identify the most relevant keep Phrases that can summarize the whole either the topic of the most important moment and then then show those key phrases as a by-product as a as a representation of those topics of the the important moments in the conversation. So so that brings us to how we do that keyphrase extraction from the from the technical standpoint. So so what we do is it's a two step process. So we're in in one in one step what we do is we extract the For the what could be what is the what is the what is the word or a or a set of forms that can be that is likely a key face which has no context of this at this point. We don't have any context of what is important and what is not important. All we know is this this this diagram or the three words combination sounds as if it's an important people. So let me just put it in my queuing for the scoring in the next steps. So for that we use we use the graph right? So this is this is live. This is not the ether graph that we talked about but this is just another algorithm to you know, to find out what are the candidate key places within the text that Wicket. So once we pass this, you know that this text or the segment text segment through the through the algorithm what we get is a set of candidates, I would say with lot of noise around it which we need to denoise in the next steps set of candidates which which kind of \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"b85dddabb62f4cc6896abc7ebdde9618\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T13:58:10Z\", \"duration\": 72, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T13:58:11.657001471Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:56:58Z\", \"updatedAt\": \"2019-10-31T13:58:52.953925621Z\", \"confidence\": 0.8262545000000001, \"recordingId\": \"fcb0a153d24244aaba1486c333f731bc\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Whole context of the either the topic of the important movement. So once we have this candidate key phrases what we do is we just pass it through our scoring service, which will ran all these candidates against the channel Minds because as we as we mentioned earlier the channel main caps is what is what is most relevant to the team and it is ever updating. So so any any any Downstream task could pass through this domain Channel Minds in a different in its own person in it. Own way and then gets conditioned or scored against the channel Point like so so what we do is we score each key plays against the channel Minds to get a relevance of that key phrase to the channel when let's say in software engineering even though we talked at length about environment or environmental issues. It may not be really relevant because someone just brought up that discussion briefly before the call or after the call, but still the call is ongoing so it doesn't really need. Need to be captured as a as an important environmental issues as an important topic of the people. So we just we just so Channel Minds just drops it or even the associated key phrases. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"b11cd18c8a1e4b2cbd3ea53775db9f64\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:00:15Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:00:16.417858562Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T13:58:15Z\", \"updatedAt\": \"2019-10-31T14:01:18.41463498Z\", \"confidence\": 0.8656769475, \"recordingId\": \"6eedf70307dd4a57a8cb12ef6b3287e9\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"So when you have see how it comes is when you have let's say the conversation about machine learning or or any software engineering related. So there could be there could be key phrases that are not really they could be like a general language key phrases that we always use but they get they get picked up by the graph Rank and we don't want to show them. So what we do is we just we just pass it through the channel Minds on the channel Minds just says that these are these are not sounding as if they belong to this conversation so you can you can just ignore them. So That's that's how it works. And then coming to come into the training part of it. What we do is so we actually again so we use we use the same language model that we talked about for the channel minds and then we fine tune it a little bit giving it and so actually you can see language model as a fairly thick soup flexible ecosystem. Where in you just you just tell it how to do certain things it will it will it can it can certainly take it into consideration without forgetting what it already knows. So what we do is we just give it give this domain language model of the chat Channel language Model A little boost on how to actually score the key phrases from using different learning paradigms. And then the data sources that are available. So one of these data sets that we use for our general benchmarking is this SST wherein it captures all the sentence similarity task action. So it in it you have a set of sentences to say two sets of sentences and it will tell you how similar on A scale of zero to a hundred or a hundred percent how similar they are. So what we do is we take all this language models that we train for the domain and we just keep we just do a one pass learning on those sentences by giving it an ability to differentiate how two sentences or two key phrases are different so that way we just fine-tune the language model specific to the to the giving it an ability to differentiate and then once we have this ability on the language model fine tuned on this. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"f4773b1607784818b1f9ec84d60f497d\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:01:18Z\", \"duration\": 63, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:01:19.227561757Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:00:15Z\", \"updatedAt\": \"2019-10-31T14:02:00.51192773Z\", \"confidence\": 0.8476079333333333, \"recordingId\": \"8c813d1c02d549ea89ffe1f2d8c708ba\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"We do is we take we take each candidate key face on the contacts associated with it. When I say context you could be the sentence in which it is part of or it could be, you know, a part of the sentence if the sentence is been there. We just passed the context through this algorithm fine-tuned and then at the end we get a we get a feature representation and then what we do is become politically this feature representation and and then correlate it with the channel Minds which are which is again a set of related, you know. Text and then we correlate that with the context associated with the channel Minds at the end of this this whole process this multi step three step process. What we have is the score of each keep is against the how relevant is it to the channel that it is associated with and then at the end we have a set of key phrases that are finely filtered out by ignoring all the noisy key phrases from the graph Rank and then summarizing whole context of the of the top either the topic of the segment. Come to those Down Citizen a little while. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"45f9dc69f29441e8b517565893255f0c\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:04:48Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:04:49.82394123Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:02:48Z\", \"updatedAt\": \"2019-10-31T14:05:52.537937436Z\", \"confidence\": 0.8426946424999999, \"recordingId\": \"2f65032054a4424aab4cef8c2de5c9d9\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Yeah, sure. So I think I got a use case level. That's right. Actually we what we do is one when as the conversation happens, so and we have this speech to text transcripts coming out of the transcript or service what we really continuously in the real time. We continue is core those conversations as if you know and then we pass it through this channel minds and then the whole process that that that we just talked about. I'll come to the training part A little later, but we just passed it through that are. A box and then what comes out is? What is the score for a certain segment when you say segment? It's like a Texan which is actually fairly self-contained either either. It just captures that you know a text that is Captain between fairly small pauses between or even when there is a speaker change and then we started the text and then we just score against the channel Minds to get up the relevant score of that segment to the conversation that's currently happening and So to the as a whole to the team and for the channel That that it is that in which that call is happen, right? So and then we score the in the scoring process is what the training aspect is. It's fairly similar to how we actually trained key phrase scoring service wherein we have a we have a sentence similarity tasks on which feed on which we have fine-tuned the associated channel channel model and the channel minds and then what we do is we we actually really one important aspect of it is the validation because because what what we get out of the model, I mean the the numbers the validation number that the model used could be fairly could be deceiving a time. So we really cannot rely only on those numbers. So what we do is when we do this the similarity task, you know fine-tuning of the language model. What we do is we have we we continuously create a list of summaries. That means we have set of like \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"e23d686b4b614aba8e082a32c44d539a\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:07:06Z\", \"duration\": 18, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:07:06.892302536Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:06:48Z\", \"updatedAt\": \"2019-10-31T14:07:27.784108967Z\", \"confidence\": 0.7352117, \"recordingId\": \"875828fca23f4a3c8107a7dc97d73a50\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Whole lot of huge HR conversation within in software engineering team so it could it could so it could still give out some important moments. But even if there is a slight mention of software engineering its prices for it higher than than the rest but it gives you a very good summary of what is happening. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"86d5c1d132554eaf8b26567de70faa9d\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:06:48Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:06:49.765223832Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:04:48Z\", \"updatedAt\": \"2019-10-31T14:07:51.324598076Z\", \"confidence\": 0.9007707075000001, \"recordingId\": \"830ddfda3f314ae7a5e42e4c5372e330\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Up tents of or hundreds of calls and then we manually curate what are the most important moments given the converse given the meeting given that this meeting is part of a certain Channel. And then what we do is the we just we just have a huge Corpus on which we have to validate so during the cleaning process. We generate like lot of candidate Al Gore models that will come out for that. We come up from the initial filtration of their performance and then we pass all those candidate models. Through this validation data on this we keep updating this validation data so that we can we can safely say that if it passes the validation set because it hasn't seen those validation satyr during training if it passes the validation set we can fairly say that we can deploy that into the, you know, Ever Changing dynamics of the team. So without without much Concepts so once so, that's how we validate. So this this kind of a semi-automatic autonomous approach because we just don't want to automate it we can we can definitely automate but but the whole point Of the channel Minds to ever evolve at the same time. We should also, you know, improve our improve or adapt to the validation data that we create. So it's a semi-auto autonomous approach that we adopt the pews for the validation. And then once we have this scoring scoring algorithm in place, the inference is pretty straightforward. We just we just passed this, you know, all the segments. So this scoring box and then what what comes out is again. It is as I said, it is conditioned when I say we are actually scoring something what it means is that it is code against the channel minds or the relevance of the of the meat clicking on the relevant context. And then what comes out is the score. I mean we needn't worry about the what do you call the quantifiable T of the score because you know, it's all relative. So each segment will have relative score on of course passes through certain minimal threshold. That means so what if there is a \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"bfe619410c4e4688abeebeb959407df4\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:07:58Z\", \"duration\": 13, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:07:58.240453845Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:07:45Z\", \"updatedAt\": \"2019-10-31T14:08:19.221995184Z\", \"confidence\": 0.75564015, \"recordingId\": \"69e2636e83044d0db30edcab53f8c5e9\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Yep, so it's kind of a very folks summer day would say so so it just it just cuts out the noise very aggressive price to stick to what what the team needs actually. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"85355cce55d6421999d8baa188a65d87\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:07:45Z\", \"duration\": 19, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:07:45.949193961Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:07:26Z\", \"updatedAt\": \"2019-10-31T14:08:06.962974771Z\", \"confidence\": 0.74823344, \"recordingId\": \"8b8f882355524de399d78b620ef7c8ba\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Rot it. Okay. So that's how for example in an engineering Channel. If you're spending the five ten in the first five minutes talking about the latest IPL score Kidd game as that happen, you're able to easily filter those unimportant moments out and get right into the important moments, which is what are the rest of the team can \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"61039d2e58bc47388248ccd2f568e533\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:08:29Z\", \"duration\": 4, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:08:29.748190319Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:08:25Z\", \"updatedAt\": \"2019-10-31T14:08:50.925231515Z\", \"confidence\": 0.8270104, \"recordingId\": \"b20b58062b5b4c24a071b225b01e49d4\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"I can just show the give you the key phrases instead of the entire discussion. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"0fd2a6dbbea446c88b98dd6cd79958e7\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:11:01Z\", \"duration\": 18, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:11:01.355151397Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:10:43Z\", \"updatedAt\": \"2019-10-31T14:11:22.564894879Z\", \"confidence\": 0.8055697, \"recordingId\": \"e6f58e994ee349be813db539d937f80b\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Perfect. Yeah, okay. So so maybe like for a say in this case when cat like for example, because because use ether graph for this as we do we use any like standardized Community detection algorithms that are graph-based for this. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"bfb6232992d24ca7af89c09a807be6f6\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:10:43Z\", \"duration\": 101, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:10:44.104300776Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:09:02Z\", \"updatedAt\": \"2019-10-31T14:11:46.121550131Z\", \"confidence\": 0.8409623, \"recordingId\": \"e7472da1f633482f8e63b96f8e01fa80\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"So the the whole intent of this app doesn't topic extraction is to be as you rightly said to be able to slice the whole meeting into certain, you know chunk such that each turn would represent a fairly independent context in which it is on what it is talking about. So to keep it simple. It's just a stock affection adult done done in a very flexible way. Let's say what we do is when you are as you have shown in the example at wearing one Call we are run through all the multiple channels of then we got different sub arrays. So what it does is if there are if there is a broader, you know, the broader categories of topics that are being discussed in certain calls. So so what this some topic extraction does is it will be very very, you know Broad in its topic identification also, so so let's say a certain Paul covered software engineering in which we talked about Lambda and Learning deployment and then in the same pot. We talked about Recruitment and also the product strategy what it does is most likely could put Lambda and deployment the same topic because you know, they've been discussed together. But otherwise if in another call we talked about Lambda base deployments for a whole lot of 45 minutes. What it does is it will be able to defeat evil to slice machine learning related components into one one topic and Lambda base if there is a fair segregation it will be able To slice them. So it's like a fairly flexible topic slicing algorithm that we that we have in place a topic extraction. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"6052f095303e4390a3bef29c89cf8f28\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:21:55Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:21:56.277766819Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:19:55Z\", \"updatedAt\": \"2019-10-31T14:22:58.170804396Z\", \"confidence\": 0.8411417175, \"recordingId\": \"309a6fa4018d4edcad430acb79dae633\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"So again the action item as a whole is not just a model here sigh that's what I wanted to highlight here. This is again a funneling process wherein in the first step, we actually captured the likelihood of an action item under the prime motor of this step is not to miss any candidate action items at an expense of I'm okay to have a lot of noise in the carrot patch. Sorry, but I don't want to miss even a single action item. That would be a likely candidate. So that's why we find you this classifier to have a highest possible. You'll recall or wherein we don't want to miss anything that has an accent. Mmm, but we are okay to have lot of that means if I capture 15 out of 15 candidate action items. I'm okay to have only four of them being the real action items, but I don't want to miss even one of the action items one of those pork perfect. So so so we have trained a language model to fine-tune such that we can be we have adopted that language model to be a binary classifier. Aaron it just says it just gives us a you know, spectrum of likelihood of being an action item from zero to a hundred percent. And then and then what we do is just I'll talk I'll talk briefly about the validation of this and then we'll move on to the next steps in the pie pan. So how we evaluate this binary classifier is again the same thing we manually tag it and then we just look at we just look at the coverage in terms of the if I if we give you like a thousand sentences out of which hundred are action items. So we validate this model for the coverage that means the model that covers almost all the hundred action items at an expense of you know, having a hundred born on action items would be preferred more than a model that actually captured only 50 action items. But as a total it has actually it has identified only 70, even though the noise is list. We don't consider that because as we as I said in the further funneling process, we will be able to funnel it further so that we didn't only candidate action items. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"ef4868f848594956b91214db33188407\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:12:20Z\", \"duration\": 80, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:12:20.994393223Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:11:00Z\", \"updatedAt\": \"2019-10-31T14:13:02.270593046Z\", \"confidence\": 0.8516666266666667, \"recordingId\": \"cefcfec99a6d46df80285973a6edaaee\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"That's right. So that's the second part of it sighs. So where in you know as I said the whole the whole the whole meeting is actually can be organized very well elegantly into a meeting structure the graph such as what it gives us is abuse as the abilities of being able to form the interactions between topics as if you know, they're moving back and forth. So what we do is we just organized the meeting into the graph that that's fairly simple because you know, you have lot of this text segments and then we we have eat them as the node. And then we have a graph a graph with with all these conversations as a notes. Then what we do is to this is where the Elegance of graph algorithms + The Machine learning the language model combination comes we start forming the relationships between this text segments using the language model associated with the with the channel That is there. So what we have at the end of this step is we have kind of a graph in which the relationships are formed based on the And pixelated so that means in any Docker lated sentences would be actually formed. It just would be found between them but no, it should be formed between Docker or the recruitment because even though they're talked in the same conversation or even inverse cos they are actually talked right one after the other because \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"e44d0a245c4d403d8b2c46bb0b9bb9c0\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:23:30Z\", \"duration\": 94, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:23:31.366095267Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:21:56Z\", \"updatedAt\": \"2019-10-31T14:24:33.315172954Z\", \"confidence\": 0.7880530199999999, \"recordingId\": \"96addb5832ab45d6a042f0bf086972e6\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"So the inference is fairly simple. What we do is we just we just pass the center. So that means as soon as we get this speech segments from specific sequence from the call, right? So we slice the segments into individual sentences. And then we just pass each of the segment sentence through this action item detection, which will give you will give us a score likely its core of being an action item for each of the sentence and then if it hits the threshold any any sentence that has individual would be passed through the You know a post processing wherein we have we have learned a lot of grammar rules and then and then the pattern mining algorithms which which actually gives the this action item detection ability to find out the pattern that can form as a grammar pattern that can be called as an action item. Say for example, if there is an action item detected by the candidate, but I didn't find any grammatically relevant subject in them. So we would prefer to drop that instead of trying to Label it as an action item in the post processing which just passed this candidates that we have identified in the previous step. So this grammar rules and then the patterns that we have identified and anything that's not actually qualifying enough for from the pattern should be disregarded. That's why we are okay to have lot of noise because the noise will get filtered out in this step and then eventually will end up having only, you know, very small and we are very we are very aggressive in this aspect. So it will filter out Lord of all the noise and then be the end up having fairly good set of concentrated set of action. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"0fa0f76cb5be43f08be43c9959ac79fc\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:24:39Z\", \"duration\": 10, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:24:39.662446607Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:24:29Z\", \"updatedAt\": \"2019-10-31T14:25:00.820250235Z\", \"confidence\": 0.83868504, \"recordingId\": \"9d1ed632ee294516a9fb079f9c0fdd1c\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"See we are not very static about this size. So it can say it the action team is assigned to one person or even multiple. It's very flexible in that aspect. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"c8c884f333ad49ce8447248a4a41dfc8\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:13:42Z\", \"duration\": 80, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:13:43.800214405Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:12:22Z\", \"updatedAt\": \"2019-10-31T14:14:24.997799695Z\", \"confidence\": 0.84512857, \"recordingId\": \"d520f1f97ce449b8bf210ace9a4d91fc\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"In the first step. We don't even form a relationship actually. So so once we have this relationships that the graph that is where in you have certain lationship Saint and then certain relationships proud what we do is we form communities within within within the whole meeting. So that means eventually what it boils down is a community is nothing but to keep it simple if it does nothing but a identifying a very concentrated relationships that means there are set of you know, Call it as a close close group within your friends. Actually, there was strong association between within 10 people where in most of the people are connected with most of the others. So so what it means is that if you bring the same notion into the communities Community something where in you have a very strong association between the text that is that is protects notes that are part of so what so when we see such communities what it says is it talks about something that is that is highly relevant within but not so relevant when Go out. I mean, I mean relatively so sure so that's what the topic is. So once we once we form this communities, which we go with various Community algorithms like Logan and Cody composition as an end product, we have like a candidate communities that can talk with that that we can fairly confidently say that this community talks about certain topics. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"30bc41434e934feda77e6c6b1cd02de7\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:15:37Z\", \"duration\": 7, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:15:37.739939154Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:15:30Z\", \"updatedAt\": \"2019-10-31T14:15:58.998984733Z\", \"confidence\": 0.7288544, \"recordingId\": \"c8aad39ed1554d41b3dacd44cd5e2c13\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"We always argument that with the with the with the manual golden said that we create. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"86cb8028bf7b4297a4fe79f1350262b1\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:28:01Z\", \"duration\": 3, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:28:02.30348151Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:27:58Z\", \"updatedAt\": \"2019-10-31T14:28:23.397600542Z\", \"confidence\": 0.6521717, \"recordingId\": \"4f939c14839a42b599e2d1a462a89583\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Bye-bye, bye-bye. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"7e78f8c1912640d5a7874ed1673c3bdc\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:15:17Z\", \"duration\": 93, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:15:18.742846582Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:13:44Z\", \"updatedAt\": \"2019-10-31T14:16:00.318792093Z\", \"confidence\": 0.846585705, \"recordingId\": \"09a356a82efc4430827b4b73429656f2\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"So just to just to be just to be aligned with the with the flow. What happens is if you talk about Docker, you know, once in the start of the conversation and to also towards the end of the meeting what we do is we can be as we as a community formation the community forming algorithm can can safely put them into a single Community. But as a most person what we do is just to do the slicing properly, which is separate them and then highlight that these were talked about You know darker but they are actually separated. So we are just giving you as a different conversation. So okay, that's all the community algorithms desk aside. And then how we trial how we trial is as I said, it's more of it. It comes from the language model fine-tuning which has nothing to do with the communities. But we we we actually played around with this community algorithms and then we have come up with a set of parameters that works the best for the associated language model. So it's a it's a combination. A nation of community algorithm parameters and the language model performance. So there's no profit business straight for training gear. Its if it comes as a bits and pieces and then how will validate as I said, I think as we always go with the with the topics that we manually created like like we can even use the manual markers that the users created as you have created multiple topics here. What we do is if you create a Copic Marker we use that as our validation as a human validation so sub truth. And then the validator community formation. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"0586a3fbdb6e4276ae19b4f04f5e3311\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:16:29Z\", \"duration\": 52, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:16:29.979246266Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:15:37Z\", \"updatedAt\": \"2019-10-31T14:16:51.245892135Z\", \"confidence\": 0.8214141450000001, \"recordingId\": \"6224f9d6c5f94e94b270755147abfc93\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Got it tigers have wanted to show a quick example of how this works. Right? So here's a call that happened and where there's a bunch of discussion that that happened between a group of people and we are able to very nicely kind of pull out the fact that the shunt and Arjun talked about deployment and configuration devops related subjects and then the Russians and blanket in then moved on and talked about some database subjects like Seattle and Cloud SQL exactly exactly. Things like that and then they went back Arjun venkat and three shots now talked about databases. Right? And so there's these top. This is one says even the whole subject is about about software. I guess, you know different topics are different groups of topics and the people involved in those conversations are pulled out a very elegant way I think is very cool. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"fb2a244ad7bf446f9b97a082430c6a27\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:17:13Z\", \"duration\": 44, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:17:13.983825674Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:16:29Z\", \"updatedAt\": \"2019-10-31T14:17:35.15701946Z\", \"confidence\": 0.727065625, \"recordingId\": \"a018a21cc4b54aa7a0262a3261342223\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"That's right. So possibly it did actually you can observe one things. I hear it didn't actually slice the whole databases databases singled out because it just reaches talked about database and run something related to deployment together. So so it just it's just able to put them together aggressive. That means if we talk only about databases on the deployment, it would have done the other way like click diplomatic conversations and then they database so that's the level of flexible. Granular Kappa can get you can go from the top level where in you have totally relevant unrelated topics discussed. So they end up as a topics or to the as we go granular. We just get what you see right now make sense. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"e7071f03540e41928e524f6358fce509\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:19:55Z\", \"duration\": 120, \"spokenBy\": \"3f01f2032f584b178fafde6b437058ae\", \"createdAt\": \"2019-10-31T14:19:57.210801327Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:17:55Z\", \"updatedAt\": \"2019-10-31T14:20:58.928039587Z\", \"confidence\": 0.8769464325, \"recordingId\": \"93ab9ea8beca4125bc5a44b20d0d971d\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-US\", \"originalText\": \"Should I just like I think when we say action items, let's let me put one point before we talk about it. So actually we are talking about action items that are being in the free-flow conversation like what we are doing right now. So which means that we need not explicitly mention some way upward for the action item to be captured which most of this, you know, transcription services or sorry Services. There's if they have seen a lot of exotic references like that. So what what what we do differently is we don't we don't need need ether use it explicitly mention any anything to for either to be able to record the action item so you can you just talk as usual in the conversation so it can it can it can certainly capture. What are the action items that are that it that they either bought things as an action items and then and then and then gives gives away. So for this it is it's this this whole action item algorithm or the other approach is little different from what we have been discussing earlier. So this This fairly has nothing to do with the with the language model that we discussed earlier. But what we do is we use we use a generic language model that means a language model that has actually fairly good understanding of that language in general English language in general then what we did is we fine-tune that language model to to set up action item to build an action item classifier, which will take as an Put the sentence and then outputs whether the sentence contains an actual likelihood actually likely action candidate or not. So how do we do that is we have actually collected lot of training data for this. So this is kind of a exercise that we did for us to be able to train this model and then we create a lot of free flow conversation sentences where in there are action items and then obviously the counter examples where there are action items then we built a binary classifier for this. \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}, {\"id\": \"f853870e10f44514a6f048debee2a416\", \"status\": \"completed\", \"deleted\": false, \"endTime\": \"2019-10-31T14:27:02Z\", \"duration\": 120, \"spokenBy\": \"8fff81b5b2f14aa5ad67405f3e8127f3\", \"createdAt\": \"2019-10-31T14:27:03.575531292Z\", \"deletedAt\": null, \"startTime\": \"2019-10-31T14:25:02Z\", \"updatedAt\": \"2019-10-31T14:28:05.859284334Z\", \"confidence\": 0.85776935, \"recordingId\": \"1d7944880d2b46d693866902c520754c\", \"transcriber\": \"google_speech_api\", \"languageCode\": \"en-IN\", \"originalText\": \"Course most of it. Maybe we can just spend a couple of minutes really quick on a couple of minor other things that we are working on. So obviously and and I'll just talk to this right we working on a vision pipeline as well because obviously a lot of business meetings involve slide presentations and all that. Right? So we're working on a technique where we can detect that the slide is being presented and then automatically pull out information from that using Standard computer vision techniques and then once the vision was a slight information is converted to text a lot of the same information can be used in terms of applying what is relevant and what's not and what's the topic and what's a keyword and so on so forth, right? So I'm just going to skip past this and then similarly the other kind of very interesting subject that we you know, we are looking into we've done some amount of proof of Concepts and investigation into it and we plan to do more on this. In the coming weeks and months is that in a shed setting? For example, you and I are in a desktop call. It's very clear. Who's the speaker button shared setting. Let's say a bunch of people are crowded around the conference call content speakerphone or whatever, right? We want to be able to detect that there are two different speakers are three different speakers and then also be able to associate a particular speaker with a particular, you know, identify the speaker I guess right. So we we kind of use at least a wave. We're approaching this. Is that as we speak? We are continually creating a library of our signature using standard MFC sea bass techniques and creating patterns and storing them for every speaker that we know and then as we detect changes in those patterns, we we kind of apply that to speaker dilation and then if the signature actually matches someone in our database \", \"transcriptId\": \"252ee37f-f7fd-4638-91f3-01d05ba4849a\"}], \"mindId\": \"01E1V0BSBR82C3BVPZWJDVFB16\", \"contextId\": \"01DBB3SN874B4V18DCP4ATMRXA\", \"instanceId\": \"abc\"}}"