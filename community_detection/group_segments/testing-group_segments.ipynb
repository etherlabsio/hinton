{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T13:39:35.426641Z",
     "start_time": "2019-10-15T13:39:35.366425Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T13:39:37.281711Z",
     "start_time": "2019-10-15T13:39:36.697446Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T13:39:43.191674Z",
     "start_time": "2019-10-15T13:39:43.097399Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('topic_testing/sync_ml_24_10.txt', 'rb') as f:\n",
    "    request = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T15:21:40.346341Z",
     "start_time": "2019-10-04T15:21:40.253050Z"
    }
   },
   "outputs": [],
   "source": [
    "# for segi in request[\"body\"][\"segments\"]:\n",
    "#     if segi[\"transcriber\"]==\"aws\":\n",
    "#         print (segi[\"originalText\"])\n",
    "#         print (\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T13:44:53.144310Z",
     "start_time": "2019-10-15T13:39:45.462395Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from main import handler\n",
    "\n",
    "res = handler(request, None)\n",
    "group = json.loads(res['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in group['group'].keys():\n",
    "    if len(group['group'][g])>1:\n",
    "        print (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T15:05:16.630280Z",
     "start_time": "2019-10-13T15:05:16.588296Z"
    }
   },
   "outputs": [],
   "source": [
    "# json.dumps(group)\n",
    "# with open(\"result_testing.json\", \"w\") as f:\n",
    "#     f.write(json.dumps(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import iso8601\n",
    "from backports.datetime_fromisoformat import MonkeyPatch\n",
    "MonkeyPatch.patch_fromisoformat()\n",
    "from datetime import datetime\n",
    "# meeting start time.\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n",
    "\n",
    "#m_time = formatTime(\"2019-09-19T06:05:00Z\", True) #eng_19\n",
    "#m_time = formatTime(\"2019-09-20T07:12:00Z\", True) #eng_front_end_20\n",
    "#m_time = formatTime(\"2019-09-24T06:11:00Z\", True) #eng_24\n",
    "#m_time = formatTime(\"2019-10-04T05:44:00Z\", True)  #podcast_04\n",
    "#m_time = formatTime(\"2019-10-08T11:55:00Z\", True)  #podcast_08\n",
    "m_time = formatTime(\"2019-10-24T09:32:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-16T09:53:21Z\", True)\n",
    "#m_time = formatTime(\"2019-07-04T12:15:14Z\", True)\n",
    "for i in group['group'].keys():\n",
    "    #print (\"\\n\\n\\nPIMs \", i)\n",
    "    if len(group['group'][i])<2:\n",
    "        continue\n",
    "    print (\"\\n\\n PIM Discussion:\\n\\n \")\n",
    "    for seg in group['group'][i]:\n",
    "        print (\"Minutes from the start of the meeting: \", formatTime(seg['startTime'], True) - m_time , seg['id'],\"\\n\")\n",
    "        print (seg['originalText'],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T13:47:03.428496Z",
     "start_time": "2019-10-15T13:47:03.274758Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "Once this is done. We need to create a step function.  \n",
      "\n",
      "We need to have a proxy from landscape it is. even  \n",
      "\n",
      "what this do what this basically does is if we issue a command that's that's messages published research topics. So there's one process of this insist are all the toppings and Bryce and map them to the corresponding A Douglas even Bridge traffic.  \n",
      "\n",
      "An effective that is you can use those events to take a laptop that can be for example, when we say part next stop after meeting ended or summary generated. We will begin go push another message that gets proxy to enrich that starts the you can Pop the Trunk.  \n",
      "\n",
      "That'll be Lambda. So the step function is basically yes. It is a smaller Lambda functions. You can actually start reading about it. So can you modern Lambda Lambda is connected like to build a pipeline. It takes one input process and it's inaudible paralyzed the flow will errors and wheat rice and all.  \n",
      "\n",
      "This can be useful for the ability to visualize the flow for the business logic this kind of detail to explain political processes. Okay. That sort of is the prerequisite of the Indian troops have function median can be triggered.  \n",
      "\n",
      "so the creativeness step function is similar to Lambda IJ or is  \n",
      "\n",
      "You double step function is actually an encapsulation of pipeline lambdas. Okay? Okay, so in graded like a workflow.  \n",
      "\n",
      "Start do this on both adverse and ended the surface from the South Port again these lambdas next Lambda. I would do something else and then have it right to multiple things to order it.  \n",
      "\n",
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "Generating customized we need to start we need to populate other source of information such as snap messages. anymore  \n",
      "\n",
      "All the other information that we can get like Channel created towards bot substitute. I believe it's just simple as pouring into tables and just  \n",
      "\n",
      "and then for this debuff perfect, is there a schema that that we it's already there or  \n",
      "\n",
      "Schema files and so we have tried getting even Ultra segment instance meeting ID my contacts. Are you okay be populating. So everything ready to this?  \n",
      "\n",
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "Yeah, it is. Huh? Okay scription ID can be verbage segment arrays good if it's unit because we generate them with more and more frequency that we are by nature always sorted in my and they started as a teller.  \n",
      "\n",
      "markers because expose it outside exposed permalinks hot already Kate Parker so that in your menu your user ID can also remain you  \n",
      "\n",
      "computer stuff we can use unit 4  \n",
      "\n",
      "I think in our DB schema we have specified type else who you writing. Okay, some my I think we need to change the same are then going to sling then going to your lives.  \n",
      "\n",
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "well, you know lets you can finish the  \n",
      "\n",
      "so action and then we can decide whether you want to create a new data set like the air embolus provider or first baby is great the parent see what breaks before you and me.  \n",
      "\n",
      "they say the existing VPC that with the Lambda values cannot be deleted or cannot be changed because the danger signature.  \n",
      "\n",
      "Back, that's one week and we will need to do that because that's the functions lambdas need to access the graph. I mean the databases and let's of Deadbeats.  \n",
      "\n",
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "Really if you have any written already any peer go this case of beer hoping more than storing in a no baby after this I use that to listen for zoom body. Okay? Okay, if you didn't already written a Handler, yeah every time but I just logging that and this painting that just raised that Pier. Okay, I'll use that. Meanwhile, let's finish. This will create a so basically there are few items for the graph population. Okay. Alright, so I'll actually create an epic.  \n",
      "\n",
      "Commented on one of the issues which I had created about action the teams would like he'll be giving two separate his for action on for decision.  \n",
      "\n",
      "Okay. Okay. I'll take a look at an update from one. Is there a bunch of times for us Black Market? this process 40 Grand population  \n",
      "\n",
      "Well, we need to get into production and all the time. That is one. Okay. So basically that means every time we need to continue see populate one Knowledge Graph when data comes in. So that is four occasions. Not of the meaning of a graph paper example on the shank is done using the he actually at the meeting early creates a graph object makes an even you need to go pull the destiny object run it through against this code and have a great writer mystery bucket.  \n",
      "\n",
      "Our have it not right as you get here in this case right to Lambda and just populate Earth roughly doubles.  \n",
      "\n",
      "that's like the fact somewhat later states that first thing is actually meeting IDs migration and  \n",
      "\n",
      "Everything would be everything related to what is populating in the graph database and affording IDs. transcription IDs if I think most likely would remain positively IDs IDs that might not be seen and  \n",
      "\n",
      "So all these things is he's already storing it in the graph. Yeah, and then his chair to this trivet.  \n",
      "\n",
      "user ID so he is depending on these as the identifiers in this resources to determine uniqueness because if you want to maintain symmetry to be on the agenda business ability to so that is one thing so I actually asked them during my education possible for now this and hyphens, but he doesn't know which place where I wish number of little better if we start working on this also. way to migrate in sanitizer monster  \n",
      "\n",
      "Daddy's I think beautiful to you ladies. That's wrong. Nothing stopping us don't expose amongst a walk.  \n",
      "\n",
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "Phyllis we have to do in addition to this.  \n",
      "\n",
      "All the other information that we can get like Channel created towards bot substitute. I believe it's just simple as pouring into tables and just  \n",
      "\n",
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "Most things are committed to populate so the graph sort of expands.  \n",
      "\n",
      "So this one so that we can start incrementing poverty programs. They can use whatever they want to do the okay.  \n",
      "\n",
      "It will be good because we already started the first one we have.  \n",
      "\n",
      "So that way sort of gives it either.  \n",
      "\n",
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "I'm not sure how much of a priority that needs to be discussed tomorrow trials. Okay?  \n",
      "\n",
      "I mean I didn't take a look at when Workforce have been impacted with this or not in your  \n",
      "\n",
      "Where to reach a period of great the telephone providers and we can work the glanders between inside me PCS problem.  \n",
      "\n",
      "Back, that's one week and we will need to do that because that's the functions lambdas need to access the graph. I mean the databases and let's of Deadbeats.  \n",
      "\n",
      "Correct. They do try to do an automatic update and there's not small Twigs expect us to my Malika.  \n",
      "\n",
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "Was the theme playing I found some curve which will save us try to do it on my test this code. So I was just testing it around and the other thing which I was working on was like saving all this white Mages in our dynamodb.  \n",
      "\n",
      "So that before the events I've attended this event handler for that is just about creating the N mod be installing if they're so is the second coat to store that dynamodb. So I was going through the websocket changes with me tonight and here is Pena moderate. So this sitting there  \n",
      "\n",
      "Really if you have any written already any peer go this case of beer hoping more than storing in a no baby after this I use that to listen for zoom body. Okay? Okay, if you didn't already written a Handler, yeah every time but I just logging that and this painting that just raised that Pier. Okay, I'll use that. Meanwhile, let's finish. This will create a so basically there are few items for the graph population. Okay. Alright, so I'll actually create an epic.  \n",
      "\n",
      "Okay. Okay. I'll take a look at an update from one. Is there a bunch of times for us Black Market? this process 40 Grand population  \n",
      "\n",
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "Daddy's I think beautiful to you ladies. That's wrong. Nothing stopping us don't expose amongst a walk.  \n",
      "\n",
      "This one more here a little less.  \n",
      "\n",
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "I think in our DB schema we have specified type else who you writing. Okay, some my I think we need to change the same are then going to sling then going to your lives.  \n",
      "\n",
      "You can use be one version 1 because version one gives you an attack sorted man, okay?  \n",
      "\n",
      "After the meeting Eddie migration, maybe it'll be good if we can take a look at completing the actions.  \n",
      "\n",
      "And actually started on that so I ran some getting the previous hip speaker code. And then I dropped out.  \n",
      "\n",
      "Continue for instance. We can see you take a I mean take a look at it.  \n",
      "\n",
      "\n",
      "\n",
      " Chapter Discussion:\n",
      "\n",
      " \n",
      "I'll probably change it today. I'll see what if there are any issues with that. Maybe we can talk tomorrow. Yeah document portal compatibility issues that that I found.  \n",
      "\n",
      "So it's the spending side after that you can take what is still baking?  \n",
      "\n",
      "No Karthik, I mean this the TV Department then I do is open up your terminal immediately open up here on the stack.  \n",
      "\n",
      "Yeah, you just do that so that I can I'll work on that today.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import iso8601\n",
    "from datetime import datetime\n",
    "# meeting start time.\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n",
    "\n",
    "#m_time = formatTime(\"2019-09-19T06:05:00Z\", True) #eng_19\n",
    "#m_time = formatTime(\"2019-09-20T07:12:00Z\", True) #eng_front_end_20\n",
    "#m_time = formatTime(\"2019-09-24T06:11:00Z\", True) #eng_24\n",
    "#m_time = formatTime(\"2019-10-04T05:44:00Z\", True)  #podcast_04\n",
    "#m_time = formatTime(\"2019-10-08T11:55:00Z\", True)  #podcast_08\n",
    "#m_time = formatTime(\"2019-10-14T06:04:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-16T09:53:21Z\", True)\n",
    "#m_time = formatTime(\"2019-07-04T12:15:14Z\", True)\n",
    "for i in group['group'].keys():\n",
    "    #print (\"\\n\\n\\nPIMs \", i)\n",
    "    print (\"\\n\\n Chapter Discussion:\\n\\n \")\n",
    "    for seg in group['group'][i]:\n",
    "        #print (\"Minutes from the start of the meeting: \", formatTime(seg['startTime'], True) - m_time , seg['id'],\"\\n\")\n",
    "        print (seg['originalText'],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-08T12:47:50.839356Z",
     "start_time": "2019-10-08T12:47:50.777983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import iso8601\n",
    "import datetime.datetime.fromtimestamp\n",
    "# meeting start time.\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n",
    "\n",
    "#m_time = formatTime(\"2019-09-19T06:05:00Z\", True) #eng_19\n",
    "#m_time = formatTime(\"2019-09-20T07:12:00Z\", True) #eng_front_end_20\n",
    "#m_time = formatTime(\"2019-09-24T06:11:00Z\", True) #eng_24\n",
    "#m_time = formatTime(\"2019-10-04T05:44:00Z\", True) #podcast_04\n",
    "m_time = formatTime(\"2019-10-08T11:55:00Z\", True) #podcast_08\n",
    "#m_time = formatTime(\"2019-09-22T09:37:00Z\", True)\n",
    "#m_time = formatTime(\"2019-09-16T09:53:21Z\", True)\n",
    "#m_time = formatTime(\"2019-07-04T12:15:14Z\", True)\n",
    "for i in group['group'].keys():\n",
    "    print (\"\\n\\n\\nPIMs \", i)\n",
    "    print (\"\\n\\nDiscussion:\\n\\n \")\n",
    "    for seg in group['group'][i]:\n",
    "        print (\"Minutes from the start of the meeting: \", formatTime(seg['startTime'], True) - m_time , seg['id'],\"\\n\")\n",
    "        print (seg['originalText'],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T18:40:48.961370Z",
     "start_time": "2019-10-03T18:40:48.876945Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "j = 1\n",
    "if i !=1 or j !=1:\n",
    "    print (True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T08:10:44.971728Z",
     "start_time": "2019-10-03T08:10:44.896970Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../ai-engine/pkg/\")\n",
    "\n",
    "from graphrank.core import GraphRank\n",
    "from graphrank.utils import GraphUtils, TextPreprocess\n",
    "\n",
    "gr = GraphRank()\n",
    "tp = TextPreprocess()\n",
    "gu = GraphUtils()\n",
    "\n",
    "def get_desc(sentence):\n",
    "    original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(sentence, filter_by_pos=True, stop_words=False)\n",
    "    word_graph = gr.build_word_graph(graph_obj=None, input_pos_text=pos_tuple, window=4, preserve_common_words=False)\n",
    "    normal_keyphrase = gr.get_keyphrases(word_graph, pos_tuple, post_process=True)\n",
    "    desc_keyphrase = gr.get_keyphrases(word_graph, pos_tuple, descriptive=True, post_process_descriptive=True)\n",
    "    desc_keyphrase = sorted(desc_keyphrase, key=lambda kv:kv[1], reverse=True)\n",
    "    normal_kp = [phrase for phrase, score in normal_keyphrase]\n",
    "    desc_kp = [phrase for phrase, score in desc_keyphrase]\n",
    "    \n",
    "    return normal_kp, desc_kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T08:10:46.342803Z",
     "start_time": "2019-10-03T08:10:45.419169Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_id_map = {}\n",
    "user_id_map = {\"84fbaa66a2474ea29ae053f3a2e519d6\":\"Mithun\",\"75bdf310110b4b8fab88b16fafce920e\":\"Trishanth Diwate\",\"b1e8787a9a1f4859ac11cbb6a8124fd9\": \"Venkata Dikshit\", \"fb52cb663aec4795aee38ccfd904d315\":\"Reagan Rewop\", \"81a3e15469374fceba1cf972faa209b2\":\"Arjun Kini\", \"ecfeeb757f0a4d47af1ebd513929264a\":\"Shubham\", \"62b6ae1d7f834b0bb2055f7c72bc3368\":\"Karthik Muralidharan\", \"1a21542584494fcaba957d768b595b80\":\"Vamshi Krishna\", \"7e7ccbba232d411aa95ad3f244a35f40\":\"Shashank\", \"65bb83952fb54409a4bb59bb707f1375\":\"Vani\", \"0bbbfe84c66145af8d0ffcd5258bba38\":\"Parshwa Nemi Jain\"}\n",
    "\n",
    "for groupid in group[\"group\"]:\n",
    "    user_list =[]\n",
    "    seg_list = []\n",
    "    keyphrase = []\n",
    "    if len(group[\"group\"][groupid])>1:\n",
    "        for segi in group[\"group\"][groupid]:\n",
    "            if segi['spokenBy'] not in user_list:\n",
    "                user_list.append(segi['spokenBy'])\n",
    "            seg_list.append(segi['originalText'])\n",
    "            #keyphrase.append(get_desc(segi['originalText']))\n",
    "        #print (\"User\", end=\" \")\n",
    "        print (*[user_id_map[user] for user in user_list], sep=\", \", end=\" \")\n",
    "        print (\"Discussed \\n\\n Text: \", *seg_list, \"\\n\\n\")\n",
    "        print (*get_desc(\" \".join(sent for sent in seg_list))[1][:5], sep=\", \")\n",
    "        print ( \"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T12:51:14.312281Z",
     "start_time": "2019-09-27T12:51:14.250175Z"
    }
   },
   "outputs": [],
   "source": [
    "groups = []\n",
    "for i in group['group'].keys():\n",
    "    if len(group['group'][i])==1:\n",
    "        continue\n",
    "    else:\n",
    "        temp = []\n",
    "        for seg in group['group'][i]:\n",
    "            temp.append(seg['originalText'])\n",
    "        groups.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T08:18:37.987596Z",
     "start_time": "2019-09-23T08:18:37.053975Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/processed/master_tag+para_cluster_df.csv\")\n",
    "import ast\n",
    "tag = [] \n",
    "for tags in df['tags']:\n",
    "    for t in ast.literal_eval(tags):\n",
    "        tag.append(t)\n",
    "print (\"no of tags present:\", len(tag))\n",
    "unique_tag_unr = list(set(tag))\n",
    "unique_tag = []\n",
    "for u in unique_tag_unr:\n",
    "    if tag.count(u) > 10:\n",
    "        unique_tag.append(u)\n",
    "print (\"no of unique tags present:\", len(unique_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T10:13:29.302228Z",
     "start_time": "2019-09-23T10:13:29.152755Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "aws_config = Config(\n",
    "        connect_timeout=60,\n",
    "        read_timeout=300,\n",
    "        retries={\"max_attempts\": 0},\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "lambda_client = client(\"lambda\", config=aws_config)\n",
    "\n",
    "def get_embeddings(input_list, req_data=None):\n",
    "\n",
    "    if req_data is None:\n",
    "        lambda_payload = {\"body\": {\"text_input\": input_list}}\n",
    "    else:\n",
    "        lambda_payload = {\"body\": {\"request\": req_data, \"text_input\": input_list}}\n",
    "\n",
    "    try:\n",
    "        #logger.info(\"Invoking lambda function\")\n",
    "        invoke_response = lambda_client.invoke(\n",
    "            FunctionName=\"keyphrase_ranker\",\n",
    "            InvocationType=\"RequestResponse\",\n",
    "            Payload=json.dumps(lambda_payload),\n",
    "        )\n",
    "\n",
    "        lambda_output = (\n",
    "            invoke_response[\"Payload\"].read().decode(\"utf8\").replace(\"'\", '\"')\n",
    "        )\n",
    "        response = json.loads(lambda_output)\n",
    "        status_code = response[\"statusCode\"]\n",
    "        response_body = response[\"body\"]\n",
    "\n",
    "        if status_code == 200:\n",
    "            embedding_vector = np.asarray(json.loads(response_body)[\"embeddings\"])\n",
    "\n",
    "        else:\n",
    "            embedding_vector = np.asarray(json.loads(response_body)[\"embeddings\"])\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        pass\n",
    "    return embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T08:19:28.634538Z",
     "start_time": "2019-09-23T08:18:38.038084Z"
    }
   },
   "outputs": [],
   "source": [
    "fv = get_embeddings(groups, req_data=None)\n",
    "fv_tag = get_embeddings([i for i in unique_tag if i!=\"\"], req_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T08:19:28.725587Z",
     "start_time": "2019-09-23T08:19:28.636633Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "for index, tex in enumerate(groups):\n",
    "    closest = -1\n",
    "    closest_tag = None\n",
    "    closest_2= -1\n",
    "    closest_tag_2 = None\n",
    "    closest_3 = -1\n",
    "    closest_tag_3 = None\n",
    "    closest_4= -1\n",
    "    closest_tag_4 = None\n",
    "    for index2, t in enumerate([i for i in unique_tag if i!=\"\"]):\n",
    "        if t!=\"\":\n",
    "            score = 1 - cosine(fv[index],fv_tag[index2])\n",
    "            if score > closest:\n",
    "                closest = score\n",
    "                closest_tag = t\n",
    "            elif score > closest_2:\n",
    "                closest_tag_2 = t\n",
    "                closest_2 = score\n",
    "            elif score > closest_3:\n",
    "                closest_tag_3 = t\n",
    "                closest_3 = score\n",
    "            elif score > closest_4:\n",
    "                closest_tag_4 = t\n",
    "                closest_4 = score\n",
    "    print (\"\\n\\n\\nsentence: \\n\\n\", tex)\n",
    "    print (\"\\n most similar tags: \", str(closest_tag) + \" , \" + str(closest_tag_2) + \" , \" + str(closest_tag_3) + \" , \" + str(closest_tag_4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"topic_testing/cullen_test.json\", \"rb\") as f:\n",
    "    req = json.load(f)\n",
    "full_req = {\"body\": {\"contextId\": \"01DBB3SN99AVJ8ZWJDQ57X9TGX\", \"instanceId\":\"522c6cac-46ef-4ee1-8b1d-81e0e7d53943\",\"mindId\":\"01DADP74WFV607KNPCB6VVXGTG\", \"segments\":[json.loads(val['value']) for val in req]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cullen_test.json\",\"w\") as f:\n",
    "    json.dump(full_req, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sri_gpt",
   "language": "python3",
   "name": "sri_gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
