{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:55:55.302542Z",
     "start_time": "2019-06-19T07:55:55.295289Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertPreTrainingHeads\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "device = 'cpu'\n",
    "import sys\n",
    "import os\n",
    "import iso8601\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:55:58.143228Z",
     "start_time": "2019-06-19T07:55:56.839228Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertForPreTraining_custom(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForPreTraining_custom, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n",
    "        output_all_encoded_layers=True\n",
    "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                                   output_all_encoded_layers=output_all_encoded_layers)\n",
    "        if output_all_encoded_layers:\n",
    "            sequence_output_pred = sequence_output[-1]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output_pred, pooled_output)\n",
    "        return prediction_scores, seq_relationship_score, sequence_output, pooled_output \n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_json_file('../data/bert_config.json')\n",
    "bert_model = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:55:59.646732Z",
     "start_time": "2019-06-19T07:55:59.629545Z"
    }
   },
   "outputs": [],
   "source": [
    "def getNSPScore(sample_text):\n",
    "    \n",
    "    m = torch.nn.Softmax()\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(sample_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [0]*tokenized_text.index('[SEP]')+[1]*(len(tokenized_text)-tokenized_text.index('[SEP]'))\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    pred_score, seq_rel, seq_out, pool_out = model1(tokens_tensor, segments_tensors)\n",
    "    return m(seq_rel).detach().numpy()[0][0] #returns probability of being next sentence\n",
    "\n",
    "def getSentMatchScore(sent1, sent2, nsp_dampening_factor = 0.7):\n",
    "    \n",
    "    sent1_feats = getBERTFeatures(model1, sent1, attn_head_idx)\n",
    "    sent2_feats = getBERTFeatures(model1, sent2, attn_head_idx)\n",
    "    \n",
    "    cosine_distance = 1- cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = np.mean([nsp_score_1,nsp_score_2])*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    #print (\"nsp score -> \" + str(nsp_score))\n",
    "    #print (\"cosine score -> \" + str(cosine_distance))\n",
    "    return score\n",
    "    \n",
    "\n",
    "def getSentMatchScore_wfeature(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    #return cosine_distance\n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    #nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    #nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    \n",
    "    nsp_score = nsp_score_1 * nsp_dampening_factor\n",
    "    #nsp_score = nsp_score_1*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    return score\n",
    "\n",
    "def getSentMatchScore_wfeature_cosine(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    return cosine_distance\n",
    "    \n",
    "\n",
    "def getSentMatchScore_wfeature_test(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    \n",
    "    nsp_score = np.mean([nsp_score_1,nsp_score_2])*nsp_dampening_factor\n",
    "    #nsp_score = nsp_score_1*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    return score, cosine_distance, nsp_score\n",
    "def getBERTFeatures(model, text, attn_head_idx = -1): #attn_head_idx - index o[]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    if len(tokenized_text)>200:\n",
    "        tokenized_text = tokenized_text[0:200]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    _, _, seq_out, pool_out = model(tokens_tensor)\n",
    "    seq_out = list(getPooledFeatures(seq_out[attn_head_idx]).T)\n",
    "    #pool_out = list(pool_out.detach().numpy().T)\n",
    "    \n",
    "    return seq_out\n",
    "\n",
    "def getPooledFeatures(np_array):\n",
    "    np_array = np_array.reshape(np_array.shape[1],np_array.shape[2]).detach().numpy()\n",
    "    np_array_mp = np.mean(np_array, axis=0).reshape(1, -1)\n",
    "    return np_array_mp\n",
    "\n",
    "def replaceContractions(text):\n",
    "    #text = text.lower()\n",
    "    c_filt_text = ''\n",
    "    for word in text.split(' '):\n",
    "        if word in contractions:\n",
    "            c_filt_text = c_filt_text+' '+contractions[word]\n",
    "        else:\n",
    "            c_filt_text = c_filt_text+' '+word\n",
    "    return c_filt_text\n",
    "\n",
    "def cleanText(text):\n",
    "        \n",
    "    text = text.replace('\\\\n','')\n",
    "    text = text.replace('\\\\','')\n",
    "    #text = text.replace('\\t', '')\n",
    "    #text = re.sub('\\[(.*?)\\]','',text) #removes [this one]\n",
    "    text = re.sub('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\\s',\n",
    "                ' __url__ ',text) #remove urls\n",
    "    #text = re.sub('\\'','',text)\n",
    "    #text = re.sub(r'\\d+', ' __number__ ', text) #replaces numbers\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:56:03.247401Z",
     "start_time": "2019-06-19T07:56:01.204137Z"
    }
   },
   "outputs": [],
   "source": [
    "model1 = BertForPreTraining_custom(config)\n",
    "model1.to(device)\n",
    "#state_dict_1 = torch.load('/home/ether/domain_mind/ai/bert_10epc_ai_ds_1e-6_sl40.bin')\n",
    "state_dict_1 = torch.load('../data/bert_10epc_se_1e-6_sl40.bin')\n",
    "model1.load_state_dict(state_dict_1)\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:54:47.841886Z",
     "start_time": "2019-06-19T07:54:47.836591Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Meetings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:25:32.023162Z",
     "start_time": "2019-06-19T07:25:32.020296Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Timeline json meeting Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:22:32.043950Z",
     "start_time": "2019-06-19T09:22:32.040628Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def parsemeeting(text):\n",
    "    with open(text, 'r') as f:\n",
    "        parsed_text = json.load(f)\n",
    "    return parsed_text\n",
    "text = parsemeeting('../data/timeline_result2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:22:39.048539Z",
     "start_time": "2019-06-19T09:22:39.046169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = ''\n",
    "temp = ' '\n",
    "for t in text['timeline']['transcriptSegments']:\n",
    "    temp = ' ' +  t['text']\n",
    "    texts = texts + temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T06:26:58.101349Z",
     "start_time": "2019-06-19T06:26:58.083952Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Get top 10 pims instead of all the segments.    \n",
    "# texts = ''\n",
    "# temp = ' '\n",
    "# for t in text['segments']:\n",
    "#     if t['transcriber'] == \"google_speech_api\":\n",
    "#         temp = ' ' +  t['originalText']\n",
    "#         print (\"---text-----  \\n\\nspoken by: \" + t['spokenBy'] + \"\\n\")\n",
    "#         print (temp, \"\\n\\n\\n\")\n",
    "#         texts = texts + temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load meeting as csv and get only the top10 pims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:56:10.346289Z",
     "start_time": "2019-06-19T07:56:10.339489Z"
    }
   },
   "outputs": [],
   "source": [
    "## load it as csv.\n",
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('../data/ee944a7260c341f39525885f68fd0338.csv', index_col=False, header=0);\n",
    "df = df.sort_values(by='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:56:11.862052Z",
     "start_time": "2019-06-19T07:56:11.844338Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "data= {}\n",
    "segment_contents = {}\n",
    "texts = ''\n",
    "for index,segment in enumerate(df['value'].tolist()):\n",
    "    data = json.loads(segment)\n",
    "    if \"google\" in data['transcriber']:\n",
    "        texts+= (' ' + data['originalText'])\n",
    "        segment_contents[index]= [' '.join(tp.preprocess(data['originalText'], stop_words=False, remove_punct=False)),formatTime(data['startTime'], True), data['spokenBy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get Communities from meetings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:01:22.227203Z",
     "start_time": "2019-06-19T09:01:16.849137Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "# with open('../data/Engineering_Mind_Test_Transcripts.txt','r') as fp:\n",
    "#     texts = fp.read()\n",
    "\n",
    "mod_texts_unfiltered = tp.preprocess(texts, stop_words=False, remove_punct=False)\n",
    "mod_texts = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts_unfiltered):\n",
    "    if len(sent.split(' '))>250:\n",
    "        length = len(sent.split(' '))\n",
    "        split1 = ' '.join([i for i in sent.split(' ')[:round(length/2)]])\n",
    "        split2 = ' '.join([i for i in sent.split(' ')[round(length/2):]])\n",
    "        mod_texts.append(split1)\n",
    "        mod_texts.append(split2)\n",
    "        continue\n",
    "        #mod_texts.pop(index)\n",
    "    if len(sent.split(' '))<=6:\n",
    "        continue\n",
    "    mod_texts.append(sent)\n",
    "\n",
    "print(len(mod_texts))\n",
    "fv = {}\n",
    "\n",
    "for index, sent in enumerate(mod_texts):\n",
    "    fv[index] = getBERTFeatures(model1, sent, attn_head_idx=-1)\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:01:29.342698Z",
     "start_time": "2019-06-19T09:01:29.337083Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_graph(doc_list):\n",
    "    eng_graph = nx.Graph()\n",
    "    try:\n",
    "        eng_graph.add_nodes_from(range(len(doc_list)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return eng_graph\n",
    "\n",
    "\n",
    "tg = build_graph(mod_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:14:41.440698Z",
     "start_time": "2019-06-19T09:01:36.353407Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attn_head_idx = -1\n",
    "\n",
    "node_edge = []\n",
    "\n",
    "for index1, sent1 in enumerate(mod_texts):\n",
    "    print (index1)\n",
    "    for index2, sent2 in enumerate(mod_texts):\n",
    "        if index1!=index2:\n",
    "            score = getSentMatchScore_wfeature(sent1, sent2,fv[index1],fv[index2])\n",
    "#             if score > 0.8:\n",
    "#                 #tg.add_edge(index1,index2,{'weight': score})\n",
    "#                 tg.add_edge(index1,index2)\n",
    "            tg.add_edge(index1,index2,weight=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:18:44.330449Z",
     "start_time": "2019-06-19T09:18:44.315033Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_community_graph(tg, mod_texts):\n",
    "    com_graph = nx.Graph()\n",
    "    for sent in list(tg.nodes()):\n",
    "        com_graph.add_node(sent)\n",
    "    for nodea in tg.nodes():\n",
    "        for nodeb in tg.nodes():\n",
    "            if nodea!=nodeb:\n",
    "                if tg.edges[nodea,nodeb]['weight'] > 0.75:\n",
    "                    com_graph.add_edge(nodea,nodeb)\n",
    "    return com_graph\n",
    "com_graph = build_community_graph(tg, mod_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:18:51.761798Z",
     "start_time": "2019-06-19T09:18:51.373636Z"
    }
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "partition = community.best_partition(com_graph)\n",
    "\n",
    "values = [partition.get(node) for node in com_graph.nodes()]\n",
    "values=[partition.get(node) for node in com_graph.nodes()]\n",
    "plt.rcParams['figure.figsize']= [16, 10]\n",
    "measure_name = \"Louviin Algorithm Community Structure\"\n",
    "pos = nx.spring_layout(com_graph, k=0.2, iterations=20)\n",
    "nodes_plot=nx.draw_networkx_nodes(com_graph, pos, node_size=140, label=True, cmap=plt.get_cmap('magma', len(com_graph.nodes())/4),node_color=values, alpha=0.95)\n",
    "edges_plot=nx.draw_networkx_edges(com_graph, pos, edge_color='r', alpha=0.1)\n",
    "plt.title(measure_name, fontsize=22, fontname='Arial')\n",
    "plt.colorbar(nodes_plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:18:58.674770Z",
     "start_time": "2019-06-19T09:18:58.669573Z"
    }
   },
   "outputs": [],
   "source": [
    "community.modularity(partition, com_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:19:05.664269Z",
     "start_time": "2019-06-19T09:19:05.662199Z"
    }
   },
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:24:01.208426Z",
     "start_time": "2019-06-19T09:24:01.191286Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        print (mod_texts[word])\n",
    "        current=cluster\n",
    "    else:\n",
    "        print (mod_texts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:19:19.676509Z",
     "start_time": "2019-06-19T09:19:19.674371Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# current = 0\n",
    "# print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "# for word, cluster in partition:\n",
    "#     if cluster!=current:\n",
    "#         print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "#         print (word)\n",
    "#         current=cluster\n",
    "#     else:\n",
    "#         print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:19:26.599179Z",
     "start_time": "2019-06-19T09:19:26.596445Z"
    }
   },
   "outputs": [],
   "source": [
    "com_graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:19:33.702279Z",
     "start_time": "2019-06-19T09:19:33.699688Z"
    }
   },
   "outputs": [],
   "source": [
    "tg.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:30:07.027802Z",
     "start_time": "2019-06-19T07:30:07.022745Z"
    }
   },
   "source": [
    "# Redefine the resulatant communities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:19:40.662333Z",
     "start_time": "2019-06-19T09:19:40.659821Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters = []\n",
    "temp = []\n",
    "prev_com = 0\n",
    "for word,cluster in partition:\n",
    "    if prev_com==cluster:\n",
    "        temp.append(word)\n",
    "        #print (temp)\n",
    "    else:\n",
    "        clusters.append(temp)\n",
    "        temp = []\n",
    "        prev_com = cluster\n",
    "        temp.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:22:04.205077Z",
     "start_time": "2019-06-19T09:22:04.198810Z"
    }
   },
   "outputs": [],
   "source": [
    "segment_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:19:47.679691Z",
     "start_time": "2019-06-19T09:19:47.673738Z"
    }
   },
   "outputs": [],
   "source": [
    "timerange = []\n",
    "temp = []\n",
    "for cluster in clusters:\n",
    "    temp= []\n",
    "    for sent in cluster:\n",
    "        temp2 = [(sentence,start_time,user) for sentence,start_time,user in segment_contents.values() if mod_texts[sent] in sentence]\n",
    "        if len(temp2)!=0:\n",
    "            temp.append(temp2[0])\n",
    "    if len(temp)!=0:\n",
    "        temp = sorted(temp,key=lambda kv: kv[1])\n",
    "        timerange.append(temp)\n",
    "    else:\n",
    "        print (cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:19:54.704322Z",
     "start_time": "2019-06-19T09:19:54.698109Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timerange_detailed = []\n",
    "temp = []\n",
    "flag = False\n",
    "pims = {}\n",
    "index_pim = 0\n",
    "index_segment = 0\n",
    "for index,com in enumerate(timerange):\n",
    "    temp = []\n",
    "    flag = False\n",
    "    #print (\"-----community-----\", index)\n",
    "    for (index1,(sent1,time1,user1)), (index2,(sent2,time2,user2)) in zip(enumerate(com[0:]),enumerate(com[1:])):\n",
    "        if sent1!=sent2:\n",
    "            #print (time1, time2, (time2-time1).seconds)\n",
    "            if ((time2-time1).seconds<=240):\n",
    "                if not flag:\n",
    "                    pims[index_pim] = {'segment'+str(index_segment):[sent1,time1,user1]}\n",
    "                    index_segment+=1\n",
    "                    temp.append((sent1,time1,user1))\n",
    "                #else:\n",
    "                    #print ('removing',time1, time2)\n",
    "                    #temp.pop()\n",
    "                pims[index_pim]['segment'+str(index_segment)] = [sent2,time2,user2]\n",
    "                index_segment+=1\n",
    "                temp.append((sent2,time2,user2))\n",
    "                flag=True\n",
    "            else:\n",
    "                #print (time2, time1)\n",
    "                if flag==True:\n",
    "                    index_pim+=1\n",
    "                    index_segment=0\n",
    "                flag=False\n",
    "    if flag==True:\n",
    "        index_pim+=1\n",
    "        index_segment=0\n",
    "    #print (\"-----timeRange-----\\n\", [j for i,j,k in temp])\n",
    "    timerange_detailed.append(temp)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alter communities based on time range and get Keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:20:01.645353Z",
     "start_time": "2019-06-19T09:20:01.640248Z"
    }
   },
   "outputs": [],
   "source": [
    "from graphrank.graphrank import GraphRank\n",
    "from graphrank.utils import GraphUtils, TextPreprocess\n",
    "\n",
    "keyphrase_text = \"\"\n",
    "keyphrases_list = []\n",
    "keyphrases = []\n",
    "for index_pim in pims.keys():\n",
    "    keyphrase_text = \"\"\n",
    "    for seg in pims[index_pim]:\n",
    "        if seg != \"keyphrase\":\n",
    "            keyphrase_text += (' ' + pims[index_pim][seg][0])\n",
    "    gr = GraphRank()\n",
    "    tp = TextPreprocess()\n",
    "    utils = GraphUtils()\n",
    "\n",
    "    original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(keyphrase_text, filter_by_pos=True, pos_filter=['NOUN', 'PROPN', 'ADJ', 'FW'], stop_words=False)\n",
    "\n",
    "    word_graph = gr.build_word_graph(filtered_pos_tuple, original_tokens=original_tokens, window=4, reset_graph_context=True, preserve_common_words=False)\n",
    "\n",
    "    keyphrases = gr.get_keyphrases(word_graph, normalize_nodes='degree')\n",
    "    pims[index_pim]['keyphrase'] = [phrases for phrases, score in keyphrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:20:08.640631Z",
     "start_time": "2019-06-19T09:20:08.636159Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## remove overlapping communities\n",
    "combined_pims = []\n",
    "extra_keyphrase = []\n",
    "removed_pims = []\n",
    "for i in pims.keys():\n",
    "    for j in pims.keys():\n",
    "        if i!=j:\n",
    "            if pims[i]['segment0'][1] >= pims[j]['segment0'][1] and pims[i]['segment0'][1] <= pims[j]['segment'+str(len(pims[j].values())-2)][1]:\n",
    "                #if j not in combined_pims:\n",
    "                for k in pims[i]:\n",
    "                    if k!=\"keyphrase\":\n",
    "                        if pims[i][k] not in pims[j].values():\n",
    "                            pims[j]['segment'+str(len(pims[j].values())-1)] = pims[i][k]\n",
    "                            #print (pims[i][k])\n",
    "                            continue\n",
    "                    else:\n",
    "                        extra_keyphrase = list(set(pims[i]['keyphrase'] + pims[j]['keyphrase']))\n",
    "                        pims[j]['keyphrase']=extra_keyphrase\n",
    "                        removed_pims.append(i)\n",
    "                combined_pims.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:20:15.571408Z",
     "start_time": "2019-06-19T09:20:15.568411Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_pims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:17:59.654854Z",
     "start_time": "2019-06-19T09:17:59.652811Z"
    }
   },
   "outputs": [],
   "source": [
    "removed_pims = set(list(removed_pims))\n",
    "for i in removed_pims:\n",
    "    pims.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T09:18:06.536644Z",
     "start_time": "2019-06-19T09:18:06.534075Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## final pims\n",
    "for i in pims.keys():\n",
    "    print (\"\\n\\n\\nPIMs \", i)\n",
    "    print (\"\\n\\nDiscussion:\\n\\n \")\n",
    "    for seg in pims[i]:\n",
    "        if seg!=\"keyphrase\":\n",
    "            print (pims[i][seg][1], \" \", pims[i][seg][0],\"\\n\")\n",
    "    print (\"Keyphrases:\\n\\n \", pims[i]['keyphrase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing topic modelling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:03:40.394625Z",
     "start_time": "2019-06-12T16:03:40.375521Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in new_text]\n",
    "#doc_clean = [clean(new_text[1]).split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:03:52.012109Z",
     "start_time": "2019-06-12T16:03:51.997576Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:05:59.081754Z",
     "start_time": "2019-06-12T16:05:59.077554Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:06:26.310170Z",
     "start_time": "2019-06-12T16:06:25.966545Z"
    }
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:07:13.872792Z",
     "start_time": "2019-06-12T16:07:13.864096Z"
    }
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "community",
   "language": "python",
   "name": "community"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
