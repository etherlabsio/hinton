{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T11:36:09.215678Z",
     "start_time": "2019-07-02T11:35:58.956212Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertPreTrainingHeads\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "device = 'cpu'\n",
    "import sys\n",
    "import os\n",
    "import iso8601\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T11:36:10.417303Z",
     "start_time": "2019-07-02T11:36:09.226465Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertForPreTraining_custom(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForPreTraining_custom, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n",
    "        output_all_encoded_layers=True\n",
    "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                                   output_all_encoded_layers=output_all_encoded_layers)\n",
    "        if output_all_encoded_layers:\n",
    "            sequence_output_pred = sequence_output[-1]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output_pred, pooled_output)\n",
    "        return prediction_scores, seq_relationship_score, sequence_output, pooled_output \n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_json_file('../data/bert_config.json')\n",
    "bert_model = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:13:55.844347Z",
     "start_time": "2019-07-02T12:13:55.824790Z"
    }
   },
   "outputs": [],
   "source": [
    "def getNSPScore(sample_text):\n",
    "    \n",
    "    m = torch.nn.Softmax()\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(sample_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [0]*tokenized_text.index('[SEP]')+[1]*(len(tokenized_text)-tokenized_text.index('[SEP]'))\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    pred_score, seq_rel, seq_out, pool_out = model1(tokens_tensor, segments_tensors)\n",
    "    return m(seq_rel).detach().numpy()[0][0] #returns probability of being next sentence\n",
    "\n",
    "def getSentMatchScore(sent1, sent2, nsp_dampening_factor = 0.7):\n",
    "    \n",
    "    sent1_feats = getBERTFeatures(model1, sent1, attn_head_idx)\n",
    "    sent2_feats = getBERTFeatures(model1, sent2, attn_head_idx)\n",
    "    \n",
    "    cosine_distance = 1- cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = np.mean([nsp_score_1,nsp_score_2])*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    #print (\"nsp score -> \" + str(nsp_score))\n",
    "    #print (\"cosine score -> \" + str(cosine_distance))\n",
    "    return score\n",
    "    \n",
    "\n",
    "def getSentMatchScore_wfeature(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    #nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    #nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    \n",
    "    nsp_score = nsp_score_1 * nsp_dampening_factor\n",
    "    #nsp_score = nsp_score_1*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    return score\n",
    "\n",
    "def getSentMatchScore_wfeature_cosine(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    return cosine_distance\n",
    "    \n",
    "\n",
    "def getBERTFeatures(model, text, attn_head_idx = -1): #attn_head_idx - index o[]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    if len(tokenized_text)>200:\n",
    "        tokenized_text = tokenized_text[0:200]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    _, _, seq_out, pool_out = model(tokens_tensor)\n",
    "    seq_out = list(getPooledFeatures(seq_out[attn_head_idx]).T)\n",
    "    #pool_out = list(pool_out.detach().numpy().T)\n",
    "    \n",
    "    return seq_out\n",
    "\n",
    "def getPooledFeatures(np_array):\n",
    "    np_array = np_array.reshape(np_array.shape[1],np_array.shape[2]).detach().numpy()\n",
    "    np_array_mp = np.mean(np_array, axis=0).reshape(1, -1)\n",
    "    return np_array_mp\n",
    "\n",
    "def replaceContractions(text):\n",
    "    #text = text.lower()\n",
    "    c_filt_text = ''\n",
    "    for word in text.split(' '):\n",
    "        if word in contractions:\n",
    "            c_filt_text = c_filt_text+' '+contractions[word]\n",
    "        else:\n",
    "            c_filt_text = c_filt_text+' '+word\n",
    "    return c_filt_text\n",
    "\n",
    "def cleanText(text):\n",
    "        \n",
    "    text = text.replace('\\\\n','')\n",
    "    text = text.replace('\\\\','')\n",
    "    #text = text.replace('\\t', '')\n",
    "    #text = re.sub('\\[(.*?)\\]','',text) #removes [this one]\n",
    "    text = re.sub('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\\s',\n",
    "                ' __url__ ',text) #remove urls\n",
    "    #text = re.sub('\\'','',text)\n",
    "    #text = re.sub(r'\\d+', ' __number__ ', text) #replaces numbers\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T13:45:02.968529Z",
     "start_time": "2019-07-02T13:45:02.846598Z"
    }
   },
   "outputs": [],
   "source": [
    "c_text = \"So one thing we can try is take the same domain names and join as a user to accept a PICC lines on the back end if you want to initialize a conference.Right so that we can spend one and I can say hey this is the last nominee nominee is a number of last in chance really to create because it is you are allocating a room in Trinity.So one thing we can try is take the same domain names and join as a user to accept a PICC lines on the back end if you want to initialize a conference.Right so that we can spend one and I can say hey this is the last nominee nominee is a number of last in chance really to create because it is you are allocating a room in Trinity.\"\n",
    "c_fv = getBERTFeatures(model1, \"Were going to be talking about why is many people as possible should be learning sequel or stand structure through we language and it is the primary language responsible for managing data and data structures contained within relational database systems.\", attn_head_idx = -1)\n",
    "c_fv2 = getBERTFeatures(model1, \"a circle call everything we can to is going to be is easy that so just turn it and we are going it short two\", attn_head_idx = -1)\n",
    "#getSentMatchScore_wfeature(c_text, c_text, c_fv, c_fv2, nsp_dampening_factor = 0.7)\n",
    "1-cosine(c_fv,c_fv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:14:04.563709Z",
     "start_time": "2019-07-02T12:14:02.481308Z"
    }
   },
   "outputs": [],
   "source": [
    "model1 = BertForPreTraining_custom(config)\n",
    "model1.to(device)\n",
    "#state_dict_1 = torch.load('../data/bert_10epc_inc_se+etherdata_1e-6_sl40_bt64.bin')\n",
    "state_dict_1 = torch.load('../data/bert_10epc_se_1e-6_sl40.bin')\n",
    "model1.load_state_dict(state_dict_1)\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Meetings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:25:32.023162Z",
     "start_time": "2019-06-19T07:25:32.020296Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Timeline json meeting Input (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T13:31:06.700956Z",
     "start_time": "2019-07-01T13:31:06.667964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def parsemeeting(text):\n",
    "    with open(text, 'r') as f:\n",
    "        parsed_text = json.load(f)\n",
    "    return parsed_text\n",
    "text = parsemeeting('../data/timeline_result2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T13:31:25.076921Z",
     "start_time": "2019-07-01T13:31:25.074619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = ''\n",
    "temp = ' '\n",
    "for t in text['timeline']['transcriptSegments']:\n",
    "    temp = ' ' +  t['text']\n",
    "    texts = texts + temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T06:26:58.101349Z",
     "start_time": "2019-06-19T06:26:58.083952Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Get top 10 pims instead of all the segments.    \n",
    "# texts = ''\n",
    "# temp = ' '\n",
    "# for t in text['segments']:\n",
    "#     if t['transcriber'] == \"google_speech_api\":\n",
    "#         temp = ' ' +  t['originalText']\n",
    "#         print (\"---text-----  \\n\\nspoken by: \" + t['spokenBy'] + \"\\n\")\n",
    "#         print (temp, \"\\n\\n\\n\")\n",
    "#         texts = texts + temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load meeting as csv and get only the top10 pims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:14:07.523112Z",
     "start_time": "2019-07-02T12:14:07.515999Z"
    }
   },
   "outputs": [],
   "source": [
    "## load it as csv.\n",
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('../data/subtopic_010.csv', index_col=False, header=0);\n",
    "df = df.sort_values(by='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:51:17.576049Z",
     "start_time": "2019-07-02T12:51:17.561489Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "data= {}\n",
    "curl_test = {}\n",
    "segment_contents = {}\n",
    "texts = ''\n",
    "curl_test[\"contextId\"] =\"6baa3490-69d6-48fc-b5d4-3994e3e8fae0\"\n",
    "curl_test[\"mindId\"] = \"01daayheky5f4e02qvrjptftxv\"\n",
    "curl_test[\"segments\"] = []\n",
    "for index,segment in enumerate(df['value'].tolist()):\n",
    "    data = json.loads(segment)\n",
    "    if \"google\" not in data['transcriber'] and data['originalText']!='':\n",
    "        #print (str(index)+\"\\t\"+data['originalText'], end=\"\\n\\n\\n\")\n",
    "        curl_test['segments'].append(json.loads(segment))\n",
    "        texts+= (' ' + data['originalText'])\n",
    "        segment_contents[index]= [' '.join(tp.preprocess(data['originalText'], stop_words=False, remove_punct=False)),formatTime(data['startTime'], True), data['spokenBy']]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get Communities from meetings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T13:29:15.139295Z",
     "start_time": "2019-07-02T13:29:11.955060Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "\n",
    "mod_texts_unfiltered = tp.preprocess(texts, stop_words=False, remove_punct=False)\n",
    "mod_texts = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts_unfiltered):\n",
    "    if len(sent.split(' '))>250:\n",
    "        length = len(sent.split(' '))\n",
    "        split1 = ' '.join([i for i in sent.split(' ')[:round(length/2)]])\n",
    "        split2 = ' '.join([i for i in sent.split(' ')[round(length/2):]])\n",
    "        mod_texts.append(split1)\n",
    "        mod_texts.append(split2)\n",
    "        continue\n",
    "        #mod_texts.pop(index)\n",
    "#     if len(sent.split(' '))<=6:\n",
    "#         continue\n",
    "    mod_texts.append(sent)\n",
    "\n",
    "print(len(mod_texts))\n",
    "fv = {}\n",
    "\n",
    "for index, sent in enumerate(mod_texts):\n",
    "    fv[index] = getBERTFeatures(model1, sent, attn_head_idx=-1)\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T13:33:04.253505Z",
     "start_time": "2019-07-02T13:33:02.292748Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for index1, sent1 in enumerate(mod_texts):\n",
    "#     for index2, sent2 in enumerate(mod_texts):\n",
    "#         print (\"Sentence 1:\\n \" + sent1 + \"\\nSentence 2:\\n \" + sent2 + \"\\n Cosine score:\" + str(1-cosine(fv[index1],fv[index2])), end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:14:28.970540Z",
     "start_time": "2019-07-02T12:14:27.490696Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_graph(doc_list):\n",
    "    eng_graph = nx.Graph()\n",
    "    try:\n",
    "        eng_graph.add_nodes_from(range(len(doc_list)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return eng_graph\n",
    "\n",
    "\n",
    "tg = build_graph(mod_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:21:26.826376Z",
     "start_time": "2019-07-02T12:18:46.755650Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attn_head_idx = -1\n",
    "\n",
    "node_edge = []\n",
    "\n",
    "for index1, sent1 in enumerate(mod_texts):\n",
    "    print (index1)\n",
    "    for index2, sent2 in enumerate(mod_texts):\n",
    "        if index1!=index2 and index2>index1:\n",
    "            score = getSentMatchScore_wfeature(sent1, sent2,fv[index1],fv[index2])\n",
    "            #score = getSentMatchScore_wfeature_cosine(sent1, sent2,fv[index1],fv[index2])\n",
    "#             if score > 0.8:\n",
    "#                 #tg.add_edge(index1,index2,{'weight': score})\n",
    "#                 tg.add_edge(index1,index2)\n",
    "            tg.add_edge(index1,index2,weight=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## build graph using metric threshold. (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T12:40:46.762541Z",
     "start_time": "2019-06-20T12:40:43.048Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_community_graph(tg, mod_texts):\n",
    "    com_graph = nx.Graph()\n",
    "    for sent in list(tg.nodes()):\n",
    "        com_graph.add_node(sent)\n",
    "    for nodea in tg.nodes():\n",
    "        for nodeb in tg.nodes():\n",
    "            if nodea!=nodeb:\n",
    "                if tg.edges[nodea,nodeb]['weight'] > 0.90:\n",
    "                    com_graph.add_edge(nodea,nodeb)\n",
    "    return com_graph\n",
    "com_graph = build_community_graph(tg, mod_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build graph using statistical percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:36:50.324314Z",
     "start_time": "2019-07-02T12:36:50.255474Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import community\n",
    "for v in [0.15, 0.10, 0.05, 0.1]:\n",
    "    flag = False\n",
    "    for count in range(5):   \n",
    "        temp_nodes = []\n",
    "        for nodea,nodeb, weight in tg.edges.data():\n",
    "            temp_nodes.append((nodea,nodeb,weight['weight']))\n",
    "        temp_nodes = sorted(temp_nodes, key=lambda kv: kv[2], reverse=True)\n",
    "\n",
    "        temp_nodes = temp_nodes[:math.ceil(len(temp_nodes)*v)+1]\n",
    "\n",
    "        com_graph = nx.Graph()\n",
    "        for nodea,nodeb, weight in temp_nodes:\n",
    "            com_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "        partition = community.best_partition(com_graph)\n",
    "\n",
    "        mod = community.modularity(partition, com_graph)\n",
    "        print (\"The pruning value 'v' and modularity is: \", v, mod)\n",
    "        if mod > 0.3:\n",
    "            flag=True\n",
    "            print (\"Modularity reached 3. The pruning value 'v' is: \", v)\n",
    "            break\n",
    "        elif mod==0:\n",
    "            temp_nodes = []\n",
    "            print (\"Modularity reached 0. The pruning value 'v' is: \", 0.15)\n",
    "            for nodea,nodeb, weight in tg.edges.data():\n",
    "                temp_nodes.append((nodea,nodeb,weight['weight']))\n",
    "            temp_nodes = sorted(temp_nodes, key=lambda kv: kv[2], reverse=True)\n",
    "\n",
    "            temp_nodes = temp_nodes[:math.ceil(len(temp_nodes)*0.15)+1]\n",
    "\n",
    "            com_graph = nx.Graph()\n",
    "            for nodea,nodeb, weight in temp_nodes:\n",
    "                com_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "            partition = community.best_partition(com_graph)\n",
    "\n",
    "            mod = community.modularity(partition, com_graph)\n",
    "            flag=True\n",
    "            break\n",
    "    if flag:\n",
    "        print()\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:36:55.390999Z",
     "start_time": "2019-07-02T12:36:55.087421Z"
    }
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "#partition = community.best_partition(com_graph)\n",
    "\n",
    "values = [partition.get(node) for node in com_graph.nodes()]\n",
    "values=[partition.get(node) for node in com_graph.nodes()]\n",
    "plt.rcParams['figure.figsize']= [16, 10]\n",
    "measure_name = \"Louviin Algorithm Community Structure\"\n",
    "pos = nx.spring_layout(com_graph, k=0.2, iterations=20)\n",
    "nodes_plot=nx.draw_networkx_nodes(com_graph, pos, node_size=140, label=True, cmap=plt.get_cmap('magma', len(com_graph.nodes())/4),node_color=values, alpha=0.95)\n",
    "edges_plot=nx.draw_networkx_edges(com_graph, pos, edge_color='r', alpha=0.1)\n",
    "plt.title(measure_name, fontsize=22, fontname='Arial')\n",
    "plt.colorbar(nodes_plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:37:00.031609Z",
     "start_time": "2019-07-02T12:37:00.028192Z"
    }
   },
   "outputs": [],
   "source": [
    "community.modularity(partition, com_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:37:15.846844Z",
     "start_time": "2019-07-02T12:37:15.842147Z"
    }
   },
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:37:20.514646Z",
     "start_time": "2019-07-02T12:37:20.509458Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        print (mod_texts[word])\n",
    "        current=cluster\n",
    "    else:\n",
    "        print (mod_texts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:37:25.146013Z",
     "start_time": "2019-07-02T12:37:25.143154Z"
    }
   },
   "outputs": [],
   "source": [
    "com_graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:37:30.156081Z",
     "start_time": "2019-07-02T12:37:30.152948Z"
    }
   },
   "outputs": [],
   "source": [
    "tg.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:30:07.027802Z",
     "start_time": "2019-06-19T07:30:07.022745Z"
    }
   },
   "source": [
    "# Redefine the resulatant communities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:37:45.357709Z",
     "start_time": "2019-07-02T12:37:45.354757Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters = []\n",
    "temp = []\n",
    "prev_com = 0\n",
    "for word,cluster in partition:\n",
    "    if prev_com==cluster:\n",
    "        temp.append(word)\n",
    "        #print (temp)\n",
    "    else:\n",
    "        clusters.append(temp)\n",
    "        temp = []\n",
    "        prev_com = cluster\n",
    "        temp.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:37:50.152927Z",
     "start_time": "2019-07-02T12:37:50.149195Z"
    }
   },
   "outputs": [],
   "source": [
    "timerange = []\n",
    "temp = []\n",
    "for index, cluster in enumerate(clusters):\n",
    "    temp= []\n",
    "    for sent in cluster:\n",
    "        temp2 = [(sentence,start_time,user) for sentence,start_time,user in segment_contents.values() if mod_texts[sent] in sentence]\n",
    "        if len(temp2)!=0:\n",
    "            temp.append(temp2[0])\n",
    "    if len(temp)!=0:\n",
    "        temp = sorted(temp,key=lambda kv: kv[1])\n",
    "        timerange.append(temp)\n",
    "    else:\n",
    "        print (cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:37:54.939694Z",
     "start_time": "2019-07-02T12:37:54.932685Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timerange_detailed = []\n",
    "temp = []\n",
    "flag = False\n",
    "pims = {}\n",
    "index_pim = 0\n",
    "index_segment = 0\n",
    "for index,com in enumerate(timerange):\n",
    "    temp = []\n",
    "    flag = False\n",
    "    #print (\"-----community-----\", index)\n",
    "    for (index1,(sent1,time1,user1)), (index2,(sent2,time2,user2)) in zip(enumerate(com[0:]),enumerate(com[1:])):\n",
    "        if sent1!=sent2:\n",
    "            #print (time1, time2, (time2-time1).seconds)\n",
    "            if ((time2-time1).seconds<=240):\n",
    "                if not flag:\n",
    "                    pims[index_pim] = {'segment'+str(index_segment):[sent1,time1,user1]}\n",
    "                    index_segment+=1\n",
    "                    temp.append((sent1,time1,user1))\n",
    "                #else:\n",
    "                    #print ('removing',time1, time2)\n",
    "                    #temp.pop()\n",
    "                pims[index_pim]['segment'+str(index_segment)] = [sent2,time2,user2]\n",
    "                index_segment+=1\n",
    "                temp.append((sent2,time2,user2))\n",
    "                flag=True\n",
    "            else:\n",
    "                #print (time2, time1)\n",
    "                if flag==True:\n",
    "                    index_pim+=1\n",
    "                    index_segment=0\n",
    "                flag=False\n",
    "    if flag==True:\n",
    "        index_pim+=1\n",
    "        index_segment=0\n",
    "    #print (\"-----timeRange-----\\n\", [j for i,j,k in temp])\n",
    "    timerange_detailed.append(temp)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Redefing the resultant communities using different approach (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:50:15.095325Z",
     "start_time": "2019-06-25T13:50:15.091774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "timerange = []\n",
    "temp = []\n",
    "for index, cluster in enumerate(clusters):\n",
    "    temp= []\n",
    "    for sent in cluster:\n",
    "        temp2 = [(sentence,start_time,user, [index]) for sentence,start_time,user in segment_contents.values() if mod_texts[sent] in sentence]\n",
    "        if len(temp2)!=0:\n",
    "            temp.append(temp2[0])\n",
    "    if len(temp)!=0:\n",
    "        temp = sorted(temp,key=lambda kv: kv[1])\n",
    "        timerange.append(temp)\n",
    "    else:\n",
    "        print (cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:50:22.147730Z",
     "start_time": "2019-06-25T13:50:22.144636Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "flattened_timerange = sorted([sent for com in timerange for sent in com], key= lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:50:29.301512Z",
     "start_time": "2019-06-25T13:50:29.290225Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flattened_timerange[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:58:20.906464Z",
     "start_time": "2019-06-25T13:58:20.894419Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ft_re = []\n",
    "# for (index1,sent1), (index2, sent2) in zip(enumerate(flattened_timerange),enumerate(flattened_timerange)):\n",
    "#     if  sent1[1]==sent2[1]:\n",
    "#         if len(ft_re)!=0 and ft_re[-1][1] == sent1[1]:\n",
    "#             temp = []\n",
    "#             temp.extend([[ft_re[-1][-1], sent1[-1], sent2[-1]]])\n",
    "#             print (temp)\n",
    "#             tot_com = list(set(temp))\n",
    "#             ft_re.pop()\n",
    "#             ft_re.append((sent1[0],sent1[1],sent1[2],tot_com))\n",
    "#         else:\n",
    "#             tot_com = list(set([sent1[-1], sent2[-1]]))\n",
    "#             ft_re.append((sent1[0],sent1[1],sent1[2],tot_com))\n",
    "#     else:\n",
    "#         if ft_re!=[] and ft_re[-1][1] == sent1[1]:\n",
    "#             tot_com = list(set([ft_re[-1][-1], sent1[-1]]))\n",
    "#             ft_re.pop()\n",
    "#             ft_re.append((sent1[0],sent1[1],sent1[2],tot_com))\n",
    "#         else:\n",
    "#             ft_re.append(sent1)\n",
    "            \n",
    "ft_re = []\n",
    "tot_com = []\n",
    "flag=False\n",
    "for index1, sent1 in enumerate(flattened_timerange):\n",
    "    if flag==True:\n",
    "        if index1!=j:\n",
    "            continue\n",
    "        else:\n",
    "            flag=False\n",
    "    if flag==False:\n",
    "        for index2, sent2 in enumerate(flattened_timerange):\n",
    "            if index2>index1:\n",
    "                if sent1[1]==sent2[1]:\n",
    "                    if flag==False:\n",
    "                        tot_com.extend([sent1[-1][-1],sent2[-1][-1]])\n",
    "                        flag=True\n",
    "                    else:\n",
    "                        tot_com.extend([sent2[-1][-1]])\n",
    "                        if index2==(len(flattened_timerange)-1):\n",
    "                                tot_com = max(set(tot_com), key = tot_com.count) \n",
    "                                ft_re.append((sent1[0],sent1[1],sent1[2],tot_com))\n",
    "                else:\n",
    "                    if flag==True:\n",
    "                        tot_com = max(set(tot_com), key = tot_com.count)\n",
    "                        ft_re.append((sent1[0],sent1[1],sent1[2],tot_com))\n",
    "                        j = index2\n",
    "                    else:\n",
    "                        ft_re.append(sent1)\n",
    "                        flag = False\n",
    "                    tot_com = []\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:58:28.121034Z",
     "start_time": "2019-06-25T13:58:28.116334Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ft_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alter communities based on time range and get Keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:38:00.003084Z",
     "start_time": "2019-07-02T12:37:59.604833Z"
    }
   },
   "outputs": [],
   "source": [
    "from graphrank.graphrank import GraphRank\n",
    "from graphrank.utils import GraphUtils, TextPreprocess\n",
    "\n",
    "keyphrase_text = \"\"\n",
    "keyphrases_list = []\n",
    "keyphrases = []\n",
    "for index_pim in pims.keys():\n",
    "    keyphrase_text = \"\"\n",
    "    for seg in pims[index_pim]:\n",
    "        if seg != \"keyphrase\":\n",
    "            keyphrase_text += (' ' + pims[index_pim][seg][0])\n",
    "    gr = GraphRank()\n",
    "    tp = TextPreprocess()\n",
    "    utils = GraphUtils()\n",
    "\n",
    "    original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(keyphrase_text, filter_by_pos=True, pos_filter=['NOUN', 'PROPN', 'ADJ', 'FW'], stop_words=False)\n",
    "\n",
    "    word_graph = gr.build_word_graph(filtered_pos_tuple, original_tokens=original_tokens, window=4, reset_graph_context=True, preserve_common_words=False)\n",
    "\n",
    "    keyphrases = gr.get_keyphrases(word_graph, normalize_nodes='degree')\n",
    "    pims[index_pim]['keyphrase'] = [phrases for phrases, score in keyphrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:38:04.691004Z",
     "start_time": "2019-07-02T12:38:04.686903Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "yet_to_combine = []\n",
    "need_to_remove = []\n",
    "for index1,i in enumerate(pims.keys()):\n",
    "    for index2,j in enumerate(pims.keys()):\n",
    "        if index1!=index2:\n",
    "            if pims[i]['segment0'][1] >= pims[j]['segment0'][1] and pims[i]['segment0'][1] <= pims[j]['segment'+str(len(pims[j].values())-2)][1]:\n",
    "                if (j,i) not in yet_to_combine and i not in need_to_remove and j not in need_to_remove:\n",
    "                    yet_to_combine.append((i,j))\n",
    "                    need_to_remove.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:38:09.367112Z",
     "start_time": "2019-07-02T12:38:09.363599Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,j in yet_to_combine:\n",
    "    for k in pims[i]:\n",
    "        if k!=\"keyphrase\":\n",
    "            if pims[i][k] not in pims[j].values():\n",
    "                pims[j]['segment'+str(len(pims[j].values())-1)] = pims[i][k]\n",
    "                #print (pims[i][k])\n",
    "                continue\n",
    "        else:\n",
    "            extra_keyphrase = list(set(pims[i]['keyphrase'] + pims[j]['keyphrase']))\n",
    "            pims[j]['keyphrase']=extra_keyphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:38:14.090365Z",
     "start_time": "2019-07-02T12:38:14.088232Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in need_to_remove:\n",
    "    pims.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T12:38:18.783760Z",
     "start_time": "2019-07-02T12:38:18.776441Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in pims.keys():\n",
    "    print (\"\\n\\n\\nPIMs \", i)\n",
    "    print (\"\\n\\nDiscussion:\\n\\n \")\n",
    "    for seg in pims[i]:\n",
    "        if seg!=\"keyphrase\":\n",
    "            print (pims[i][seg][1], \" \", pims[i][seg][0],\"\\n\")\n",
    "    print (\"Keyphrases:\\n\\n \", pims[i]['keyphrase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## testing topic modelling with LDA (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:03:40.394625Z",
     "start_time": "2019-06-12T16:03:40.375521Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in new_text]\n",
    "#doc_clean = [clean(new_text[1]).split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:03:52.012109Z",
     "start_time": "2019-06-12T16:03:51.997576Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:05:59.081754Z",
     "start_time": "2019-06-12T16:05:59.077554Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:06:26.310170Z",
     "start_time": "2019-06-12T16:06:25.966545Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:07:13.872792Z",
     "start_time": "2019-06-12T16:07:13.864096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Testing keyphrase for the timerange (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:20:16.374772Z",
     "start_time": "2019-06-28T17:20:16.366362Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pims = {\n",
    "\t\"0\": {\n",
    "\t\t\"segment0\": [\"Like my per month to date most likely as we excuse into which we can that be better more consistent but General aw strategy what they say is have one Lambda the internet coming HTML segment notify is attached a general hook for the data uploaded the Lambda function who associate to the S3 bucket and have that pan out all other language like be a dispatcher. Yeah, thats.\", \"2019-06-28T06:16:58Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"ebe8604a02c84952bb7ea9dfccd6c1df\"]\n",
    "\t},\n",
    "\t\"1\": {\n",
    "\t\t\"segment0\": [\"From by will sync up offline first trusting watertight preview will move manually create a bucket in staging environment and try to use it for testing but what I am expecting is not be a big change. What we what we can do is we can we can remove that CD and table itself from the database because the student table and other other things are actually only used for you know, this sets these things so we give you can remove them. So we will now will have a less overhead even in the future. So because after this change we are we are never going to use the stimuli. and.\", \"2019-06-28T06:11:11Z\", \"b6db6d45-1a71-41e7-bc16-ae330f655c38\", \"0ef8d453e7bb41d3bd11153827df076a\"]\n",
    "\t},\n",
    "\t\"2\": {\n",
    "\t\t\"segment0\": [\n",
    "\t\t\t[\"All right create an issue. Well track it should be a small thing even in the back end to handle. Yeah, I will see if I can look into it today.\"], \"2019-06-28T05:35:58Z\", \"b6db6d45-1a71-41e7-bc16-ae330f655c38\", \"f2d6bb774d5f4649b2e4f15bf07052ad\"\n",
    "\t\t],\n",
    "\t\t\"segment1\": [\n",
    "\t\t\t[\"Nothing was done. Headmistress, I would wear black in the public Channel. We need to prompt Channel Minds. Also, I will check with Rashon thundergun. It was another.\"], \"2019-06-28T05:39:19Z\", \"b6db6d45-1a71-41e7-bc16-ae330f655c38\", \"418c719a0f5e4888b242df011e818aa3\"\n",
    "\t\t],\n",
    "\t\t\"segment2\": [\n",
    "\t\t\t[\"And updated methods for that which I take do too. Okay. Yeah, I will look into that.\"], \"2019-06-28T05:39:35Z\", \"d65899ed-e47e-4611-8c2c-59b154ff6a3f\", \"136b9ea1ccaf461facc5e7953a520ff6\"\n",
    "\t\t]\n",
    "\t},\n",
    "\t\"3\": {\n",
    "\t\t\"segment0\": [\n",
    "\t\t\t[\"Its similar than better to go with S3. If GitHub is not giving you just artifact reading scope or something which release artifact reading Scopes even then it can be weird because if we keep tagging or create a good stuff that are not Jesus files are also generated. We need to prevent them from able to downloading do not also even if they do not we have to protect the code right should not be vulnerability this this take that what is the effort to from Reading from your front door if Jacob does give you anything fixed capacity?. .\"], \"2019-06-28T05:46:56Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"8c4e8716db754f82b1ed36c449fa15a3\"\n",
    "\t\t],\n",
    "\t\t\"segment1\": [\n",
    "\t\t\t[\"Another markers with calibration issue. I will also take a look on the back end. I know that we have to trim the edges to remove the flashin but I do think that if someone puts a empty space between two sentences that can cause an issue. I will just go check. Why is that occurring and see if it is something easy to pick up and then the back end also?.\"], \"2019-06-28T05:50:04Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"3c74a71652394d65abed891a412c008d\"\n",
    "\t\t]\n",
    "\t},\n",
    "\t\"4\": {\n",
    "\t\t\"segment0\": [\n",
    "\t\t\t[\"You things on whether can we go eat on to Lambda or whether she should whether it should be based on the current key phrase. Maybe we will just take it off way, or I will talk to Sasha Hank and understand and get those details. We did have some plans to move it along.\"], \"2019-06-28T05:57:22Z\", \"92b4588f-74cc-42f0-bf6d-eec0b6198ade\", \"6b474e34b42a4514b7689a378bc47c66\"\n",
    "\t\t],\n",
    "\t\t\"segment1\": [\n",
    "\t\t\t[\"Between making calls on Tims and waiting on the one that comes last. Yeah that still happen because we will have to wait on the consensus of the what we need to do again is somewhat more on the cause they custom server is not doing anything and then it out into my you are still paying for the execution of the Pim. Lambda looks right at is one thing you can reduce but also the fact that we are right now waiting on Transit segment to complete I think within being the change we will need to wait on transcript second segment to be analyzed. Those are the events that trigger are some regeneration. So if it makes it and because that tramp is segment competed in already in the event of are acting on making this switch to the next most relevant is not a big problem. So it is more even driven so we do not need to wait on okay. Okay. Yeah, and then on the.\"], \"2019-06-28T06:00:48Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"f5c2855a681c40c8b5a77a85c5c0fd6d\"\n",
    "\t\t],\n",
    "\t\t\"segment2\": [\n",
    "\t\t\t[\"Loved it. Thats fine. For now. I think you do. I think once you go to production, we need to figure out a way eventually to not lock sensitive information like transcript segment data Etc. And that will cause some you know issues still doing that. So but ceasing to your fine, too. Yeah.\"], \"2019-06-28T06:02:50Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"f56de51245ce4fa69c6628e5dc066369\"\n",
    "\t\t],\n",
    "\t\t\"segment3\": [\n",
    "\t\t\t[\"Make a contained environment. You cannot specify the requirements separately and then all the executable file separately so that you can put the files as layers. I was just trying to do that, but I could not find it in the process of doing a trigger. If so, why?.\"], \"2019-06-28T06:03:45Z\", \"f6b27e4a-ad4f-42b3-ac18-e09ec396685b\", \"4fddeb333fe745cdac8ee41d5785d0ba\"\n",
    "\t\t],\n",
    "\t\t\"segment4\": [\n",
    "\t\t\t[\"I think when cat had reduced it to Fighters, I think Sicilian fighter he moved to by Torchlight and side by and all the mines and this one. So overall I think the thing got to do but we ended up using layers so we might be can end up adding both the layers for five dots by touch light and maybe scifi known by all the psychic number like it has layers and learning anything network accessible routes like that, but if sound 50mb.\"], \"2019-06-28T06:06:36Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"cc29605c0446480f8ace87c4d6593d8a\"\n",
    "\t\t],\n",
    "\t\t\"segment5\": [\n",
    "\t\t\t[\"So currently was just working on the refactoring of bug book printing that is done basically it is in like the record reviews review comments have to just address it so apart from the doe those comments look straight forward or should not be a bigger big thing is basically so the other part is the pims part the the pins which are not seen in the mix does not constitute the last last few segments the meeting right? So that issue if started working on it like probably a bit Monday should be able to send the send it for review that code. Okay. Yeah that I maybe I will have to ask like more technical things. I will have to ask like if there is anything which I am missing missing I will call you again.\"], \"2019-06-28T06:08:07Z\", \"6e8408ce-1072-4209-9e82-945701c7b86c\", \"439f3d6209184e19a7de54c59a8c4533\"\n",
    "\t\t],\n",
    "\t\t\"segment6\": [\n",
    "\t\t\t[\"Make a contained environment. You cannot specify the requirements separately and then all the executable file separately so that you can put the files as layers. I was just trying to do that, but I could not find it in the process of doing a trigger. If so, why?.\"], \"2019-06-28T06:03:45Z\", \"f6b27e4a-ad4f-42b3-ac18-e09ec396685b\", \"4fddeb333fe745cdac8ee41d5785d0ba\"\n",
    "\t\t]\n",
    "\t},\n",
    "\t\"5\": {\n",
    "\t\t\"segment0\": [\"But we we have done it. We have done it only from our office and I am not confused coming from that one will because people in the calls are like variables XnumberX  scholar two persons called Waterloo. We visited a real scenario kind of thing according a minimum XnumberX people per call because we just need to load test it and because it is normal generous mood worth but under the cave under the load, how does how its behaving that we need to check? Yeah.\", \"2019-06-28T06:21:56Z\", \"b6db6d45-1a71-41e7-bc16-ae330f655c38\", \"b573665bbeae49bdb26020751122dfc1\"]\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:24:40.447940Z",
     "start_time": "2019-06-28T17:24:39.970584Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from graphrank.graphrank import GraphRank\n",
    "from graphrank.utils import GraphUtils, TextPreprocess\n",
    "\n",
    "keyphrase_text = \"\"\n",
    "keyphrases_list = []\n",
    "keyphrases = []\n",
    "for index_pim in pims.keys():\n",
    "    keyphrase_text = \"\"\n",
    "    for seg in pims[index_pim]:\n",
    "        if seg != \"keyphrase\":\n",
    "            keyphrase_text += (' ' + ''.join([i for i in pims[index_pim][seg][0]]))\n",
    "            print (keyphrase_text)\n",
    "    gr = GraphRank()\n",
    "    tp = TextPreprocess()\n",
    "    utils = GraphUtils()\n",
    "\n",
    "    original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(keyphrase_text, filter_by_pos=True, pos_filter=['NOUN', 'PROPN', 'ADJ', 'FW'], stop_words=False)\n",
    "\n",
    "    word_graph = gr.build_word_graph(filtered_pos_tuple, original_tokens=original_tokens, window=4, reset_graph_context=True, preserve_common_words=False)\n",
    "\n",
    "    keyphrases = gr.get_keyphrases(word_graph, normalize_nodes='degree')\n",
    "    pims[index_pim]['keyphrase'] = [phrases for phrases, score in keyphrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:24:52.412467Z",
     "start_time": "2019-06-28T17:24:52.407043Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## keyphrases comparision with word2vec (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:25:36.521105Z",
     "start_time": "2019-07-01T16:25:36.503427Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/meetings_slack_embeddings.pkl\",\"rb\") as f:\n",
    "    emb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:44:03.316368Z",
     "start_time": "2019-07-01T16:44:03.175132Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nodea = 'sql'\n",
    "nodea_fv = emb[nodea]\n",
    "closest_match =[]\n",
    "for nodeb in emb.keys():\n",
    "    if nodeb!=nodea:\n",
    "        diff = 1 - cosine(emb[nodeb], nodea_fv)\n",
    "        closest_match.append((nodeb, diff))\n",
    "closest_match_sorted = sorted(closest_match, key= lambda kv:kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:44:32.718468Z",
     "start_time": "2019-07-01T16:44:32.715059Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "closest_match_sorted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:43:25.113499Z",
     "start_time": "2019-07-01T17:43:25.037486Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('../data/query_resultsync2.csv', index_col=False, header=0);\n",
    "\n",
    "import json\n",
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "data= {}\n",
    "\n",
    "segment_contents = {}\n",
    "texts = ''\n",
    "\n",
    "for index,segment in enumerate(df['value'].tolist()):\n",
    "    data = json.loads(segment)\n",
    "    if \"google\" not in data['transcriber'] and data['originalText']!='':\n",
    "    #if \"google\" not in data['transcriber']:\n",
    "        texts+= (' ' + data['originalText'])\n",
    "        segment_contents[index]= [' '.join(tp.preprocess(data['originalText'], stop_words=False, remove_punct=False)),formatTime(data['startTime'], True), data['spokenBy']]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:46:37.246275Z",
     "start_time": "2019-07-01T17:46:37.232827Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:43:55.744377Z",
     "start_time": "2019-07-01T17:43:55.326468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from graphrank.graphrank import GraphRank\n",
    "from graphrank.utils import GraphUtils, TextPreprocess\n",
    "\n",
    "gr = GraphRank()\n",
    "tp = TextPreprocess()\n",
    "utils = GraphUtils()\n",
    "\n",
    "original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(keyphrase_text, filter_by_pos=True, pos_filter=['NOUN', 'PROPN', 'ADJ', 'FW'], stop_words=False)\n",
    "\n",
    "word_graph = gr.build_word_graph(filtered_pos_tuple, original_tokens=original_tokens, window=4, reset_graph_context=True, preserve_common_words=False)\n",
    "\n",
    "#keyphrases = gr.get_keyphrases(word_graph, normalize_nodes='degree')\n",
    "#pims[index_pim]['keyphrase'] = [phrases for phrases, score in keyphrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:44:26.282278Z",
     "start_time": "2019-07-01T17:44:26.183078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "com_graph = GraphRank()\n",
    "nodes_list = []\n",
    "for indexa, nodea in enumerate(list(word_graph.nodes())):\n",
    "    for indexb, nodeb in enumerate(list(word_graph.nodes())):\n",
    "        if indexb>indexa:\n",
    "            if nodea not in emb.keys() or nodeb not in emb.keys():\n",
    "                #nodes_list.append((nodea,nodeb,0.5))            \n",
    "                continue\n",
    "            nodes_list.append((nodea,nodeb,cosine(emb[nodea],emb[nodeb])))\n",
    "#print (*nodes_list, sep=\"\\n\")\n",
    "nodes_list_sorted = sorted(nodes_list, key=lambda kv: kv[2], reverse=True)\n",
    "\n",
    "nodes_list = nodes_list_sorted[:math.ceil(len(nodes_list_sorted)*0.05)+1]\n",
    "\n",
    "com2_graph = nx.Graph()\n",
    "for nodea,nodeb, weight in nodes_list:\n",
    "    com2_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "partition = community.best_partition(com2_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:44:56.761046Z",
     "start_time": "2019-07-01T17:44:56.757951Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "community.modularity(partition, com2_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:45:26.415917Z",
     "start_time": "2019-07-01T17:45:26.413695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:45:56.160268Z",
     "start_time": "2019-07-01T17:45:56.156494Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        print (word)\n",
    "        current=cluster\n",
    "    else:\n",
    "        print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "community",
   "language": "python",
   "name": "community"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
