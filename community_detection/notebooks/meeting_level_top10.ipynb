{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:34:33.229897Z",
     "start_time": "2019-07-31T09:34:33.222106Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertPreTrainingHeads\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "device = 'cpu'\n",
    "import sys\n",
    "import os\n",
    "import iso8601\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:34:35.816525Z",
     "start_time": "2019-07-31T09:34:33.955547Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertForPreTraining_custom(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForPreTraining_custom, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n",
    "        output_all_encoded_layers=True\n",
    "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                                   output_all_encoded_layers=output_all_encoded_layers)\n",
    "        if output_all_encoded_layers:\n",
    "            sequence_output_pred = sequence_output[-1]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output_pred, pooled_output)\n",
    "        return prediction_scores, seq_relationship_score, sequence_output, pooled_output \n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_json_file('../data/bert_config.json')\n",
    "bert_model = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:34:35.855700Z",
     "start_time": "2019-07-31T09:34:35.826125Z"
    }
   },
   "outputs": [],
   "source": [
    "def getNSPScore(sample_text):\n",
    "    \n",
    "    m = torch.nn.Softmax()\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(sample_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [0]*tokenized_text.index('[SEP]')+[1]*(len(tokenized_text)-tokenized_text.index('[SEP]'))\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    pred_score, seq_rel, seq_out, pool_out = model1(tokens_tensor, segments_tensors)\n",
    "    return m(seq_rel).detach().numpy()[0][0] #returns probability of being next sentence\n",
    "\n",
    "def getSentMatchScore(sent1, sent2, nsp_dampening_factor = 0.7):\n",
    "    \n",
    "    sent1_feats = getBERTFeatures(model1, sent1, attn_head_idx)\n",
    "    sent2_feats = getBERTFeatures(model1, sent2, attn_head_idx)\n",
    "    \n",
    "    cosine_distance = 1- cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = np.mean([nsp_score_1,nsp_score_2])*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    #print (\"nsp score -> \" + str(nsp_score))\n",
    "    #print (\"cosine score -> \" + str(cosine_distance))\n",
    "    return score\n",
    "    \n",
    "\n",
    "def getSentMatchScore_wfeature(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    #nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    #nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    \n",
    "    nsp_score = nsp_score_1 * nsp_dampening_factor\n",
    "    #nsp_score = nsp_score_1*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    return score\n",
    "\n",
    "def getSentMatchScore_wfeature_cosine(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    return cosine_distance\n",
    "    \n",
    "\n",
    "def getBERTFeatures(model, text, attn_head_idx = -1): #attn_head_idx - index o[]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    if len(tokenized_text)>200:\n",
    "        tokenized_text = tokenized_text[0:200]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    _, _, seq_out, pool_out = model(tokens_tensor)\n",
    "    seq_out = list(getPooledFeatures(seq_out[attn_head_idx]).T)\n",
    "    #pool_out = list(pool_out.detach().numpy().T)\n",
    "    \n",
    "    return seq_out\n",
    "\n",
    "def getPooledFeatures(np_array):\n",
    "    np_array = np_array.reshape(np_array.shape[1],np_array.shape[2]).detach().numpy()\n",
    "    np_array_mp = np.mean(np_array, axis=0).reshape(1, -1)\n",
    "    return np_array_mp\n",
    "\n",
    "def replaceContractions(text):\n",
    "    #text = text.lower()\n",
    "    c_filt_text = ''\n",
    "    for word in text.split(' '):\n",
    "        if word in contractions:\n",
    "            c_filt_text = c_filt_text+' '+contractions[word]\n",
    "        else:\n",
    "            c_filt_text = c_filt_text+' '+word\n",
    "    return c_filt_text\n",
    "\n",
    "def cleanText(text):\n",
    "        \n",
    "    text = text.replace('\\\\n','')\n",
    "    text = text.replace('\\\\','')\n",
    "    #text = text.replace('\\t', '')\n",
    "    #text = re.sub('\\[(.*?)\\]','',text) #removes [this one]\n",
    "    text = re.sub('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\\s',\n",
    "                ' __url__ ',text) #remove urls\n",
    "    #text = re.sub('\\'','',text)\n",
    "    #text = re.sub(r'\\d+', ' __number__ ', text) #replaces numbers\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T13:13:43.528532Z",
     "start_time": "2019-07-29T13:13:43.337983Z"
    }
   },
   "outputs": [],
   "source": [
    "# c_text = \"So one thing we can try is take the same domain names and join as a user to accept a PICC lines on the back end if you want to initialize a conference.Right so that we can spend one and I can say hey this is the last nominee nominee is a number of last in chance really to create because it is you are allocating a room in Trinity.So one thing we can try is take the same domain names and join as a user to accept a PICC lines on the back end if you want to initialize a conference.Right so that we can spend one and I can say hey this is the last nominee nominee is a number of last in chance really to create because it is you are allocating a room in Trinity.\"\n",
    "# c_fv = getBERTFeatures(model1, \"Were going to be talking about why is many people as possible should be learning sequel or stand structure through we language and it is the primary language     responsible for managing data and data structures contained within relational database systems.\", attn_head_idx = -1)\n",
    "# c_fv2 = getBERTFeatures(model1, \"a circle call everything we can to is going to be is easy that so just turn it and we are going it short two\", attn_head_idx = -1)\n",
    "# #getSentMatchScore_wfeature(c_text, c_text, c_fv, c_fv2, nsp_dampening_factor = 0.7)\n",
    "# 1-cosine(c_fv,c_fv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:34:59.493175Z",
     "start_time": "2019-07-31T09:34:37.419380Z"
    }
   },
   "outputs": [],
   "source": [
    "model1 = BertForPreTraining_custom(config)\n",
    "model1.to(device)\n",
    "#state_dict_1 = torch.load('../data/bert_10epc_inc_se+etherdata_1e-6_sl40_bt64.bin')\n",
    "state_dict_1 = torch.load('../data/bert_10epc_se_1e-6_sl40.bin')\n",
    "model1.load_state_dict(state_dict_1)\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Topic extraction with keyphrase approach as cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T07:31:36.327190Z",
     "start_time": "2019-07-30T07:31:36.311922Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getKPBasedSimilarity(text1,text2,model=None,tup1=None,tup2=None,layer = -1):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates similarity between two sentences - based on noun phrases, verb<>noun phrases. Calculates pair-wise cosine similarity between candidate phrases\n",
    "    returns mean of top-3 similar phrases as similarity\n",
    "\n",
    "    Args:\n",
    "    text1: required \n",
    "    text2: required\n",
    "    model optional, required if tup1 and tup2 are provided\n",
    "    tup1: bert features of `sent1` as returned by getBERTFeatures()\n",
    "    tup2: bert features of `sent2` as returned by getBERTFeatures()\n",
    "    layer: optional, BERT layer to extract features from\n",
    "\n",
    "    Output:\n",
    "    returns key-phrase based cosine similarity between the sentences\n",
    "    \"\"\"\n",
    "\n",
    "    #text1 = customPreprocess(text1)\n",
    "    #text2 = customPreprocess(text2)\n",
    "\n",
    "    if (tup1 is not None) and (tup2 is not None):\n",
    "        token_feats_1,final_feats1,text1_bert_tokenized = tup1\n",
    "        token_feats_2,final_feats2,text2_bert_tokenized = tup2\n",
    "    else:\n",
    "        if model is not None:\n",
    "            token_feats_1,final_feats1,text1_bert_tokenized = getBERTFeatures(model, text1, attn_head_idx=layer)\n",
    "            token_feats_2,final_feats2,text2_bert_tokenized = getBERTFeatures(model, text2, attn_head_idx=layer)\n",
    "        else:\n",
    "            assert model is not None\n",
    "\n",
    "    text1_sent_tokens = tokenize(text1)\n",
    "    text2_sent_tokens = tokenize(text2)\n",
    "\n",
    "    merged_feats_text1 = getWordFeatsFromBertTokenFeats(text1_sent_tokens,text1_bert_tokenized,token_feats_1)\n",
    "    merged_feats_text2 = getWordFeatsFromBertTokenFeats(text2_sent_tokens,text2_bert_tokenized,token_feats_2)\n",
    "\n",
    "    #get candidate key-phrases for both sentences\n",
    "    kps_sent1,kps_loc_sent1 = getCandidatePhrases(text1)\n",
    "    kps_sent2,kps_loc_sent2 = getCandidatePhrases(text2)\n",
    "    \n",
    "    sent1_kp_feats = getKeyPhraseFeatures(kps_sent1,kps_loc_sent1,merged_feats_text1,text1_sent_tokens)\n",
    "    sent2_kp_feats = getKeyPhraseFeatures(kps_sent2,kps_loc_sent2,merged_feats_text2,text2_sent_tokens)\n",
    "\n",
    "    sim_list = []\n",
    "    for sent1_kp, feats1 in zip(kps_sent1,sent1_kp_feats):\n",
    "        for sent2_kp, feats2 in zip(kps_sent2,sent2_kp_feats):\n",
    "            if len(sent1_kp.split(' '))+len(sent2_kp.split(' '))==2:\n",
    "                if len(sent1_kp.split(' ')[0])<4 or len(sent2_kp.split(' ')[0])<4:\n",
    "                    continue\n",
    "                curr_sim = 1-spatial.distance.cosine(feats1,feats2)\n",
    "            else:\n",
    "                if len(sent1_kp.split(' '))==1 or len(sent2_kp.split(' '))==1:\n",
    "                    continue\n",
    "                else:\n",
    "                    curr_sim = 1-spatial.distance.cosine(feats1,feats2)\n",
    "            sim_list.append(curr_sim)\n",
    "\n",
    "    if len(sim_list)>3:\n",
    "        sim_list = sim_list[0:3]\n",
    "    if len(sim_list)>0:\n",
    "        mean_dist = np.mean(sim_list)\n",
    "    else:\n",
    "        mean_dist = 0.0\n",
    "        \n",
    "    return mean_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:04:27.534090Z",
     "start_time": "2019-07-30T09:04:27.451223Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## utils for the above\n",
    "\n",
    "import unicodedata\n",
    "import itertools\n",
    "from scipy import spatial\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from itertools import chain\n",
    "\n",
    "def lambda_unpack(f):\n",
    "    return lambda args: f(*args)\n",
    "\n",
    "def tokenize(text, never_split = [], do_lower_case = True):\n",
    "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "    text = _clean_text(text)\n",
    "    orig_tokens = whitespace_tokenize(text)\n",
    "    split_tokens = []\n",
    "    for token in orig_tokens:\n",
    "        if do_lower_case and token not in never_split:\n",
    "            token = token.lower()\n",
    "            token = _run_strip_accents(token)\n",
    "        split_tokens.extend(_run_split_on_punc(token))\n",
    "\n",
    "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "    return output_tokens\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _run_split_on_punc(text):\n",
    "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "    chars = list(text)\n",
    "    i = 0\n",
    "    start_new_word = True\n",
    "    output = []\n",
    "    while i < len(chars):\n",
    "        char = chars[i]\n",
    "        if _is_punctuation(char):\n",
    "            output.append([char])\n",
    "            start_new_word = True\n",
    "        else:\n",
    "            if start_new_word:\n",
    "                output.append([])\n",
    "            start_new_word = False\n",
    "            output[-1].append(char)\n",
    "        i += 1\n",
    "\n",
    "    return [\"\".join(x) for x in output]\n",
    "\n",
    "def _clean_text(text):\n",
    "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "    output = []\n",
    "    for char in text:\n",
    "        cp = ord(char)\n",
    "        if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "            continue\n",
    "        if _is_whitespace(char):\n",
    "            output.append(\" \")\n",
    "        else:\n",
    "            output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "def _is_control(char):\n",
    "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _is_whitespace(char):\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def _run_strip_accents(text):\n",
    "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    output = []\n",
    "    for char in text:\n",
    "        cat = unicodedata.category(char)\n",
    "        if cat == \"Mn\":\n",
    "            continue\n",
    "        output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "def getWordFeatsFromBertTokenFeats(sent_tokens,bert_tokens,bert_token_feats):\n",
    "    \"\"\"\n",
    "    #steps for merging the bert tokens to get the BERT features for actual words\n",
    "    #1. iterate over the BERT base tokenizer\n",
    "    #2. lookup for the actual word in the current BERT lookup postions\n",
    "    #3. If found:\n",
    "        #3a. the word is not tokenized further - use the current BERT features as word embedding\n",
    "    #else:\n",
    "        #3b. the word is tokenized in BERT - find the sequence of tokens and sum up the features to get the word vector\n",
    "    \"\"\"\n",
    "    base_ctr = 0\n",
    "    bert_ctr = 0\n",
    "    word_feat_list = []\n",
    "\n",
    "    for word in sent_tokens:\n",
    "        if bert_tokens[bert_ctr] == word:#word not further tokenized, use the same feature vector\n",
    "            if type(bert_token_feats[bert_ctr]) == np.ndarray:\n",
    "                word_feat_list.append(np.array(bert_token_feats[bert_ctr]))\n",
    "            else:\n",
    "                word_feat_list.append(np.array(bert_token_feats[bert_ctr].detach().numpy()))\n",
    "            base_ctr+=1\n",
    "            bert_ctr+=1\n",
    "        else:\n",
    "            if type(bert_token_feats[bert_ctr]) == np.ndarray:\n",
    "                aggr_feats = np.array(bert_token_feats[bert_ctr])\n",
    "            else:\n",
    "                aggr_feats = np.array(bert_token_feats[bert_ctr].detach().numpy())\n",
    "            aggr_word = bert_tokens[bert_ctr]\n",
    "            merge_next = True\n",
    "            while merge_next and bert_ctr<len(bert_tokens)-1:\n",
    "                if '#' in bert_tokens[bert_ctr+1]:\n",
    "                    aggr_word = aggr_word+bert_tokens[bert_ctr+1]\n",
    "                    bert_ctr+=1\n",
    "                    if type(bert_token_feats[bert_ctr])==np.ndarray:\n",
    "                        aggr_feats+=np.array(bert_token_feats[bert_ctr])\n",
    "                    else:\n",
    "                        aggr_feats+=np.array(bert_token_feats[bert_ctr].detach().numpy())\n",
    "                else:\n",
    "                    merge_next = False\n",
    "                    bert_ctr+=1\n",
    "            word_feat_list.append(aggr_feats)\n",
    "    assert len(sent_tokens)==len(word_feat_list)\n",
    "    return word_feat_list\n",
    "\n",
    "def getCandidatePhrases(text, pos_search_pattern_list=[r\"\"\"base: {(<JJ.*>*<NN.*>+<IN>)?<JJ>*<NN.*>+}\"\"\",\n",
    "                                           r\"\"\"nounverb:{<NN.*>+<VB.*>+}\"\"\",\n",
    "                                           r\"\"\"verbnoun:{<VB.*>+<NN.*>+}\"\"\"]):\n",
    "    \n",
    "    #text = customPreprocess(text)\n",
    "    punct = set(string.punctuation)\n",
    "    all_chunks = []\n",
    "\n",
    "    candidate_phrases = []\n",
    "    for pattern in pos_search_pattern_list:\n",
    "        curr_chunks=getregexChunks(text, pattern)\n",
    "        candidate_phrases+=[' '.join(word for word, pos, \n",
    "                           chunk,ctr in group).lower() \n",
    "                  for key, group in itertools.groupby(curr_chunks, \n",
    "                  lambda_unpack(lambda word, pos, chunk, ctr: chunk != 'O')) if key]\n",
    "    \n",
    "    filtered_candidates = []\n",
    "    for key_phrase in candidate_phrases:\n",
    "        curr_filtr_phrase = stripStopWordsFromText(key_phrase,stop_words)\n",
    "        if curr_filtr_phrase!=key_phrase and curr_filtr_phrase in candidate_phrases:\n",
    "            curr_filtr_phrase = '' #can be considered duplicate\n",
    "        if len(curr_filtr_phrase)>0:\n",
    "            filtered_candidates.append(curr_filtr_phrase)\n",
    "    candidate_phrases = filterCandidatePhrases(text,filtered_candidates)\n",
    "    candidate_phrases,candidate_locs = getPhraseListLocations(text, candidate_phrases)\n",
    "\n",
    "    return candidate_phrases,candidate_locs\n",
    "\n",
    "def getregexChunks(text, grammar):\n",
    "\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
    "                                                    for tagged_sent in tagged_sents))\n",
    "    return [(ele[0], ele[1], ele[2], ctr) for ele,ctr in zip(all_chunks,range(len(all_chunks)))]\n",
    "\n",
    "def filterCandidatePhrases(text, candidate_phrases_list):\n",
    "    \"\"\"\n",
    "    * Merges sub phrases into single phrase\n",
    "    * Concatenate adjecent phrases\n",
    "    * Retains duplicate phrases occurring at different positions\n",
    "    \"\"\"\n",
    "    drop_list = []\n",
    "    merge_list = []\n",
    "    merge_list_start = []\n",
    "    merge_list_end = []\n",
    "\n",
    "    filtered_sent = removeStopwords(text)\n",
    "    filtered_phrase_list = [removeStopwords(phrase) for phrase in candidate_phrases_list]\n",
    "\n",
    "    start_pos_list, end_pos_list = getStartEndPOSList(text,candidate_phrases_list)\n",
    "    filtered_start_pos_list, filtered_end_pos_list = getStartEndPOSList(filtered_sent,filtered_phrase_list)\n",
    "    assert len(filtered_start_pos_list)==len(filtered_phrase_list)\n",
    "\n",
    "    for i in range(len(start_pos_list)):\n",
    "        curr_start,curr_end,ctr = start_pos_list[i],end_pos_list[i],i\n",
    "\n",
    "        for j in range(i+1, len(start_pos_list)):\n",
    "            lookup_start, lookup_end, lookup_ctr = start_pos_list[j], end_pos_list[j], j\n",
    "            if curr_start==lookup_start and curr_end==lookup_end:\n",
    "                continue\n",
    "            if (curr_start<=lookup_start and curr_end>=lookup_end) or (lookup_start<=curr_start and lookup_end>=curr_end):\n",
    "                if len(candidate_phrases_list[i])<len(candidate_phrases_list[j]):\n",
    "                    drop_list.append(candidate_phrases_list[i])\n",
    "                else:\n",
    "                    drop_list.append(candidate_phrases_list[j])\n",
    "\n",
    "        for k in range(len(start_pos_list)):\n",
    "            if filtered_start_pos_list[i]-filtered_end_pos_list[k]==1:\n",
    "                merge_list.append([candidate_phrases_list[i],candidate_phrases_list[k]])\n",
    "                drop_list.append(candidate_phrases_list[i])\n",
    "                drop_list.append(candidate_phrases_list[k])\n",
    "                merge_list_start.append(min(start_pos_list[i],start_pos_list[k]))\n",
    "                merge_list_end.append(max(end_pos_list[i],end_pos_list[k]))\n",
    "\n",
    "    for ctr in range(len(merge_list)):\n",
    "        candidate_phrases_list.append(text[merge_list_start[ctr]:merge_list_end[ctr]])\n",
    "        \n",
    "    doup_list = []   \n",
    "    for kp1 in candidate_phrases_list:\n",
    "        for kp2 in candidate_phrases_list:\n",
    "            if kp1!=kp2 and kp2 in kp1:\n",
    "                doup_list.append(kp2)\n",
    "    #do not do set operation            \n",
    "    for ele in drop_list:\n",
    "        if ele in candidate_phrases_list:\n",
    "            candidate_phrases_list.remove(ele)\n",
    "            \n",
    "    for ele in doup_list:\n",
    "        if ele in candidate_phrases_list:\n",
    "            candidate_phrases_list.remove(ele)\n",
    "\n",
    "    return candidate_phrases_list\n",
    "\n",
    "def removeStopwords(text):\n",
    "    sent = ' '.join([tok for tok in text.split(' ') if tok not in stop_words])\n",
    "    return sent\n",
    "\n",
    "\n",
    "def getStartEndPOSList(text,candidate_phrases_list):\n",
    "    start_pos_list = []\n",
    "    end_pos_list = []\n",
    "    processed_list = []\n",
    "    for candidate in candidate_phrases_list:\n",
    "        start_pos = [match.start() for match in re.finditer(candidate, text)]\n",
    "        if len(start_pos)==1:\n",
    "            processed_list.append(candidate)\n",
    "            start_pos_list.append(start_pos[0])\n",
    "            end_pos_list.append(start_pos[0]+len(candidate))\n",
    "        else: \n",
    "            tok_ctr = processed_list.count(candidate)\n",
    "            start_pos_list.append(start_pos[tok_ctr])\n",
    "            end_pos_list.append(start_pos[tok_ctr]+len(candidate))\n",
    "            processed_list.append(candidate)\n",
    "    return start_pos_list, end_pos_list\n",
    "\n",
    "def getPhraseListLocations(text, candidate_phrases):\n",
    "    \"\"\"\n",
    "    locates the word indices of the key-phrase in the input text\n",
    "    \"\"\"\n",
    "    phrase_idx_list = []\n",
    "    token_sent_list = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text)]\n",
    "    token_list = list(chain(*token_sent_list))\n",
    "    \n",
    "    for phrase in candidate_phrases:\n",
    "        phrase_tokens = nltk.word_tokenize(phrase)\n",
    "        phrase_idx = find_sub_list(phrase_tokens,token_list)\n",
    "        phrase_idx_list.append(phrase_idx)\n",
    "     \n",
    "    processed_phrase_list = []\n",
    "    processed_idx_list = []\n",
    "    for phrase, loc_idx in zip(candidate_phrases,phrase_idx_list):\n",
    "        if len(loc_idx)==0:\n",
    "            processed_phrase_list.append(phrase)\n",
    "            processed_idx_list.append(['-1']) #mismatch between nltk and bert tokenizers :-(\n",
    "\n",
    "        if len(loc_idx)==1:\n",
    "            processed_phrase_list.append(phrase)\n",
    "            processed_idx_list.append(loc_idx[0])\n",
    "        else:\n",
    "            #count number of times the phrase has occurred in the list\n",
    "            if phrase not in processed_phrase_list:\n",
    "                kp_occ_ctr = candidate_phrases.count(phrase)\n",
    "                if kp_occ_ctr == len(loc_idx):\n",
    "                    #append current key-phrase `kp_occ_ctr` times into the lists\n",
    "                    processed_phrase_list+=[phrase]*kp_occ_ctr\n",
    "                    processed_idx_list+=loc_idx\n",
    "                else: \n",
    "                    idx_drop_list = []\n",
    "                    #the phrase index is calculated as part of another key-phrase index\n",
    "                    #check other sublists that are \n",
    "                    #find other locations \n",
    "                    for lookup_loc in phrase_idx_list:\n",
    "                        if lookup_loc!=loc_idx and len(lookup_loc[0])!=len(loc_idx[0]):\n",
    "                            for i in range(len(loc_idx)):\n",
    "                                if((set(loc_idx[i]) & set(lookup_loc[0]))== set(loc_idx[i])):\n",
    "                                    idx_drop_list.append(loc_idx[i])\n",
    "                    for to_insert_loc in loc_idx:\n",
    "                        if to_insert_loc not in idx_drop_list:\n",
    "                            processed_phrase_list.append(phrase)\n",
    "                            processed_idx_list.append(to_insert_loc)\n",
    "                            \n",
    "    str_loc_list = []\n",
    "    for ele in processed_idx_list:\n",
    "        str_loc = ''\n",
    "        for tok in ele:\n",
    "            str_loc = str_loc+' '+str(tok)\n",
    "        str_loc_list.append(str_loc.strip())\n",
    "            \n",
    "    return processed_phrase_list,str_loc_list\n",
    "\n",
    "def getKeyPhraseFeatures(kp_list, kp_loc_idx,text_feats, text_tokens):\n",
    "    \n",
    "    key_phrase_feats = []\n",
    "    for ele,loc_list in zip(kp_list,kp_loc_idx):\n",
    "        if len(ele.split(' '))==1:\n",
    "            if loc_list[0]== '-': #check getPhraseListLocations() for the reason\n",
    "                idx_val = int(loc_list)\n",
    "            else:\n",
    "                idx_val = int(loc_list[0])\n",
    "            key_phrase_feats.append(getTokenFeature(ele,idx_val,text_feats,text_tokens))\n",
    "        else:\n",
    "            curr_feature_vec = []\n",
    "            for tok,tok_idx in zip(ele.split(' '),loc_list.split(' ')):\n",
    "                curr_feature_vec.append(getTokenFeature(tok,int(tok_idx),text_feats,text_tokens))\n",
    "            key_phrase_feats.append(sum(curr_feature_vec))\n",
    "    return key_phrase_feats\n",
    "\n",
    "def getTokenFeature(token, token_idx, text_feats, text_tokens):    \n",
    "    if text_tokens[token_idx]==token:\n",
    "        feat_vec = text_feats[token_idx]\n",
    "    else:\n",
    "        #print('Token not found in the location, searching entire text.: ', token)\n",
    "        if token in text_tokens:\n",
    "            idx_val = text_tokens.index(token)\n",
    "            feat_vec = text_feats[idx_val]\n",
    "        else:\n",
    "            #print('Token not found.. returning default feature vector: ', token)\n",
    "            feat_vec = np.full(len(text_feats[0]),0.01)\n",
    "    return feat_vec\n",
    "\n",
    "def getWordFeatsFromBertTokenFeats(sent_tokens,bert_tokens,bert_token_feats):\n",
    "    \"\"\"\n",
    "    #steps for merging the bert tokens to get the BERT features for actual words\n",
    "    #1. iterate over the BERT base tokenizer\n",
    "    #2. lookup for the actual word in the current BERT lookup postions\n",
    "    #3. If found:\n",
    "        #3a. the word is not tokenized further - use the current BERT features as word embedding\n",
    "    #else:\n",
    "        #3b. the word is tokenized in BERT - find the sequence of tokens and sum up the features to get the word vector\n",
    "    \"\"\"\n",
    "    base_ctr = 0\n",
    "    bert_ctr = 0\n",
    "    word_feat_list = []\n",
    "\n",
    "    for word in sent_tokens:\n",
    "        if bert_tokens[bert_ctr] == word:#word not further tokenized, use the same feature vector\n",
    "            if type(bert_token_feats[bert_ctr]) == np.ndarray:\n",
    "                word_feat_list.append(np.array(bert_token_feats[bert_ctr]))\n",
    "            else:\n",
    "                word_feat_list.append(np.array(bert_token_feats[bert_ctr].detach().numpy()))\n",
    "            base_ctr+=1\n",
    "            bert_ctr+=1\n",
    "        else:\n",
    "            if type(bert_token_feats[bert_ctr]) == np.ndarray:\n",
    "                aggr_feats = np.array(bert_token_feats[bert_ctr])\n",
    "            else:\n",
    "                aggr_feats = np.array(bert_token_feats[bert_ctr].detach().numpy())\n",
    "            aggr_word = bert_tokens[bert_ctr]\n",
    "            merge_next = True\n",
    "            while merge_next and bert_ctr<len(bert_tokens)-1:\n",
    "                if '#' in bert_tokens[bert_ctr+1]:\n",
    "                    aggr_word = aggr_word+bert_tokens[bert_ctr+1]\n",
    "                    bert_ctr+=1\n",
    "                    if type(bert_token_feats[bert_ctr])==np.ndarray:\n",
    "                        aggr_feats+=np.array(bert_token_feats[bert_ctr])\n",
    "                    else:\n",
    "                        aggr_feats+=np.array(bert_token_feats[bert_ctr].detach().numpy())\n",
    "                else:\n",
    "                    merge_next = False\n",
    "                    bert_ctr+=1\n",
    "            word_feat_list.append(aggr_feats)\n",
    "    assert len(sent_tokens)==len(word_feat_list)\n",
    "    return word_feat_list\n",
    "\n",
    "def stripStopWordsFromText(sent, stop_words):\n",
    "    \"\"\"\n",
    "    Removes stop-words at the start and end of the inputs\n",
    "    \"\"\"\n",
    "    fw_ctr = 0\n",
    "    bw_ctr = 0\n",
    "    for tok in sent.split(' '):\n",
    "        if tok in stop_words:\n",
    "            fw_ctr+=1\n",
    "        else:\n",
    "            break\n",
    "    for tok in reversed(sent.split(' ')):\n",
    "        if tok in stop_words:\n",
    "            bw_ctr-=1\n",
    "        else:\n",
    "            break\n",
    "    if bw_ctr!=0:\n",
    "        stripped_kp = ' '.join(sent.split(' ')[fw_ctr:bw_ctr])\n",
    "    else:\n",
    "        stripped_kp = ' '.join(sent.split(' ')[fw_ctr:])\n",
    "            \n",
    "    return stripped_kp.strip()\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    results=[]\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            results.append((ind,ind+sll-1))        \n",
    "    range_list = [list(range(ele[0],ele[1]+1)) for ele in results]\n",
    "    \n",
    "    return range_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:04:54.581976Z",
     "start_time": "2019-07-30T09:04:54.576608Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getBERTFeatures_KP(model, text, attn_head_idx=-1):  # attn_head_idx - index o[]\n",
    "\n",
    "    \"\"\"\n",
    "    Get BERT features for the `text`\n",
    "    Args:\n",
    "        model: BERT model of type `BertForPreTrainingCustom`\n",
    "        text: required, get features for this text\n",
    "        attn_head_idx: optional, defaults to last layer\n",
    "    Returns:\n",
    "        tuple - {token_feats[attn_head_idx][0],final_feats,tokenized_text}\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    if len(tokenized_text) > 200:\n",
    "        tokenized_text = tokenized_text[0:200]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    #print('indexed_tokens: ', indexed_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    #print('tokens_tensor: ', tokens_tensor)\n",
    "    _, _, token_feats, pool_out = model(tokens_tensor)\n",
    "    final_feats = list(getPooledFeatures(token_feats[attn_head_idx]).T)\n",
    "    return token_feats[attn_head_idx][0],final_feats,tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Meetings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:25:32.023162Z",
     "start_time": "2019-06-19T07:25:32.020296Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Timeline json meeting Input (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T13:31:06.700956Z",
     "start_time": "2019-07-01T13:31:06.667964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def parsemeeting(text):\n",
    "    with open(text, 'r') as f:\n",
    "        parsed_text = json.load(f)\n",
    "    return parsed_text\n",
    "text = parsemeeting('../data/timeline_result2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T13:31:25.076921Z",
     "start_time": "2019-07-01T13:31:25.074619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = ''\n",
    "temp = ' '\n",
    "for t in text['timeline']['transcriptSegments']:\n",
    "    temp = ' ' +  t['text']\n",
    "    texts = texts + temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T06:26:58.101349Z",
     "start_time": "2019-06-19T06:26:58.083952Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Get top 10 pims instead of all the segments.    \n",
    "# texts = ''\n",
    "# temp = ' '\n",
    "# for t in text['segments']:\n",
    "#     if t['transcriber'] == \"google_speech_api\":\n",
    "#         temp = ' ' +  t['originalText']\n",
    "#         print (\"---text-----  \\n\\nspoken by: \" + t['spokenBy'] + \"\\n\")\n",
    "#         print (temp, \"\\n\\n\\n\")\n",
    "#         texts = texts + temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load meeting as csv and get only the top10 pims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:41:11.728707Z",
     "start_time": "2019-07-31T09:41:10.469720Z"
    }
   },
   "outputs": [],
   "source": [
    "## load it as csv.\n",
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('../data/segments_201908021635.csv', index_col=False, header=0);\n",
    "df = df.sort_values(by='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:41:23.115800Z",
     "start_time": "2019-07-31T09:41:14.266069Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "data= {}\n",
    "curl_test = {}\n",
    "segment_contents = {}\n",
    "texts = ''\n",
    "curl_test[\"contextId\"] =\"6baa3490-69d6-48fc-b5d4-3994e3e8fae0\"\n",
    "curl_test[\"mindId\"] = \"01daayheky5f4e02qvrjptftxv\"\n",
    "curl_test[\"segments\"] = []\n",
    "for index,segment in enumerate(df['value'].tolist()):\n",
    "    data = json.loads(segment)\n",
    "    if \"google\" not in data['transcriber'] and data['originalText']!='':\n",
    "        print (segment, end=\",\")\n",
    "        curl_test['segments'].append(json.loads(segment))\n",
    "        texts+= (' ' + data['originalText'])\n",
    "        segment_contents[index]= [' '.join(tp.preprocess(data['originalText'], stop_words=False, remove_punct=False)),formatTime(data['startTime'], True), data['spokenBy']]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:31:51.512347Z",
     "start_time": "2019-07-29T14:31:51.503761Z"
    }
   },
   "outputs": [],
   "source": [
    "segment_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get Communities from meetings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:41:48.622898Z",
     "start_time": "2019-07-31T09:41:28.199149Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "\n",
    "mod_texts_unfiltered = tp.preprocess(texts, stop_words=False, remove_punct=False)\n",
    "mod_texts = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts_unfiltered):\n",
    "    if len(sent.split(' '))>250:\n",
    "        length = len(sent.split(' '))\n",
    "        split1 = ' '.join([i for i in sent.split(' ')[:round(length/2)]])\n",
    "        split2 = ' '.join([i for i in sent.split(' ')[round(length/2):]])\n",
    "        mod_texts.append(split1)\n",
    "        mod_texts.append(split2)\n",
    "        continue\n",
    "        #mod_texts.pop(index)\n",
    "    if len(sent.split(' '))<=6:\n",
    "        continue\n",
    "    mod_texts.append(sent)\n",
    "\n",
    "print(len(mod_texts))\n",
    "fv = {}\n",
    "\n",
    "for index, sent in enumerate(mod_texts):\n",
    "    fv[index] = getBERTFeatures(model1, sent, attn_head_idx=-1)\n",
    "    #fv[index] = getBERTFeatures_KP(model1, sent, attn_head_idx=-3)\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T13:33:04.253505Z",
     "start_time": "2019-07-02T13:33:02.292748Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for index1, sent1 in enumerate(mod_texts):\n",
    "#     for index2, sent2 in enumerate(mod_texts):\n",
    "#         print (\"Sentence 1:\\n \" + sent1 + \"\\nSentence 2:\\n \" + sent2 + \"\\n Cosine score:\" + str(1-cosine(fv[index1],fv[index2])), end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:42:17.579574Z",
     "start_time": "2019-07-31T09:42:15.643449Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_graph(doc_list):\n",
    "    eng_graph = nx.Graph()\n",
    "    try:\n",
    "        eng_graph.add_nodes_from(range(len(doc_list)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return eng_graph\n",
    "\n",
    "\n",
    "tg = build_graph(mod_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:27:13.634168Z",
     "start_time": "2019-07-30T09:27:12.352006Z"
    }
   },
   "outputs": [],
   "source": [
    "# v1 = getBERTFeatures(model1, mod_texts[0], attn_head_idx=-1)\n",
    "# v2 = getBERTFeatures(model1, mod_texts[3], attn_head_idx=-1)\n",
    "# getSentMatchScore_wfeature_cosine(mod_texts[0], mod_texts[3],v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:28:29.153180Z",
     "start_time": "2019-07-30T09:28:29.070770Z"
    }
   },
   "outputs": [],
   "source": [
    "# getKPBasedSimilarity(mod_texts[10], mod_texts[20], model1, fv[10], fv[20],layer= -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:43:07.916127Z",
     "start_time": "2019-07-31T09:42:45.276728Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attn_head_idx = -1\n",
    "\n",
    "node_edge = []\n",
    "\n",
    "for index1, sent1 in enumerate(mod_texts):\n",
    "    print (index1)\n",
    "    for index2, sent2 in enumerate(mod_texts):\n",
    "        if index1!=index2 and index2>index1:\n",
    "            #score = getSentMatchScore_wfeature(sent1, sent2,fv[index1],fv[index2])\n",
    "            score = getSentMatchScore_wfeature_cosine(sent1, sent2,fv[index1],fv[index2])\n",
    "#             if score > 0.8:\n",
    "#                 #tg.add_edge(index1,index2,{'weight': score})\n",
    "#                 tg.add_edge(index1,index2)\n",
    "            tg.add_edge(index1,index2,weight=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## build graph using metric threshold. (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T12:40:46.762541Z",
     "start_time": "2019-06-20T12:40:43.048Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_community_graph(tg, mod_texts):\n",
    "    com_graph = nx.Graph()\n",
    "    for sent in list(tg.nodes()):\n",
    "        com_graph.add_node(sent)\n",
    "    for nodea in tg.nodes():\n",
    "        for nodeb in tg.nodes():\n",
    "            if nodea!=nodeb:\n",
    "                if tg.edges[nodea,nodeb]['weight'] > 0.90:\n",
    "                    com_graph.add_edge(nodea,nodeb)\n",
    "    return com_graph\n",
    "com_graph = build_community_graph(tg, mod_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build graph using statistical percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:07:34.594827Z",
     "start_time": "2019-07-31T10:07:29.396057Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import community\n",
    "max_mod = 0\n",
    "mod_v = 0\n",
    "for v in [0.15, 0.10, 0.05, 0.01]:\n",
    "    flag = False\n",
    "    for count in range(5):   \n",
    "        temp_nodes = []\n",
    "        for nodea,nodeb, weight in tg.edges.data():\n",
    "            temp_nodes.append((nodea,nodeb,weight['weight']))\n",
    "        temp_nodes = sorted(temp_nodes, key=lambda kv: kv[2], reverse=True)\n",
    "\n",
    "        temp_nodes = temp_nodes[:math.ceil(len(temp_nodes)*v)+1]\n",
    "\n",
    "        com_graph = nx.Graph()\n",
    "        for nodea,nodeb, weight in temp_nodes:\n",
    "            com_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "        partition = community.best_partition(com_graph)\n",
    "\n",
    "        mod = community.modularity(partition, com_graph)\n",
    "        if mod > max_mod:\n",
    "            max_mod=mod\n",
    "            mod_v = v\n",
    "        print (\"The pruning value 'v' and modularity is: \", v, mod)\n",
    "#         if mod > 0.3:\n",
    "#             flag=True\n",
    "#             print (\"Modularity reached 3. The pruning value 'v' is: \", v)\n",
    "#             break\n",
    "        if mod==0:\n",
    "            temp_nodes = []\n",
    "            print (\"Modularity reached 0. The pruning value 'v' is: \", 0.15)\n",
    "            for nodea,nodeb, weight in tg.edges.data():\n",
    "                temp_nodes.append((nodea,nodeb,weight['weight']))\n",
    "            temp_nodes = sorted(temp_nodes, key=lambda kv: kv[2], reverse=True)\n",
    "\n",
    "            temp_nodes = temp_nodes[:math.ceil(len(temp_nodes)*0.15)+1]\n",
    "\n",
    "            com_graph = nx.Graph()\n",
    "            for nodea,nodeb, weight in temp_nodes:\n",
    "                com_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "            partition = community.best_partition(com_graph)\n",
    "\n",
    "            mod = community.modularity(partition, com_graph)\n",
    "            flag=True\n",
    "            break\n",
    "    if flag:\n",
    "        print()\n",
    "        break\n",
    "\n",
    "for count in range(5):\n",
    "    temp_nodes = []\n",
    "    for nodea,nodeb, weight in tg.edges.data():\n",
    "        temp_nodes.append((nodea,nodeb,weight['weight']))\n",
    "    temp_nodes = sorted(temp_nodes, key=lambda kv: kv[2], reverse=True)\n",
    "\n",
    "    temp_nodes = temp_nodes[:math.ceil(len(temp_nodes)*mod_v)+1]\n",
    "\n",
    "    com_graph = nx.Graph()\n",
    "    for nodea,nodeb, weight in temp_nodes:\n",
    "        com_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "    partition = community.best_partition(com_graph)\n",
    "    mod = community.modularity(partition, com_graph)\n",
    "    if mod>=mod_v:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T14:17:42.593232Z",
     "start_time": "2019-07-30T14:17:42.571058Z"
    }
   },
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# nx.write_gexf(com_graph, \"graph.gexf\", encoding='utf-8', prettyprint=True, version='1.1draft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:08:02.349467Z",
     "start_time": "2019-07-31T10:08:01.638459Z"
    }
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "#partition = community.best_partition(com_graph)\n",
    "\n",
    "values = [partition.get(node) for node in com_graph.nodes()]\n",
    "values=[partition.get(node) for node in com_graph.nodes()]\n",
    "plt.rcParams['figure.figsize']= [16, 10]\n",
    "measure_name = \"Louviin Algorithm Community Structure\"\n",
    "pos = nx.spring_layout(com_graph, k=0.2, iterations=20)\n",
    "nodes_plot=nx.draw_networkx_nodes(com_graph, pos, node_size=140, label=True, cmap=plt.get_cmap('magma', len(com_graph.nodes())/4),node_color=values, alpha=0.95)\n",
    "edges_plot=nx.draw_networkx_edges(com_graph, pos, edge_color='r', alpha=0.1)\n",
    "plt.title(measure_name, fontsize=22, fontname='Arial')\n",
    "plt.colorbar(nodes_plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:08:30.270797Z",
     "start_time": "2019-07-31T10:08:30.260986Z"
    }
   },
   "outputs": [],
   "source": [
    "community.modularity(partition, com_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:08:58.141670Z",
     "start_time": "2019-07-31T10:08:58.138499Z"
    }
   },
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:09:26.276921Z",
     "start_time": "2019-07-31T10:09:26.252783Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        print (mod_texts[word])\n",
    "        current=cluster\n",
    "    else:\n",
    "        print (mod_texts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:49:24.377351Z",
     "start_time": "2019-07-31T09:49:24.373545Z"
    }
   },
   "outputs": [],
   "source": [
    "com_graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:49:51.804944Z",
     "start_time": "2019-07-31T09:49:51.800167Z"
    }
   },
   "outputs": [],
   "source": [
    "tg.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:30:07.027802Z",
     "start_time": "2019-06-19T07:30:07.022745Z"
    }
   },
   "source": [
    "# Redefine the resulatant communities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:09:53.281628Z",
     "start_time": "2019-07-31T10:09:53.276720Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters = []\n",
    "temp = []\n",
    "prev_com = 0\n",
    "for index,(word,cluster) in enumerate(partition):\n",
    "    if prev_com==cluster:\n",
    "        temp.append(word)\n",
    "        #print (temp)\n",
    "        if index==len(partition)-1:\n",
    "            clusters.append(temp)\n",
    "    else:\n",
    "        clusters.append(temp)\n",
    "        temp = []\n",
    "        prev_com = cluster\n",
    "        temp.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:10:20.267486Z",
     "start_time": "2019-07-31T10:10:20.255964Z"
    }
   },
   "outputs": [],
   "source": [
    "timerange = []\n",
    "temp = []\n",
    "for index, cluster in enumerate(clusters):\n",
    "    temp= []\n",
    "    for sent in cluster:\n",
    "        temp2 = [(sentence,start_time,user) for sentence,start_time,user in segment_contents.values() if mod_texts[sent] in sentence]\n",
    "        if len(temp2)!=0:\n",
    "            temp.append(temp2[0])\n",
    "    if len(temp)!=0:\n",
    "        temp = sorted(temp,key=lambda kv: kv[1])\n",
    "        timerange.append(temp)\n",
    "    else:\n",
    "        print (cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T09:51:16.293382Z",
     "start_time": "2019-07-31T09:51:16.281663Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timerange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:10:47.609764Z",
     "start_time": "2019-07-31T10:10:47.599256Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timerange_detailed = []\n",
    "temp = []\n",
    "flag = False\n",
    "pims = {}\n",
    "index_pim = 0\n",
    "index_segment = 0\n",
    "for index,com in enumerate(timerange):\n",
    "    temp = []\n",
    "    flag = False\n",
    "    #print (\"-----community-----\", index)\n",
    "    for (index1,(sent1,time1,user1)), (index2,(sent2,time2,user2)) in zip(enumerate(com[0:]),enumerate(com[1:])):\n",
    "        if sent1!=sent2:\n",
    "            #print (time1, time2, (time2-time1).seconds)\n",
    "            if ((time2-time1).seconds<=120):\n",
    "                if not flag:\n",
    "                    pims[index_pim] = {'segment'+str(index_segment):[sent1,time1,user1]}\n",
    "                    index_segment+=1\n",
    "                    temp.append((sent1,time1,user1))\n",
    "                #else:\n",
    "                    #print ('removing',time1, time2)\n",
    "                    #temp.pop()\n",
    "                pims[index_pim]['segment'+str(index_segment)] = [sent2,time2,user2]\n",
    "                index_segment+=1\n",
    "                temp.append((sent2,time2,user2))\n",
    "                flag=True\n",
    "            else:\n",
    "                #print (time2, time1)\n",
    "                if flag==True:\n",
    "                    index_pim+=1\n",
    "                    index_segment=0\n",
    "                flag=False\n",
    "    if flag==True:\n",
    "        index_pim+=1\n",
    "        index_segment=0\n",
    "    #print (\"-----timeRange-----\\n\", [j for i,j,k in temp])\n",
    "    timerange_detailed.append(temp)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Redefing the resultant communities using different approach (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:50:15.095325Z",
     "start_time": "2019-06-25T13:50:15.091774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "timerange = []\n",
    "temp = []\n",
    "for index, cluster in enumerate(clusters):\n",
    "    temp= []\n",
    "    for sent in cluster:\n",
    "        temp2 = [(sentence,start_time,user, [index]) for sentence,start_time,user in segment_contents.values() if mod_texts[sent] in sentence]\n",
    "        if len(temp2)!=0:\n",
    "            temp.append(temp2[0])\n",
    "    if len(temp)!=0:\n",
    "        temp = sorted(temp,key=lambda kv: kv[1])\n",
    "        timerange.append(temp)\n",
    "    else:\n",
    "        print (cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:50:22.147730Z",
     "start_time": "2019-06-25T13:50:22.144636Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "flattened_timerange = sorted([sent for com in timerange for sent in com], key= lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:50:29.301512Z",
     "start_time": "2019-06-25T13:50:29.290225Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flattened_timerange[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:58:20.906464Z",
     "start_time": "2019-06-25T13:58:20.894419Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ft_re = []\n",
    "# for (index1,sent1), (index2, sent2) in zip(enumerate(flattened_timerange),enumerate(flattened_timerange)):\n",
    "#     if  sent1[1]==sent2[1]:\n",
    "#         if len(ft_re)!=0 and ft_re[-1][1] == sent1[1]:\n",
    "#             temp = []\n",
    "#             temp.extend([[ft_re[-1][-1], sent1[-1], sent2[-1]]])\n",
    "#             print (temp)\n",
    "#             tot_com = list(set(temp))\n",
    "#             ft_re.pop()\n",
    "#             ft_re.append((sent1[0],sent1[1],sent1[2],tot_com))\n",
    "#         else:\n",
    "#             tot_com = list(set([sent1[-1], sent2[-1]]))\n",
    "#             ft_re.append((sent1[0],sent1[1],sent1[2],tot_com))\n",
    "#     else:\n",
    "#         if ft_re!=[] and ft_re[-1][1] == sent1[1]:\n",
    "#             tot_com = list(set([ft_re[-1][-1], sent1[-1]]))\n",
    "#             ft_re.pop()\n",
    "#             ft_re.append((sent1[0],sent1[1],sent1[2],tot_com))\n",
    "#         else:\n",
    "#             ft_re.append(sent1)\n",
    "            \n",
    "ft_re = []\n",
    "tot_com = []\n",
    "flag=False\n",
    "for index1, sent1 in enumerate(flattened_timerange):\n",
    "    if flag==True:\n",
    "        if index1!=j:\n",
    "            continue\n",
    "        else:\n",
    "            flag=False\n",
    "    if flag==False:\n",
    "        for index2, sent2 in enumerate(flattened_timerange):\n",
    "            if index2>index1:\n",
    "                if sent1[1]==sent2[1]:\n",
    "                    if flag==False:\n",
    "                        tot_com.extend([sent1[-1][-1],sent2[-1][-1]])\n",
    "                        flag=True\n",
    "                    else:\n",
    "                        tot_com.extend([sent2[-1][-1]])\n",
    "                        if index2==(len(flattened_timerange)-1):\n",
    "                                tot_com = max(set(tot_com), key = tot_com.count) \n",
    "                                ft_re.append((sent1[0],sent1[1],sent1[2],tot_com))\n",
    "                else:\n",
    "                    if flag==True:\n",
    "                        tot_com = max(set(tot_com), key = tot_com.count)\n",
    "                        ft_re.append((sent1[0],sent1[1],sent1[2],tot_com))\n",
    "                        j = index2\n",
    "                    else:\n",
    "                        ft_re.append(sent1)\n",
    "                        flag = False\n",
    "                    tot_com = []\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:58:28.121034Z",
     "start_time": "2019-06-25T13:58:28.116334Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ft_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alter communities based on time range and get Keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:29:33.845421Z",
     "start_time": "2019-07-31T10:29:26.282488Z"
    }
   },
   "outputs": [],
   "source": [
    "from graphrank.graphrank import GraphRank\n",
    "from graphrank.utils import GraphUtils, TextPreprocess\n",
    "\n",
    "keyphrase_text = \"\"\n",
    "keyphrases_list = []\n",
    "keyphrases = []\n",
    "for index_pim in pims.keys():\n",
    "    keyphrase_text = \"\"\n",
    "    for seg in pims[index_pim]:\n",
    "        if seg != \"keyphrase\":\n",
    "            keyphrase_text += (' ' + pims[index_pim][seg][0])\n",
    "    gr = GraphRank()\n",
    "    tp = TextPreprocess()\n",
    "    utils = GraphUtils()\n",
    "\n",
    "    original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(keyphrase_text, filter_by_pos=True, pos_filter=['NOUN', 'PROPN', 'ADJ', 'FW'], stop_words=False)\n",
    "\n",
    "    word_graph = gr.build_word_graph(filtered_pos_tuple, original_tokens=original_tokens, window=4, reset_graph_context=True, preserve_common_words=False)\n",
    "\n",
    "    keyphrases = gr.get_keyphrases(word_graph, normalize_nodes='degree')\n",
    "    pims[index_pim]['keyphrase'] = [phrases for phrases, score in keyphrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:30:01.246110Z",
     "start_time": "2019-07-31T10:30:01.239427Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "yet_to_combine = []\n",
    "need_to_remove = []\n",
    "for index1,i in enumerate(pims.keys()):\n",
    "    for index2,j in enumerate(pims.keys()):\n",
    "        if index1!=index2:\n",
    "            if pims[i]['segment0'][1] >= pims[j]['segment0'][1] and pims[i]['segment0'][1] <= pims[j]['segment'+str(len(pims[j].values())-2)][1]:\n",
    "                if (j,i) not in yet_to_combine and i not in need_to_remove and j not in need_to_remove:\n",
    "                    yet_to_combine.append((i,j))\n",
    "                    need_to_remove.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:30:28.527088Z",
     "start_time": "2019-07-31T10:30:28.517612Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,j in yet_to_combine:\n",
    "    for k in pims[i]:\n",
    "        if k!=\"keyphrase\":\n",
    "            if pims[i][k] not in pims[j].values():\n",
    "                pims[j]['segment'+str(len(pims[j].values())-1)] = pims[i][k]\n",
    "                #print (pims[i][k])\n",
    "                continue\n",
    "        else:\n",
    "            extra_keyphrase = list(set(pims[i]['keyphrase'] + pims[j]['keyphrase']))\n",
    "            pims[j]['keyphrase']=extra_keyphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:30:56.304955Z",
     "start_time": "2019-07-31T10:30:56.301646Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in need_to_remove:\n",
    "    pims.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T10:31:23.772427Z",
     "start_time": "2019-07-31T10:31:23.727777Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# meeting start time.\n",
    "m_time = formatTime(\"2019-08-01T12:56:00Z\", True)\n",
    "for i in pims.keys():\n",
    "    print (\"\\n\\n\\nPIMs \", i)\n",
    "    print (\"\\n\\nDiscussion:\\n\\n \")\n",
    "    for seg in pims[i]:\n",
    "        if seg!=\"keyphrase\":\n",
    "            print (\"Minutes from the start of the meeting: \", pims[i][seg][1] - m_time , \"\\n\")\n",
    "            print (pims[i][seg][1], \" \", pims[i][seg][0],\"\\n\")\n",
    "    print (\"Keyphrases:\\n\\n \", pims[i]['keyphrase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## testing topic modelling with LDA (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:03:40.394625Z",
     "start_time": "2019-06-12T16:03:40.375521Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in new_text]\n",
    "#doc_clean = [clean(new_text[1]).split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:03:52.012109Z",
     "start_time": "2019-06-12T16:03:51.997576Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:05:59.081754Z",
     "start_time": "2019-06-12T16:05:59.077554Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:06:26.310170Z",
     "start_time": "2019-06-12T16:06:25.966545Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T16:07:13.872792Z",
     "start_time": "2019-06-12T16:07:13.864096Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Testing keyphrase for the timerange (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:20:16.374772Z",
     "start_time": "2019-06-28T17:20:16.366362Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pims = {\n",
    "\t\"0\": {\n",
    "\t\t\"segment0\": [\"Like my per month to date most likely as we excuse into which we can that be better more consistent but General aw strategy what they say is have one Lambda the internet coming HTML segment notify is attached a general hook for the data uploaded the Lambda function who associate to the S3 bucket and have that pan out all other language like be a dispatcher. Yeah, thats.\", \"2019-06-28T06:16:58Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"ebe8604a02c84952bb7ea9dfccd6c1df\"]\n",
    "\t},\n",
    "\t\"1\": {\n",
    "\t\t\"segment0\": [\"From by will sync up offline first trusting watertight preview will move manually create a bucket in staging environment and try to use it for testing but what I am expecting is not be a big change. What we what we can do is we can we can remove that CD and table itself from the database because the student table and other other things are actually only used for you know, this sets these things so we give you can remove them. So we will now will have a less overhead even in the future. So because after this change we are we are never going to use the stimuli. and.\", \"2019-06-28T06:11:11Z\", \"b6db6d45-1a71-41e7-bc16-ae330f655c38\", \"0ef8d453e7bb41d3bd11153827df076a\"]\n",
    "\t},\n",
    "\t\"2\": {\n",
    "\t\t\"segment0\": [\n",
    "\t\t\t[\"All right create an issue. Well track it should be a small thing even in the back end to handle. Yeah, I will see if I can look into it today.\"], \"2019-06-28T05:35:58Z\", \"b6db6d45-1a71-41e7-bc16-ae330f655c38\", \"f2d6bb774d5f4649b2e4f15bf07052ad\"\n",
    "\t\t],\n",
    "\t\t\"segment1\": [\n",
    "\t\t\t[\"Nothing was done. Headmistress, I would wear black in the public Channel. We need to prompt Channel Minds. Also, I will check with Rashon thundergun. It was another.\"], \"2019-06-28T05:39:19Z\", \"b6db6d45-1a71-41e7-bc16-ae330f655c38\", \"418c719a0f5e4888b242df011e818aa3\"\n",
    "\t\t],\n",
    "\t\t\"segment2\": [\n",
    "\t\t\t[\"And updated methods for that which I take do too. Okay. Yeah, I will look into that.\"], \"2019-06-28T05:39:35Z\", \"d65899ed-e47e-4611-8c2c-59b154ff6a3f\", \"136b9ea1ccaf461facc5e7953a520ff6\"\n",
    "\t\t]\n",
    "\t},\n",
    "\t\"3\": {\n",
    "\t\t\"segment0\": [\n",
    "\t\t\t[\"Its similar than better to go with S3. If GitHub is not giving you just artifact reading scope or something which release artifact reading Scopes even then it can be weird because if we keep tagging or create a good stuff that are not Jesus files are also generated. We need to prevent them from able to downloading do not also even if they do not we have to protect the code right should not be vulnerability this this take that what is the effort to from Reading from your front door if Jacob does give you anything fixed capacity?. .\"], \"2019-06-28T05:46:56Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"8c4e8716db754f82b1ed36c449fa15a3\"\n",
    "\t\t],\n",
    "\t\t\"segment1\": [\n",
    "\t\t\t[\"Another markers with calibration issue. I will also take a look on the back end. I know that we have to trim the edges to remove the flashin but I do think that if someone puts a empty space between two sentences that can cause an issue. I will just go check. Why is that occurring and see if it is something easy to pick up and then the back end also?.\"], \"2019-06-28T05:50:04Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"3c74a71652394d65abed891a412c008d\"\n",
    "\t\t]\n",
    "\t},\n",
    "\t\"4\": {\n",
    "\t\t\"segment0\": [\n",
    "\t\t\t[\"You things on whether can we go eat on to Lambda or whether she should whether it should be based on the current key phrase. Maybe we will just take it off way, or I will talk to Sasha Hank and understand and get those details. We did have some plans to move it along.\"], \"2019-06-28T05:57:22Z\", \"92b4588f-74cc-42f0-bf6d-eec0b6198ade\", \"6b474e34b42a4514b7689a378bc47c66\"\n",
    "\t\t],\n",
    "\t\t\"segment1\": [\n",
    "\t\t\t[\"Between making calls on Tims and waiting on the one that comes last. Yeah that still happen because we will have to wait on the consensus of the what we need to do again is somewhat more on the cause they custom server is not doing anything and then it out into my you are still paying for the execution of the Pim. Lambda looks right at is one thing you can reduce but also the fact that we are right now waiting on Transit segment to complete I think within being the change we will need to wait on transcript second segment to be analyzed. Those are the events that trigger are some regeneration. So if it makes it and because that tramp is segment competed in already in the event of are acting on making this switch to the next most relevant is not a big problem. So it is more even driven so we do not need to wait on okay. Okay. Yeah, and then on the.\"], \"2019-06-28T06:00:48Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"f5c2855a681c40c8b5a77a85c5c0fd6d\"\n",
    "\t\t],\n",
    "\t\t\"segment2\": [\n",
    "\t\t\t[\"Loved it. Thats fine. For now. I think you do. I think once you go to production, we need to figure out a way eventually to not lock sensitive information like transcript segment data Etc. And that will cause some you know issues still doing that. So but ceasing to your fine, too. Yeah.\"], \"2019-06-28T06:02:50Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"f56de51245ce4fa69c6628e5dc066369\"\n",
    "\t\t],\n",
    "\t\t\"segment3\": [\n",
    "\t\t\t[\"Make a contained environment. You cannot specify the requirements separately and then all the executable file separately so that you can put the files as layers. I was just trying to do that, but I could not find it in the process of doing a trigger. If so, why?.\"], \"2019-06-28T06:03:45Z\", \"f6b27e4a-ad4f-42b3-ac18-e09ec396685b\", \"4fddeb333fe745cdac8ee41d5785d0ba\"\n",
    "\t\t],\n",
    "\t\t\"segment4\": [\n",
    "\t\t\t[\"I think when cat had reduced it to Fighters, I think Sicilian fighter he moved to by Torchlight and side by and all the mines and this one. So overall I think the thing got to do but we ended up using layers so we might be can end up adding both the layers for five dots by touch light and maybe scifi known by all the psychic number like it has layers and learning anything network accessible routes like that, but if sound 50mb.\"], \"2019-06-28T06:06:36Z\", \"2982d23c-1e59-476d-88c8-a42376324142\", \"cc29605c0446480f8ace87c4d6593d8a\"\n",
    "\t\t],\n",
    "\t\t\"segment5\": [\n",
    "\t\t\t[\"So currently was just working on the refactoring of bug book printing that is done basically it is in like the record reviews review comments have to just address it so apart from the doe those comments look straight forward or should not be a bigger big thing is basically so the other part is the pims part the the pins which are not seen in the mix does not constitute the last last few segments the meeting right? So that issue if started working on it like probably a bit Monday should be able to send the send it for review that code. Okay. Yeah that I maybe I will have to ask like more technical things. I will have to ask like if there is anything which I am missing missing I will call you again.\"], \"2019-06-28T06:08:07Z\", \"6e8408ce-1072-4209-9e82-945701c7b86c\", \"439f3d6209184e19a7de54c59a8c4533\"\n",
    "\t\t],\n",
    "\t\t\"segment6\": [\n",
    "\t\t\t[\"Make a contained environment. You cannot specify the requirements separately and then all the executable file separately so that you can put the files as layers. I was just trying to do that, but I could not find it in the process of doing a trigger. If so, why?.\"], \"2019-06-28T06:03:45Z\", \"f6b27e4a-ad4f-42b3-ac18-e09ec396685b\", \"4fddeb333fe745cdac8ee41d5785d0ba\"\n",
    "\t\t]\n",
    "\t},\n",
    "\t\"5\": {\n",
    "\t\t\"segment0\": [\"But we we have done it. We have done it only from our office and I am not confused coming from that one will because people in the calls are like variables XnumberX  scholar two persons called Waterloo. We visited a real scenario kind of thing according a minimum XnumberX people per call because we just need to load test it and because it is normal generous mood worth but under the cave under the load, how does how its behaving that we need to check? Yeah.\", \"2019-06-28T06:21:56Z\", \"b6db6d45-1a71-41e7-bc16-ae330f655c38\", \"b573665bbeae49bdb26020751122dfc1\"]\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:24:40.447940Z",
     "start_time": "2019-06-28T17:24:39.970584Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from graphrank.graphrank import GraphRank\n",
    "from graphrank.utils import GraphUtils, TextPreprocess\n",
    "\n",
    "keyphrase_text = \"\"\n",
    "keyphrases_list = []\n",
    "keyphrases = []\n",
    "for index_pim in pims.keys():\n",
    "    keyphrase_text = \"\"\n",
    "    for seg in pims[index_pim]:\n",
    "        if seg != \"keyphrase\":\n",
    "            keyphrase_text += (' ' + ''.join([i for i in pims[index_pim][seg][0]]))\n",
    "            print (keyphrase_text)\n",
    "    gr = GraphRank()\n",
    "    tp = TextPreprocess()\n",
    "    utils = GraphUtils()\n",
    "\n",
    "    original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(keyphrase_text, filter_by_pos=True, pos_filter=['NOUN', 'PROPN', 'ADJ', 'FW'], stop_words=False)\n",
    "\n",
    "    word_graph = gr.build_word_graph(filtered_pos_tuple, original_tokens=original_tokens, window=4, reset_graph_context=True, preserve_common_words=False)\n",
    "\n",
    "    keyphrases = gr.get_keyphrases(word_graph, normalize_nodes='degree')\n",
    "    pims[index_pim]['keyphrase'] = [phrases for phrases, score in keyphrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:24:52.412467Z",
     "start_time": "2019-06-28T17:24:52.407043Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## keyphrases comparision with word2vec (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:25:36.521105Z",
     "start_time": "2019-07-01T16:25:36.503427Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/meetings_slack_embeddings.pkl\",\"rb\") as f:\n",
    "    emb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:44:03.316368Z",
     "start_time": "2019-07-01T16:44:03.175132Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nodea = 'sql'\n",
    "nodea_fv = emb[nodea]\n",
    "closest_match =[]\n",
    "for nodeb in emb.keys():\n",
    "    if nodeb!=nodea:\n",
    "        diff = 1 - cosine(emb[nodeb], nodea_fv)\n",
    "        closest_match.append((nodeb, diff))\n",
    "closest_match_sorted = sorted(closest_match, key= lambda kv:kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:44:32.718468Z",
     "start_time": "2019-07-01T16:44:32.715059Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "closest_match_sorted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:43:25.113499Z",
     "start_time": "2019-07-01T17:43:25.037486Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('../data/query_resultsync2.csv', index_col=False, header=0);\n",
    "\n",
    "import json\n",
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "data= {}\n",
    "\n",
    "segment_contents = {}\n",
    "texts = ''\n",
    "\n",
    "for index,segment in enumerate(df['value'].tolist()):\n",
    "    data = json.loads(segment)\n",
    "    if \"google\" not in data['transcriber'] and data['originalText']!='':\n",
    "    #if \"google\" not in data['transcriber']:\n",
    "        texts+= (' ' + data['originalText'])\n",
    "        segment_contents[index]= [' '.join(tp.preprocess(data['originalText'], stop_words=False, remove_punct=False)),formatTime(data['startTime'], True), data['spokenBy']]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:46:37.246275Z",
     "start_time": "2019-07-01T17:46:37.232827Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:43:55.744377Z",
     "start_time": "2019-07-01T17:43:55.326468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from graphrank.graphrank import GraphRank\n",
    "from graphrank.utils import GraphUtils, TextPreprocess\n",
    "\n",
    "gr = GraphRank()\n",
    "tp = TextPreprocess()\n",
    "utils = GraphUtils()\n",
    "\n",
    "original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(keyphrase_text, filter_by_pos=True, pos_filter=['NOUN', 'PROPN', 'ADJ', 'FW'], stop_words=False)\n",
    "\n",
    "word_graph = gr.build_word_graph(filtered_pos_tuple, original_tokens=original_tokens, window=4, reset_graph_context=True, preserve_common_words=False)\n",
    "\n",
    "#keyphrases = gr.get_keyphrases(word_graph, normalize_nodes='degree')\n",
    "#pims[index_pim]['keyphrase'] = [phrases for phrases, score in keyphrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:44:26.282278Z",
     "start_time": "2019-07-01T17:44:26.183078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "com_graph = GraphRank()\n",
    "nodes_list = []\n",
    "for indexa, nodea in enumerate(list(word_graph.nodes())):\n",
    "    for indexb, nodeb in enumerate(list(word_graph.nodes())):\n",
    "        if indexb>indexa:\n",
    "            if nodea not in emb.keys() or nodeb not in emb.keys():\n",
    "                #nodes_list.append((nodea,nodeb,0.5))            \n",
    "                continue\n",
    "            nodes_list.append((nodea,nodeb,cosine(emb[nodea],emb[nodeb])))\n",
    "#print (*nodes_list, sep=\"\\n\")\n",
    "nodes_list_sorted = sorted(nodes_list, key=lambda kv: kv[2], reverse=True)\n",
    "\n",
    "nodes_list = nodes_list_sorted[:math.ceil(len(nodes_list_sorted)*0.05)+1]\n",
    "\n",
    "com2_graph = nx.Graph()\n",
    "for nodea,nodeb, weight in nodes_list:\n",
    "    com2_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "partition = community.best_partition(com2_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:44:56.761046Z",
     "start_time": "2019-07-01T17:44:56.757951Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "community.modularity(partition, com2_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:45:26.415917Z",
     "start_time": "2019-07-01T17:45:26.413695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T17:45:56.160268Z",
     "start_time": "2019-07-01T17:45:56.156494Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        print (word)\n",
    "        current=cluster\n",
    "    else:\n",
    "        print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T07:00:10.110975Z",
     "start_time": "2019-07-03T07:00:10.106961Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text_1 = \"We have now created a simple network with Terraform on GCP. You can do a lot more with terraform such as setting up compute instances, firewall rules and much more. I thought this was necessary for the kinds of apps I was working on. And with that in mind, I tried many approaches and frameworks for implementing the same pattern: Single-page applications (SPA). The next thing we shall do is get the necessary credentials from GCP. Go to the GCP console menu and select APIs & Services then Credentials. Now click the Create Credentials button and choose Service Account Key. On the next screen choose Compute Engine default service account and JSON then click Create. This will download a json file to your computer, move this file to the folder where we shall write our terraform code. This file contains sensitive information so do not store it in a public repository or make it public.\"\n",
    "text_1 = [sent for sent in text_1.split('. ')]\n",
    "text_2 = \"properly applied HR strategies ensure that  strategic recruiting and retention processes are created that fit the organization.There's a clear need for increased visibility to drastically improve sales. But in order to get more visibility, businesses have to spend more money. When that well runs dry, what are you supposed to do?.  strategically applying smart HR insights into different areas of the business tends to improve employee engagement throughout the company. However, identifying the right strategies to market your business is often likened to rocket science.  Some businesses have been built solely on the backs of social media. How do you get your message to the right audience and do it effectively? How do you boost visibility and increase sales while sustaining a profit with a converting offer? Today, with so much vying for our attention from social media, to search engine optimization, blogging and pay-per-click advertising, it's easy to see why most are ready to pull their hair out.The truth is that what got you to this point in business is likely not going to get you to the next level. If you're feeling stuck, join the fray. Most entrepreneurs are so busy working 'in' their businesses that they fail to work 'on' their businesses. As a result of dealing with the day-to-day operations of a company that includes customer hand-holding, supply-chain demands and more, we often neglect to wield the right marketing strategies that will help fuel our business's growth.\"\n",
    "text_2 = [sent for sent in text_2.split('. ')]\n",
    "print(len(text_1),len(text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_fv = getBERTFeatures(model1,\"\", attn_head_idx = -1)\n",
    "c_fv2 = getBERTFeatures(model1, \"\", attn_head_idx = -1)\n",
    "#getSentMatchScore_wfeature(c_text, c_text, c_fv, c_fv2, nsp_dampening_factor = 0.7)\n",
    "1-cosine(c_fv,c_fv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:55:30.785830Z",
     "start_time": "2019-07-03T06:55:30.783164Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label = {}\n",
    "for x in range(10):\n",
    "    label[x]=\"in-domain\" \n",
    "for x in range(10,20):\n",
    "    label[x]=\"out-of-domain\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:55:37.038062Z",
     "start_time": "2019-07-03T06:55:37.035284Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:58:26.654404Z",
     "start_time": "2019-07-03T06:57:54.602632Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index1, text1 in enumerate(text_1+(text_2)):\n",
    "    for index2, text2 in enumerate(text_1+(text_2)):\n",
    "        if index2>index1:\n",
    "            text1_fv = getBERTFeatures(model1,text1, attn_head_idx = -1)\n",
    "            text2_fv = getBERTFeatures(model1,text2, attn_head_idx = -1)\n",
    "            cosine_similarity = 1 - cosine(text1_fv, text2_fv)\n",
    "            metric_similarity = getSentMatchScore_wfeature(text1, text2, text1_fv, text2_fv, nsp_dampening_factor = 0.7)\n",
    "            \n",
    "            print (label[index1] + \"  and   \" + label[index2] + \"   scores: \" )\n",
    "            print (\"cosine similarity: \" + str(cosine_similarity))\n",
    "            print (\"metric similarity: \" + str(metric_similarity))\n",
    "            print (\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T07:26:39.062425Z",
     "start_time": "2019-07-03T07:26:39.059027Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## subtopics\n",
    "\n",
    "topic_1 = \"We have now created a simple network with Terraform on GCP. You can do a lot more with terraform such as setting up compute instances, firewall rules and much more. The next thing we shall do is get the necessary credentials from GCP. Go to the GCP console menu and select APIs & Services then Credentials. Now click the Create Credentials button and choose Service Account Key. On the next screen choose Compute Engine default service account and JSON then click Create. This will download a json file to your computer, move this file to the folder where we shall write our terraform code. This file contains sensitive information so do not store it in a public repository or make it public.\"\n",
    "topic_1 = [sent for sent in topic_1.split('. ') if sent!=\"\"]\n",
    "topic_2 = \"I thought this was necessary for the kinds of apps I was working on. And with that in mind, I tried many approaches and frameworks for implementing the same pattern: Single-page applications. If you work in the JavaScript ecosystem, you are most likely aware of how difficult it is to manage dependencies.When one of your core libraries releases a major version, it is time to start thinking about upgrading. Imagine that a library named X has a peer dependency on React and you are migrating your application to React. Maintaining an application with a large codebase requires a lot of discipline and having your dependencies up to date should be a top priority for your team and organization. \"\n",
    "topic_2 = [sent for sent in topic_2.split('. ') if sent!=\"\"]\n",
    "print(len(topic_1),len(topic_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T07:26:45.377137Z",
     "start_time": "2019-07-03T07:26:45.374938Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label2 = {}\n",
    "for x in range(8):\n",
    "    label2[x]=\"topic_1\" \n",
    "for x in range(8,14):\n",
    "    label2[x]=\"topic_2\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T07:27:21.954789Z",
     "start_time": "2019-07-03T07:27:09.472666Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index1, text1 in enumerate(topic_1+(topic_2)):\n",
    "    for index2, text2 in enumerate(topic_1+(topic_2)):\n",
    "        if index2>index1:\n",
    "            text1_fv = getBERTFeatures(model1,text1, attn_head_idx = -1)\n",
    "            text2_fv = getBERTFeatures(model1,text2, attn_head_idx = -1)\n",
    "            cosine_similarity = 1 - cosine(text1_fv, text2_fv)\n",
    "            metric_similarity = getSentMatchScore_wfeature(text1, text2, text1_fv, text2_fv, nsp_dampening_factor = 0.7)\n",
    "            \n",
    "            print (label2[index1] + \"  and   \" + label2[index2] + \"   scores: \" )\n",
    "            print (\"cosine similarity: \" + str(cosine_similarity))\n",
    "            print (\"metric similarity: \" + str(metric_similarity))\n",
    "            print (\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test = {\n",
    "    \"0\": {\n",
    "        \"segment0\": [\n",
    "            [\n",
    "                \"Yes cannot think still senior you sure that is coming.\"\n",
    "            ],\n",
    "            \"2019-07-24T15:59:40Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"0399616ea53f4fa7ad42970ed8d78db4\"\n",
    "        ],\n",
    "        \"segment1\": [\n",
    "            [\n",
    "                \"Give hope I channel Okay right okay so topic what I thought we will do is I just wanted to have like a bunch of discussions. Firstly what is the test how we is working on that, but also I thought when use is just go back and forth in accommodation format. There will have some discussions about you ether platform technology choices. What we do where we can improve you know what are some of the problems challenges that we faced coming up to this point cycle. You know how are we doing compared to industry standards or what folks are talking about industry and all that so I just wanted to can keep it interactive. They would not keep talking about it month like for so let me start with maybe just at high level right so architecture choices. You know, I know we started off from literally from brand sheet of paper. You know they we said we built this using a variety of different technologies can gone that traditionally Commerce more that three based for example and Java based my chris.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:00:07Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"34c5024cdf914228b8501e4585098e4f\"\n",
    "        ],\n",
    "        \"segment2\": [\n",
    "            [\n",
    "                \"And and more more tell Amazon for example, we how went the part of you know either not just going a tomorrow at the same time not going with a full services architecture and also going as well dont core main choice with back. So can you walk me through that process sure so before diving into the.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:02:01Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"c0a1f97f95b64c8aba82813e99489eae\"\n",
    "        ],\n",
    "        \"segment3\": [\n",
    "            [\n",
    "                \"Let is talk about the decision around that they we wanted to go with model this services or why we went to something in green. Yes, it good point to start because that master that the language that you choose hmmm from you know from I mean most of this is come from experiences both working at living to where we working on a model and moving to Groupon, which was a heavy presented company a with a huge model in between so some of the learning that we took from that was before getting into model the micro services. There is something that I be very was do not come a framework frameworks like Ruby or something that is too allows some invoices from kind of practices upon the developer and sort of expects the about the right code such that will only work well written in that order and to word away from the same words, you know specific in a workings and things will kind of make you words from the community around view also right. So especially when we work then like one of model and rails backend group on one of the biggest problems are things like handling database connections across different regions. You know using the appropriate data connection database connection or you only your master et cetera and you have to do a lot of to get that in where we go in have had to had chosen the right form of abstraction and design patterns that are put in place and will allow us to easily move with logic or handle much well the second thing was at least some other framework.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:02:36Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"0360786073c64971864925d05327ff36\"\n",
    "        ],\n",
    "        \"segment4\": [\n",
    "            [\n",
    "                \"And and more more tell Amazon for example, we how went the part of you know either not just going a tomorrow at the same time not going with a full services architecture and also going as well dont core main choice with back. So can you walk me through that process sure so before diving into the.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:02:01Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"c0a1f97f95b64c8aba82813e99489eae\"\n",
    "        ]\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"segment0\": [\n",
    "            [\n",
    "                \"Is very important for us to handy so that allows us to so you do not want to focus on things like collection things like that, especially when you are small you want to focus on you know and figure how to make come currency like like by myself for example or do be a blue and logs just incredibly hard to work make it the sustainable small advocate application. There is just a lot of extra work that we need to do this to get it working well so I show them out at least for the core platform is mainly our call platform actually, but for us just to go with items because the so comfortable with it and the variety of the same most of it are implemented. So so with respect to that the problem with Java the one of the two main part Java of was types so when you had to great clases and yet to create interfaces. We have to say this clas implemented interface and make sure that you try to not try to get in exactly the dependency place and worry about those things and unless the clas does not provide implement interface even though it has all the messages. It does not work out definitely at that point time was somewhat shaky if the Java had been introduced because not as efficient as go she was or not easy to manage the way go had done with respect to go things in the way managers in admin manner scale was interesting because it is a functional program language for the problem was there were just not easy for developers to get onboard with and the bigger problem was that it had a functional.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:42:48Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"485300f6f5df4b02bc1e2ed86c3db17b\"\n",
    "        ],\n",
    "        \"segment1\": [\n",
    "            [\n",
    "                \"Do it, but also allow you to do manner which I will call it at the bad thing but it is just that there was no proper guidelines on how you use specific based on what choices. So most of the experience support were like is just incredibly hard to do reviews developers would constantly argue us, which is the right or the most way to write code and this is focused on those just figure on how what is a good design how the this code scale that and so we sort of makes call know now let us just put that aside look at closure. I ideally like that a lot but again I because it to the list of the foundation of the language is very small. There is not a lot of things that learn about the language to go work. Yeah, but that because also that it is list most people uncomfortable, especially because when we started it. We need people with media processing background. These will go at work especially on lower level systems and plus plus with handling network related itself are heavily comfortable of as a language and to train them do not just learn like how to write high level software was one concern that Wouldve happen regardless of what language. We choose but on top of that piece then my whole different paradigm. I am of writing go reading code would just be another concern at that point right it also had good teams in so also nice and the way the languages was defined and maintained by the core team was very nice open changes they are are now going to do introduce changes are already stable you can use one want which is a big bonus because we get to follow the and also.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:44:48Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"6f4b21e7a4cd429ea7ebc15e0466cfdd\"\n",
    "        ]\n",
    "    },\n",
    "    \"4\": {\n",
    "        \"segment0\": [\n",
    "            [\n",
    "                \"But those do not this Thursday they all implement these instances because then you try to between packages which is not great. I just wanted to use that package termination whether or not I take it to it in this system or not is a different. I just wanted to use that isolated manner and if it stop and software. So that ability was really nice which go really put thought out on and that allows get a lot of code in that manner that was very important at least for me there is very important and definitely simplicity that android division management to move it in.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:50:48Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"4716393165334ea1babd6349af31719c\"\n",
    "        ],\n",
    "        \"segment1\": [\n",
    "            [\n",
    "                \"And there was a lot of opinion views and how you should be writing good go there were things like go format VR respect to how you I mean this still are but it is not that back right like with respect to format my team style guide that you dont discuss the team is already opinion. Lot of things are pretty good things like in there are some inheritance which is when you are not a experience program or integrating them and understanding them understand things like you know solid principles etc takes a lot of time and takes a lot of experience for them to understand you know interface segments and cost principal etc. Then you should use a channel clas and you should do the parent clas cetera and that sometimes is not easily available. The clarify using composition over intelligence is very useful and the way goal does to is pretty nice that is very interesting the old font the language and does not have more than keywords. So this is very small language at its score. So there is useful and they also goes on explicit code right? So there were not things like lot of use of reflection at least every everywhere like would use an base programming like one would add in Java like frame. Basically next spring or where you use the like you use dependency injection to solve the dependency version and most of it was this random time dependency and spring is basically branded independently injection framework. So a lot of times I have seen there is a lot of complexity. I just how systems away and most of the time in go you just start provider it in the main you do not have to do a lot of the.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:51:34Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"24c32599e13b4ec299fc70c36276eb72\"\n",
    "        ],\n",
    "        \"segment2\": [\n",
    "            [\n",
    "                \"Most of the time every balance if we good going to write good more code that is but that is of the language. There is saying that do not use AI like banks injection as much responsive that is not two. So simplicity was very important like how the more simple code the more understanding to more reusable right and the concept using one defined interfaces with least number of methods in it was also good of this. So that you increase a usability of the code all is three times like interfaces revenue this remote part which we do now use a lot of effect we do now use a lot of to but when started we to so that is not like the biggest thing but was definitely going to yes not having to do corpus collection management not having to to yeah, the Java selection speed staging path and still less to this day never had a problem with asuming it I we are all the changes. So how quickly you get integrate through now definitely one thing I miss about what things like with the process with closure was an development you try to think of process you come up with the thought process you try to implement it just dynamically see if it works and you written the core is copied it and done so that kind of provided in the evaluation strategy and think the process solving problem is not still something that go application still not to so that is something I do definitely miss and alert seen that.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:53:34Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"e13950b5882f484e9f89e72d13a432ab\"\n",
    "        ]\n",
    "    },\n",
    "    \"5\": {\n",
    "        \"segment0\": [\n",
    "            [\n",
    "                \"I would want to negotiate it or would be for a lot of work like exact some some load where data a lot of places they do not need to info types around things or want force integrate. Now with the closure a really good content gradual typing. So it is essentially you can create it is still still of G everything just boils into a tight internally when it jump compliance by code but at the higher level. It is a dynamic program, you will language to the end user right? So when they they dont define types to everything so getting errors address is not so what they do is we can define like metadata on functions like crackers saying that this been engineers should be of this between this time I automate make errors is and top that background words. So that is pretty interesting but definitely would you flows for lot of work where I just straightforward code as data to lot of lead on of code possible and ID user doing data the team things on this like that maybe writing scheduled lot of complex code that you would not want to use like low level. I think that is definitely one thing and so that is pretty interesting like.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:55:38Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"1ad3240eb67d4a0ea34f871c5824379c\"\n",
    "        ],\n",
    "        \"segment1\": [\n",
    "            [\n",
    "                \"Failures cetera or you can use the messaging system which lacks and support on domain but definitely moving down even system system makes more sense than just going to something like right road above its serial I am not really opinionated that I am not really that on comments because at our because we do not have a lot of moving mobile parts in the part system. So definitely saving compute time by creating my of for utilization will not give us any benefit at this point or which will be to the point where users use any benefit the and is can benefit so it is hard to comment on whether we should have used in the beginning or in the beginning, but what we do some days we are thing about using for for concessions to of data metadata data like models all of that stuff like you know marketing heavy data like case and responses maybe total for Pierre stuff that may be something that we can explore but definitely for process communication will be core platform. It is not really something we looking into the other a problem with part of us was it least a lot of core generation. It is a good thing in a bad thing the good thing with systems is you just define back. Once generate clients every everywhere that everyone wants to use but the problem with I think things like there is a lot of more coordinated generated with respect to structures define just for that they can be receive.\"\n",
    "            ],\n",
    "            \"2019-07-24T17:00:52Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"25b34b08c838427c90e38deec43d321d\"\n",
    "        ]\n",
    "    },\n",
    "    \"7\": {\n",
    "        \"segment0\": [\n",
    "            [\n",
    "                \"Clarify before that we are not truly a model in the traditional sense as Google as or Facebook case because if you look at it back all of base everything is almost all open source integrations, but everything that is go to their system is in one single april and definitely as we mentioned they have have taken a lot of the done lot of special teams completely focused on improving the productivity developers developers working on main so that is definitely becomes a road at some features for a small complaint like us that we would not want to spend so much time on efforts on those things hooks on the post product now in that sense what so let us try to address with one by one. So what we do we do not have a traditional transfer not all every based in the same. So what we do is we separate the by that concern like by the team points are using it or core group of grouping and the high working up who users it and for what corpus so the front end has a own report but the front end related stuff which is for the mid use case I will not save for everything that we will eventually put some share how are the minor push it elsewhere, but we see that enough work that can use across up use cases for the core platform which completely in goal line at least ninety two right nine percent of it is in goal line that and platform related a P are all in a moral because the entire platform team, which is currently than five people need to maintain in all of that stuff. So if that is one of the use case, this is very essential to our company which is the AI engine right the so that we need to have mentioned respective access to who.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:11:58Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"fae2ecb9d0814afe95a84ad956988bb4\"\n",
    "        ],\n",
    "        \"segment1\": [\n",
    "            [\n",
    "                \"So access that code base and this will look at base so that becomes its own which is completely python icon as that is we thing most AI developers developers are comfortable with. So we chose a language based on the use case and choose to four days on the moment code to the so in that sense a lot of problems with Google and favor at with the structure management everything comes into problem when you are having to put code based of different management the same and that least a lot of hierarchy depth of going into code and looking at that things that brings a lot of complexity which we try to avoid by doing it this way and fourteen people that and of which like no team is there then five know would intent so right now this works well for us the second thing is to respect to managing that is definitely good point. So at least the thing that gets deployed most frequently for us is a platform and the front end code is now the front end goal base right now it is not totally more more or that we have separate approvals for IOs and separate the web, but we will a converge by use case like the mid case we have the first one into a own and share with shared the reach. They can use but in terms of their own bill everything they only choose to build the files that they need. They will not pas so the entire holder structure to read all the files they need to come. It is like they go from the index and they try to go to the graph and directory and.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:13:58Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"566413246226417c891f0be99701acf9\"\n",
    "        ],\n",
    "        \"segment2\": [\n",
    "            [\n",
    "                \"Go through the three and they try to resolve up it. So right now it is pretty fast at least from the front end point it is not go. Now this is where for the platforms has been a pretty good thing with one of the good productions go with going with the language here was good because go perspective of the size of the code base the time does not generally beyond five seconds for us and we have seen case studies and experience supports by different companies around their experiences writing large managing large base of code goal and how has been placing parts to them so for and concern but definitely something that will come into to column CEO steps right how do you test everything once you make change or you back to something that is is you know that has like permissions across different it is the code based or that is something we need to continue to work on, but even then one of the thing is in our systems based on the deployment that we have so we deploy the a or the slack or we deploy so that socket a gateway or the lambda functions. Whenever you be configure as such way when you trigger a push to one of those code basis. It basically runs a simple from the main main file for that specific base and try see that files or not at least that right now seems to be working but definitely and because lot of the code.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:15:58Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"92c33f93e4cf46a498dcbaedb2e5d02c\"\n",
    "        ],\n",
    "        \"segment3\": [\n",
    "            [\n",
    "                \"Shared I sort of on to almost everything but because we use go again the time everything seems to pretty pas so the test cases they are all run family so they do work but definitely if you look at in terms of size the code base there are two of big concerns with smaller post right one is size of the code base and growing test cases and time it takes so execute so many current across the code base. So five code is definitely concerned especially with the platform because this platform, but at least because platform. Keep deploying for the are more three countries in the who and deploying machine models frequently, then change. So that is a different concern there, but from platform benefits. There is a lot more code involved. So that is something that Google and Facebook have worked extensively on coming coming with a virtual file systems forget where they download specific parts of the core basis based on main stages are the teams who are on specific parts have that kind of sophistication right now and and and and I imagine by the time that we grow so big do such big team we probably ended up breaking the more we go down to services and asigning team because it is not generally at least a company of our sites to be with managing the tools in that question. Now definitely one can say that we can adopt tools that Google or Facebook built and that is something to us, but they not use enough ID to be able to commit to something at that skill this point I have looked at things like get file system by microsoft it only use works windows those.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:17:58Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"d1dc92db4bc04ebfa9a2f5dfd70aefc4\"\n",
    "        ],\n",
    "        \"segment4\": [\n",
    "            [\n",
    "                \"So and Google will also have certain time, but different of usage and terms of community support. So we will see where it goes but definitely gives the way we written our more core for that platform. We can easily break down like meeting service or transcription service is done. So on and deployment procedure and once we have a team. That is large enough to maintain that specific for the or the code goes too big for the go platform work on so that is how long break at least that is the thought process right now.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:19:58Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"14cebf347c434f3db6e126c5bcb706a1\"\n",
    "        ]\n",
    "    },\n",
    "    \"10\": {\n",
    "        \"segment0\": [\n",
    "            [\n",
    "                \"Language the way you handle database and like they sort of introduce all of the conferences that our database management into the language which is very incredibly useful like transactions in the language and instead using new Texas and and ability to and like persistent data fixes is very nice basically whenever muted is you will get a new Richard back just someone more like, but essentially was not and how it does make its own three et, I was pretty interesting and was thought out and it would definitely I would go back and I would not go back, but basically if the team grows and we able to split into smaller teams where we start a microphone services if I had to use under another language. I would definitely choose slow here as all of language because it was easy because you never muted code right in our muted by not to do it as as much possible mode which means there automatically reached leads to like code and code and you to get a history of how thing has seen to what time and all of this. So which is very incredibly useful like and sort of symptoms terms of how developers that ability so that definitely one choice. I would go for but definitely at that time it is not the direct as first because generally, the average time to get on onboard to be a good project developers around six months. So they months road media processing guys an understanding program language conference with I was one concern. Now go was very interesting in that order because.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:46:48Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"46e342ffb73945a0b8bbcdd0a41d9344\"\n",
    "        ],\n",
    "        \"segment1\": [\n",
    "            [\n",
    "                \"The core focus of the program language. It is very similar to see like emergency systems language, but it also has really good standard libraries for C T p for H C D B and our task and I could just go into the library and just look at the chronic and and what has being done by they need position with that. So code pretty high the concerns the other thing was definitely good was a really big role of the how come currency managed and go is very nice based on force paper communicating three its own thesis et cetera, but essentially the way you reason about how systems should interact between how different parts system can interact not to sharing memory your communicating is a very interesting in a good concept. Similarly to essentially very different, but different top using threats not using processes and the community was growing in I had a feeling that the community was kind was going to increase it especially in India on the word which it did there is is a good but to give the attraction that lot of companies had adopted it like Uber that and they are are locally go very vocal about their experience process go to be important, but one of the really good reasons choose go was that ability or that concept around implicit interfaces and since you the defined interfaces, which is very which is very an architecture way right you define interface inside the package you want to operate on and have when you acquiring system. You the inject implementation asociations of implementation.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:48:48Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"97ee876792fd4899b24fdca4423060c0\"\n",
    "        ]\n",
    "    },\n",
    "    \"13\": {\n",
    "        \"segment0\": [\n",
    "            [\n",
    "                \"Saying that the fundamentals in how you approach different these right as of concern. So database need to be separated to its own layer. It needs to be separated from the branch layer, which could be sheet we can we did not know because we have enough time to are so many different things that one shot, but if you choose to separate delays independently you could easily stop in our different different or at least progress in know unless but get a successful fashion to update migrate into the you could have a migration part flow to process somewhere right? So that is where we thought the key architecture that was as that has been spoken about for so long was useful in that process where they force use of but not frameworks right. So using the right interface was important for us. So it does not matter what remove it as long it user. The right set of abstractions to how doing so that is the first thing right and and it starts to choosing name model versus Micro service. Now once you have clean architecture in place you do not really need to think about whether guess when come to place there are automatically good design patterns and concerns that automatically get in force by which you can commit code to the team report at least before getting to tomorrow services and later decide whether they orders split them or move them based on deployment concerns or how they are scale. So what are concerns they were we could choose to move them out independently in a slow fashion.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:06:44Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"26696a3db44446bb83f99fa8dbd94c69\"\n",
    "        ],\n",
    "        \"segment1\": [\n",
    "            [\n",
    "                \"Because when we started we were about three people right. So three a first managing services is big problem because we seen that before when also so I think choosing like right design part concerns will definitely put a long way into deciding whether they want to remain in a model or local service. It is a model that is the right word model fine means that entire deployment sort of lies in one process, which it is not so we do separate the contents when is required like the end separated from the backend end the platform go platform some of the deployment deployment concerns like let us socket gateway or something else which requires that own scale concerns or maybe need to be exposed to the outside world, but does not need to track me they are generally they deprived separately this, but they do end up who be hosted in same people. So we follow model architecture see and domain driven design this help defining what bound context the systems interact with and how you go about creating like the this separating deployment or wiring audio system from the core business logical systems containing packages. So that they help us a lot way and we still in a on fashion at least for the platform front is definitely separated and is also a separate so the problem is now lets want to languages before I go to.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:08:44Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"a268ac1f9a774b3ca327ea43122bab65\"\n",
    "        ]\n",
    "    },\n",
    "    \"14\": {\n",
    "        \"segment0\": [\n",
    "            [\n",
    "                \"Layer and then you can in the service layer itself. You can have a multiple concerns one is we want to log certain things right we do not want to last you want to headers back up and log it in a common place. See you but this but when you do and just long you want the log and define sort things like watch service method you it. What are the input that you define how much time did take to perform just operating at the service layer not involving the time to transfer to the network or doing anything like that. So you have a mid very symbol right? So we want to push metrics business metrics or not is not the respect to end points. But this is metrics do you want to do some logging maybe be some so of several that it is where you put it. But you can have some user that a pattern as you want would stay where you pas the data interface how what the methods essentially this service should be doing on the piece of code like a meeting server should be doing and you wrap the same implementation of a logging system of the metrics publishing system and you ask them in words and the most thing would be the core so is that actually does the work. So so that is one level middle based the business or the service later then you back up to perform to think Singapore. Okay now dial dealing with how my system interact with other systems in the company and how it performs. What it needs to the status are the business requirements. Now how now is to now let us worry about how systems communicate with each other right. So then transfer put it layer and work. Now in that that you have to worry about okay, how will I transfer information between two services also.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:23:25Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"63f3a7e75044439fb4bc22fd06f54b1d\"\n",
    "        ],\n",
    "        \"segment1\": [\n",
    "            [\n",
    "                \"Company right need worry about Okay how would I create a representation of the clases of languages into a common save format that can be transferred over the wire. Maybe I strong representation or by representation whatever it. We so that other systems can understand into and interest and perform some other of compettion and give back and talk to each so those concerns definitely come up with the transitions we need to worry about what protocol you want to use the time to talk to between services. What is the utilization you want format much wanna is protocol buffer and they want to use R P C or case an whatever it is so you move those up to a layer about and you have interfaces around that right. So one should be easily be able to swap out an implementation with the implementation without having to change the systems the service layer positively layer. The cash systems are any business logically. So you want to when you make it change your goal should be not or your changes down to every part system right if you are touching layer the changes is that you implement should only touch that layer so that you do not have to test every products of that by making a code service is robust right so that is the concern of the transition there. So how you ideally do these of stuff now having now now when we are in the service layer or the controller earlier layer you do do some I related work you do talk to other services you talk to the databases you talked to cash systems.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:25:25Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"7513544127fe4fb8a57cf4808d960256\"\n",
    "        ],\n",
    "        \"segment2\": [\n",
    "            [\n",
    "                \"Now how we do that how do you do that because you can have this fund implementation we have the so many database to use right now my sequel or you can even now the problem is if you commit yourself to one database and actually write this queries in one directly in the controller the problem that it very hard to couple of your database level concerns on a code business level in the controller level right. So what one does is move create from interfaces around how your service interact the data database later we create the database active objects just the technical terms that people use so database access to sorry. Let is say for example and leaving with the so entty call meeting some of the common tasks that would expect to do with the would be to get a meeting by identifier to store the meeting object if I give it and two be able will do query meeting days on certain attributes if I need to which again, those interfaces. Sometimes can be very databases certain databases. Sometimes cannot this important for one to think about can I use a light while you store in general, but sometimes you might have to query by certain with different differently in narrow. You might want to have a initial index which table with an index on which is your one in base systems you can just maybe just create an directly understand table itself but that dollar consider the database how you optimized query inside the database should be separated from your service layer. So that is why you can create that kind of way.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:27:25Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"1058c8d49d70468babb600d4e25fb928\"\n",
    "        ],\n",
    "        \"segment3\": [\n",
    "            [\n",
    "                \"And you talk to those things through that kind of interface. So can just when you wiring your service or application. We can just initiate a different implementation with call repository pattern. We can see the different implementation may see instead of both move to my sequel data organizations. We concern but once we do that you can create in translation on my sequel repository and and just it to the and of your service. So internally product intel where does not seem because the scope database. So isolate testing by the layers that you want that are concern with it that point of the architecture. So because we do that if right and maintain maintaining you just by layers and you can if you may you change latest a specific specific component and does not format. So you do not have to worry. So in a way you can say the marker for good software is by how many lines of code or how many lines of core added, but more importantly how many clients of code not altering it every i.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:29:25Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"9032f6520e014231967dd5f9dfbf1fee\"\n",
    "        ],\n",
    "        \"segment4\": [\n",
    "            [\n",
    "                \"That is say from why understanding is the importance of choosing from the making architecture because we are separating different concerns. Now as I mentioned how different systems need to talk to each other. Now that is that as a problem right. So generate arranged approach people tend to be each new entty its own service just because this thing that is the way it should be, but does not have to be right? So you need to figure out how different parts system can work independently from each other and what system parts need to talk to other parts system right so you try to in your mind right to form abound context. What are the roles and responsibility for a ttle system and you define whether the roles in response and you try to fix it to some extent and if decide whether it needs or does not meet those constraint and decide to break the system down or to create it on create separate system whether constant and have interact between things. So to give an example, list like at least in the ruby, you would have a forward all models right and you put the meeting entty put the transcription entty you put like contextual core object like an entty detail into the same folder essentially and by doing so what is happening is within that photo people do not try to create tight between those entties. This is really the is this one and there. It is easier to paint the relations to things.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:30:44Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"38e098c22ee2468983f3d7021cdfbc0a\"\n",
    "        ],\n",
    "        \"segment5\": [\n",
    "            [\n",
    "                \"So once you do because seems hard to break the system. Now because there there can be sometimes dependency that client is is support, but we just will not be able to figure how to bring this now right? So one thing that is important is figure out okay what entty should be isolated from what we are the entties and what were like higher level entties high deposit that need to talk to lower levels systems there to common an operation. So in my mind I try to think of it as zero one to services where zero means complete we operate on the input. They do not talk to anything else tier one basically auto to one or two loud mobile less. Well will we talked to lower and zero or one different. So so he try to buy different systems based and me from what environments. It is important to move systems around to pay to the bottom case much as possible. So that they are more reusable but sometimes it is not possible. So to give an example, there are far system where in our company that want to build the platform right our core l related services are t zero and they to operate like a zero or at to some extent one service depending talk to some other services, but they should never go beyond that so because we want high the usability of those things. So we try to figure out okay we want a contextual score you want a keyword factor we have maybe transcription engine. We have use service and we have some other services that know other service in the same level should in the same can talk to only service in the same level but not.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:32:44Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"edf791e6d3304a0fa24a7a3748d4ae43\"\n",
    "        ],\n",
    "        \"segment6\": [\n",
    "            [\n",
    "                \"So we try to create that kind of boundary so that anyone opening on that if you want to build that is use case it just clarity high level service that why is systems together or a group of one tier two services that why I talked to different services in a different manner, but end up reducing most of the components right so to be able to do that you will define out team what are the roles in the responsibilities of a context is quarter and how it from perspective how does it from a indian. So you try to Pakistan in its small folder including how it interact with each other et cetera and lot of that comes in with beginning the domain design and also some other countries like patterns parts sort of also going with some of the cleanup architecture conferences like create repository around things we do not want to interact directly that databases etcetera and use them accordingly create separate the business concern from the obviously that is also very important right. Let is say if say maybe determined mind. What is the next transcription for meeting right you would try to go get the current meeting based ID that the servers will do get the meeting object and then it might talk to some other service also to get some more information, then it will pas it down to the business package the meeting or domain layer here here are the data that has gathered for you go come to the next transition that you need to make and give me all the data that you will generate for me to work on operate on those phones. So those are all IO completely non ios based decisions that a system should be doing those are like business operations lot of can put them and.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:34:44Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"48dd9386be754c3d92dffb298350f3e4\"\n",
    "        ],\n",
    "        \"segment7\": [\n",
    "            [\n",
    "                \"Company right need worry about Okay how would I create a representation of the clases of languages into a common save format that can be transferred over the wire. Maybe I strong representation or by representation whatever it. We so that other systems can understand into and interest and perform some other of compettion and give back and talk to each so those concerns definitely come up with the transitions we need to worry about what protocol you want to use the time to talk to between services. What is the utilization you want format much wanna is protocol buffer and they want to use R P C or case an whatever it is so you move those up to a layer about and you have interfaces around that right. So one should be easily be able to swap out an implementation with the implementation without having to change the systems the service layer positively layer. The cash systems are any business logically. So you want to when you make it change your goal should be not or your changes down to every part system right if you are touching layer the changes is that you implement should only touch that layer so that you do not have to test every products of that by making a code service is robust right so that is the concern of the transition there. So how you ideally do these of stuff now having now now when we are in the service layer or the controller earlier layer you do do some I related work you do talk to other services you talk to the databases you talked to cash systems.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:25:25Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"7513544127fe4fb8a57cf4808d960256\"\n",
    "        ]\n",
    "    },\n",
    "    \"15\": {\n",
    "        \"segment0\": [\n",
    "            [\n",
    "                \"Meeting now it to schedule go the recording now instance for the recording the media server to go create media entty and maybe six server to and aggregate type for holding for identifying a bunch just a couple task segments by a common. So I do all of the stuff. So it tries to change bunch of operations that would generally have to do across ourselves send one place right ensure that they happen in a specific recorder of does not initially have to do chain by even if tries to make sure all of them happen if I app we can try to figure our role back by calling the corporate back on the services that this might be required. So that kind of that is one thing we have had to do right now respect to handling know of the challenges just not going with all of the initially, but given but we made a context to not worry what at that time because most the the systems that we will do not have to responding they eventually consistent. So as better mostly able systems in that map.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:38:44Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"57a185e4f25c461dbb6802cff67d5500\"\n",
    "        ],\n",
    "        \"segment1\": [\n",
    "            [\n",
    "                \"Going with the language now we started the company around two thousand seventeen twenty seventeen like two and more in the language a lot of what the language is import to look at the everything team to see how quickly we can go prototype implementation and to see like whether all developers will be able to comfortably work with the language and try good in being both in that manner at that point setting it did try to validate different languages. I try to evaluate and I look that language is super Awhile Java and bit closure also which sort of real cloud and upload that go and link out to some extent. Now let me talk about little little bit about why rejected few of them but the biggest important part was when we started the bill. It is important for us to maintain a velocity that we have this worry about operational warehouse supporting the platform that supports the language. We make sure developers are comfortable are able to treat the code as a code case and do not have issues core right. There is no there is not a profession going on is not enough things that they do not understand and try to you notice to manage they can just go jump and see what happens and make call okay explicit enough for me to understand all system works and I will trying to look at a language that allowed me to create systems that that allow to help create software that was that can be easily.\"\n",
    "            ],\n",
    "            \"2019-07-24T16:40:44Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"9a2d50dfc5b04cc980371ebf079bc4f5\"\n",
    "        ]\n",
    "    },\n",
    "    \"1\": {\n",
    "        \"segment0\": [\n",
    "            \"Speaking no testing one two three and.\",\n",
    "            \"2019-07-24T15:59:58Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"ae396e02e5084d9484969032d8b6067c\"\n",
    "        ]\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"segment0\": [\n",
    "            \"\",\n",
    "            \"2019-07-24T16:00:02Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"2a5a387ba5884ec49a7e92b35b4383ad\"\n",
    "        ]\n",
    "    },\n",
    "    \"6\": {\n",
    "        \"segment0\": [\n",
    "            \"I want say all the same the same way from my experience where you form you to an five kind of pattern model will be controller with the problem as a controller at least there was no strict guidelines on how the database interactions should be done and what are the business how do we update the some database functions from business logic concerns. So most of the time once point and that when we were dealing with managing you model we almost never want to touch any database query because we did not know what the permissions were across the work right because directly call from the controller and just who join slack right and you would to know what is break at any point in time. So the problem was be able to handle code that changes is so frequently that code that pro so quickly developers code be able to make a progress in the same manner as we used to and at a level in a of company that we were where we about at least whether the use case was defined or not we knew that he wanted to build a platform around a round and registry initially. So we knew that we need some things flexible enough to work for different cases and definitely commit to a framework a sort of I guess load on the progress as we once get started. So what is more important for us was not to use screen, but also to use interfaces like gateways are around different parts of the system where it can easily log in way or swap and swap our implementation and is easily move forward quickly like that was the more important and that is one of the fundamentals of for disclose the.\",\n",
    "            \"2019-07-24T16:04:36Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"6935c0c8e82c4af5ac6f8a7d46e38fab\"\n",
    "        ]\n",
    "    },\n",
    "    \"8\": {\n",
    "        \"segment0\": [\n",
    "            \"\",\n",
    "            \"2019-07-24T16:06:41Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"eb37e78f10f049748ddcebe148d08b1b\"\n",
    "        ]\n",
    "    },\n",
    "    \"9\": {\n",
    "        \"segment0\": [\n",
    "            \"Scan of dig to that it because it is a interesting subject right perspective. For example seamlessly I two company right base all developers developers get access to the entire base, you know build the whole thing together deploy so and so forth but they also put in a lot of work to own built i do not have to build everything right? So you can share working piece of the core so they put in a lot of work for it to more right, but as some know okay, obviously for us build are not yet concerned. It is not problem but as we grow the let us say we start to do whole bunch service that do need to have separate the databases on so of level of service and so on and so forth. We still see them that continuing to be a moment or do things that we can and of what right.\",\n",
    "            \"2019-07-24T16:10:41Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"daebdb93723d44caad387093cd91ba77\"\n",
    "        ]\n",
    "    },\n",
    "    \"11\": {\n",
    "        \"segment0\": [\n",
    "            \"More discussion one more question you talked about about like architecture domain driven design. We what twelve apps bunch of things right. So when we look at architecture paradigm can you maybe give an example of how we are playing each one of those principles in our platform like just give one example of for example to domain design like where will create it and change it no architecture right.\",\n",
    "            \"2019-07-24T16:20:48Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"968968d3834847bea1d6d5e953323455\"\n",
    "        ]\n",
    "    },\n",
    "    \"12\": {\n",
    "        \"segment0\": [\n",
    "            \"At least clean architecture domain design are somewhat similar yet they are different. So the clean architecture domain design essentially the idea behind that is the philosophy that you have domain experts in know really well about how systems work especially any commerce platform. Youre have in monthly products in inventory units et cetera around those jargon should be introduced in the system and we use the way it meant to be used like in a traditional supply same. So you have to domain person or something about just mean you have developers developers who can write code. So it was to bring that knowledge on developers or to sort reach that gap and I have them usually work on the business logic that is one other core pieces clean architecture. What it was was template essentially of how one should be thinking about affecting your business from code business data that only updates on changes in the business logic or things like is the lowest level unit basically lowest level unit that the knowledge of the right and then you sort of add layers on top of them about maybe you can have a business where you give all the data the compettion gives you some insights or something and that you need to operate on then you have them around me a service layer or a controller basically that talks to different features of the system. Maybe you want to talk to a database you want to talk to cash system going to do some io. We will our some services to get some data you to gather all the information that you want and send it down to the layer down to perform the compettion give you the result that we can probably back words that is like the control or the service we call this.\",\n",
    "            \"2019-07-24T16:21:25Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"0dd153c300eb4684b0d6a592e305cd6d\"\n",
    "        ]\n",
    "    },\n",
    "    \"16\": {\n",
    "        \"segment0\": [\n",
    "            \"\",\n",
    "            \"2019-07-24T16:30:42Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"6ebabde2b54e4137abce3d422523c54a\"\n",
    "        ]\n",
    "    },\n",
    "    \"17\": {\n",
    "        \"segment0\": [\n",
    "            \"So you have a meeting package as we have transcript like that. So that sort of I iO from the core important parts of system right. So that is also some important link domain and it is also good back especially you see in option is where try to separate Io from just less the because network or any kind of other kind of that so they just on input and give a conversation result and it choose operate decide. What we do based on the results that you get and even though and can even go down to the latest. So that is also part the domain driven design. Now definitely one of the problems are I was call it all but some of the challenges with building something like a or the system that we built right now is transactions are extremely heard distributed transactions pretty hard is we can just supply ID acquire a database lock or a transaction log with an excellent. We can just do a computer role, but if you want to do which given the nature of the system that we are building is it does not require that amount of consistency just just we lucky in that manner, but in some place you been my so we end up having to create another layer of sections or things you tag up bton and like that where you want to have a some an might use and responsibilities, but they operate on the events. The system generates try to tie different services to get the and tries to hold information and generate new right for the track on and form that kind so they of old okay I need the talk a meeting service creating.\",\n",
    "            \"2019-07-24T16:36:44Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"55861b2d9f574b348c65550ccc1f34fb\"\n",
    "        ]\n",
    "    },\n",
    "    \"18\": {\n",
    "        \"segment0\": [\n",
    "            \"Maybe we have bought like fifteen ten fifteen minutes. Let is talk about the choice of languages right yeah, right forward with.\",\n",
    "            \"2019-07-24T16:40:09Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"1661ebce84074a118425d05c7e1ad8c9\"\n",
    "        ]\n",
    "    },\n",
    "    \"19\": {\n",
    "        \"segment0\": [\n",
    "            \"\",\n",
    "            \"2019-07-24T16:40:41Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"cf3d5599749546509bdbe2cec752526b\"\n",
    "        ]\n",
    "    },\n",
    "    \"20\": {\n",
    "        \"segment0\": [\n",
    "            \"\",\n",
    "            \"2019-07-24T16:42:44Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"41856466b6a2446186b59ba3e7e08473\"\n",
    "        ]\n",
    "    },\n",
    "    \"21\": {\n",
    "        \"segment0\": [\n",
    "            \"\",\n",
    "            \"2019-07-24T16:51:30Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"3ec2531fb9924c21a48b52d9f03a1776\"\n",
    "        ]\n",
    "    },\n",
    "    \"22\": {\n",
    "        \"segment0\": [\n",
    "            \"We will write to do you have?\",\n",
    "            \"2019-07-24T16:55:30Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"739f43d08a88440188dac195667ab2b3\"\n",
    "        ]\n",
    "    },\n",
    "    \"23\": {\n",
    "        \"segment0\": [\n",
    "            \"There like Resource Manager example, resource i.\",\n",
    "            \"2019-07-24T16:57:08Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"544209b4f97d4556a7ce5136e782438d\"\n",
    "        ]\n",
    "    },\n",
    "    \"24\": {\n",
    "        \"segment0\": [\n",
    "            \"Use them for that I will necessarily want to use it for simple services when you try to break the system around the small services you can use this from half in is something wrong with it, but is is that with proper with go maybe people are more use of the work with this memory case his binary all of those stuff. Yeah, right that part is definitely a lot of work to manage with the not respect to that is the biggest concern now with and advantage in the state. We forgetting native images is going to improve is going to take some time to piece tables. So we can be with that once they come up with a good skill all right.\",\n",
    "            \"2019-07-24T16:57:14Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"6faf584e372a40c18544005f78639f6b\"\n",
    "        ]\n",
    "    },\n",
    "    \"25\": {\n",
    "        \"segment0\": [\n",
    "            \"I have an architecture call from language what about things like you know communication into service process communication right. So we obviously rest right internally we just the calls we top offline should we be thinking about moving to brought of and and you ask you apIs and our client consume.\",\n",
    "            \"2019-07-24T16:58:08Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"0764775efcca4fc8a5a9f31fb3191be1\"\n",
    "        ]\n",
    "    },\n",
    "    \"26\": {\n",
    "        \"segment0\": [\n",
    "            \"So we problem with so I me.\",\n",
    "            \"2019-07-24T16:58:47Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"fad94c2443544bd8829d2b45e2276446\"\n",
    "        ]\n",
    "    },\n",
    "    \"27\": {\n",
    "        \"segment0\": [\n",
    "            \"Go one by one so we do not use rest in a suggestions sense we do use Json civilization, but the way which somewhat like our PC kind of type basic of pasion and it is that or slack APi essentially if go to slack platform domestic try to that concept, but eventually beat is going to be a very heavily event system right and we are already removing to heavy event system. I think so there is not a lot of communication request their reply and like use across the board so what we and and that a most scalable of approach is to use and even design pattern for system communication and handling to changes systems. So for that we need none the that all requests reply kind of items right or all that what we need is so we use that which is the system that we use for to support request like also that it the good thing about it is it does support responsive, but more importantly we use it for messages and aiming that is so what we should be doing for all internal communication. We should be using which is more reliable for sending for creating more events and having with and on them. What would be interesting is to create a small server system that would just subscribe all the topics and lax and you create an event log that one can just read from and you will create like it is on use the data for whatever you it needs to do like creating state back in case of.\",\n",
    "            \"2019-07-24T16:58:52Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"b15cfcd972a8461cb108979ef0b12788\"\n",
    "        ]\n",
    "    },\n",
    "    \"28\": {\n",
    "        \"segment0\": [\n",
    "            \"Publish eight is outside yeah, but we at that point given the know the makes easy for people to just item so sixteen about that.\",\n",
    "            \"2019-07-24T17:02:52Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"bdbcddb8dc764343ade5158e751e74d1\"\n",
    "        ]\n",
    "    },\n",
    "    \"29\": {\n",
    "        \"segment0\": [\n",
    "            \"Still evolving so those who want directly it on the browser definitely will have something right it is evolving its it is going and the authentication mechanism is are very opportunity. It will also improve but basically essentially we can even use certificates and create roles in scope and does that value whether discussions should be lot to access et etc, but generally right now at this point, it is useful for process communication playlist somehow you want to have this time start to each with that worry a call pause get pads delete and what happens for us our PC was important call then using this because it is not a lot of data end client gives us we have a lot of transcript system and we are very specific specific inputs that we request and we do a lot of combination on that January new data rather then is blindly taking some input and updating on the database. If you almost never do so that was never really that useful for us to begin with in terms of the traditional center. How people can do use it like systems is not useful for us, but generally still if you look at the a problem of is protocol making testing things out is not easy right and sense that you do not have a simple call replacement for I can just not give them like a maybe flag that maybe be my open i said they might be supporting it now but we can cannot just simply give you like a request to just test it out and say on that. So that is far reasons why like things like platforms slack and other companies have not been explored supporting this at least from the end a Google. I think is the only something course and solution. That is also because the are the ones so.\",\n",
    "            \"2019-07-24T17:03:08Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"0b09291a0d9343e9adac1c057ef4a8e6\"\n",
    "        ]\n",
    "    },\n",
    "    \"30\": {\n",
    "        \"segment0\": [\n",
    "            \"Taken it so it is important to way are the framework also more efficient and.\",\n",
    "            \"2019-07-24T17:05:08Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"17dafeb21023497b96376b8a950a84ca\"\n",
    "        ]\n",
    "    },\n",
    "    \"31\": {\n",
    "        \"segment0\": [\n",
    "            \"Moving text around when you moving to lot data the like Google definitely and Google does not have.\",\n",
    "            \"2019-07-24T17:05:16Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"7e6a6d9440094973a431f6babd1febe0\"\n",
    "        ]\n",
    "    },\n",
    "    \"32\": {\n",
    "        \"segment0\": [\n",
    "            \"So there is a lot of chatter chase chatter that goes on between systems to give you the end result so definitely that that is one of that is probably the only reason entty use protocol offers efficient because are like compute cycles it is overall and but generally text is good enough from scale or even general companies. I have something to test live wrong, but you know it is just like an efficient if we super fast in memory database users text simple text representation of items to down the data is still extremely fast. It is very hard to argue the case of just for being the supporting manually protocol you might want to use the or at this point and respect is also and getting updated. So for above three is just released and they are working on four and they trying to improve the productive of how you define this thoughts so it is worth waiting and seeing how the thing was commit you need a lot more.\",\n",
    "            \"2019-07-24T17:05:28Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"e82737e6816f49579b4c621c2f6216dd\"\n",
    "        ]\n",
    "    },\n",
    "    \"33\": {\n",
    "        \"segment0\": [\n",
    "            \"In the end of coming to it.\",\n",
    "            \"2019-07-24T17:06:50Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"ddce8f8f6fb54a2880b00808f4b9bfa6\"\n",
    "        ]\n",
    "    },\n",
    "    \"34\": {\n",
    "        \"segment0\": [\n",
    "            \"Well, I did look at into it, but it just generates a lot of complexity at least in the platform side essentially it is very hard tell you have to create resolve words and you to sort of top of business logic and they are database interactions then you have to use the extreme work like it is on framework. We use a libraries we support use it.\",\n",
    "            \"2019-07-24T17:06:58Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"e405ac95cd0b4f96b983ff015d2d6041\"\n",
    "        ]\n",
    "    },\n",
    "    \"35\": {\n",
    "        \"segment0\": [\n",
    "            \"Hours a very simple in nature yeah, Okay, especially the ones that lines concern like a big comment application where you need to pull data from high different services you know, quite the long them and all that and we get them and change them all definitely i mean there is not a lot of.\",\n",
    "            \"2019-07-24T17:07:30Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"bc0c0982eab143588261c640906893e4\"\n",
    "        ]\n",
    "    },\n",
    "    \"36\": {\n",
    "        \"segment0\": [\n",
    "            \"Dynamic quitting and efficient related of that client and client us and there is not a lot of information they just can just write to the database from graph also we always do a lot of state detection transitions and then give the data. So it is important to keep the code more and more like extreme.\",\n",
    "            \"2019-07-24T17:07:52Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"4fa758f2a1324f69b68881d224a4f1c6\"\n",
    "        ]\n",
    "    },\n",
    "    \"37\": {\n",
    "        \"segment0\": [\n",
    "            \"Easy to build yeah yeah, definitely we basic can we used like maybe.\",\n",
    "            \"2019-07-24T17:08:20Z\",\n",
    "            \"8d6db5f7d9b74c54ba38fe710ffcaf3f\",\n",
    "            \"c642febac6d043b087c953f37cbea320\"\n",
    "        ]\n",
    "    },\n",
    "    \"38\": {\n",
    "        \"segment0\": [\n",
    "            \"You can create the different implementation like in server connecting to post is different separately and drive graphical queries to look at the data we will and different manner more efficiently one one can start with and then understand all better project graphical and then go on to see how we can fit in front.\",\n",
    "            \"2019-07-24T17:08:26Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"c2072c68c3f943098b71ea215e70967e\"\n",
    "        ]\n",
    "    },\n",
    "    \"39\": {\n",
    "        \"segment0\": [\n",
    "            \"Karthik I think we covered bunch of.\",\n",
    "            \"2019-07-24T17:08:52Z\",\n",
    "            \"62b6ae1d7f834b0bb2055f7c72bc3368\",\n",
    "            \"e2edbc50987b4508b489150c909f65b1\"\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topics = {}\n",
    "topics['topics']=[]\n",
    "for i in test:\n",
    "    if (len(test[i]))>=4:\n",
    "        new_topic={}\n",
    "        new_topic['id']=test[i]['segment0'][3]\n",
    "        new_topic['text']=test[i]['segment0'][0]\n",
    "        new_topic['authors']=test[i]['segment0'][2]\n",
    "        new_topic['authoredAt']=test[i]['segment0'][1]\n",
    "        topics['topics'].append(new_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(topics['topics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# similarity comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T12:17:36.655469Z",
     "start_time": "2019-07-30T12:17:36.625530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sa = []\n",
    "for edgea, edgeb, weight in tg.edges.data():\n",
    "    #print (\"Text A: \")\n",
    "    #print (mod_texts[edgea])\n",
    "    #print (\"\\n\\nText b: \")\n",
    "    #print (mod_texts[edgeb])\n",
    "    #print (\"\\n\\nsimilarity score:\", weight['weight'])\n",
    "    #print (\"\\n\\n\\n\")\n",
    "    sa.append((edgea, edgeb, weight['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T12:18:03.872790Z",
     "start_time": "2019-07-30T12:18:03.846212Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sa_sorted = sorted(sa, key=lambda kv: kv[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T14:48:14.254358Z",
     "start_time": "2019-07-30T14:47:46.504583Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for edgea, edgeb, weight in sa_sorted:\n",
    "    print (\"Text A: \")\n",
    "    print (mod_texts[edgea], edgea)\n",
    "    print (\"\\n\\nText b: \")\n",
    "    print (mod_texts[edgeb], edgeb)\n",
    "    print (\"\\n\\nsimilarity score:\", weight)\n",
    "    print (\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T12:34:14.008571Z",
     "start_time": "2019-07-30T12:34:14.003526Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tg.get_edge_data(36,94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T11:21:05.539897Z",
     "start_time": "2019-07-30T11:20:45.291547Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for edgea, edgeb, weight in tg.edges.data():\n",
    "    print (\"Text A: \")\n",
    "    print (mod_texts[edgea])\n",
    "    print (\"\\n\\nText b: \")\n",
    "    print (mod_texts[edgeb])\n",
    "    print (\"\\n\\nsimilarity score:\", weight['weight'])\n",
    "    print (\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "nteract-on-jupyter@2.1.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
