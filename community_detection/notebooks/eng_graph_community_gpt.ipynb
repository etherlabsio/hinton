{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T06:56:24.817955Z",
     "start_time": "2019-06-11T06:56:20.999861Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "import nltk\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import combinations, product\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from pytorch_pretrained_bert import (OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
    "                                     OpenAIAdam, cached_path, WEIGHTS_NAME, CONFIG_NAME)\n",
    "from pytorch_pretrained_bert.modeling_openai import OpenAIGPTPreTrainedModel,OpenAIGPTDoubleHeadsModel,OpenAIGPTConfig,OpenAIGPTModel,OpenAIGPTLMHead\n",
    "\n",
    "from scipy.spatial.distance import cosine, cityblock\n",
    "import iso8601\n",
    "from datetime import datetime\n",
    "import json\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIGPTLMHead_custom(nn.Module):\n",
    "    \"\"\" Language Model Head for the transformer \"\"\"\n",
    "\n",
    "    def __init__(self, model_embeddings_weights, config):\n",
    "        super(OpenAIGPTLMHead_custom, self).__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.predict_special_tokens = config.predict_special_tokens\n",
    "        embed_shape = model_embeddings_weights.shape\n",
    "        #print(\"shape check\",(model_embeddings_weights[1]))\n",
    "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
    "        self.set_embeddings_weights(model_embeddings_weights)\n",
    "\n",
    "    def set_embeddings_weights(self, model_embeddings_weights, predict_special_tokens=True):\n",
    "        self.predict_special_tokens = predict_special_tokens\n",
    "        embed_shape = model_embeddings_weights.shape\n",
    "        self.decoder.weight = model_embeddings_weights  # Tied weights\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "#         print('decoder weight')\n",
    "#         print((hidden_state.shape))\n",
    "        lm_logits = self.decoder(hidden_state)\n",
    "#         print(lm_logits.shape)\n",
    "        if not self.predict_special_tokens:\n",
    "            lm_logits = lm_logits[..., :self.vocab_size]\n",
    "#             print(\"lm_logits.shape: \",lm_logits.shape)\n",
    "        return lm_logits\n",
    "\n",
    "class OpenAIGPTMultipleChoiceHead_custom(nn.Module):\n",
    "    \"\"\" Classifier Head for the transformer \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(OpenAIGPTMultipleChoiceHead_custom, self).__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = nn.Dropout2d(config.resid_pdrop)  # To reproduce the noise_shape parameter of TF implementation\n",
    "        self.linear = nn.Linear(config.n_embd, 1)\n",
    "\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, hidden_states, mc_token_ids):\n",
    "        # Classification logits\n",
    "        # hidden_state (bsz, num_choices, seq_length, hidden_size)\n",
    "        # mc_token_ids (bsz, num_choices)\n",
    "        mc_token_ids = mc_token_ids.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, hidden_states.size(-1))\n",
    "        # (bsz, num_choices, 1, hidden_size)\n",
    "        #print('mc_token_ids: ', mc_token_ids[0][0].shape,mc_token_ids[0][1].shape)\n",
    "        #print('mc_token_ids.shape: ', mc_token_ids.shape)\n",
    "        #print('Hidden states before compute: ', hidden_states.shape)\n",
    "#         print(\"Token IDS:\",mc_token_ids)\n",
    "        multiple_choice_h = hidden_states.gather(2, mc_token_ids).squeeze(2)\n",
    "        #print('After transformation: ', multiple_choice_h.shape)\n",
    "        # (bsz, num_choices, hidden_size)\n",
    "#         multiple_choice_h = self.dropout(multiple_choice_h.transpose(1, 2)).transpose(1, 2)\n",
    "#         multiple_choice_logits = self.linear(multiple_choice_h).squeeze(-1)\n",
    "        # (bsz, num_choices)\n",
    "        return multiple_choice_h\n",
    "\n",
    "class OpenAIGPTDoubleHeadsModel_custom(OpenAIGPTPreTrainedModel):\n",
    "    \"\"\"\n",
    "    OpenAI GPT model with a Language Modeling and a Multiple Choice head (\"Improving Language Understanding by Generative Pre-Training\").\n",
    "    OpenAI GPT use a single embedding matrix to store the word and special embeddings.\n",
    "    Special tokens embeddings are additional tokens that are not pre-trained: [SEP], [CLS]...\n",
    "    Special tokens need to be trained during the fine-tuning if you use them.\n",
    "    The number of special embeddings can be controled using the `set_num_special_tokens(num_special_tokens)` function.\n",
    "    The embeddings are ordered as follow in the token embeddings matrice:\n",
    "        [0,                                                         ----------------------\n",
    "         ...                                                        -> word embeddings\n",
    "         config.vocab_size - 1,                                     ______________________\n",
    "         config.vocab_size,\n",
    "         ...                                                        -> special embeddings\n",
    "         config.vocab_size + config.n_special - 1]                  ______________________\n",
    "    where total_tokens_embeddings can be obtained as config.total_tokens_embeddings and is:\n",
    "        total_tokens_embeddings = config.vocab_size + config.n_special\n",
    "    You should use the associate indices to index the embeddings.\n",
    "    Params:\n",
    "        `config`: a OpenAIGPTConfig class instance with the configuration to build a new model\n",
    "        `output_attentions`: If True, also output attentions weights computed by the model at each layer. Default: False\n",
    "        `keep_multihead_output`: If True, saves output of the multi-head attention module with its gradient.\n",
    "            This can be used to compute head importance metrics. Default: False\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length] with the BPE token\n",
    "            indices selected in the range [0, total_tokens_embeddings[\n",
    "        `mc_token_ids`: a torch.LongTensor of shape [batch_size, num_choices] with the index of the token from\n",
    "            which we should take the hidden state to feed the multiple choice classifier (usually last token of the sequence)\n",
    "        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n",
    "            with the position indices (selected in the range [0, config.n_positions - 1[.\n",
    "        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n",
    "            You can use it to add a third type of embedding to each input token in the sequence\n",
    "            (the previous two being the word and position embeddings).\n",
    "            The input, position and token_type embeddings are summed inside the Transformer before the first\n",
    "            self-attention block.\n",
    "        `lm_labels`: optional language modeling labels: torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n",
    "            with indices selected in [-1, 0, ..., total_tokens_embeddings]. All labels set to -1 are ignored (masked), the loss\n",
    "            is only computed for the labels set in [0, ..., total_tokens_embeddings]\n",
    "        `multiple_choice_labels`: optional multiple choice labels: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_choices].\n",
    "        `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
    "            It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
    "    Outputs:\n",
    "        if `lm_labels` and `multiple_choice_labels` are not `None`:\n",
    "            Outputs a tuple of losses with the language modeling loss and the multiple choice loss.\n",
    "        else: a tuple with\n",
    "            `lm_logits`: the language modeling logits as a torch.FloatTensor of size [batch_size, num_choices, sequence_length, total_tokens_embeddings]\n",
    "            `multiple_choice_logits`: the multiple choice logits as a torch.FloatTensor of size [batch_size, num_choices]\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into BPE token ids\n",
    "    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]]])  # (bsz, number of choice, seq length)\n",
    "    mc_token_ids = torch.LongTensor([[2], [1]]) # (bsz, number of choice)\n",
    "    config = modeling_openai.OpenAIGPTOpenAIGPTMultipleChoiceHead_customOpenAIGPTMultipleChoiceHead_customConfig()\n",
    "    model = modeling_openai.OpenAIGPTDoubleHeadsModel(config)\n",
    "    lm_logits, multiple_choice_logits = model(input_ids, mc_token_ids)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(OpenAIGPTDoubleHeadsModel_custom, self).__init__(config)\n",
    "        self.transformer = OpenAIGPTModel(config)\n",
    "        self.lm_head = OpenAIGPTLMHead_custom(self.transformer.tokens_embed.weight, config)\n",
    "        self.multiple_choice_head = OpenAIGPTMultipleChoiceHead_custom(config)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def set_num_special_tokens(self, num_special_tokens, predict_special_tokens=True):\n",
    "        \"\"\" Update input and output embeddings with new embedding matrice\n",
    "            Make sure we are sharing the embeddings\n",
    "        \"\"\"\n",
    "        #self.config.predict_special_tokens = self.transformer.config.predict_special_tokens = predict_special_tokens\n",
    "        self.transformer.set_num_special_tokens(num_special_tokens)\n",
    "        self.lm_head.set_embeddings_weights(self.transformer.tokens_embed.weight, predict_special_tokens=predict_special_tokens)\n",
    "\n",
    "    def forward(self, input_ids, mc_token_ids, lm_labels=None, mc_labels=None, token_type_ids=None,\n",
    "                position_ids=None, head_mask=None):\n",
    "        hidden_states = self.transformer(input_ids, position_ids, token_type_ids, head_mask)\n",
    "        if self.transformer.output_attentions:\n",
    "            all_attentions, hidden_states = hidden_states\n",
    "#         print('hidden states',len(hidden_states))\n",
    "        \n",
    "        hidden_states = hidden_states[-1] #layer #\n",
    "#         lm_logits = self.lm_head(hidden_states)\n",
    "#         hidden_feats = self.multiple_choice_head(hidden_states, mc_token_ids)\n",
    "#         print(\"FEAT.\",hidden_feats)\n",
    "#         losses = []\n",
    "#         if lm_labels is not None:\n",
    "#             shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "#             shift_labels = lm_labels[..., 1:].contiguous()\n",
    "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "#             losses.append(loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)))\n",
    "#         print(\"hidden state shape\",hidden_states.shape)\n",
    "        lm_logits = 0\n",
    "        return lm_logits, hidden_states #token #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "def listRightIndex(alist, value):\n",
    "    return len(alist) - alist[-1::-1].index(value) -1\n",
    "\n",
    "\n",
    "def pre_process_datasets(encoded_datasets, input_len, cap_length, start_token, delimiter_token, clf_token):\n",
    "    \"\"\" Pre-process datasets containing lists of tuples(story, 1st continuation, 2nd continuation, label)\n",
    "\n",
    "        To Transformer inputs of shape (n_batch, n_alternative, length) comprising for each batch, continuation:\n",
    "        input_ids[batch, alternative, :] = [start_token] + story[:cap_length] + [delimiter_token] + cont1[:cap_length] + [clf_token]\n",
    "    \"\"\"\n",
    "\n",
    "    tensor_datasets = []\n",
    "    for dataset in encoded_datasets:\n",
    "        n_batch = ceil(len(dataset[0][0])/cap_length)\n",
    "        input_ids = np.zeros((n_batch, 1, input_len), dtype=np.int64)\n",
    "        mc_token_ids = np.zeros((n_batch, 1), dtype=np.int64)\n",
    "        i = 0\n",
    "        init_pos = 0\n",
    "        end_pos = cap_length\n",
    "        for story, cont1, cont2, mc_label in dataset:\n",
    "            if n_batch!=0:\n",
    "                if n_batch==1:\n",
    "                    with_cont1 = [start_token] + story[:cap_length] + [clf_token]\n",
    "                    input_ids[i, 0, :len(with_cont1)] = with_cont1\n",
    "                    mc_token_ids[i, 0] = len(with_cont1) - 1\n",
    "                    i+=1\n",
    "                else:\n",
    "                    while i!=n_batch and end_pos<len(story):\n",
    "                        try:\n",
    "                            end_pos = init_pos + listRightIndex(story[init_pos:end_pos],story[-1])\n",
    "                        except ValueError:\n",
    "                            end_pos = init_pos+story[init_pos:].index(story[-1])\n",
    "                        with_cont1 = [start_token] + story[init_pos:end_pos+1] + [clf_token]\n",
    "                        input_ids[i, 0, :len(with_cont1)] = with_cont1\n",
    "                        mc_token_ids[i, 0] = len(with_cont1) - 1\n",
    "                        i+=1\n",
    "                        init_pos = end_pos+1\n",
    "                        end_pos = min(init_pos+cap_length-1,len(story))\n",
    "        all_inputs = (input_ids, mc_token_ids)#, lm_labels, mc_labels)\n",
    "        tensor_datasets.append(tuple(torch.tensor(t) for t in all_inputs))\n",
    "    return tensor_datasets\n",
    "\n",
    "def load_rocstories_dataset(dataset_path):\n",
    "    \"\"\" Output a list of tuples(story, 1st continuation, 2nd continuation, label) \"\"\"\n",
    "    with open(dataset_path, encoding='utf_8') as f:\n",
    "        f = csv.reader(f)\n",
    "        output = []\n",
    "        next(f) # skip the first line\n",
    "        for line in tqdm(f):\n",
    "            output.append(('.'.join(line[0 :4]), line[4], line[5], int(line[-1])))\n",
    "    return output\n",
    "\n",
    "def tokenize_and_encode(obj):\n",
    "    \"\"\" Tokenize and encode a nested object \"\"\"\n",
    "    if isinstance(obj, str):\n",
    "        return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "    elif isinstance(obj, int):\n",
    "        return obj\n",
    "    return list(tokenize_and_encode(o) for o in obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNSPScore(sample_text):\n",
    "    \n",
    "    m = torch.nn.Softmax()\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(sample_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [0]*tokenized_text.index('[SEP]')+[1]*(len(tokenized_text)-tokenized_text.index('[SEP]'))\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    pred_score, seq_rel, seq_out, pool_out = model1(tokens_tensor, segments_tensors)\n",
    "    return m(seq_rel).detach().numpy()[0][0] #returns probability of being next sentence\n",
    "\n",
    "def getSentMatchScore(sent1, sent2, nsp_dampening_factor = 0.7):\n",
    "    \n",
    "    sent1_feats = getBERTFeatures(model1, sent1, attn_head_idx)\n",
    "    sent2_feats = getBERTFeatures(model1, sent2, attn_head_idx)\n",
    "    \n",
    "    cosine_distance = 1- cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = np.mean([nsp_score_1,nsp_score_2])*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    #print (\"nsp score -> \" + str(nsp_score))\n",
    "    #print (\"cosine score -> \" + str(cosine_distance))\n",
    "    return score\n",
    "    \n",
    "\n",
    "def getSentMatchScore_wfeature(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    #nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    #nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    \n",
    "    nsp_score = nsp_score_1 * nsp_dampening_factor\n",
    "    #nsp_score = nsp_score_1*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    return score\n",
    "\n",
    "def getSentMatchScore_wfeature_cosine(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    return cosine_distance\n",
    "    \n",
    "\n",
    "def getBERTFeatures(model, text, attn_head_idx = -1): #attn_head_idx - index o[]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    if len(tokenized_text)>200:\n",
    "        tokenized_text = tokenized_text[0:200]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    _, _, seq_out, pool_out = model(tokens_tensor)\n",
    "    seq_out = list(getPooledFeatures(seq_out[attn_head_idx]).T)\n",
    "    #pool_out = list(pool_out.detach().numpy().T)\n",
    "    \n",
    "    return seq_out\n",
    "\n",
    "def getPooledFeatures(np_array):\n",
    "    np_array = np_array.reshape(np_array.shape[1],np_array.shape[2]).detach().numpy()\n",
    "    np_array_mp = np.mean(np_array, axis=0).reshape(1, -1)\n",
    "    return np_array_mp\n",
    "\n",
    "def replaceContractions(text):\n",
    "    #text = text.lower()\n",
    "    c_filt_text = ''\n",
    "    for word in text.split(' '):\n",
    "        if word in contractions:\n",
    "            c_filt_text = c_filt_text+' '+contractions[word]\n",
    "        else:\n",
    "            c_filt_text = c_filt_text+' '+word\n",
    "    return c_filt_text\n",
    "\n",
    "def cleanText(text):\n",
    "        \n",
    "    text = text.replace('\\\\n','')\n",
    "    text = text.replace('\\\\','')\n",
    "    #text = text.replace('\\t', '')\n",
    "    #text = re.sub('\\[(.*?)\\]','',text) #removes [this one]\n",
    "    text = re.sub('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\\s',\n",
    "                ' __url__ ',text) #remove urls\n",
    "    #text = re.sub('\\'','',text)\n",
    "    #text = re.sub(r'\\d+', ' __number__ ', text) #replaces numbers\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/13/2019 14:45:44 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json from cache at /home/ray__/.pytorch_pretrained_bert/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
      "09/13/2019 14:45:44 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading merges file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt from cache at /home/ray__/.pytorch_pretrained_bert/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n",
      "09/13/2019 14:45:44 - WARNING - pytorch_pretrained_bert.tokenization_openai -   ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "09/13/2019 14:45:44 - INFO - pytorch_pretrained_bert.tokenization_openai -   Special tokens {'_start_': 40478, '_delimiter_': 40479, '_classify_': 40480}\n",
      "09/13/2019 14:45:44 - INFO - pytorch_pretrained_bert.modeling_openai -   loading weights file ../models/gpt/pytorch_model.bin\n",
      "09/13/2019 14:45:44 - INFO - pytorch_pretrained_bert.modeling_openai -   loading configuration file ../models/gpt/config.json\n",
      "09/13/2019 14:45:44 - INFO - pytorch_pretrained_bert.modeling_openai -   Model config {\n",
      "  \"afn\": \"gelu\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 3,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "09/13/2019 14:45:47 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading special tokens file ../models/gpt/special_tokens.txt\n",
      "09/13/2019 14:45:47 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading vocabulary file ../models/gpt/vocab.json\n",
      "09/13/2019 14:45:47 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading merges file ../models/gpt/merges.txt\n",
      "09/13/2019 14:45:47 - WARNING - pytorch_pretrained_bert.tokenization_openai -   ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "09/13/2019 14:45:47 - INFO - pytorch_pretrained_bert.tokenization_openai -   Special tokens {'_start_': 40478, '_delimiter_': 40479, '_classify_': 40480}\n",
      "09/13/2019 14:45:47 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading special tokens file ../models/gpt/special_tokens.txt\n",
      "09/13/2019 14:45:47 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading vocabulary file ../models/gpt/vocab.json\n",
      "09/13/2019 14:45:47 - INFO - pytorch_pretrained_bert.tokenization_openai -   loading merges file ../models/gpt/merges.txt\n",
      "09/13/2019 14:45:47 - WARNING - pytorch_pretrained_bert.tokenization_openai -   ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "09/13/2019 14:45:47 - INFO - pytorch_pretrained_bert.tokenization_openai -   Special tokens {'_start_': 40478, '_delimiter_': 40479, '_classify_': 40480}\n",
      "09/13/2019 14:45:47 - INFO - __main__ -   Encoding dataset...\n"
     ]
    }
   ],
   "source": [
    "## Defining constants over here\n",
    "seed = 42 \n",
    "model_name = 'openai-gpt'\n",
    "output_dir = '../models/gpt/'\n",
    "train_batch_size = 1\n",
    "n_valid = 374\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "#n_gpu = torch.cuda.device_count()\n",
    "#logger.info(\"device: {}, n_gpu {}\".format(device, n_gpu))\n",
    "device = 'cuda'\n",
    "special_tokens = ['_start_', '_delimiter_', '_classify_']\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name, special_tokens=special_tokens)\n",
    "special_tokens_ids = list(tokenizer.convert_tokens_to_ids(token) for token in special_tokens)\n",
    "\n",
    "# model = OpenAIGPTDoubleHeadsModel.from_pretrained(output_dir)\n",
    "# tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "model1 = OpenAIGPTDoubleHeadsModel_custom.from_pretrained(output_dir)\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)\n",
    "model1.to(device)\n",
    "model1.eval()\n",
    "#print(type(model))\n",
    "#print('model1')\n",
    "#print(model1)\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)\n",
    "logger.info(\"Encoding dataset...\")\n",
    "\n",
    "def feature_extractor(model1,text):\n",
    "    trn_dt = ([text,'','',0],)   \n",
    "    datasets = (trn_dt,)\n",
    "    encoded_datasets = tokenize_and_encode(datasets)\n",
    "    max_length = model1.config.n_positions//2 - 2\n",
    "    input_length = len(encoded_datasets[0][0][0])+2\n",
    "    input_length = min(input_length, model1.config.n_positions)  # Max size of input for the pre-trained model\n",
    "\n",
    "    # Prepare inputs tensors and dataloaders\n",
    "    n_batches = ceil(len(encoded_datasets[0][0][0])/max_length)\n",
    "    \n",
    "    tensor_datasets = pre_process_datasets(encoded_datasets, input_length, max_length, *special_tokens_ids)\n",
    "    train_tensor_dataset = tensor_datasets[0]\n",
    "    train_data = TensorDataset(*train_tensor_dataset)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=1)\n",
    "    '''\n",
    "    config = OpenAIGPTConfig.from_json_file('/home/shubham/Project/domain_mind/gpt2_experiment/model/config.json')\n",
    "    model1 = OpenAIGPTMultipleChoiceHead_custom(config)\n",
    "    '''\n",
    "    #eval_loss, eval_accuracy = 0, 0\n",
    "    #nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    final_clf=[]\n",
    "    final_lm=[]\n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, mc_token_ids = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            a, clf_text_feature = model1(input_ids, mc_token_ids)\n",
    "            final_clf.append(clf_text_feature[:,:,-1])\n",
    "    if n_batches>1:\n",
    "        clf_torch = torch.sum(torch.stack(final_clf),0)\n",
    "        return clf_torch\n",
    "    else:\n",
    "        return clf_text_feature[:,:,-1,:]#, lm_text_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse csv medium dump data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load it as csv.\n",
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('../data/raw/se_medium_.csv', index_col=False, header=0);\n",
    "#df = df.sort_values(by='created_at')\n",
    "texts = ' '.join(list(df['Data'])[:30])\n",
    "#texts_org = list(df['Data'][:100])\n",
    "#texts = '. '.join(str(i) for i in list(df['Heading']))\n",
    "#texts_org = list(df['Heading'])\n",
    "#texts = '. '.join(list(map(lambda x: cleanText(x), list(df['Data']))))\n",
    "#print (texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "\n",
    "mod_texts_unfiltered = tp.preprocess(texts, stop_words=False, remove_punct=False)\n",
    "mod_texts = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts_unfiltered[:]):\n",
    "    if len(sent.split(' '))>250:\n",
    "        length = len(sent.split(' '))\n",
    "        split1 = ' '.join([i for i in sent.split(' ')[:round(length/2)]])\n",
    "        split2 = ' '.join([i for i in sent.split(' ')[round(length/2):]])\n",
    "        mod_texts.append(split1)\n",
    "        mod_texts.append(split2)\n",
    "        continue\n",
    "        #mod_texts.pop(index)\n",
    "    if len(sent.split(' '))<=6:\n",
    "        continue\n",
    "    mod_texts.append(sent)\n",
    "\n",
    "print(len(mod_texts))\n",
    "fv = {}\n",
    "\n",
    "for index, sent in enumerate(mod_texts):\n",
    "    fv[index] = feature_extractor(model1,sent)\n",
    "    #fv[index] = getBERTFeatures(model1, sent, attn_head_idx=-1)\n",
    "    #fv[index] = getBERTFeatures_KP(model1, sent, attn_head_idx=-3)\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_graph(doc_list):\n",
    "    eng_graph = nx.Graph()\n",
    "    try:\n",
    "        eng_graph.add_nodes_from(range(len(doc_list)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return eng_graph\n",
    "\n",
    "\n",
    "tg = build_graph(mod_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attn_head_idx = -1\n",
    "\n",
    "node_edge = []\n",
    "\n",
    "for index1, sent1 in enumerate(mod_texts):\n",
    "    print (index1)\n",
    "    for index2, sent2 in enumerate(mod_texts):\n",
    "        if index1!=index2 and index2>index1:\n",
    "            #score = getSentMatchScore_wfeature(sent1, sent2,fv[index1],fv[index2])\n",
    "            #score = getSentMatchScore_wfeature_cosine(sent1, sent2,fv[index1],fv[index2])\n",
    "            score = 1 - cosine(fv[index1].cpu(),fv[index2].cpu())\n",
    "#             if score > 0.8:\n",
    "#                 #tg.add_edge(index1,index2,{'weight': score})\n",
    "#                 tg.add_edge(index1,index2)\n",
    "            tg.add_edge(index1,index2,weight=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import community\n",
    "max_mod = 0\n",
    "mod_v = 0\n",
    "for v in [0.15, 0.10, 0.05, 0.01]:\n",
    "    flag = False\n",
    "    for count in range(5):   \n",
    "        temp_nodes = []\n",
    "        for nodea,nodeb, weight in tg.edges.data():\n",
    "            temp_nodes.append((nodea,nodeb,weight['weight']))\n",
    "        temp_nodes = sorted(temp_nodes, key=lambda kv: kv[2], reverse=True)\n",
    "\n",
    "        temp_nodes = temp_nodes[:math.ceil(len(temp_nodes)*v)+1]\n",
    "\n",
    "        com_graph = nx.Graph()\n",
    "        for nodea,nodeb, weight in temp_nodes:\n",
    "            com_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "        partition = community.best_partition(com_graph)\n",
    "\n",
    "        mod = community.modularity(partition, com_graph)\n",
    "        if mod > max_mod and (mod < 0.4 or max_mod==0):\n",
    "            max_mod=mod\n",
    "            mod_v = v\n",
    "        print (\"The pruning value 'v' and modularity is: \", v, mod)\n",
    "#         if mod > 0.3:\n",
    "#             flag=True\n",
    "#             print (\"Modularity reached 3. The pruning value 'v' is: \", v)\n",
    "#             break\n",
    "        if mod==0:\n",
    "            temp_nodes = []\n",
    "            print (\"Modularity reached 0. The pruning value 'v' is: \", 0.15)\n",
    "            for nodea,nodeb, weight in tg.edges.data():\n",
    "                temp_nodes.append((nodea,nodeb,weight['weight']))\n",
    "            temp_nodes = sorted(temp_nodes, key=lambda kv: kv[2], reverse=True)\n",
    "\n",
    "            temp_nodes = temp_nodes[:math.ceil(len(temp_nodes)*0.15)+1]\n",
    "\n",
    "            com_graph = nx.Graph()\n",
    "            for nodea,nodeb, weight in temp_nodes:\n",
    "                com_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "            partition = community.best_partition(com_graph)\n",
    "\n",
    "            mod = community.modularity(partition, com_graph)\n",
    "            flag=True\n",
    "            break\n",
    "    if flag:\n",
    "        print()\n",
    "        break\n",
    "\n",
    "for count in range(5):\n",
    "    temp_nodes = []\n",
    "    for nodea,nodeb, weight in tg.edges.data():\n",
    "        temp_nodes.append((nodea,nodeb,weight['weight']))\n",
    "    temp_nodes = sorted(temp_nodes, key=lambda kv: kv[2], reverse=True)\n",
    "    #print (len(temp_nodes), mod_v)\n",
    "    temp_nodes = temp_nodes[:math.ceil(len(temp_nodes)*mod_v)+1]\n",
    "    #print (len(temp_nodes))\n",
    "    com_graph = nx.Graph()\n",
    "    for nodea,nodeb, weight in temp_nodes:\n",
    "        com_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "    partition = community.best_partition(com_graph)\n",
    "    mod = community.modularity(partition, com_graph)\n",
    "    #print (mod)\n",
    "    if mod>=max_mod:\n",
    "        break\n",
    "print (\"The final modularity is \", mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "#partition = community.best_partition(com_graph)\n",
    "\n",
    "values = [partition.get(node) for node in com_graph.nodes()]\n",
    "values=[partition.get(node) for node in com_graph.nodes()]\n",
    "plt.rcParams['figure.figsize']= [16, 10]\n",
    "measure_name = \"Louviin Algorithm Community Structure\"\n",
    "pos = nx.spring_layout(com_graph, k=0.2, iterations=20)\n",
    "nodes_plot=nx.draw_networkx_nodes(com_graph, pos, node_size=140, label=True, cmap=plt.get_cmap('magma', len(com_graph.nodes())/4),node_color=values, alpha=0.95)\n",
    "edges_plot=nx.draw_networkx_edges(com_graph, pos, edge_color='r', alpha=0.1)\n",
    "plt.title(measure_name, fontsize=22, fontname='Arial')\n",
    "plt.colorbar(nodes_plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community.modularity(partition, com_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        print (mod_texts[word] + \"\\n\\n\")\n",
    "        current=cluster\n",
    "    else:\n",
    "        print (mod_texts[word] + \"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to backlink to the documents\n",
    "\n",
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "doc_split = []\n",
    "for t in texts_org:\n",
    "    mod_texts_unfiltered_new = tp.preprocess(t, stop_words=False, remove_punct=False)\n",
    "    mod_texts_new = []\n",
    "\n",
    "    for index, sent in enumerate(mod_texts_unfiltered_new[:]):\n",
    "        if len(sent.split(' '))>250:\n",
    "            length = len(sent.split(' '))\n",
    "            split1 = ' '.join([i for i in sent.split(' ')[:round(length/2)]])\n",
    "            split2 = ' '.join([i for i in sent.split(' ')[round(length/2):]])\n",
    "            mod_texts_new.append(split1)\n",
    "            mod_texts_new.append(split2)\n",
    "            continue\n",
    "            #mod_texts.pop(index)\n",
    "        if len(sent.split(' '))<=6:\n",
    "            continue\n",
    "        mod_texts_new.append(sent)\n",
    "    doc_split.append(mod_texts_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "new_list = []\n",
    "new_list_temp = []\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        #print (mod_texts[word])\n",
    "        doc = -1\n",
    "        for index, doc_s in enumerate(doc_split):\n",
    "            if mod_texts[word] in doc_s:\n",
    "                doc = index\n",
    "        print (doc)\n",
    "        new_list.append(new_list_temp)\n",
    "        new_list_temp = []\n",
    "        new_list_temp.append(doc)\n",
    "        current=cluster\n",
    "    else:\n",
    "        #print (mod_texts[word] + \"\\n\\n\\n\\n\\n\")\n",
    "        doc = 0\n",
    "        for index, doc_s in enumerate(doc_split):\n",
    "            if mod_texts[word] in doc_s:\n",
    "                doc = index\n",
    "        print (doc)\n",
    "        new_list_temp.append(doc)\n",
    "new_list.append(new_list_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "count_list = []\n",
    "for c in new_list:\n",
    "    count_list.append(Counter(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# without pruning (doesn't work w/ cosine similarity as weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "com_graph_full = nx.Graph()\n",
    "for nodea,nodeb, weight1 in tg.edges.data():\n",
    "    com_graph_full.add_edge(nodea,nodeb, weight=weight1['weight'])\n",
    "\n",
    "partition = community.best_partition(com_graph_full)\n",
    "mod = community.modularity(partition, com_graph_full)\n",
    "print (mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "#partition = community.best_partition(com_graph)\n",
    "\n",
    "values = [partition.get(node) for node in com_graph_full.nodes()]\n",
    "values=[partition.get(node) for node in com_graph_full.nodes()]\n",
    "plt.rcParams['figure.figsize']= [16, 10]\n",
    "measure_name = \"Louviin Algorithm Community Structure\"\n",
    "pos = nx.spring_layout(com_graph_full, k=0.2, iterations=20)\n",
    "nodes_plot=nx.draw_networkx_nodes(com_graph_full, pos, node_size=140, label=True, cmap=plt.get_cmap('magma', len(com_graph_full.nodes())/4),node_color=values, alpha=0.95)\n",
    "edges_plot=nx.draw_networkx_edges(com_graph_full, pos, edge_color='r', alpha=0.1)\n",
    "plt.title(measure_name, fontsize=22, fontname='Arial')\n",
    "plt.colorbar(nodes_plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        print (mod_texts[word])\n",
    "        current=cluster\n",
    "    else:\n",
    "        print (mod_texts[word] + \"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "com_graph_full = nx.Graph()\n",
    "for nodea,nodeb, weight1 in tg.edges.data():\n",
    "    com_graph_full.add_edge(nodea,nodeb, weight=weight1['weight'])\n",
    "\n",
    "partition = community.best_partition(com_graph_full)\n",
    "mod = community.modularity(partition, com_graph_full)\n",
    "print (mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# old approach (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T06:57:15.046763Z",
     "start_time": "2019-06-11T06:56:54.541064Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "with open('../data/engg_text_06May19_recency.pkl','rb') as fp:\n",
    "    file = pickle.load(fp)\n",
    "\n",
    "texts = '. '.join(sent for sent in file)\n",
    "\n",
    "mod_texts_unfiltered = tp.preprocess(texts, stop_words=False, remove_punct=True)\n",
    "mod_texts = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts_unfiltered[:500]):\n",
    "    if len(sent.split(' '))>250:\n",
    "        length = len(sent.split(' '))\n",
    "        split1 = ' '.join([i for i in sent.split(' ')[:round(length/2)]])\n",
    "        split2 = ' '.join([i for i in sent.split(' ')[round(length/2):]])\n",
    "        mod_texts.append(split1)\n",
    "        mod_texts.append(split2)\n",
    "        continue\n",
    "        #mod_texts.pop(index)\n",
    "    if len(sent.split(' '))<=6:\n",
    "        continue\n",
    "    mod_texts.append(sent)\n",
    "\n",
    "print(len(mod_texts))\n",
    "fv = {}\n",
    "\n",
    "for index, sent in enumerate(mod_texts):\n",
    "    fv[index] = getBERTFeatures(model1, sent, attn_head_idx=-1)\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T07:03:58.226383Z",
     "start_time": "2019-06-11T07:03:57.465436Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_graph(doc_list):\n",
    "    eng_graph = nx.Graph()\n",
    "    try:\n",
    "        eng_graph.add_nodes_from(range(len(doc_list)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return eng_graph\n",
    "\n",
    "\n",
    "tg = build_graph(mod_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for index1, sent1 in enumerate(mod_texts):\n",
    "    for index2, sent2 in enumerate(mod_texts):\n",
    "        if sent1!=sent2:\n",
    "            score = getSentMatchScore_wfeature(sent1, sent2, fv[index1], fv[index2])\n",
    "            print (\"sentence 1 \\n \\n\", sent1)\n",
    "            print (\"\\n\\nsentence 2 \\n \\n\", sent2)\n",
    "            print (\"\\n\\n Score -> \", score, \"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:11:22.256624Z",
     "start_time": "2019-06-07T11:10:16.574980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "attn_head_idx = -1\n",
    "start = time.time()\n",
    "end = 0\n",
    "node_edge = []\n",
    "runs = len(mod_texts)*(len(mod_texts)-1)\n",
    "\n",
    "sent1counter = 0\n",
    "sent2counter = 0\n",
    "counter = 0\n",
    "try:\n",
    "    i=0\n",
    "    #tqdm()\n",
    "    for index1, sent1 in enumerate(mod_texts):\n",
    "        sent1counter=index1\n",
    "        print (index1, time.time()-start)\n",
    "        for index2, sent2 in enumerate(mod_texts):\n",
    "            counter+=1\n",
    "            #updt(runs, counter)\n",
    "            sent2counter=index2\n",
    "            if index1!=index2:\n",
    "                score = getSentMatchScore_wfeature(sent1, sent2, fv[index1], fv[index2])\n",
    "                if score>0.8:\n",
    "                    #tg.add_edge(index1,index2,{'weight': score})\n",
    "                    tg.add_edge(index1,index2)\n",
    "except RuntimeError as e:\n",
    "    print (e)\n",
    "    print (sent1counter, sent2counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores_round = [round(i,1) for i in scores]\n",
    "scores_freq = {}\n",
    "for i in scores_round:\n",
    "    scores_freq[i] = scores_round.count(i)\n",
    "scores_round = set(scores_freq.keys())\n",
    "scores_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sorted(scores_freq.items(), key=lambda kv: kv[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sent1counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tg.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:36:40.808135Z",
     "start_time": "2019-06-07T11:36:40.474903Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "partition = community.best_partition(tg)\n",
    "\n",
    "#drawing\n",
    "# size = float(len(set(partition.values())))\n",
    "# pos = nx.spring_layout(tg)\n",
    "# count = 0.\n",
    "# for com in set(partition.values()) :\n",
    "#     count = count + 1.\n",
    "#     list_nodes = [nodes for nodes in partition.keys()\n",
    "#                                 if partition[nodes] == com]\n",
    "#     nx.draw_networkx_nodes(tg, pos, list_nodes, node_size = 20,\n",
    "#                                 node_color = str(count / size))\n",
    "\n",
    "values = [partition.get(node) for node in tg.nodes()]\n",
    "#nx.draw_networkx_edges(tg, pos, alpha=0.5)\n",
    "#plt.show()\n",
    "#plt.margins(0.1, 0.1)\n",
    "#plt.figure(figsize=(10, 9))\n",
    "#plt.axis('off')\n",
    "#nx.draw_spring(tg, cmap = plt.get_cmap('jet'), node_color = values, node_size=300)\n",
    "#draw_graph(tg, None)\n",
    "\n",
    "\n",
    "values=[partition.get(node) for node in tg.nodes()]\n",
    "plt.rcParams['figure.figsize']= [16, 10]\n",
    "measure_name = \"Louviin Algorithm Community Structure\"\n",
    "pos = nx.spring_layout(tg, k=0.2, iterations=20)\n",
    "nodes_plot=nx.draw_networkx_nodes(tg, pos, node_size=140, label=True, cmap=plt.get_cmap('magma', len(tg.nodes())/4),node_color=values, alpha=0.95)\n",
    "edges_plot=nx.draw_networkx_edges(tg, pos, edge_color='r', alpha=0.1)\n",
    "plt.title(measure_name, fontsize=22, fontname='Arial')\n",
    "plt.colorbar(nodes_plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:36:55.246506Z",
     "start_time": "2019-06-07T11:36:55.244074Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:36:58.283193Z",
     "start_time": "2019-06-07T11:36:58.279757Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:40:07.625227Z",
     "start_time": "2019-06-07T11:40:07.620025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        print (mod_texts[word])\n",
    "        current=cluster\n",
    "    else:\n",
    "        print (mod_texts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cluster0 = []\n",
    "cluster_dict = {}\n",
    "cluster1= []\n",
    "cluster2 = []\n",
    "cluster3 = []\n",
    "cluster4= []\n",
    "cluster5 = []\n",
    "cluster6= []\n",
    "cluster7 = []\n",
    "for sent, cluster in partition:\n",
    "    if cluster == 0:\n",
    "        cluster0.append(sent)\n",
    "        cluster_dict[sent] = 2\n",
    "    elif cluster == 1:\n",
    "        cluster1.append(sent)\n",
    "    elif cluster == 2:\n",
    "        cluster2.append(sent)\n",
    "    elif cluster == 3:\n",
    "        cluster3.append(sent)\n",
    "    elif cluster == 4:\n",
    "        cluster4.append(sent)\n",
    "    elif cluster == 5:\n",
    "        cluster5.append(sent)\n",
    "    elif cluster == 6:\n",
    "        cluster6.append(sent)\n",
    "    elif cluster == 7:\n",
    "        cluster7.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"------------Cluster 0--------------- \\n\\n\")\n",
    "for sent in cluster0:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 1--------------- \\n\\n\")\n",
    "for sent in cluster1:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 2--------------- \\n\\n\")\n",
    "for sent in cluster2:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "print (\"------------Cluster 3--------------- \\n\\n\")\n",
    "for sent in cluster3:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 4--------------- \\n\\n\")\n",
    "for sent in cluster4:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 5--------------- \\n\\n\")\n",
    "for sent in cluster5:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 6--------------- \\n\\n\")\n",
    "for sent in cluster6:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 7--------------- \\n\\n\")\n",
    "for sent in cluster7:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## POC on slack dump (json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from text_preprocessing import preprocess\n",
    "\n",
    "from graphrank.graphrank import GraphRank\n",
    "\n",
    "from graphrank.utils import GraphUtils\n",
    "import networkx as nx\n",
    "import json as js\n",
    "import keyphrase_extraction as kp\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def getslacktext(location):\n",
    "    with open(location) as f:\n",
    "        meeting = json.load(f)\n",
    "    \n",
    "#     text = []\n",
    "    \n",
    "#     for i in range(len(list(meeting['segments']))):\n",
    "#         #unfil_text = meeting['segments'][i]['filteredText']\n",
    "#         unfil_text = meeting['segments'][i]['originalText']\n",
    "#         if len(unfil_text.split(' '))<6:\n",
    "#             continue\n",
    "#         if len(unfil_text.split(' '))>250:\n",
    "#             length = len(unfil_text.split(' '))\n",
    "#             split1 = ' '.join([i for i in unfil_text.split(' ')[:round(length/2)]])\n",
    "#             split2 = ' '.join([i for i in unfil_text.split(' ')[round(length/2):]])\n",
    "#             text.append(split1)\n",
    "#             text.append(split2)\n",
    "#             continue\n",
    "#         text.append(unfil_text)\n",
    "    \n",
    "    return meeting\n",
    "\n",
    "text = getslacktext('engineering_6thMay2019.json')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getusermsg(text, userid = 'U9GLH098C'): #Venkat\n",
    "    \n",
    "    usermsg = []\n",
    "    for index, msg in enumerate(text):\n",
    "        if msg['type']=='message' and 'bot_id' not in msg.keys() and 'user' in msg.keys():\n",
    "            if msg['user']==userid and msg['text']!='':\n",
    "                    text = preprocess.preprocess(msg['text'], remove_punct=True,word_tokenize=False ,stop_words=False)\n",
    "                    #print (text)\n",
    "                    if len(text)!=0:\n",
    "                        for t in text:\n",
    "                            if len(t.split(' '))>6:\n",
    "                                usermsg.append(t)\n",
    "    return usermsg\n",
    "\n",
    "def getusermsg_alluser(text): \n",
    "    \n",
    "    usermsg = []\n",
    "    user = {}\n",
    "    cnt=0\n",
    "    for index, msg in enumerate(text):\n",
    "        if msg['type']=='message' and 'bot_id' not in msg.keys() and 'user' in msg.keys():\n",
    "            if msg['text']!='':\n",
    "                    text = preprocess.preprocess(msg['text'], remove_punct=True,word_tokenize=False ,stop_words=False)\n",
    "                    #print (text)\n",
    "                    if len(text)!=0:\n",
    "                        for t in text:\n",
    "#                             if len(t.split(' '))>6:\n",
    "#                                 if msg['user'] in usermsg.keys() and len(usermsg[msg['user']])<10:\n",
    "#                                     usermsg[msg['user']].append(t)\n",
    "#                                 elif msg['user'] not in usermsg.keys():\n",
    "#                                     usermsg[msg['user']] = []\n",
    "#                                     usermsg[msg['user']].append(t)\n",
    "                            if len(t.split(' '))>6:\n",
    "                                usermsg.append(t) \n",
    "                                user[cnt] = msg['user']\n",
    "                                cnt+=1\n",
    "    return usermsg, user\n",
    "\n",
    "#mod_texts = getusermsg(text,userid = 'U4QK2H8RL')[:20]\n",
    "mod_texts, user_list = getusermsg_alluser(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mod_texts = mod_texts[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(len(mod_texts))\n",
    "fv = {}\n",
    "\n",
    "for index, sent in enumerate(mod_texts):\n",
    "    fv[index] = getBERTFeatures(model1, sent, attn_head_idx=-1)\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_graph(doc_list):\n",
    "    eng_graph = nx.Graph()\n",
    "    try:\n",
    "        eng_graph.add_nodes_from(range(len(doc_list)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return eng_graph\n",
    "\n",
    "\n",
    "tg = build_graph(mod_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "attn_head_idx = -1\n",
    "start = time.time()\n",
    "end = 0\n",
    "node_edge = []\n",
    "runs = len(mod_texts)*(len(mod_texts)-1)\n",
    "\n",
    "sent1counter = 0\n",
    "sent2counter = 0\n",
    "counter = 0\n",
    "try:\n",
    "    i=0\n",
    "    #tqdm()\n",
    "    for index1, sent1 in enumerate(mod_texts):\n",
    "        sent1counter=index1\n",
    "        print (index1, time.time()-start)\n",
    "        for index2, sent2 in enumerate(mod_texts):\n",
    "            \n",
    "            #updt(runs, counter)\n",
    "            sent2counter=index2\n",
    "            if index1!=index2:\n",
    "                score = getSentMatchScore_wfeature_test(sent1, sent2, fv[index1], fv[index2], user_list[index1], user_list[index1])\n",
    "                if score>0:\n",
    "                    counter+=1\n",
    "                    tg.add_edge(index1,index2,weight=score)\n",
    "                    #tg.add_edge(index1,index2)\n",
    "except RuntimeError as e:\n",
    "    print (e)\n",
    "    print (sent1counter, sent2counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(nx.betweenness_centrality(tg).items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#tg.remove_node(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mod_texts[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (len(mod_texts)*(len(mod_texts)-1))\n",
    "(len(mod_texts)*(len(mod_texts)-1)) - counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "partition = community.best_partition(tg)\n",
    "\n",
    "#drawing\n",
    "# size = float(len(set(partition.values())))\n",
    "# pos = nx.spring_layout(tg)\n",
    "# count = 0.\n",
    "# for com in set(partition.values()) :\n",
    "#     count = count + 1.\n",
    "#     list_nodes = [nodes for nodes in partition.keys()\n",
    "#                                 if partition[nodes] == com]\n",
    "#     nx.draw_networkx_nodes(tg, pos, list_nodes, node_size = 20,\n",
    "#                                 node_color = str(count / size))\n",
    "\n",
    "values = [partition.get(node) for node in tg.nodes()]\n",
    "#nx.draw_networkx_edges(tg, pos, alpha=0.5)\n",
    "#plt.show()\n",
    "#plt.margins(0.1, 0.1)\n",
    "#plt.figure(figsize=(10, 9))\n",
    "#plt.axis('off')\n",
    "#nx.draw_spring(tg, cmap = plt.get_cmap('jet'), node_color = values, node_size=300)\n",
    "#draw_graph(tg, None)\n",
    "\n",
    "\n",
    "values=[partition.get(node) for node in tg.nodes()]\n",
    "plt.rcParams['figure.figsize']= [16, 10]\n",
    "measure_name = \"Louviin Algorithm Community Structure\"\n",
    "pos = nx.spring_layout(tg, k=0.2, iterations=20)\n",
    "nodes_plot=nx.draw_networkx_nodes(tg, pos, node_size=140, label=True, cmap=plt.get_cmap('magma', len(tg.nodes())/4),node_color=values, alpha=0.95)\n",
    "edges_plot=nx.draw_networkx_edges(tg, pos, edge_color='r', alpha=0.1)\n",
    "plt.title(measure_name, fontsize=22, fontname='Arial')\n",
    "plt.colorbar(nodes_plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cluster0 = []\n",
    "cluster_dict = {}\n",
    "cluster1= []\n",
    "cluster2 = []\n",
    "cluster3 = []\n",
    "cluster4= []\n",
    "cluster5 = []\n",
    "cluster6= []\n",
    "cluster7 = []\n",
    "for sent, cluster in partition:\n",
    "    if cluster == 0:\n",
    "        cluster0.append(sent)\n",
    "        cluster_dict[sent] = 2\n",
    "    elif cluster == 1:\n",
    "        cluster1.append(sent)\n",
    "    elif cluster == 2:\n",
    "        cluster2.append(sent)\n",
    "    elif cluster == 3:\n",
    "        cluster3.append(sent)\n",
    "    elif cluster == 4:\n",
    "        cluster4.append(sent)\n",
    "    elif cluster == 5:\n",
    "        cluster5.append(sent)\n",
    "    elif cluster == 6:\n",
    "        cluster6.append(sent)\n",
    "    elif cluster == 7:\n",
    "        cluster7.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"------------Cluster 0--------------- \\n\\n\")\n",
    "for sent in cluster0:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 1--------------- \\n\\n\")\n",
    "for sent in cluster1:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 2--------------- \\n\\n\")\n",
    "for sent in cluster2:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "print (\"------------Cluster 3--------------- \\n\\n\")\n",
    "for sent in cluster3:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 4--------------- \\n\\n\")\n",
    "for sent in cluster4:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 5--------------- \\n\\n\")\n",
    "for sent in cluster5:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 6--------------- \\n\\n\")\n",
    "for sent in cluster6:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 7--------------- \\n\\n\")\n",
    "for sent in cluster7:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster1_user = []\n",
    "print (\"------------Cluster 0--------------- \\n\\n\")\n",
    "for sent in cluster0:\n",
    "    if user_list[sent] not in cluster1_user:\n",
    "        print (user_list[sent] + \"\\n\\n\")\n",
    "        cluster1_user.append(user_list[sent])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "cluster2_user = []\n",
    "print (\"------------Cluster 1--------------- \\n\\n\")\n",
    "for sent in cluster1:\n",
    "    if user_list[sent] not in cluster2_user:\n",
    "        print (user_list[sent] + \"\\n\\n\")\n",
    "        cluster2_user.append(user_list[sent])\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print (\"------ Cluster 0 - Cluster 1---------\")\n",
    "for i in cluster1_user:\n",
    "    if i in cluster2_user:\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "part ={}\n",
    "for sent, cluster in partition:\n",
    "    part[sent] = cluster\n",
    "\n",
    "print (part[19],part[1],part[8],part[6],part[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in [19,1,8]:\n",
    "    print (mod_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for index1, sent1 in enumerate(mod_texts):\n",
    "    for index2, sent2 in enumerate(mod_texts):\n",
    "        if sent1!=sent2:\n",
    "            score = getSentMatchScore_wfeature(sent1, sent2, fv[index1], fv[index2])\n",
    "            print (\"sentence 1 \\n \\n\", sent1)\n",
    "            print (\"\\n\\nsentence 2 \\n \\n\", sent2)\n",
    "            print (\"\\n\\n Score -> \", score, \"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/home/ether/domain_mind/engineering/se_minds_new.pkl', 'rb') as f:\n",
    "    file = pickle.load(f)\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(list(file['feature_vector'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_score = []\n",
    "sent_com = {}\n",
    "max_index = 0\n",
    "max_score = 0\n",
    "for i in range(10):\n",
    "    sent_com[i] = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts[:]):\n",
    "    temp_score = []\n",
    "    for index2, vec in enumerate(list(file['feature_vector'])):\n",
    "        temp_score.append(getSentMatchScore_wfeature(sent, list(file['sentence'])[index2], fv[index], vec, model1))\n",
    "#         print (\"-------cluster-------\")\n",
    "#         print (sent)\n",
    "#         print (\"score -> \" + str(temp_score[index2]))\n",
    "    max_score = max(temp_score)\n",
    "    max_index = temp_score.index(max_score)\n",
    "    sent_com[max_index].append(sent)\n",
    "    #print (max_index)\n",
    "    #break\n",
    "#for i in mod_texts[:10]:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for cluster, sent in sent_com.items():\n",
    "    print (\"--------------community \" + str(cluster) + \"------------\")\n",
    "    for sentence in sent:\n",
    "        print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list(file['sentence'])[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text =\"The only thing I can think of is that I had both the production and staging builds installed at that time vs now.\"\n",
    "#text = \"It is a very good time to re consider game of thrones story line\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## select 3 clusters instead of max-score while calculating belongingness of a sentences..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_com = {}\n",
    "max_index = 0\n",
    "max_score = 0\n",
    "for i in range(100):\n",
    "    sent_com[i] = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts[:]):\n",
    "    temp_score = {}\n",
    "    for index2, vec in enumerate(list(file['feature_vector'])):\n",
    "        temp_score[index2] = getSentMatchScore_wfeature(sent, list(file['sentence'])[index2], fv[index], vec)\n",
    "#         print (\"-------cluster-------\")\n",
    "#         print (sent)\n",
    "#         print (\"score -> \" + str(temp_score[index2]))\n",
    "    temp_score = dict(sorted(temp_score.items(), key = lambda fv : fv[1], reverse=True))\n",
    "    max_indexes = [i for i in temp_score.keys()][:3]\n",
    "    #max_index = temp_score.index(max_score)\n",
    "    print (max_indexes)\n",
    "#     for indexes in max_indexes:\n",
    "#         sent_com[indexes].append(index)\n",
    "    sent_com[index] = max_indexes\n",
    "    #print (max_index)\n",
    "    #break\n",
    "#for i in mod_texts[:10]:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in sent_com.keys():\n",
    "    for j in sent_com.keys():\n",
    "        if i!=j:\n",
    "            if sorted(sent_com[i])==(sent_com[j]):\n",
    "                print (\"-------similar sentence--------\")\n",
    "                print (mod_texts[i])\n",
    "                print (\"\\n\")\n",
    "                print (mod_texts[j])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## select 2 clusters instead of max-score while calculating belongingness of a sentences..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sent_com = {}\n",
    "max_index = 0\n",
    "max_score = 0\n",
    "for i in range(100):\n",
    "    sent_com[i] = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts[:]):\n",
    "    temp_score = {}\n",
    "    for index2, vec in enumerate(list(file['feature_vector'])):\n",
    "        temp_score[index2] = getSentMatchScore_wfeature(sent, list(file['sentence'])[index2], fv[index], vec)\n",
    "#         print (\"-------cluster-------\")\n",
    "#         print (sent)\n",
    "#         print (\"score -> \" + str(temp_score[index2]))\n",
    "    temp_score = dict(sorted(temp_score.items(), key = lambda fv : fv[1], reverse=True))\n",
    "    max_indexes = [i for i in temp_score.keys()][:2]\n",
    "    #max_index = temp_score.index(max_score)\n",
    "    #print (max_indexes)\n",
    "#     for indexes in max_indexes:\n",
    "#         sent_com[indexes].append(index)\n",
    "    sent_com[index] = max_indexes\n",
    "    #print (max_index)\n",
    "    #break\n",
    "#for i in mod_texts[:10]:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in sent_com.keys():\n",
    "    for j in sent_com.keys():\n",
    "        if i!=j:\n",
    "            if sorted(sent_com[i])==(sent_com[j]):\n",
    "                print (\"-------similar sentence--------\")\n",
    "                print (mod_texts[i])\n",
    "                print (\"\\n\")\n",
    "                print (mod_texts[j])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph = \"I have a couple S3 buckets. One for my static home page, one for holding images and one for holding the application version. As far as I know, ELB automatically creates the one for managing the application versions.\"\n",
    "\n",
    "paragraph = preprocess.preprocess(paragraph, stop_words=False, remove_punct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sent_vec = {}\n",
    "for index, sentence in enumerate(paragraph):\n",
    "    sent_vec[index] = getBERTFeatures(model1, sentence, attn_head_idx=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "score = {}\n",
    "for index, sent in enumerate(paragraph):\n",
    "    temp_score = {}\n",
    "    for index2, vec in enumerate(list(file['feature_vector'])):\n",
    "        temp_score[index2] = getSentMatchScore_wfeature(sent, list(file['sentence'])[index2], sent_vec[index], vec)\n",
    "    score[index] = max(temp_score.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_norm_value(score):\n",
    "    weighted_score = {}\n",
    "    min_score = min(score.values())\n",
    "    max_score = max(score.values())\n",
    "    if min_score == max_score:\n",
    "        return [1]*len(score.values())\n",
    "    for index, s in enumerate(score.values()):\n",
    "        weighted_score[index] = (s - min_score)/ (max_score - min_score)\n",
    "    return weighted_score\n",
    "\n",
    "def get_weighted_norm_value(score):\n",
    "    weighted_score = {}\n",
    "    tot_score = sum(score.values())\n",
    "    for index, s in enumerate(score.values()):\n",
    "        weighted_score[index] = s/tot_score\n",
    "    return weighted_score\n",
    "\n",
    "#weighted_score = list(get_weighted_norm_value(score).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph = []\n",
    "paragraph_vec = np.zeros((768,), dtype=float)\n",
    "for index, sent in enumerate(paragraph):\n",
    "    sent_vec[index] = list(np.array(sent_vec[index])*weighted_score[index])\n",
    "    \n",
    "#paragraph_vec = np.sum(np.array(sent_vec.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for s in sent_vec.values():\n",
    "    print (len(np.array(s)))\n",
    "    paragraph_vec = np.add(paragraph_vec,np.array(s))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph_vec = np.zeros((768,1), dtype=float)\n",
    "paragraph3_vec = np.array(list(file['feature_vector'])[0])\n",
    "(paragraph_vec + paragraph3_vec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.array(list(file['feature_vector'])[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_paragraph_vec(paragraph, file):\n",
    "    sent_vec = {}\n",
    "    for index, sentence in enumerate(paragraph):\n",
    "        sent_vec[index] = getBERTFeatures(model1, sentence, attn_head_idx=-2)\n",
    "    score = {}\n",
    "    for index, sent in enumerate(paragraph):\n",
    "        temp_score = {}\n",
    "        for index2, vec in enumerate(list(file['feature_vector'])):\n",
    "            temp_score[index2] = getSentMatchScore_wfeature(sent, list(file['sentence'])[index2], sent_vec[index], vec)\n",
    "        score[index] = max(temp_score.values())\n",
    "        \n",
    "    weighted_score = list(get_weighted_norm_value(score).values())\n",
    "    \n",
    "    paragraph = []\n",
    "    paragraph_vec = np.zeros((768,1), dtype=float)\n",
    "    for index, sent in enumerate(paragraph):\n",
    "        sent_vec[index] = list(np.array(sent_vec[index])*weighted_score[index])\n",
    "    \n",
    "\n",
    "    for s in sent_vec.values():\n",
    "        #print(np.add(paragraph_vec,np.array(s)).shape)\n",
    "        paragraph_vec = np.add(paragraph_vec,np.array(s))\n",
    "    \n",
    "    print (paragraph_vec.shape)\n",
    "    return paragraph_vec\n",
    "    #paragraph_vec = np.sum(np.array(sent_vec.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph = \"I have a couple S3 buckets. One for my static home page, one for holding images and one for holding the application version. As far as I know, ELB automatically creates the one for managing the application versions.\"\n",
    "paragraph2 = \"Having a development environment and a production running at the same time is easy, but it’s expensive. It doubles it, in fact. Therefore, I usually destroy the dev environment as soon as I’m done with it.\"\n",
    "paragraph4 = \" I actually got a couple of S3 buckets. One for my the home page, one for images and one for the application version.  I know that, ELB will automatically create the one to managing the application versions. \"\n",
    "paragraph3 = \"You find out that Harvard Business Review looked into how personality traits factor into group dynamics. While each person should have a functional role within the group, they also have a less obvious psychological role. You already know what functional roles you need to hire for — a product manager, two engineers, an analyst, and a designer. While a functional role is of course important, HBR found that the psychological role someone has to play is just as important to a team’s viability and productivity. Through their research, HBR came up with five different personality traits that are imperative to group success.\"\n",
    "paragraph = preprocess.preprocess(paragraph, stop_words=False, remove_punct=False)\n",
    "paragraph2 = preprocess.preprocess(paragraph2, stop_words=False, remove_punct=False)\n",
    "paragraph3 = preprocess.preprocess(paragraph3, stop_words=False, remove_punct=False)\n",
    "paragraph4 = preprocess.preprocess(paragraph4, stop_words=False, remove_punct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph_fv = get_paragraph_vec(paragraph, file)\n",
    "paragraph2_fv = get_paragraph_vec(paragraph2, file)\n",
    "paragraph3_fv = get_paragraph_vec(paragraph3, file)\n",
    "paragraph4_fv = get_paragraph_vec(paragraph4, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "#cosine(list(paragraph_fv), list(paragraph2_fv))\n",
    "1-cosine(paragraph_fv, paragraph3_fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph2_fv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# predifine communities and add tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/12/2019 16:05:59 - WARNING - pytorch_transformers.tokenization_openai -   ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "09/12/2019 16:06:00 - INFO - pytorch_transformers.tokenization_utils -   Adding _start_ to the vocabulary\n",
      "09/12/2019 16:06:00 - INFO - pytorch_transformers.tokenization_utils -   Adding _delimiter_ to the vocabulary\n",
      "09/12/2019 16:06:00 - INFO - pytorch_transformers.tokenization_utils -   Adding _classify_ to the vocabulary\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "init_weights() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-1652c4d75a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0mmodel_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../models/gpt-se/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m \u001b[0mpt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT_Inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-1652c4d75a4a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_dir, device, model_type)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"pytorch_model.bin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"MC\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIGPTDoubleHeadsModel_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"SIM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIGPTSimHeadsModel_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-1652c4d75a4a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_choice_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequenceSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtie_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pipenv/gpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pipenv/gpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pipenv/gpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: init_weights() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_transformers.modeling_openai import OpenAIGPTPreTrainedModel,OpenAIGPTConfig,OpenAIGPTModel, \\\n",
    "                                            OpenAIGPTDoubleHeadsModel,SequenceSummary\n",
    "from pytorch_transformers.tokenization_openai import OpenAIGPTTokenizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def getScore(vec1, vec2):\n",
    "    return dot(vec1, vec2)/(norm(vec1)*norm(vec2))\n",
    "\n",
    "def splitText(text):\n",
    "    # returns list of sentences\n",
    "    text = text.strip()\n",
    "    if not text.endswith((\".\",\"?\",\"!\")):\n",
    "        text+=\".\"\n",
    "    \n",
    "    text = text.replace(\"?.\",\"?\")\n",
    "    split_text = nltk.sent_tokenize(text)\n",
    "    return split_text\n",
    "\n",
    "\n",
    "\n",
    "class OpenAIGPTDoubleHeadsModel_custom(OpenAIGPTPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(OpenAIGPTDoubleHeadsModel_custom, self).__init__(config)\n",
    "        self.transformer = OpenAIGPTModel(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.multiple_choice_head = SequenceSummary(config)\n",
    "        self.apply(self.init_weights)\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
    "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
    "        \"\"\"\n",
    "        self._tie_or_clone_weights(self.lm_head,\n",
    "                                   self.transformer.tokens_embed)\n",
    "\n",
    "    def forward(self, input_ids, mc_token_ids=None, lm_labels=None, mc_labels=None, token_type_ids=None,position_ids=None, head_mask=None):\n",
    "        \n",
    "        transformer_outputs = self.transformer(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,head_mask=head_mask)\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        return hidden_states\n",
    "\n",
    "class OpenAIGPTSimHeadsModel_custom(OpenAIGPTPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(OpenAIGPTSimHeadsModel_custom, self).__init__(config)\n",
    "        self.transformer = OpenAIGPTModel(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.multiple_choice_head = SequenceSummary(config)\n",
    "        self.similarity_head = SequenceSimilarity(config)\n",
    "        self.apply(self.init_weights)\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
    "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
    "        \"\"\"\n",
    "        self._tie_or_clone_weights(self.lm_head,\n",
    "                                   self.transformer.tokens_embed)\n",
    "\n",
    "    def forward(self, input_ids, mc_token_ids=None, lm_labels=None, mc_labels=None, token_type_ids=None,position_ids=None, head_mask=None):\n",
    "        \n",
    "        transformer_outputs = self.transformer(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,head_mask=head_mask)\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        return hidden_states\n",
    "\n",
    "class GPT_Inference:\n",
    "    def __init__(self,model_dir, device = torch.device(\"cuda\"),model_type=\"MC\"):\n",
    "        \"\"\"\n",
    "        Declare a GPT Model for inference.\n",
    "        \n",
    "        Usage Example:\n",
    "        gpt_model = GPT_Inference(\"/home/ether/Desktop/gpt_experiments/models/model_trans/\")\n",
    "        feature1 = gpt_model.get_text_features(\"Enter text 1 here\") \n",
    "        feature2 = gpt_model.get_text_features(\"Enter text 2 here\")\n",
    "        feature3 = gpt_model.get_para_feats(\"Enter paragraph text here...\")\n",
    "        score = getScore(feature1,feature2)\n",
    "        \"\"\"\n",
    "        self.special_tokens = ['_start_', '_delimiter_', '_classify_']\n",
    "        self.tokenizer = OpenAIGPTTokenizer(model_dir+\"vocab.json\",model_dir+\"merges.txt\")\n",
    "        self.tokenizer.add_tokens(self.special_tokens)\n",
    "        self.config = OpenAIGPTConfig(model_dir+\"config.json\")\n",
    "        self.config.vocab_size = self.config.vocab_size+self.config.n_special\n",
    "        self.device = device\n",
    "        self.state_dict = torch.load(model_dir+\"pytorch_model.bin\",map_location=self.device)\n",
    "        if model_type==\"MC\":\n",
    "            self.model = OpenAIGPTDoubleHeadsModel_custom(config=self.config)\n",
    "        elif model_type==\"SIM\":\n",
    "            self.model = OpenAIGPTSimHeadsModel_custom(config=self.config)\n",
    "        self.model.load_state_dict(self.state_dict)\n",
    "        self.special_tokens_ids = list(self.tokenizer.convert_tokens_to_ids(token) for token in self.special_tokens)\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def getReshapedFeatures(self,np_array):\n",
    "        np_array_mp = np_array.reshape(-1).detach().cpu().numpy()\n",
    "        return np_array_mp\n",
    "\n",
    "    def get_text_feats(self,text):\n",
    "        #This function takes text as input and return feature vector (tensor) of shape (1,1,768) as output\n",
    "        \n",
    "        # Compute the max input length for the Transformer\n",
    "        max_length = self.config.n_positions  - 2\n",
    "        \n",
    "        \n",
    "        encoded_data = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
    "        input_ids = [self.special_tokens_ids[0]] + encoded_data[:max_length] + [self.special_tokens_ids[2]]\n",
    "        input_tensor = torch.tensor([[input_ids]],device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            lm_text_feature = self.model(input_tensor)[:,:,-1]\n",
    "        lm_text_feature = self.getReshapedFeatures(lm_text_feature)\n",
    "        return lm_text_feature\n",
    "    \n",
    "    def get_para_feats(self,para):\n",
    "        #This function takes text as input and return feature vector (tensor) of shape (1,1,768) as output\n",
    "        \n",
    "        sent_features = [self.get_text_feats(text) for text in splitText(para)]\n",
    "        sent_features = np.mean(np.array(sent_features),0).reshape(1,-1)\n",
    "        return sent_features\n",
    "\n",
    "class GPT_OOB_Inference:\n",
    "    def __init__(self, device=torch.device(\"cpu\")):\n",
    "        self.tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "        self.config = OpenAIGPTConfig()\n",
    "        self.config.output_hidden_states = True\n",
    "        self.model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt',config = self.config)\n",
    "\n",
    "        self.tokenizer.add_special_tokens({'bos_token':'_start_','eos_token':'_delimiter_','cls_token': '_classify_'})  # Add a [CLS] to the vocabulary (we should train it also!)\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    def get_text_feats(self,text):\n",
    "        #This function takes text as input and return feature vector (tensor) of shape (1,1,768) as output\n",
    "        text = \"_start_ \"+text+\" _classify_\"\n",
    "        # Compute the max input length for the Transformer\n",
    "        max_length = self.config.n_positions  - 2\n",
    "        \n",
    "        encoded_data = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
    "        input_ids =  encoded_data[:max_length]\n",
    "        mc_token_ids = len(input_ids)-1\n",
    "        input_tensor = torch.tensor([[input_ids]],device=self.device)\n",
    "        mc_token_tensor = torch.tensor([[mc_token_ids]],device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lm_text_feature = self.model(input_tensor, mc_token_tensor)[2][-1].detach().cpu().numpy()\n",
    "        return lm_text_feature[0,0,-1]\n",
    "    \n",
    "    def get_para_feats(self,para):\n",
    "        sent_features = [self.get_text_feats(text) for text in splitText(para)]\n",
    "        sent_features = np.mean(np.array(sent_features),0)\n",
    "        return sent_features\n",
    "\n",
    "\n",
    "model_loc = '../models/gpt-se/'\n",
    "pt_model = GPT_Inference(model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## load it as csv.\n",
    "import pandas\n",
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "df = pandas.read_csv('../data/raw/se_medium_.csv', index_col=False, header=0);\n",
    "\n",
    "article = {}\n",
    "article_org = {}\n",
    "for index, para in enumerate(list(df['Data'])):\n",
    "    article_org[index] = para\n",
    "    article[index] = tp.preprocess(para, stop_words=False, remove_punct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_para_feats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a5d5df8e1e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_para_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#fv[index] = getBERTFeatures(model1, sent, attn_head_idx=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#fv[index] = getBERTFeatures_KP(model1, sent, attn_head_idx=-3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_para_feats' is not defined"
     ]
    }
   ],
   "source": [
    "fv = {}\n",
    "\n",
    "for index, sent in enumerate(article.keys()):\n",
    "    fv[index] = get_para_feats(model1,' '.join(str(i) for i in article[sent]))\n",
    "    #fv[index] = getBERTFeatures(model1, sent, attn_head_idx=-1)\n",
    "    #fv[index] = getBERTFeatures_KP(model1, sent, attn_head_idx=-3)\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     gpt_model = GPT_Inference(\"/home/ether/Desktop/gpt_experiments/models/model_lm+mc+sim/\",model_type=\"SIM\")\n",
    "#     text1 = \"I'm Kyle and welcome to text with Edie we're gonna be running a ton of services from this 23 terabyte server in an upcoming video before we get started. We have to go over Docker. Thank you to hover.com for sponsoring this episode. You can get 10% off your own custom domain name at hover.com forward slash text with what is Docker Docker is mainly a software development platform and kind of the virtualization technology that makes it easy for us to develop and deploy apps inside of neatly packaged virtual containerized environments meaning apps run the same no matter where they are or what machine they're running Docker containers can be deployed to just about any machine without any compatibility issues. So your software stay is system agnostic making software simpler to use less work to develop and easier to maintain and deploy these containers running on your computer or server act like little micro heaters each with very specific jobs each with their own operating system their own isolated CPU processes memory and network resources and because of this they can be easily added removed stop and start it again without affecting each.\"\n",
    "#     text2 = \"welcome\"\n",
    "#     feature1 = gpt_model.get_para_feats(text1)\n",
    "#     for text2 in text1.split():\n",
    "#         feature2 = gpt_model.get_text_feats(text2)\n",
    "#         score = getScore(feature1,feature2)\n",
    "#         if score>0.8:\n",
    "#             print(\"\\n<2>c \",text2,\"\\tScorec: \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tags testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/processed/master_tag+para_cluster_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>As an example of such approach, lets take a ja...</td>\n",
       "      <td>['Scala', 'Java', 'Api Design']</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>You might be wondering why Repr is not a type ...</td>\n",
       "      <td>['Scala', 'Java', 'Api Design']</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>So its turned out that we can build genericall...</td>\n",
       "      <td>['Scala', 'Java', 'Api Design']</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The reasons, as we previously mentioned, used ...</td>\n",
       "      <td>['Agile', 'Software Architecture']</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>But, how do these autonomous and fast-paced te...</td>\n",
       "      <td>['Agile', 'Software Architecture']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>However, the assumption that the database tech...</td>\n",
       "      <td>['Domain Driven Design', 'Cqrs', 'Distributed ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>These concerns could be overcome by letting th...</td>\n",
       "      <td>['Domain Driven Design', 'Cqrs', 'Distributed ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>The biggest pain with UUIDs are cost on the st...</td>\n",
       "      <td>['Domain Driven Design', 'Cqrs', 'Distributed ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Even when youre not travelling, as a public re...</td>\n",
       "      <td>['Developer Relations', 'Burnout', 'Motivation...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>If you have some tips to share, please leave a...</td>\n",
       "      <td>['Developer Relations', 'Burnout', 'Motivation...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>By Brett Luckabaugh, Account Principal, Contin...</td>\n",
       "      <td>['DevOps', 'Risk Management', 'Team Building',...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Here is an exhaustive list of the things many ...</td>\n",
       "      <td>['DevOps', 'Risk Management', 'Team Building',...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Project lead time shoots through the roof. Eve...</td>\n",
       "      <td>['DevOps', 'Risk Management', 'Team Building',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>At this point, Ive hopefully gotten at least a...</td>\n",
       "      <td>['DevOps', 'Risk Management', 'Team Building',...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>To get out of this hole, something has to give...</td>\n",
       "      <td>['DevOps', 'Risk Management', 'Team Building',...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Another example that actually does increase ri...</td>\n",
       "      <td>['DevOps', 'Risk Management', 'Team Building',...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>The budget Give developers a financial andon c...</td>\n",
       "      <td>['DevOps', 'Risk Management', 'Team Building',...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>For detailed information, you can refer to his...</td>\n",
       "      <td>['Clean Architecture', 'Angular', 'Architectur...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>This continuous adaptation to business demands...</td>\n",
       "      <td>['Microservices', 'Organic Architecture', 'Ser...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Always remember to build semantic, well-struct...</td>\n",
       "      <td>['JavaScript', 'Reactjs', 'Spring Framework', ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>A Passthrough setup is one specific to Network...</td>\n",
       "      <td>['DevOps', 'Load Balancing', 'Rust']</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Internally, the architecture is a manager-work...</td>\n",
       "      <td>['DevOps', 'Load Balancing', 'Rust']</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>The number of workers are configurable in the ...</td>\n",
       "      <td>['DevOps', 'Load Balancing', 'Rust']</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>To run Convey in Passthrough mode, we need a c...</td>\n",
       "      <td>['DevOps', 'Load Balancing', 'Rust']</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Another difference between DSR and Passthrough...</td>\n",
       "      <td>['DevOps', 'Load Balancing', 'Rust']</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Whether you say yes to REST, so-called-REST, o...</td>\n",
       "      <td>['GraphQL', 'Rest Api', 'API', 'Software Devel...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Sales Lofts core platform has thousands of use...</td>\n",
       "      <td>['Data Warehouse', 'Data Modeling', 'SaaS', 'C...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>Dimensions (tables) contain textual descriptio...</td>\n",
       "      <td>['Data Warehouse', 'Data Modeling', 'SaaS', 'C...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>One important concept to understand here is th...</td>\n",
       "      <td>['Data Warehouse', 'Data Modeling', 'SaaS', 'C...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>After the groundwork of designing the dimensio...</td>\n",
       "      <td>['Data Warehouse', 'Data Modeling', 'SaaS', 'C...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3647</th>\n",
       "      <td>3647</td>\n",
       "      <td>Gerald Weinbergs The Psychology of Computer Pr...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>3648</td>\n",
       "      <td>Gerald Weinbergs An Introduction to General Sy...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>3649</td>\n",
       "      <td>Henry Hazlitts Economics in One Lesson enhance...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650</th>\n",
       "      <td>3650</td>\n",
       "      <td>Keith Stanovichs How to Think Straight About P...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>3651</td>\n",
       "      <td>John Allen Paulos Innumeracy gives programmers...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3652</th>\n",
       "      <td>3652</td>\n",
       "      <td>Barry Boehm, Jo Ann Lane, Supannika Koolmanojw...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3653</th>\n",
       "      <td>3653</td>\n",
       "      <td>John Lakos Large-Scale C++ Software Design bol...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3654</th>\n",
       "      <td>3654</td>\n",
       "      <td>Steve Krugs Dont Make Me Think widens the curr...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>3655</td>\n",
       "      <td>Robert Martins Agile Software Development, Pri...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>3656</td>\n",
       "      <td>Adam Shostacks Threat Modeling bolsters the cu...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>3657</td>\n",
       "      <td>Robert Martins Clean Code provides a set of pr...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>3658</td>\n",
       "      <td>Many attacks are required to handle the compli...</td>\n",
       "      <td>['Programming', 'Software Development', 'Softw...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>3659</td>\n",
       "      <td>The primary purpose of the OOPs concept is to ...</td>\n",
       "      <td>['Programming', 'Java', 'Java Oops Concept', '...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>3660</td>\n",
       "      <td>It started with an innocent question: Is it wo...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>3661</td>\n",
       "      <td>Its creator, Ilya did the heavy-lifting for us...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3662</th>\n",
       "      <td>3662</td>\n",
       "      <td>What does a top repository mean? Is it the mos...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>3663</td>\n",
       "      <td>Whatever repositories youre looking for, you c...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>3664</td>\n",
       "      <td>Forking a project takes more technical experti...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3665</th>\n",
       "      <td>3665</td>\n",
       "      <td>The third option Github offers is to explore t...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>3666</td>\n",
       "      <td>A good starting point is to look at blockchain...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>3667</td>\n",
       "      <td>How many projects does a repository have? Its ...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>3668</td>\n",
       "      <td>Pagination is not the biggest problem here. We...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>3669</td>\n",
       "      <td>Graph QL was created to address these woes. Yo...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3670</th>\n",
       "      <td>3670</td>\n",
       "      <td>But Graph QL is not a graph database by itself...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>3671</td>\n",
       "      <td>How do I run a query for the 100 repos I colle...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>3672</td>\n",
       "      <td>I can finally run a single query instead of 15...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>3673</td>\n",
       "      <td>I said goodbye to the editor and started to lo...</td>\n",
       "      <td>['Github', 'Google Big Query', 'GraphQL', 'Pro...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3674</th>\n",
       "      <td>3674</td>\n",
       "      <td>One of the biggest issues is that in order to ...</td>\n",
       "      <td>['GraphQL', 'Nodejs', 'API', 'Backend', 'Micro...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>3675</td>\n",
       "      <td>Within the confines of a single service, the d...</td>\n",
       "      <td>['GraphQL', 'Nodejs', 'API', 'Backend', 'Micro...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3676</th>\n",
       "      <td>3676</td>\n",
       "      <td>It will open up the gates to the abstraction h...</td>\n",
       "      <td>['']</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3677 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "0              0  As an example of such approach, lets take a ja...   \n",
       "1              1  You might be wondering why Repr is not a type ...   \n",
       "2              2  So its turned out that we can build genericall...   \n",
       "3              3  The reasons, as we previously mentioned, used ...   \n",
       "4              4  But, how do these autonomous and fast-paced te...   \n",
       "...          ...                                                ...   \n",
       "3672        3672  I can finally run a single query instead of 15...   \n",
       "3673        3673  I said goodbye to the editor and started to lo...   \n",
       "3674        3674  One of the biggest issues is that in order to ...   \n",
       "3675        3675  Within the confines of a single service, the d...   \n",
       "3676        3676  It will open up the gates to the abstraction h...   \n",
       "\n",
       "                                                   tags  cluster  \n",
       "0                       ['Scala', 'Java', 'Api Design']       15  \n",
       "1                       ['Scala', 'Java', 'Api Design']       15  \n",
       "2                       ['Scala', 'Java', 'Api Design']       15  \n",
       "3                    ['Agile', 'Software Architecture']       12  \n",
       "4                    ['Agile', 'Software Architecture']        1  \n",
       "...                                                 ...      ...  \n",
       "3672  ['Github', 'Google Big Query', 'GraphQL', 'Pro...        8  \n",
       "3673  ['Github', 'Google Big Query', 'GraphQL', 'Pro...       18  \n",
       "3674  ['GraphQL', 'Nodejs', 'API', 'Backend', 'Micro...       19  \n",
       "3675  ['GraphQL', 'Nodejs', 'API', 'Backend', 'Micro...        8  \n",
       "3676                                               ['']       19  \n",
       "\n",
       "[3677 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of tags present: 15918\n",
      "no of unique tags present: 259\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "tag = [] \n",
    "for tags in df['tags']:\n",
    "    for t in ast.literal_eval(tags):\n",
    "        tag.append(t)\n",
    "print (\"no of tags present:\", len(tag))\n",
    "unique_tag_unr = list(set(tag))\n",
    "unique_tag = []\n",
    "for u in unique_tag_unr:\n",
    "    if tag.count(u) > 10:\n",
    "        unique_tag.append(u)\n",
    "print (\"no of unique tags present:\", len(unique_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.client import Config\n",
    "aws_config = Config(\n",
    "        connect_timeout=60,\n",
    "        read_timeout=300,\n",
    "        retries={\"max_attempts\": 0},\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "lambda_client = client(\"lambda\", config=aws_config)\n",
    "\n",
    "def get_embeddings(input_list, req_data=None):\n",
    "\n",
    "    if req_data is None:\n",
    "        lambda_payload = {\"body\": {\"text_input\": input_list}}\n",
    "    else:\n",
    "        lambda_payload = {\"body\": {\"request\": req_data, \"text_input\": input_list}}\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Invoking lambda function\")\n",
    "        invoke_response = lambda_client.invoke(\n",
    "            FunctionName=\"keyphrase_ranker\",\n",
    "            InvocationType=\"RequestResponse\",\n",
    "            Payload=json.dumps(lambda_payload),\n",
    "        )\n",
    "\n",
    "        lambda_output = (\n",
    "            invoke_response[\"Payload\"].read().decode(\"utf8\").replace(\"'\", '\"')\n",
    "        )\n",
    "        response = json.loads(lambda_output)\n",
    "        status_code = response[\"statusCode\"]\n",
    "        response_body = response[\"body\"]\n",
    "\n",
    "        if status_code == 200:\n",
    "            embedding_vector = np.asarray(json.loads(response_body)[\"embeddings\"])\n",
    "\n",
    "        else:\n",
    "            embedding_vector = np.asarray(json.loads(response_body)[\"embeddings\"])\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        pass\n",
    "    return embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/13/2019 18:58:54 - INFO - __main__ -   Invoking lambda function\n",
      "09/13/2019 18:58:56 - INFO - __main__ -   Invoking lambda function\n"
     ]
    }
   ],
   "source": [
    "text_2 = [\"I think filter the it mostly likely we see this like the like like and all they they should get filtered though so that is that. So there nine to fifty. \", \" Okay But when we mute right? And if you speak it is still it is able to record everything and gives a chapter motors.\", \"We are basically yeah, like trial error things like check is few things for like help first optimized few issues in increase the page scores so okay.\",\"I think we can be can can enable the recording or the following we know give even on the some of the browser that we disable live we enable the dealer and more than recorder it so so us bit like that.\", \" That is we can build a small thing but big ticket items that I am coming to about search and and the basics as we should be able to start with this getting on our services to use my elastic that search something so we should start with the basics. So that can you know. \",\"So yesterday I was one changes related to and today we partnered one issue with the booking meeting so like from the night whenever we create in meeting it because all because all creating and Janice and the merging so that still speaks other than also start looking into this C P s pipe that was getting if there have thing because for the I see like always are running at percent CPU. So Janice might be less, but at least the I will also go through this page anything that you can introduce so yeah, I listen into that one. So way through four screen screen shared quality and and yeah, let us let us do is screenshot and we wanna try the screen to you guys are able to see the screen share right. \", \" Nice has and like four P s our feature that so usually the even if the network list. There is there is not have the transfer that will just feature it quality. Okay can then we can side was not able to do me can go that is first we may any yeah I do not know need maybe S p need will two cents we was in from my and like. \", \"Yeah so what are we can we can lay with the inside so when the fully that difference will build and the second term will be get. Let is should also be a smaller consultants. Yeah yeah. \", \"  Yeah so I am figure out how to change like call the URL learning meeting but one issue is that when like ios is the first user to join in the meeting at that time call video ID D there is something identifier to interact with call errors. So like when there is is only one person in the call and the Ios called you ID is not and call quick call is not started. So like I am this to end this. So like I think I should be able to be very by today then and you so he will even if the call so but is not call he like tightly okay that because can we can give existing change also because he you are working to whatever the thing that is taking time basically be everybody what change that we have done. So that I I can experience and if they have the administration we can go it on that give by today what.\"]\n",
    "text = [\"I think filter the it mostly likely we see this like the like like and all they they should get filtered though so that is that. So there nine to fifty. Okay But when we mute right? And if you speak it is still it is able to record everything and gives a chapter motors.\",\" That is that and what about the.  That that easy that will be earlier like were everything so.  We are basically yeah, like trial error things like check is few things for like help first optimized few issues in increase the page scores so okay.\",\"  If you mute and if you play some audio hide reach to the leads there so that audio transcript. They have the audience when it we are.I think we can be can can enable the recording or the following we know give even on the some of the browser that we disable live we enable the dealer and more than recorder it so so us bit like that. Is first up all Disable for that because I have to test few website issues as well. Not actually so what is the things that we are looking so optimize side side.\",\" Everything is done I week do implement on release. I have the ether so I just need to implement add and implement it that is it. My myself this image web configuration complete and production also it is all up underneath that task working was like removing in the old IPS in the media commit proxy.\",\" It is like this go through the network load balance a lot of our time. So if we think that a like to separate issue with new way we can directly accept the can fix at least we can do it by doing that header side right. Yeah. That is we can build a small thing but big ticket items that I am coming to about search and and the basics as we should be able to start with this getting on our services to use my elastic that search something so we should start with the basics. So that can you know.\",\" So we need to send this segments to web socket right now that there is a new requirement of when the segments also next to the markers right. Yes that the Transcription was that yeah that once once why is done we will saying those things would be like threatening an ocean right so it includes be a contact. So it is like we need to do like the way that we are sending markers we we need. This is basic of can add some client ID kind of like API with and always push the header and we can it.\",\" Yeah so I know what let us do that first and then we can spend you know after that we can spend time on you my U S screen this one we say exactly search Jenkins like can you can hand and not multiple. I think know those are high priority oh. So if I define me and audio still go go go to the recorder.\"]\n",
    "#fv = list(map(lambda x:get_embeddings(x, req_data=None), text))\n",
    "fv = get_embeddings(text, req_data=None)\n",
    "fv_tag = get_embeddings([i for i in unique_tag if i!=\"\"], req_data=None)\n",
    "#fv_tag = list(map(lambda x:get_embeddings(x, req_data=None), [i for i in unique_tag if i!=\"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sentence: \n",
      "\n",
      " I think filter the it mostly likely we see this like the like like and all they they should get filtered though so that is that. So there nine to fifty. Okay But when we mute right? And if you speak it is still it is able to record everything and gives a chapter motors.\n",
      "\n",
      " most similar tags:  Internet of Things , API , User Experience , UX\n",
      "\n",
      "\n",
      "\n",
      "sentence: \n",
      "\n",
      "  That is that and what about the.  That that easy that will be earlier like were everything so.  We are basically yeah, like trial error things like check is few things for like help first optimized few issues in increase the page scores so okay.\n",
      "\n",
      " most similar tags:  Unit Testing , Test Automation , Software Estimation , Scalability\n",
      "\n",
      "\n",
      "\n",
      "sentence: \n",
      "\n",
      "   If you mute and if you play some audio hide reach to the leads there so that audio transcript. They have the audience when it we are.I think we can be can can enable the recording or the following we know give even on the some of the browser that we disable live we enable the dealer and more than recorder it so so us bit like that. Is first up all Disable for that because I have to test few website issues as well. Not actually so what is the things that we are looking so optimize side side.\n",
      "\n",
      " most similar tags:  Monitoring , Test Automation , User Experience , Unit Testing\n",
      "\n",
      "\n",
      "\n",
      "sentence: \n",
      "\n",
      "  Everything is done I week do implement on release. I have the ether so I just need to implement add and implement it that is it. My myself this image web configuration complete and production also it is all up underneath that task working was like removing in the old IPS in the media commit proxy.\n",
      "\n",
      " most similar tags:  Github , Gitlab , Microservices , Unit Testing\n",
      "\n",
      "\n",
      "\n",
      "sentence: \n",
      "\n",
      "  It is like this go through the network load balance a lot of our time. So if we think that a like to separate issue with new way we can directly accept the can fix at least we can do it by doing that header side right. Yeah. That is we can build a small thing but big ticket items that I am coming to about search and and the basics as we should be able to start with this getting on our services to use my elastic that search something so we should start with the basics. So that can you know.\n",
      "\n",
      " most similar tags:  Test Automation , User Experience , Scalability , Agile Methodology\n",
      "\n",
      "\n",
      "\n",
      "sentence: \n",
      "\n",
      "  So we need to send this segments to web socket right now that there is a new requirement of when the segments also next to the markers right. Yes that the Transcription was that yeah that once once why is done we will saying those things would be like threatening an ocean right so it includes be a contact. So it is like we need to do like the way that we are sending markers we we need. This is basic of can add some client ID kind of like API with and always push the header and we can it.\n",
      "\n",
      " most similar tags:  API , Scalability , Domain Driven Design , Golang\n",
      "\n",
      "\n",
      "\n",
      "sentence: \n",
      "\n",
      "  Yeah so I know what let us do that first and then we can spend you know after that we can spend time on you my U S screen this one we say exactly search Jenkins like can you can hand and not multiple. I think know those are high priority oh. So if I define me and audio still go go go to the recorder.\n",
      "\n",
      " most similar tags:  Software , Internet of Things , Unit Testing , API\n"
     ]
    }
   ],
   "source": [
    "for index, tex in enumerate(text):\n",
    "    closest = -1\n",
    "    closest_tag = None\n",
    "    closest_2= -1\n",
    "    closest_tag_2 = None\n",
    "    closest_3 = -1\n",
    "    closest_tag_3 = None\n",
    "    closest_4= -1\n",
    "    closest_tag_4 = None\n",
    "    for index2, t in enumerate([i for i in unique_tag if i!=\"\"]):\n",
    "        if t!=\"\":\n",
    "            score = 1 - cosine(fv[index],fv_tag[index2])\n",
    "            if score > closest:\n",
    "                closest = score\n",
    "                closest_tag = t\n",
    "            elif score > closest_2:\n",
    "                closest_tag_2 = t\n",
    "                closest_2 = score\n",
    "            elif score > closest_3:\n",
    "                closest_tag_3 = t\n",
    "                closest_3 = score\n",
    "            elif score > closest_4:\n",
    "                closest_tag_4 = t\n",
    "                closest_4 = score\n",
    "    print (\"\\n\\n\\nsentence: \\n\\n\", tex)\n",
    "    print (\"\\n most similar tags: \", closest_tag + \" , \" + closest_tag_2 + \" , \" + closest_tag_3 + \" , \" + closest_tag_4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/CSO.3.1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tags = list(set((df[df.keys()[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = extra_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<https://cso.kmi.open.ac.uk/topics/sequent_calculus>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
