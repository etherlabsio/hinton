{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T06:56:24.817955Z",
     "start_time": "2019-06-11T06:56:20.999861Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "import nltk\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import combinations, product\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from pytorch_pretrained_bert import (OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer,\n",
    "                                     OpenAIAdam, cached_path, WEIGHTS_NAME, CONFIG_NAME)\n",
    "from pytorch_pretrained_bert.modeling_openai import OpenAIGPTPreTrainedModel,OpenAIGPTDoubleHeadsModel,OpenAIGPTConfig,OpenAIGPTModel,OpenAIGPTLMHead\n",
    "\n",
    "from scipy.spatial.distance import cosine, cityblock\n",
    "import iso8601\n",
    "from datetime import datetime\n",
    "import json\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIGPTLMHead_custom(nn.Module):\n",
    "    \"\"\" Language Model Head for the transformer \"\"\"\n",
    "\n",
    "    def __init__(self, model_embeddings_weights, config):\n",
    "        super(OpenAIGPTLMHead_custom, self).__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.predict_special_tokens = config.predict_special_tokens\n",
    "        embed_shape = model_embeddings_weights.shape\n",
    "        #print(\"shape check\",(model_embeddings_weights[1]))\n",
    "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
    "        self.set_embeddings_weights(model_embeddings_weights)\n",
    "\n",
    "    def set_embeddings_weights(self, model_embeddings_weights, predict_special_tokens=True):\n",
    "        self.predict_special_tokens = predict_special_tokens\n",
    "        embed_shape = model_embeddings_weights.shape\n",
    "        self.decoder.weight = model_embeddings_weights  # Tied weights\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "#         print('decoder weight')\n",
    "#         print((hidden_state.shape))\n",
    "        lm_logits = self.decoder(hidden_state)\n",
    "#         print(lm_logits.shape)\n",
    "        if not self.predict_special_tokens:\n",
    "            lm_logits = lm_logits[..., :self.vocab_size]\n",
    "#             print(\"lm_logits.shape: \",lm_logits.shape)\n",
    "        return lm_logits\n",
    "\n",
    "class OpenAIGPTMultipleChoiceHead_custom(nn.Module):\n",
    "    \"\"\" Classifier Head for the transformer \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(OpenAIGPTMultipleChoiceHead_custom, self).__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = nn.Dropout2d(config.resid_pdrop)  # To reproduce the noise_shape parameter of TF implementation\n",
    "        self.linear = nn.Linear(config.n_embd, 1)\n",
    "\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, hidden_states, mc_token_ids):\n",
    "        # Classification logits\n",
    "        # hidden_state (bsz, num_choices, seq_length, hidden_size)\n",
    "        # mc_token_ids (bsz, num_choices)\n",
    "        mc_token_ids = mc_token_ids.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, hidden_states.size(-1))\n",
    "        # (bsz, num_choices, 1, hidden_size)\n",
    "        #print('mc_token_ids: ', mc_token_ids[0][0].shape,mc_token_ids[0][1].shape)\n",
    "        #print('mc_token_ids.shape: ', mc_token_ids.shape)\n",
    "        #print('Hidden states before compute: ', hidden_states.shape)\n",
    "#         print(\"Token IDS:\",mc_token_ids)\n",
    "        multiple_choice_h = hidden_states.gather(2, mc_token_ids).squeeze(2)\n",
    "        #print('After transformation: ', multiple_choice_h.shape)\n",
    "        # (bsz, num_choices, hidden_size)\n",
    "#         multiple_choice_h = self.dropout(multiple_choice_h.transpose(1, 2)).transpose(1, 2)\n",
    "#         multiple_choice_logits = self.linear(multiple_choice_h).squeeze(-1)\n",
    "        # (bsz, num_choices)\n",
    "        return multiple_choice_h\n",
    "\n",
    "class OpenAIGPTDoubleHeadsModel_custom(OpenAIGPTPreTrainedModel):\n",
    "    \"\"\"\n",
    "    OpenAI GPT model with a Language Modeling and a Multiple Choice head (\"Improving Language Understanding by Generative Pre-Training\").\n",
    "    OpenAI GPT use a single embedding matrix to store the word and special embeddings.\n",
    "    Special tokens embeddings are additional tokens that are not pre-trained: [SEP], [CLS]...\n",
    "    Special tokens need to be trained during the fine-tuning if you use them.\n",
    "    The number of special embeddings can be controled using the `set_num_special_tokens(num_special_tokens)` function.\n",
    "    The embeddings are ordered as follow in the token embeddings matrice:\n",
    "        [0,                                                         ----------------------\n",
    "         ...                                                        -> word embeddings\n",
    "         config.vocab_size - 1,                                     ______________________\n",
    "         config.vocab_size,\n",
    "         ...                                                        -> special embeddings\n",
    "         config.vocab_size + config.n_special - 1]                  ______________________\n",
    "    where total_tokens_embeddings can be obtained as config.total_tokens_embeddings and is:\n",
    "        total_tokens_embeddings = config.vocab_size + config.n_special\n",
    "    You should use the associate indices to index the embeddings.\n",
    "    Params:\n",
    "        `config`: a OpenAIGPTConfig class instance with the configuration to build a new model\n",
    "        `output_attentions`: If True, also output attentions weights computed by the model at each layer. Default: False\n",
    "        `keep_multihead_output`: If True, saves output of the multi-head attention module with its gradient.\n",
    "            This can be used to compute head importance metrics. Default: False\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length] with the BPE token\n",
    "            indices selected in the range [0, total_tokens_embeddings[\n",
    "        `mc_token_ids`: a torch.LongTensor of shape [batch_size, num_choices] with the index of the token from\n",
    "            which we should take the hidden state to feed the multiple choice classifier (usually last token of the sequence)\n",
    "        `position_ids`: an optional torch.LongTensor with the same shape as input_ids\n",
    "            with the position indices (selected in the range [0, config.n_positions - 1[.\n",
    "        `token_type_ids`: an optional torch.LongTensor with the same shape as input_ids\n",
    "            You can use it to add a third type of embedding to each input token in the sequence\n",
    "            (the previous two being the word and position embeddings).\n",
    "            The input, position and token_type embeddings are summed inside the Transformer before the first\n",
    "            self-attention block.\n",
    "        `lm_labels`: optional language modeling labels: torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n",
    "            with indices selected in [-1, 0, ..., total_tokens_embeddings]. All labels set to -1 are ignored (masked), the loss\n",
    "            is only computed for the labels set in [0, ..., total_tokens_embeddings]\n",
    "        `multiple_choice_labels`: optional multiple choice labels: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_choices].\n",
    "        `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
    "            It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
    "    Outputs:\n",
    "        if `lm_labels` and `multiple_choice_labels` are not `None`:\n",
    "            Outputs a tuple of losses with the language modeling loss and the multiple choice loss.\n",
    "        else: a tuple with\n",
    "            `lm_logits`: the language modeling logits as a torch.FloatTensor of size [batch_size, num_choices, sequence_length, total_tokens_embeddings]\n",
    "            `multiple_choice_logits`: the multiple choice logits as a torch.FloatTensor of size [batch_size, num_choices]\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into BPE token ids\n",
    "    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]]])  # (bsz, number of choice, seq length)\n",
    "    mc_token_ids = torch.LongTensor([[2], [1]]) # (bsz, number of choice)\n",
    "    config = modeling_openai.OpenAIGPTOpenAIGPTMultipleChoiceHead_customOpenAIGPTMultipleChoiceHead_customConfig()\n",
    "    model = modeling_openai.OpenAIGPTDoubleHeadsModel(config)\n",
    "    lm_logits, multiple_choice_logits = model(input_ids, mc_token_ids)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(OpenAIGPTDoubleHeadsModel_custom, self).__init__(config)\n",
    "        self.transformer = OpenAIGPTModel(config)\n",
    "        self.lm_head = OpenAIGPTLMHead_custom(self.transformer.tokens_embed.weight, config)\n",
    "        self.multiple_choice_head = OpenAIGPTMultipleChoiceHead_custom(config)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def set_num_special_tokens(self, num_special_tokens, predict_special_tokens=True):\n",
    "        \"\"\" Update input and output embeddings with new embedding matrice\n",
    "            Make sure we are sharing the embeddings\n",
    "        \"\"\"\n",
    "        #self.config.predict_special_tokens = self.transformer.config.predict_special_tokens = predict_special_tokens\n",
    "        self.transformer.set_num_special_tokens(num_special_tokens)\n",
    "        self.lm_head.set_embeddings_weights(self.transformer.tokens_embed.weight, predict_special_tokens=predict_special_tokens)\n",
    "\n",
    "    def forward(self, input_ids, mc_token_ids, lm_labels=None, mc_labels=None, token_type_ids=None,\n",
    "                position_ids=None, head_mask=None):\n",
    "        hidden_states = self.transformer(input_ids, position_ids, token_type_ids, head_mask)\n",
    "        if self.transformer.output_attentions:\n",
    "            all_attentions, hidden_states = hidden_states\n",
    "#         print('hidden states',len(hidden_states))\n",
    "        \n",
    "        hidden_states = hidden_states[-1] #layer #\n",
    "#         lm_logits = self.lm_head(hidden_states)\n",
    "#         hidden_feats = self.multiple_choice_head(hidden_states, mc_token_ids)\n",
    "#         print(\"FEAT.\",hidden_feats)\n",
    "#         losses = []\n",
    "#         if lm_labels is not None:\n",
    "#             shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "#             shift_labels = lm_labels[..., 1:].contiguous()\n",
    "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "#             losses.append(loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)))\n",
    "#         print(\"hidden state shape\",hidden_states.shape)\n",
    "        lm_logits = 0\n",
    "        return lm_logits, hidden_states #token #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "def listRightIndex(alist, value):\n",
    "    return len(alist) - alist[-1::-1].index(value) -1\n",
    "\n",
    "\n",
    "def pre_process_datasets(encoded_datasets, input_len, cap_length, start_token, delimiter_token, clf_token):\n",
    "    \"\"\" Pre-process datasets containing lists of tuples(story, 1st continuation, 2nd continuation, label)\n",
    "\n",
    "        To Transformer inputs of shape (n_batch, n_alternative, length) comprising for each batch, continuation:\n",
    "        input_ids[batch, alternative, :] = [start_token] + story[:cap_length] + [delimiter_token] + cont1[:cap_length] + [clf_token]\n",
    "    \"\"\"\n",
    "\n",
    "    tensor_datasets = []\n",
    "    for dataset in encoded_datasets:\n",
    "        n_batch = ceil(len(dataset[0][0])/cap_length)\n",
    "        input_ids = np.zeros((n_batch, 1, input_len), dtype=np.int64)\n",
    "        mc_token_ids = np.zeros((n_batch, 1), dtype=np.int64)\n",
    "        i = 0\n",
    "        init_pos = 0\n",
    "        end_pos = cap_length\n",
    "        for story, cont1, cont2, mc_label in dataset:\n",
    "            if n_batch!=0:\n",
    "                if n_batch==1:\n",
    "                    with_cont1 = [start_token] + story[:cap_length] + [clf_token]\n",
    "                    input_ids[i, 0, :len(with_cont1)] = with_cont1\n",
    "                    mc_token_ids[i, 0] = len(with_cont1) - 1\n",
    "                    i+=1\n",
    "                else:\n",
    "                    while i!=n_batch and end_pos<len(story):\n",
    "                        try:\n",
    "                            end_pos = init_pos + listRightIndex(story[init_pos:end_pos],story[-1])\n",
    "                        except ValueError:\n",
    "                            end_pos = init_pos+story[init_pos:].index(story[-1])\n",
    "                        with_cont1 = [start_token] + story[init_pos:end_pos+1] + [clf_token]\n",
    "                        input_ids[i, 0, :len(with_cont1)] = with_cont1\n",
    "                        mc_token_ids[i, 0] = len(with_cont1) - 1\n",
    "                        i+=1\n",
    "                        init_pos = end_pos+1\n",
    "                        end_pos = min(init_pos+cap_length-1,len(story))\n",
    "        all_inputs = (input_ids, mc_token_ids)#, lm_labels, mc_labels)\n",
    "        tensor_datasets.append(tuple(torch.tensor(t) for t in all_inputs))\n",
    "    return tensor_datasets\n",
    "\n",
    "def load_rocstories_dataset(dataset_path):\n",
    "    \"\"\" Output a list of tuples(story, 1st continuation, 2nd continuation, label) \"\"\"\n",
    "    with open(dataset_path, encoding='utf_8') as f:\n",
    "        f = csv.reader(f)\n",
    "        output = []\n",
    "        next(f) # skip the first line\n",
    "        for line in tqdm(f):\n",
    "            output.append(('.'.join(line[0 :4]), line[4], line[5], int(line[-1])))\n",
    "    return output\n",
    "\n",
    "def tokenize_and_encode(obj):\n",
    "    \"\"\" Tokenize and encode a nested object \"\"\"\n",
    "    if isinstance(obj, str):\n",
    "        return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "    elif isinstance(obj, int):\n",
    "        return obj\n",
    "    return list(tokenize_and_encode(o) for o in obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNSPScore(sample_text):\n",
    "    \n",
    "    m = torch.nn.Softmax()\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(sample_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [0]*tokenized_text.index('[SEP]')+[1]*(len(tokenized_text)-tokenized_text.index('[SEP]'))\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    pred_score, seq_rel, seq_out, pool_out = model1(tokens_tensor, segments_tensors)\n",
    "    return m(seq_rel).detach().numpy()[0][0] #returns probability of being next sentence\n",
    "\n",
    "def getSentMatchScore(sent1, sent2, nsp_dampening_factor = 0.7):\n",
    "    \n",
    "    sent1_feats = getBERTFeatures(model1, sent1, attn_head_idx)\n",
    "    sent2_feats = getBERTFeatures(model1, sent2, attn_head_idx)\n",
    "    \n",
    "    cosine_distance = 1- cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = np.mean([nsp_score_1,nsp_score_2])*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    #print (\"nsp score -> \" + str(nsp_score))\n",
    "    #print (\"cosine score -> \" + str(cosine_distance))\n",
    "    return score\n",
    "    \n",
    "\n",
    "def getSentMatchScore_wfeature(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    #nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    #nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    \n",
    "    nsp_score = nsp_score_1 * nsp_dampening_factor\n",
    "    #nsp_score = nsp_score_1*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    return score\n",
    "\n",
    "def getSentMatchScore_wfeature_cosine(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    return cosine_distance\n",
    "    \n",
    "\n",
    "def getBERTFeatures(model, text, attn_head_idx = -1): #attn_head_idx - index o[]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    if len(tokenized_text)>200:\n",
    "        tokenized_text = tokenized_text[0:200]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    _, _, seq_out, pool_out = model(tokens_tensor)\n",
    "    seq_out = list(getPooledFeatures(seq_out[attn_head_idx]).T)\n",
    "    #pool_out = list(pool_out.detach().numpy().T)\n",
    "    \n",
    "    return seq_out\n",
    "\n",
    "def getPooledFeatures(np_array):\n",
    "    np_array = np_array.reshape(np_array.shape[1],np_array.shape[2]).detach().numpy()\n",
    "    np_array_mp = np.mean(np_array, axis=0).reshape(1, -1)\n",
    "    return np_array_mp\n",
    "\n",
    "def replaceContractions(text):\n",
    "    #text = text.lower()\n",
    "    c_filt_text = ''\n",
    "    for word in text.split(' '):\n",
    "        if word in contractions:\n",
    "            c_filt_text = c_filt_text+' '+contractions[word]\n",
    "        else:\n",
    "            c_filt_text = c_filt_text+' '+word\n",
    "    return c_filt_text\n",
    "\n",
    "def cleanText(text):\n",
    "        \n",
    "    text = text.replace('\\\\n','')\n",
    "    text = text.replace('\\\\','')\n",
    "    #text = text.replace('\\t', '')\n",
    "    #text = re.sub('\\[(.*?)\\]','',text) #removes [this one]\n",
    "    text = re.sub('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\\s',\n",
    "                ' __url__ ',text) #remove urls\n",
    "    #text = re.sub('\\'','',text)\n",
    "    #text = re.sub(r'\\d+', ' __number__ ', text) #replaces numbers\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining constants over here\n",
    "seed = 42 \n",
    "model_name = 'openai-gpt'\n",
    "output_dir = '../models/gpt/'\n",
    "train_batch_size = 1\n",
    "n_valid = 374\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "#n_gpu = torch.cuda.device_count()\n",
    "#logger.info(\"device: {}, n_gpu {}\".format(device, n_gpu))\n",
    "device = 'cuda'\n",
    "special_tokens = ['_start_', '_delimiter_', '_classify_']\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(model_name, special_tokens=special_tokens)\n",
    "special_tokens_ids = list(tokenizer.convert_tokens_to_ids(token) for token in special_tokens)\n",
    "\n",
    "# model = OpenAIGPTDoubleHeadsModel.from_pretrained(output_dir)\n",
    "# tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "model1 = OpenAIGPTDoubleHeadsModel_custom.from_pretrained(output_dir)\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)\n",
    "model1.to(device)\n",
    "model1.eval()\n",
    "#print(type(model))\n",
    "#print('model1')\n",
    "#print(model1)\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)\n",
    "logger.info(\"Encoding dataset...\")\n",
    "\n",
    "def feature_extractor(model1,text):\n",
    "    trn_dt = ([text,'','',0],)   \n",
    "    datasets = (trn_dt,)\n",
    "    encoded_datasets = tokenize_and_encode(datasets)\n",
    "    max_length = model1.config.n_positions//2 - 2\n",
    "    input_length = len(encoded_datasets[0][0][0])+2\n",
    "    input_length = min(input_length, model1.config.n_positions)  # Max size of input for the pre-trained model\n",
    "\n",
    "    # Prepare inputs tensors and dataloaders\n",
    "    n_batches = ceil(len(encoded_datasets[0][0][0])/max_length)\n",
    "    \n",
    "    tensor_datasets = pre_process_datasets(encoded_datasets, input_length, max_length, *special_tokens_ids)\n",
    "    train_tensor_dataset = tensor_datasets[0]\n",
    "    train_data = TensorDataset(*train_tensor_dataset)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=1)\n",
    "    '''\n",
    "    config = OpenAIGPTConfig.from_json_file('/home/shubham/Project/domain_mind/gpt2_experiment/model/config.json')\n",
    "    model1 = OpenAIGPTMultipleChoiceHead_custom(config)\n",
    "    '''\n",
    "    #eval_loss, eval_accuracy = 0, 0\n",
    "    #nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    final_clf=[]\n",
    "    final_lm=[]\n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, mc_token_ids = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            a, clf_text_feature = model1(input_ids, mc_token_ids)\n",
    "            final_clf.append(clf_text_feature[:,:,-1])\n",
    "    if n_batches>1:\n",
    "        clf_torch = torch.sum(torch.stack(final_clf),0)\n",
    "        return clf_torch\n",
    "    else:\n",
    "        return clf_text_feature[:,:,-1,:]#, lm_text_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse csv medium dump data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load it as csv.\n",
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('../data/raw/se_medium_.csv', index_col=False, header=0);\n",
    "#texts = ' '.join(list(df['Data'])[:100])\n",
    "texts_org = list(df['Data'][:100])\n",
    "#texts = '. '.join(list(map(lambda x: cleanText(x), list(df['Data']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "\n",
    "mod_texts_unfiltered = tp.preprocess(texts, stop_words=False, remove_punct=False)\n",
    "mod_texts = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts_unfiltered[:]):\n",
    "    if len(sent.split(' '))>250:\n",
    "        length = len(sent.split(' '))\n",
    "        split1 = ' '.join([i for i in sent.split(' ')[:round(length/2)]])\n",
    "        split2 = ' '.join([i for i in sent.split(' ')[round(length/2):]])\n",
    "        mod_texts.append(split1)\n",
    "        mod_texts.append(split2)\n",
    "        continue\n",
    "        #mod_texts.pop(index)\n",
    "    if len(sent.split(' '))<=6:\n",
    "        continue\n",
    "    mod_texts.append(sent)\n",
    "\n",
    "print(len(mod_texts))\n",
    "fv = {}\n",
    "\n",
    "for index, sent in enumerate(mod_texts):\n",
    "    fv[index] = feature_extractor(model1,sent)\n",
    "    #fv[index] = getBERTFeatures(model1, sent, attn_head_idx=-1)\n",
    "    #fv[index] = getBERTFeatures_KP(model1, sent, attn_head_idx=-3)\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_graph(doc_list):\n",
    "    eng_graph = nx.Graph()\n",
    "    try:\n",
    "        eng_graph.add_nodes_from(range(len(doc_list)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return eng_graph\n",
    "\n",
    "\n",
    "tg = build_graph(mod_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attn_head_idx = -1\n",
    "\n",
    "node_edge = []\n",
    "\n",
    "for index1, sent1 in enumerate(mod_texts):\n",
    "    print (index1)\n",
    "    for index2, sent2 in enumerate(mod_texts):\n",
    "        if index1!=index2 and index2>index1:\n",
    "            #score = getSentMatchScore_wfeature(sent1, sent2,fv[index1],fv[index2])\n",
    "            #score = getSentMatchScore_wfeature_cosine(sent1, sent2,fv[index1],fv[index2])\n",
    "            score = 1 - cosine(fv[index1].cpu(),fv[index2].cpu())\n",
    "#             if score > 0.8:\n",
    "#                 #tg.add_edge(index1,index2,{'weight': score})\n",
    "#                 tg.add_edge(index1,index2)\n",
    "            tg.add_edge(index1,index2,weight=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import community\n",
    "max_mod = 0\n",
    "mod_v = 0\n",
    "for v in [0.15, 0.10, 0.05, 0.01]:\n",
    "    flag = False\n",
    "    for count in range(5):   \n",
    "        temp_nodes = []\n",
    "        for nodea,nodeb, weight in tg.edges.data():\n",
    "            temp_nodes.append((nodea,nodeb,weight['weight']))\n",
    "        temp_nodes = sorted(temp_nodes, key=lambda kv: kv[2], reverse=True)\n",
    "\n",
    "        temp_nodes = temp_nodes[:math.ceil(len(temp_nodes)*v)+1]\n",
    "\n",
    "        com_graph = nx.Graph()\n",
    "        for nodea,nodeb, weight in temp_nodes:\n",
    "            com_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "        partition = community.best_partition(com_graph)\n",
    "\n",
    "        mod = community.modularity(partition, com_graph)\n",
    "        if mod > max_mod and (mod < 0.4 or max_mod==0):\n",
    "            max_mod=mod\n",
    "            mod_v = v\n",
    "        print (\"The pruning value 'v' and modularity is: \", v, mod)\n",
    "#         if mod > 0.3:\n",
    "#             flag=True\n",
    "#             print (\"Modularity reached 3. The pruning value 'v' is: \", v)\n",
    "#             break\n",
    "        if mod==0:\n",
    "            temp_nodes = []\n",
    "            print (\"Modularity reached 0. The pruning value 'v' is: \", 0.15)\n",
    "            for nodea,nodeb, weight in tg.edges.data():\n",
    "                temp_nodes.append((nodea,nodeb,weight['weight']))\n",
    "            temp_nodes = sorted(temp_nodes, key=lambda kv: kv[2], reverse=True)\n",
    "\n",
    "            temp_nodes = temp_nodes[:math.ceil(len(temp_nodes)*0.15)+1]\n",
    "\n",
    "            com_graph = nx.Graph()\n",
    "            for nodea,nodeb, weight in temp_nodes:\n",
    "                com_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "            partition = community.best_partition(com_graph)\n",
    "\n",
    "            mod = community.modularity(partition, com_graph)\n",
    "            flag=True\n",
    "            break\n",
    "    if flag:\n",
    "        print()\n",
    "        break\n",
    "\n",
    "for count in range(5):\n",
    "    temp_nodes = []\n",
    "    for nodea,nodeb, weight in tg.edges.data():\n",
    "        temp_nodes.append((nodea,nodeb,weight['weight']))\n",
    "    temp_nodes = sorted(temp_nodes, key=lambda kv: kv[2], reverse=True)\n",
    "    #print (len(temp_nodes), mod_v)\n",
    "    temp_nodes = temp_nodes[:math.ceil(len(temp_nodes)*mod_v)+1]\n",
    "    #print (len(temp_nodes))\n",
    "    com_graph = nx.Graph()\n",
    "    for nodea,nodeb, weight in temp_nodes:\n",
    "        com_graph.add_edge(nodea,nodeb)\n",
    "\n",
    "    partition = community.best_partition(com_graph)\n",
    "    mod = community.modularity(partition, com_graph)\n",
    "    #print (mod)\n",
    "    if mod>=max_mod:\n",
    "        break\n",
    "print (\"The final modularity is \", mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "#partition = community.best_partition(com_graph)\n",
    "\n",
    "values = [partition.get(node) for node in com_graph.nodes()]\n",
    "values=[partition.get(node) for node in com_graph.nodes()]\n",
    "plt.rcParams['figure.figsize']= [16, 10]\n",
    "measure_name = \"Louviin Algorithm Community Structure\"\n",
    "pos = nx.spring_layout(com_graph, k=0.2, iterations=20)\n",
    "nodes_plot=nx.draw_networkx_nodes(com_graph, pos, node_size=140, label=True, cmap=plt.get_cmap('magma', len(com_graph.nodes())/4),node_color=values, alpha=0.95)\n",
    "edges_plot=nx.draw_networkx_edges(com_graph, pos, edge_color='r', alpha=0.1)\n",
    "plt.title(measure_name, fontsize=22, fontname='Arial')\n",
    "plt.colorbar(nodes_plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community.modularity(partition, com_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        print (mod_texts[word])\n",
    "        current=cluster\n",
    "    else:\n",
    "        print (mod_texts[word] + \"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to backlink to the documents\n",
    "\n",
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "doc_split = []\n",
    "for t in texts_org:\n",
    "    mod_texts_unfiltered_new = tp.preprocess(t, stop_words=False, remove_punct=False)\n",
    "    mod_texts_new = []\n",
    "\n",
    "    for index, sent in enumerate(mod_texts_unfiltered_new[:]):\n",
    "        if len(sent.split(' '))>250:\n",
    "            length = len(sent.split(' '))\n",
    "            split1 = ' '.join([i for i in sent.split(' ')[:round(length/2)]])\n",
    "            split2 = ' '.join([i for i in sent.split(' ')[round(length/2):]])\n",
    "            mod_texts_new.append(split1)\n",
    "            mod_texts_new.append(split2)\n",
    "            continue\n",
    "            #mod_texts.pop(index)\n",
    "        if len(sent.split(' '))<=6:\n",
    "            continue\n",
    "        mod_texts_new.append(sent)\n",
    "    doc_split.append(mod_texts_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        #print (mod_texts[word])\n",
    "        doc = -1\n",
    "        for index, doc_s in enumerate(doc_split):\n",
    "            if mod_texts[word] in doc_s:\n",
    "                doc = index\n",
    "        print (doc)\n",
    "        current=cluster\n",
    "    else:\n",
    "        #print (mod_texts[word] + \"\\n\\n\\n\\n\\n\")\n",
    "        doc = 0\n",
    "        for index, doc_s in enumerate(doc_split):\n",
    "            if mod_texts[word] in doc_s:\n",
    "                doc = index\n",
    "        print (doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# without pruning (doesn't work w/ cosine similarity as weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "com_graph_full = nx.Graph()\n",
    "for nodea,nodeb, weight1 in tg.edges.data():\n",
    "    com_graph_full.add_edge(nodea,nodeb, weight=weight1['weight'])\n",
    "\n",
    "partition = community.best_partition(com_graph_full)\n",
    "mod = community.modularity(partition, com_graph_full)\n",
    "print (mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "#partition = community.best_partition(com_graph)\n",
    "\n",
    "values = [partition.get(node) for node in com_graph_full.nodes()]\n",
    "values=[partition.get(node) for node in com_graph_full.nodes()]\n",
    "plt.rcParams['figure.figsize']= [16, 10]\n",
    "measure_name = \"Louviin Algorithm Community Structure\"\n",
    "pos = nx.spring_layout(com_graph_full, k=0.2, iterations=20)\n",
    "nodes_plot=nx.draw_networkx_nodes(com_graph_full, pos, node_size=140, label=True, cmap=plt.get_cmap('magma', len(com_graph_full.nodes())/4),node_color=values, alpha=0.95)\n",
    "edges_plot=nx.draw_networkx_edges(com_graph_full, pos, edge_color='r', alpha=0.1)\n",
    "plt.title(measure_name, fontsize=22, fontname='Arial')\n",
    "plt.colorbar(nodes_plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        print (mod_texts[word])\n",
    "        current=cluster\n",
    "    else:\n",
    "        print (mod_texts[word] + \"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "com_graph_full = nx.Graph()\n",
    "for nodea,nodeb, weight1 in tg.edges.data():\n",
    "    com_graph_full.add_edge(nodea,nodeb, weight=weight1['weight'])\n",
    "\n",
    "partition = community.best_partition(com_graph_full)\n",
    "mod = community.modularity(partition, com_graph_full)\n",
    "print (mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# old approach (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T06:57:15.046763Z",
     "start_time": "2019-06-11T06:56:54.541064Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys, pickle\n",
    "sys.path.append('../')\n",
    "import text_preprocessing.preprocess as tp\n",
    "\n",
    "with open('../data/engg_text_06May19_recency.pkl','rb') as fp:\n",
    "    file = pickle.load(fp)\n",
    "\n",
    "texts = '. '.join(sent for sent in file)\n",
    "\n",
    "mod_texts_unfiltered = tp.preprocess(texts, stop_words=False, remove_punct=True)\n",
    "mod_texts = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts_unfiltered[:500]):\n",
    "    if len(sent.split(' '))>250:\n",
    "        length = len(sent.split(' '))\n",
    "        split1 = ' '.join([i for i in sent.split(' ')[:round(length/2)]])\n",
    "        split2 = ' '.join([i for i in sent.split(' ')[round(length/2):]])\n",
    "        mod_texts.append(split1)\n",
    "        mod_texts.append(split2)\n",
    "        continue\n",
    "        #mod_texts.pop(index)\n",
    "    if len(sent.split(' '))<=6:\n",
    "        continue\n",
    "    mod_texts.append(sent)\n",
    "\n",
    "print(len(mod_texts))\n",
    "fv = {}\n",
    "\n",
    "for index, sent in enumerate(mod_texts):\n",
    "    fv[index] = getBERTFeatures(model1, sent, attn_head_idx=-1)\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-11T07:03:58.226383Z",
     "start_time": "2019-06-11T07:03:57.465436Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_graph(doc_list):\n",
    "    eng_graph = nx.Graph()\n",
    "    try:\n",
    "        eng_graph.add_nodes_from(range(len(doc_list)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return eng_graph\n",
    "\n",
    "\n",
    "tg = build_graph(mod_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for index1, sent1 in enumerate(mod_texts):\n",
    "    for index2, sent2 in enumerate(mod_texts):\n",
    "        if sent1!=sent2:\n",
    "            score = getSentMatchScore_wfeature(sent1, sent2, fv[index1], fv[index2])\n",
    "            print (\"sentence 1 \\n \\n\", sent1)\n",
    "            print (\"\\n\\nsentence 2 \\n \\n\", sent2)\n",
    "            print (\"\\n\\n Score -> \", score, \"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:11:22.256624Z",
     "start_time": "2019-06-07T11:10:16.574980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "attn_head_idx = -1\n",
    "start = time.time()\n",
    "end = 0\n",
    "node_edge = []\n",
    "runs = len(mod_texts)*(len(mod_texts)-1)\n",
    "\n",
    "sent1counter = 0\n",
    "sent2counter = 0\n",
    "counter = 0\n",
    "try:\n",
    "    i=0\n",
    "    #tqdm()\n",
    "    for index1, sent1 in enumerate(mod_texts):\n",
    "        sent1counter=index1\n",
    "        print (index1, time.time()-start)\n",
    "        for index2, sent2 in enumerate(mod_texts):\n",
    "            counter+=1\n",
    "            #updt(runs, counter)\n",
    "            sent2counter=index2\n",
    "            if index1!=index2:\n",
    "                score = getSentMatchScore_wfeature(sent1, sent2, fv[index1], fv[index2])\n",
    "                if score>0.8:\n",
    "                    #tg.add_edge(index1,index2,{'weight': score})\n",
    "                    tg.add_edge(index1,index2)\n",
    "except RuntimeError as e:\n",
    "    print (e)\n",
    "    print (sent1counter, sent2counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores_round = [round(i,1) for i in scores]\n",
    "scores_freq = {}\n",
    "for i in scores_round:\n",
    "    scores_freq[i] = scores_round.count(i)\n",
    "scores_round = set(scores_freq.keys())\n",
    "scores_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sorted(scores_freq.items(), key=lambda kv: kv[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sent1counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tg.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:36:40.808135Z",
     "start_time": "2019-06-07T11:36:40.474903Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "partition = community.best_partition(tg)\n",
    "\n",
    "#drawing\n",
    "# size = float(len(set(partition.values())))\n",
    "# pos = nx.spring_layout(tg)\n",
    "# count = 0.\n",
    "# for com in set(partition.values()) :\n",
    "#     count = count + 1.\n",
    "#     list_nodes = [nodes for nodes in partition.keys()\n",
    "#                                 if partition[nodes] == com]\n",
    "#     nx.draw_networkx_nodes(tg, pos, list_nodes, node_size = 20,\n",
    "#                                 node_color = str(count / size))\n",
    "\n",
    "values = [partition.get(node) for node in tg.nodes()]\n",
    "#nx.draw_networkx_edges(tg, pos, alpha=0.5)\n",
    "#plt.show()\n",
    "#plt.margins(0.1, 0.1)\n",
    "#plt.figure(figsize=(10, 9))\n",
    "#plt.axis('off')\n",
    "#nx.draw_spring(tg, cmap = plt.get_cmap('jet'), node_color = values, node_size=300)\n",
    "#draw_graph(tg, None)\n",
    "\n",
    "\n",
    "values=[partition.get(node) for node in tg.nodes()]\n",
    "plt.rcParams['figure.figsize']= [16, 10]\n",
    "measure_name = \"Louviin Algorithm Community Structure\"\n",
    "pos = nx.spring_layout(tg, k=0.2, iterations=20)\n",
    "nodes_plot=nx.draw_networkx_nodes(tg, pos, node_size=140, label=True, cmap=plt.get_cmap('magma', len(tg.nodes())/4),node_color=values, alpha=0.95)\n",
    "edges_plot=nx.draw_networkx_edges(tg, pos, edge_color='r', alpha=0.1)\n",
    "plt.title(measure_name, fontsize=22, fontname='Arial')\n",
    "plt.colorbar(nodes_plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:36:55.246506Z",
     "start_time": "2019-06-07T11:36:55.244074Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:36:58.283193Z",
     "start_time": "2019-06-07T11:36:58.279757Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T11:40:07.625227Z",
     "start_time": "2019-06-07T11:40:07.620025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "current = 0\n",
    "print (\"--------------cluster \" + str(0) + \"------------ \\n \")\n",
    "for word, cluster in partition:\n",
    "    if cluster!=current:\n",
    "        print (\"--------------cluster \" + str(cluster) + \"------------ \\n \")\n",
    "        print (mod_texts[word])\n",
    "        current=cluster\n",
    "    else:\n",
    "        print (mod_texts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cluster0 = []\n",
    "cluster_dict = {}\n",
    "cluster1= []\n",
    "cluster2 = []\n",
    "cluster3 = []\n",
    "cluster4= []\n",
    "cluster5 = []\n",
    "cluster6= []\n",
    "cluster7 = []\n",
    "for sent, cluster in partition:\n",
    "    if cluster == 0:\n",
    "        cluster0.append(sent)\n",
    "        cluster_dict[sent] = 2\n",
    "    elif cluster == 1:\n",
    "        cluster1.append(sent)\n",
    "    elif cluster == 2:\n",
    "        cluster2.append(sent)\n",
    "    elif cluster == 3:\n",
    "        cluster3.append(sent)\n",
    "    elif cluster == 4:\n",
    "        cluster4.append(sent)\n",
    "    elif cluster == 5:\n",
    "        cluster5.append(sent)\n",
    "    elif cluster == 6:\n",
    "        cluster6.append(sent)\n",
    "    elif cluster == 7:\n",
    "        cluster7.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"------------Cluster 0--------------- \\n\\n\")\n",
    "for sent in cluster0:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 1--------------- \\n\\n\")\n",
    "for sent in cluster1:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 2--------------- \\n\\n\")\n",
    "for sent in cluster2:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "print (\"------------Cluster 3--------------- \\n\\n\")\n",
    "for sent in cluster3:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 4--------------- \\n\\n\")\n",
    "for sent in cluster4:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 5--------------- \\n\\n\")\n",
    "for sent in cluster5:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 6--------------- \\n\\n\")\n",
    "for sent in cluster6:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 7--------------- \\n\\n\")\n",
    "for sent in cluster7:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## POC on slack dump (json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from text_preprocessing import preprocess\n",
    "\n",
    "from graphrank.graphrank import GraphRank\n",
    "\n",
    "from graphrank.utils import GraphUtils\n",
    "import networkx as nx\n",
    "import json as js\n",
    "import keyphrase_extraction as kp\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def getslacktext(location):\n",
    "    with open(location) as f:\n",
    "        meeting = json.load(f)\n",
    "    \n",
    "#     text = []\n",
    "    \n",
    "#     for i in range(len(list(meeting['segments']))):\n",
    "#         #unfil_text = meeting['segments'][i]['filteredText']\n",
    "#         unfil_text = meeting['segments'][i]['originalText']\n",
    "#         if len(unfil_text.split(' '))<6:\n",
    "#             continue\n",
    "#         if len(unfil_text.split(' '))>250:\n",
    "#             length = len(unfil_text.split(' '))\n",
    "#             split1 = ' '.join([i for i in unfil_text.split(' ')[:round(length/2)]])\n",
    "#             split2 = ' '.join([i for i in unfil_text.split(' ')[round(length/2):]])\n",
    "#             text.append(split1)\n",
    "#             text.append(split2)\n",
    "#             continue\n",
    "#         text.append(unfil_text)\n",
    "    \n",
    "    return meeting\n",
    "\n",
    "text = getslacktext('engineering_6thMay2019.json')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getusermsg(text, userid = 'U9GLH098C'): #Venkat\n",
    "    \n",
    "    usermsg = []\n",
    "    for index, msg in enumerate(text):\n",
    "        if msg['type']=='message' and 'bot_id' not in msg.keys() and 'user' in msg.keys():\n",
    "            if msg['user']==userid and msg['text']!='':\n",
    "                    text = preprocess.preprocess(msg['text'], remove_punct=True,word_tokenize=False ,stop_words=False)\n",
    "                    #print (text)\n",
    "                    if len(text)!=0:\n",
    "                        for t in text:\n",
    "                            if len(t.split(' '))>6:\n",
    "                                usermsg.append(t)\n",
    "    return usermsg\n",
    "\n",
    "def getusermsg_alluser(text): \n",
    "    \n",
    "    usermsg = []\n",
    "    user = {}\n",
    "    cnt=0\n",
    "    for index, msg in enumerate(text):\n",
    "        if msg['type']=='message' and 'bot_id' not in msg.keys() and 'user' in msg.keys():\n",
    "            if msg['text']!='':\n",
    "                    text = preprocess.preprocess(msg['text'], remove_punct=True,word_tokenize=False ,stop_words=False)\n",
    "                    #print (text)\n",
    "                    if len(text)!=0:\n",
    "                        for t in text:\n",
    "#                             if len(t.split(' '))>6:\n",
    "#                                 if msg['user'] in usermsg.keys() and len(usermsg[msg['user']])<10:\n",
    "#                                     usermsg[msg['user']].append(t)\n",
    "#                                 elif msg['user'] not in usermsg.keys():\n",
    "#                                     usermsg[msg['user']] = []\n",
    "#                                     usermsg[msg['user']].append(t)\n",
    "                            if len(t.split(' '))>6:\n",
    "                                usermsg.append(t) \n",
    "                                user[cnt] = msg['user']\n",
    "                                cnt+=1\n",
    "    return usermsg, user\n",
    "\n",
    "#mod_texts = getusermsg(text,userid = 'U4QK2H8RL')[:20]\n",
    "mod_texts, user_list = getusermsg_alluser(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mod_texts = mod_texts[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(len(mod_texts))\n",
    "fv = {}\n",
    "\n",
    "for index, sent in enumerate(mod_texts):\n",
    "    fv[index] = getBERTFeatures(model1, sent, attn_head_idx=-1)\n",
    "    print (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_graph(doc_list):\n",
    "    eng_graph = nx.Graph()\n",
    "    try:\n",
    "        eng_graph.add_nodes_from(range(len(doc_list)))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return eng_graph\n",
    "\n",
    "\n",
    "tg = build_graph(mod_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "attn_head_idx = -1\n",
    "start = time.time()\n",
    "end = 0\n",
    "node_edge = []\n",
    "runs = len(mod_texts)*(len(mod_texts)-1)\n",
    "\n",
    "sent1counter = 0\n",
    "sent2counter = 0\n",
    "counter = 0\n",
    "try:\n",
    "    i=0\n",
    "    #tqdm()\n",
    "    for index1, sent1 in enumerate(mod_texts):\n",
    "        sent1counter=index1\n",
    "        print (index1, time.time()-start)\n",
    "        for index2, sent2 in enumerate(mod_texts):\n",
    "            \n",
    "            #updt(runs, counter)\n",
    "            sent2counter=index2\n",
    "            if index1!=index2:\n",
    "                score = getSentMatchScore_wfeature_test(sent1, sent2, fv[index1], fv[index2], user_list[index1], user_list[index1])\n",
    "                if score>0:\n",
    "                    counter+=1\n",
    "                    tg.add_edge(index1,index2,weight=score)\n",
    "                    #tg.add_edge(index1,index2)\n",
    "except RuntimeError as e:\n",
    "    print (e)\n",
    "    print (sent1counter, sent2counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(nx.betweenness_centrality(tg).items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#tg.remove_node(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mod_texts[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (len(mod_texts)*(len(mod_texts)-1))\n",
    "(len(mod_texts)*(len(mod_texts)-1)) - counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "partition = community.best_partition(tg)\n",
    "\n",
    "#drawing\n",
    "# size = float(len(set(partition.values())))\n",
    "# pos = nx.spring_layout(tg)\n",
    "# count = 0.\n",
    "# for com in set(partition.values()) :\n",
    "#     count = count + 1.\n",
    "#     list_nodes = [nodes for nodes in partition.keys()\n",
    "#                                 if partition[nodes] == com]\n",
    "#     nx.draw_networkx_nodes(tg, pos, list_nodes, node_size = 20,\n",
    "#                                 node_color = str(count / size))\n",
    "\n",
    "values = [partition.get(node) for node in tg.nodes()]\n",
    "#nx.draw_networkx_edges(tg, pos, alpha=0.5)\n",
    "#plt.show()\n",
    "#plt.margins(0.1, 0.1)\n",
    "#plt.figure(figsize=(10, 9))\n",
    "#plt.axis('off')\n",
    "#nx.draw_spring(tg, cmap = plt.get_cmap('jet'), node_color = values, node_size=300)\n",
    "#draw_graph(tg, None)\n",
    "\n",
    "\n",
    "values=[partition.get(node) for node in tg.nodes()]\n",
    "plt.rcParams['figure.figsize']= [16, 10]\n",
    "measure_name = \"Louviin Algorithm Community Structure\"\n",
    "pos = nx.spring_layout(tg, k=0.2, iterations=20)\n",
    "nodes_plot=nx.draw_networkx_nodes(tg, pos, node_size=140, label=True, cmap=plt.get_cmap('magma', len(tg.nodes())/4),node_color=values, alpha=0.95)\n",
    "edges_plot=nx.draw_networkx_edges(tg, pos, edge_color='r', alpha=0.1)\n",
    "plt.title(measure_name, fontsize=22, fontname='Arial')\n",
    "plt.colorbar(nodes_plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "partition = sorted(partition.items(), key=lambda kv: kv[1], reverse=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cluster0 = []\n",
    "cluster_dict = {}\n",
    "cluster1= []\n",
    "cluster2 = []\n",
    "cluster3 = []\n",
    "cluster4= []\n",
    "cluster5 = []\n",
    "cluster6= []\n",
    "cluster7 = []\n",
    "for sent, cluster in partition:\n",
    "    if cluster == 0:\n",
    "        cluster0.append(sent)\n",
    "        cluster_dict[sent] = 2\n",
    "    elif cluster == 1:\n",
    "        cluster1.append(sent)\n",
    "    elif cluster == 2:\n",
    "        cluster2.append(sent)\n",
    "    elif cluster == 3:\n",
    "        cluster3.append(sent)\n",
    "    elif cluster == 4:\n",
    "        cluster4.append(sent)\n",
    "    elif cluster == 5:\n",
    "        cluster5.append(sent)\n",
    "    elif cluster == 6:\n",
    "        cluster6.append(sent)\n",
    "    elif cluster == 7:\n",
    "        cluster7.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"------------Cluster 0--------------- \\n\\n\")\n",
    "for sent in cluster0:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 1--------------- \\n\\n\")\n",
    "for sent in cluster1:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 2--------------- \\n\\n\")\n",
    "for sent in cluster2:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "print (\"------------Cluster 3--------------- \\n\\n\")\n",
    "for sent in cluster3:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 4--------------- \\n\\n\")\n",
    "for sent in cluster4:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 5--------------- \\n\\n\")\n",
    "for sent in cluster5:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 6--------------- \\n\\n\")\n",
    "for sent in cluster6:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")\n",
    "    \n",
    "print (\"------------Cluster 7--------------- \\n\\n\")\n",
    "for sent in cluster7:\n",
    "    print (mod_texts[sent] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster1_user = []\n",
    "print (\"------------Cluster 0--------------- \\n\\n\")\n",
    "for sent in cluster0:\n",
    "    if user_list[sent] not in cluster1_user:\n",
    "        print (user_list[sent] + \"\\n\\n\")\n",
    "        cluster1_user.append(user_list[sent])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "cluster2_user = []\n",
    "print (\"------------Cluster 1--------------- \\n\\n\")\n",
    "for sent in cluster1:\n",
    "    if user_list[sent] not in cluster2_user:\n",
    "        print (user_list[sent] + \"\\n\\n\")\n",
    "        cluster2_user.append(user_list[sent])\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print (\"------ Cluster 0 - Cluster 1---------\")\n",
    "for i in cluster1_user:\n",
    "    if i in cluster2_user:\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "part ={}\n",
    "for sent, cluster in partition:\n",
    "    part[sent] = cluster\n",
    "\n",
    "print (part[19],part[1],part[8],part[6],part[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in [19,1,8]:\n",
    "    print (mod_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for index1, sent1 in enumerate(mod_texts):\n",
    "    for index2, sent2 in enumerate(mod_texts):\n",
    "        if sent1!=sent2:\n",
    "            score = getSentMatchScore_wfeature(sent1, sent2, fv[index1], fv[index2])\n",
    "            print (\"sentence 1 \\n \\n\", sent1)\n",
    "            print (\"\\n\\nsentence 2 \\n \\n\", sent2)\n",
    "            print (\"\\n\\n Score -> \", score, \"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/home/ether/domain_mind/engineering/se_minds_new.pkl', 'rb') as f:\n",
    "    file = pickle.load(f)\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(list(file['feature_vector'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_score = []\n",
    "sent_com = {}\n",
    "max_index = 0\n",
    "max_score = 0\n",
    "for i in range(10):\n",
    "    sent_com[i] = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts[:]):\n",
    "    temp_score = []\n",
    "    for index2, vec in enumerate(list(file['feature_vector'])):\n",
    "        temp_score.append(getSentMatchScore_wfeature(sent, list(file['sentence'])[index2], fv[index], vec, model1))\n",
    "#         print (\"-------cluster-------\")\n",
    "#         print (sent)\n",
    "#         print (\"score -> \" + str(temp_score[index2]))\n",
    "    max_score = max(temp_score)\n",
    "    max_index = temp_score.index(max_score)\n",
    "    sent_com[max_index].append(sent)\n",
    "    #print (max_index)\n",
    "    #break\n",
    "#for i in mod_texts[:10]:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for cluster, sent in sent_com.items():\n",
    "    print (\"--------------community \" + str(cluster) + \"------------\")\n",
    "    for sentence in sent:\n",
    "        print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list(file['sentence'])[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text =\"The only thing I can think of is that I had both the production and staging builds installed at that time vs now.\"\n",
    "#text = \"It is a very good time to re consider game of thrones story line\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## select 3 clusters instead of max-score while calculating belongingness of a sentences..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_com = {}\n",
    "max_index = 0\n",
    "max_score = 0\n",
    "for i in range(100):\n",
    "    sent_com[i] = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts[:]):\n",
    "    temp_score = {}\n",
    "    for index2, vec in enumerate(list(file['feature_vector'])):\n",
    "        temp_score[index2] = getSentMatchScore_wfeature(sent, list(file['sentence'])[index2], fv[index], vec)\n",
    "#         print (\"-------cluster-------\")\n",
    "#         print (sent)\n",
    "#         print (\"score -> \" + str(temp_score[index2]))\n",
    "    temp_score = dict(sorted(temp_score.items(), key = lambda fv : fv[1], reverse=True))\n",
    "    max_indexes = [i for i in temp_score.keys()][:3]\n",
    "    #max_index = temp_score.index(max_score)\n",
    "    print (max_indexes)\n",
    "#     for indexes in max_indexes:\n",
    "#         sent_com[indexes].append(index)\n",
    "    sent_com[index] = max_indexes\n",
    "    #print (max_index)\n",
    "    #break\n",
    "#for i in mod_texts[:10]:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in sent_com.keys():\n",
    "    for j in sent_com.keys():\n",
    "        if i!=j:\n",
    "            if sorted(sent_com[i])==(sent_com[j]):\n",
    "                print (\"-------similar sentence--------\")\n",
    "                print (mod_texts[i])\n",
    "                print (\"\\n\")\n",
    "                print (mod_texts[j])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## select 2 clusters instead of max-score while calculating belongingness of a sentences..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sent_com = {}\n",
    "max_index = 0\n",
    "max_score = 0\n",
    "for i in range(100):\n",
    "    sent_com[i] = []\n",
    "\n",
    "for index, sent in enumerate(mod_texts[:]):\n",
    "    temp_score = {}\n",
    "    for index2, vec in enumerate(list(file['feature_vector'])):\n",
    "        temp_score[index2] = getSentMatchScore_wfeature(sent, list(file['sentence'])[index2], fv[index], vec)\n",
    "#         print (\"-------cluster-------\")\n",
    "#         print (sent)\n",
    "#         print (\"score -> \" + str(temp_score[index2]))\n",
    "    temp_score = dict(sorted(temp_score.items(), key = lambda fv : fv[1], reverse=True))\n",
    "    max_indexes = [i for i in temp_score.keys()][:2]\n",
    "    #max_index = temp_score.index(max_score)\n",
    "    #print (max_indexes)\n",
    "#     for indexes in max_indexes:\n",
    "#         sent_com[indexes].append(index)\n",
    "    sent_com[index] = max_indexes\n",
    "    #print (max_index)\n",
    "    #break\n",
    "#for i in mod_texts[:10]:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in sent_com.keys():\n",
    "    for j in sent_com.keys():\n",
    "        if i!=j:\n",
    "            if sorted(sent_com[i])==(sent_com[j]):\n",
    "                print (\"-------similar sentence--------\")\n",
    "                print (mod_texts[i])\n",
    "                print (\"\\n\")\n",
    "                print (mod_texts[j])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph = \"I have a couple S3 buckets. One for my static home page, one for holding images and one for holding the application version. As far as I know, ELB automatically creates the one for managing the application versions.\"\n",
    "\n",
    "paragraph = preprocess.preprocess(paragraph, stop_words=False, remove_punct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sent_vec = {}\n",
    "for index, sentence in enumerate(paragraph):\n",
    "    sent_vec[index] = getBERTFeatures(model1, sentence, attn_head_idx=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "score = {}\n",
    "for index, sent in enumerate(paragraph):\n",
    "    temp_score = {}\n",
    "    for index2, vec in enumerate(list(file['feature_vector'])):\n",
    "        temp_score[index2] = getSentMatchScore_wfeature(sent, list(file['sentence'])[index2], sent_vec[index], vec)\n",
    "    score[index] = max(temp_score.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_norm_value(score):\n",
    "    weighted_score = {}\n",
    "    min_score = min(score.values())\n",
    "    max_score = max(score.values())\n",
    "    if min_score == max_score:\n",
    "        return [1]*len(score.values())\n",
    "    for index, s in enumerate(score.values()):\n",
    "        weighted_score[index] = (s - min_score)/ (max_score - min_score)\n",
    "    return weighted_score\n",
    "\n",
    "def get_weighted_norm_value(score):\n",
    "    weighted_score = {}\n",
    "    tot_score = sum(score.values())\n",
    "    for index, s in enumerate(score.values()):\n",
    "        weighted_score[index] = s/tot_score\n",
    "    return weighted_score\n",
    "\n",
    "#weighted_score = list(get_weighted_norm_value(score).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph = []\n",
    "paragraph_vec = np.zeros((768,), dtype=float)\n",
    "for index, sent in enumerate(paragraph):\n",
    "    sent_vec[index] = list(np.array(sent_vec[index])*weighted_score[index])\n",
    "    \n",
    "#paragraph_vec = np.sum(np.array(sent_vec.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for s in sent_vec.values():\n",
    "    print (len(np.array(s)))\n",
    "    paragraph_vec = np.add(paragraph_vec,np.array(s))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph_vec = np.zeros((768,1), dtype=float)\n",
    "paragraph3_vec = np.array(list(file['feature_vector'])[0])\n",
    "(paragraph_vec + paragraph3_vec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.array(list(file['feature_vector'])[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_paragraph_vec(paragraph, file):\n",
    "    sent_vec = {}\n",
    "    for index, sentence in enumerate(paragraph):\n",
    "        sent_vec[index] = getBERTFeatures(model1, sentence, attn_head_idx=-2)\n",
    "    score = {}\n",
    "    for index, sent in enumerate(paragraph):\n",
    "        temp_score = {}\n",
    "        for index2, vec in enumerate(list(file['feature_vector'])):\n",
    "            temp_score[index2] = getSentMatchScore_wfeature(sent, list(file['sentence'])[index2], sent_vec[index], vec)\n",
    "        score[index] = max(temp_score.values())\n",
    "        \n",
    "    weighted_score = list(get_weighted_norm_value(score).values())\n",
    "    \n",
    "    paragraph = []\n",
    "    paragraph_vec = np.zeros((768,1), dtype=float)\n",
    "    for index, sent in enumerate(paragraph):\n",
    "        sent_vec[index] = list(np.array(sent_vec[index])*weighted_score[index])\n",
    "    \n",
    "\n",
    "    for s in sent_vec.values():\n",
    "        #print(np.add(paragraph_vec,np.array(s)).shape)\n",
    "        paragraph_vec = np.add(paragraph_vec,np.array(s))\n",
    "    \n",
    "    print (paragraph_vec.shape)\n",
    "    return paragraph_vec\n",
    "    #paragraph_vec = np.sum(np.array(sent_vec.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph = \"I have a couple S3 buckets. One for my static home page, one for holding images and one for holding the application version. As far as I know, ELB automatically creates the one for managing the application versions.\"\n",
    "paragraph2 = \"Having a development environment and a production running at the same time is easy, but its expensive. It doubles it, in fact. Therefore, I usually destroy the dev environment as soon as Im done with it.\"\n",
    "paragraph4 = \" I actually got a couple of S3 buckets. One for my the home page, one for images and one for the application version.  I know that, ELB will automatically create the one to managing the application versions. \"\n",
    "paragraph3 = \"You find out that Harvard Business Review looked into how personality traits factor into group dynamics. While each person should have a functional role within the group, they also have a less obvious psychological role. You already know what functional roles you need to hire fora product manager, two engineers, an analyst, and a designer. While a functional role is of course important, HBR found that the psychological role someone has to play is just as important to a teams viability and productivity. Through their research, HBR came up with five different personality traits that are imperative to group success.\"\n",
    "paragraph = preprocess.preprocess(paragraph, stop_words=False, remove_punct=False)\n",
    "paragraph2 = preprocess.preprocess(paragraph2, stop_words=False, remove_punct=False)\n",
    "paragraph3 = preprocess.preprocess(paragraph3, stop_words=False, remove_punct=False)\n",
    "paragraph4 = preprocess.preprocess(paragraph4, stop_words=False, remove_punct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph_fv = get_paragraph_vec(paragraph, file)\n",
    "paragraph2_fv = get_paragraph_vec(paragraph2, file)\n",
    "paragraph3_fv = get_paragraph_vec(paragraph3, file)\n",
    "paragraph4_fv = get_paragraph_vec(paragraph4, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "#cosine(list(paragraph_fv), list(paragraph2_fv))\n",
    "1-cosine(paragraph_fv, paragraph3_fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "paragraph2_fv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
