{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from io import open\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_transformers import WEIGHTS_NAME, CONFIG_NAME, BertConfig\n",
    "from pytorch_transformers.modeling_bert import BertPreTrainedModel,BertModel,BertForTokenClassification\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "from torch.nn import CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForActionItemDetection(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForActionItemDetection, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.apply(self.init_weights)\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
    "                position_ids=None, head_mask=None):\n",
    "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask, head_mask=head_mask)\n",
    "        #print((outputs[0]))\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "        \n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \n",
    "    def __init__(self, input_ids, input_mask, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.label = label\n",
    "        \n",
    "class ActionItemDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, max_seq_len, tokenizer):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X_text = self.data.iloc[index,0]\n",
    "        y_label = self.data.iloc[index, 1]\n",
    "        tokenized_text = self.tokenizer.tokenize(X_text)\n",
    "        curr_feats = convert_example_to_features(tokenized_text,y_label,self.max_seq_len,self.tokenizer)\n",
    "        cur_tensors = (torch.tensor(curr_feats.input_ids),\n",
    "                        torch.tensor(curr_feats.input_mask),\n",
    "                        torch.tensor(curr_feats.label))\n",
    "        return cur_tensors\n",
    "\n",
    "def _truncate_seq_pair(tokenized_text, max_length):\n",
    "    while True:\n",
    "        if len(tokenized_text) <= max_length:\n",
    "            break\n",
    "        else:\n",
    "            tokenized_text.pop()\n",
    "            \n",
    "def convert_example_to_features(tokenized_text, label, max_seq_len, tokenizer):\n",
    "    _truncate_seq_pair(tokenized_text, max_seq_len-2)\n",
    "    tokens = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    for token in tokenized_text:\n",
    "        tokens.append(token)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    while len(input_ids) < max_seq_len:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "    \n",
    "    features = InputFeatures(input_ids=input_ids,\n",
    "                             input_mask=input_mask,\n",
    "                             label=label)\n",
    "    return features\n",
    "\n",
    "def get_ai_probabilities(text):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "    out = model(input_ids)[0]\n",
    "    return (sfmax(out).detach().numpy()[0])\n",
    "\n",
    "def getMetrics(model_df,focus_label):\n",
    "    model_tp = len(model_df[(model_df['actual_label']==focus_label) & (model_df['isCorrect']==True)])\n",
    "    model_fp = len(model_df[(model_df['actual_label']==abs(1-focus_label)) & (model_df['isCorrect']==False)])\n",
    "    model_fn = len(model_df[(model_df['actual_label']==focus_label) & (model_df['isCorrect']==False)])\n",
    "\n",
    "    if model_tp>0:\n",
    "        precision = model_tp/(model_tp+model_fp)\n",
    "        recall = model_tp/(model_tp+model_fn)\n",
    "    else:\n",
    "        precision=0.0\n",
    "        recall=0.0\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    missed_ai_preds = len(model_df[(model_df['actual_label']==focus_label) & (model_df['isCorrect']==False)])\n",
    "    print('Missed Predictions: ', missed_ai_preds)\n",
    "    hit_rate = model_tp/len(model_df[model_df['actual_label']==focus_label])\n",
    "    print('Hit rate: ', hit_rate)\n",
    "    return precision,recall,hit_rate\n",
    "\n",
    "def getValidationMetrics():\n",
    "    df_validation = pd.read_csv('/home/venkat/hdd/Venkat/action_item_detection/Shubham/training/seed_data/seed_val.csv')\n",
    "    text = list(df_validation['text'])\n",
    "    label_list = list(df_validation['is_action_item'])\n",
    "    pred_list = []\n",
    "    confidence_list = []\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    \n",
    "    for sent in text:\n",
    "        res = get_ai_probabilities(sent)\n",
    "        pred_list.append(np.argmax(res))\n",
    "        confidence_list.append(max(res))\n",
    "        \n",
    "    model_df = pd.DataFrame({'sentence': text,\n",
    "                        'actual_label': label_list,\n",
    "                        'predicted_label': pred_list,\n",
    "                        'confidence': confidence_list})\n",
    "    model_df['isCorrect'] = model_df['actual_label']==model_df['predicted_label']\n",
    "    print('Action Item Metrics: ')\n",
    "    ai_precision,ai_recall,ai_hit_rate = getMetrics(model_df,1)\n",
    "    print()\n",
    "    print('Non-Action Item Metrics: ')\n",
    "    nai_precision,nai_recall,nai_hit_rate = getMetrics(model_df,0)\n",
    "    if ai_precision!=0 and ai_recall!=0:\n",
    "        f1 = 2*(ai_precision*ai_recall)/(ai_precision+ai_recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    print('F1 score: ', f1)\n",
    "    model.train()\n",
    "    model.to('cuda')\n",
    "    \n",
    "    return ai_precision,ai_recall,f1,ai_hit_rate,nai_hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_model = 'bert-base-uncased'\n",
    "device = 'cuda'\n",
    "model = BertForActionItemDetection.from_pretrained(bert_model)\n",
    "model.to('cuda')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "# if n_gpu>1:\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=True)\n",
    "gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight True\n",
      "embeddings.position_embeddings.weight True\n",
      "embeddings.token_type_embeddings.weight True\n",
      "embeddings.LayerNorm.weight True\n",
      "embeddings.LayerNorm.bias True\n",
      "encoder.layer.0.attention.self.query.weight True\n",
      "encoder.layer.0.attention.self.query.bias True\n",
      "encoder.layer.0.attention.self.key.weight True\n",
      "encoder.layer.0.attention.self.key.bias True\n",
      "encoder.layer.0.attention.self.value.weight True\n",
      "encoder.layer.0.attention.self.value.bias True\n",
      "encoder.layer.0.attention.output.dense.weight True\n",
      "encoder.layer.0.attention.output.dense.bias True\n",
      "encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "encoder.layer.0.intermediate.dense.weight True\n",
      "encoder.layer.0.intermediate.dense.bias True\n",
      "encoder.layer.0.output.dense.weight True\n",
      "encoder.layer.0.output.dense.bias True\n",
      "encoder.layer.0.output.LayerNorm.weight True\n",
      "encoder.layer.0.output.LayerNorm.bias True\n",
      "encoder.layer.1.attention.self.query.weight True\n",
      "encoder.layer.1.attention.self.query.bias True\n",
      "encoder.layer.1.attention.self.key.weight True\n",
      "encoder.layer.1.attention.self.key.bias True\n",
      "encoder.layer.1.attention.self.value.weight True\n",
      "encoder.layer.1.attention.self.value.bias True\n",
      "encoder.layer.1.attention.output.dense.weight True\n",
      "encoder.layer.1.attention.output.dense.bias True\n",
      "encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "encoder.layer.1.intermediate.dense.weight True\n",
      "encoder.layer.1.intermediate.dense.bias True\n",
      "encoder.layer.1.output.dense.weight True\n",
      "encoder.layer.1.output.dense.bias True\n",
      "encoder.layer.1.output.LayerNorm.weight True\n",
      "encoder.layer.1.output.LayerNorm.bias True\n",
      "encoder.layer.2.attention.self.query.weight False\n",
      "encoder.layer.2.attention.self.query.bias False\n",
      "encoder.layer.2.attention.self.key.weight False\n",
      "encoder.layer.2.attention.self.key.bias False\n",
      "encoder.layer.2.attention.self.value.weight False\n",
      "encoder.layer.2.attention.self.value.bias False\n",
      "encoder.layer.2.attention.output.dense.weight False\n",
      "encoder.layer.2.attention.output.dense.bias False\n",
      "encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "encoder.layer.2.intermediate.dense.weight False\n",
      "encoder.layer.2.intermediate.dense.bias False\n",
      "encoder.layer.2.output.dense.weight False\n",
      "encoder.layer.2.output.dense.bias False\n",
      "encoder.layer.2.output.LayerNorm.weight False\n",
      "encoder.layer.2.output.LayerNorm.bias False\n",
      "encoder.layer.3.attention.self.query.weight False\n",
      "encoder.layer.3.attention.self.query.bias False\n",
      "encoder.layer.3.attention.self.key.weight False\n",
      "encoder.layer.3.attention.self.key.bias False\n",
      "encoder.layer.3.attention.self.value.weight False\n",
      "encoder.layer.3.attention.self.value.bias False\n",
      "encoder.layer.3.attention.output.dense.weight False\n",
      "encoder.layer.3.attention.output.dense.bias False\n",
      "encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "encoder.layer.3.intermediate.dense.weight False\n",
      "encoder.layer.3.intermediate.dense.bias False\n",
      "encoder.layer.3.output.dense.weight False\n",
      "encoder.layer.3.output.dense.bias False\n",
      "encoder.layer.3.output.LayerNorm.weight False\n",
      "encoder.layer.3.output.LayerNorm.bias False\n",
      "encoder.layer.4.attention.self.query.weight False\n",
      "encoder.layer.4.attention.self.query.bias False\n",
      "encoder.layer.4.attention.self.key.weight False\n",
      "encoder.layer.4.attention.self.key.bias False\n",
      "encoder.layer.4.attention.self.value.weight False\n",
      "encoder.layer.4.attention.self.value.bias False\n",
      "encoder.layer.4.attention.output.dense.weight False\n",
      "encoder.layer.4.attention.output.dense.bias False\n",
      "encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "encoder.layer.4.intermediate.dense.weight False\n",
      "encoder.layer.4.intermediate.dense.bias False\n",
      "encoder.layer.4.output.dense.weight False\n",
      "encoder.layer.4.output.dense.bias False\n",
      "encoder.layer.4.output.LayerNorm.weight False\n",
      "encoder.layer.4.output.LayerNorm.bias False\n",
      "encoder.layer.5.attention.self.query.weight False\n",
      "encoder.layer.5.attention.self.query.bias False\n",
      "encoder.layer.5.attention.self.key.weight False\n",
      "encoder.layer.5.attention.self.key.bias False\n",
      "encoder.layer.5.attention.self.value.weight False\n",
      "encoder.layer.5.attention.self.value.bias False\n",
      "encoder.layer.5.attention.output.dense.weight False\n",
      "encoder.layer.5.attention.output.dense.bias False\n",
      "encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "encoder.layer.5.intermediate.dense.weight False\n",
      "encoder.layer.5.intermediate.dense.bias False\n",
      "encoder.layer.5.output.dense.weight False\n",
      "encoder.layer.5.output.dense.bias False\n",
      "encoder.layer.5.output.LayerNorm.weight False\n",
      "encoder.layer.5.output.LayerNorm.bias False\n",
      "encoder.layer.6.attention.self.query.weight False\n",
      "encoder.layer.6.attention.self.query.bias False\n",
      "encoder.layer.6.attention.self.key.weight False\n",
      "encoder.layer.6.attention.self.key.bias False\n",
      "encoder.layer.6.attention.self.value.weight False\n",
      "encoder.layer.6.attention.self.value.bias False\n",
      "encoder.layer.6.attention.output.dense.weight False\n",
      "encoder.layer.6.attention.output.dense.bias False\n",
      "encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "encoder.layer.6.intermediate.dense.weight False\n",
      "encoder.layer.6.intermediate.dense.bias False\n",
      "encoder.layer.6.output.dense.weight False\n",
      "encoder.layer.6.output.dense.bias False\n",
      "encoder.layer.6.output.LayerNorm.weight False\n",
      "encoder.layer.6.output.LayerNorm.bias False\n",
      "encoder.layer.7.attention.self.query.weight False\n",
      "encoder.layer.7.attention.self.query.bias False\n",
      "encoder.layer.7.attention.self.key.weight False\n",
      "encoder.layer.7.attention.self.key.bias False\n",
      "encoder.layer.7.attention.self.value.weight False\n",
      "encoder.layer.7.attention.self.value.bias False\n",
      "encoder.layer.7.attention.output.dense.weight False\n",
      "encoder.layer.7.attention.output.dense.bias False\n",
      "encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "encoder.layer.7.intermediate.dense.weight False\n",
      "encoder.layer.7.intermediate.dense.bias False\n",
      "encoder.layer.7.output.dense.weight False\n",
      "encoder.layer.7.output.dense.bias False\n",
      "encoder.layer.7.output.LayerNorm.weight False\n",
      "encoder.layer.7.output.LayerNorm.bias False\n",
      "encoder.layer.8.attention.self.query.weight False\n",
      "encoder.layer.8.attention.self.query.bias False\n",
      "encoder.layer.8.attention.self.key.weight False\n",
      "encoder.layer.8.attention.self.key.bias False\n",
      "encoder.layer.8.attention.self.value.weight False\n",
      "encoder.layer.8.attention.self.value.bias False\n",
      "encoder.layer.8.attention.output.dense.weight False\n",
      "encoder.layer.8.attention.output.dense.bias False\n",
      "encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "encoder.layer.8.intermediate.dense.weight False\n",
      "encoder.layer.8.intermediate.dense.bias False\n",
      "encoder.layer.8.output.dense.weight False\n",
      "encoder.layer.8.output.dense.bias False\n",
      "encoder.layer.8.output.LayerNorm.weight False\n",
      "encoder.layer.8.output.LayerNorm.bias False\n",
      "encoder.layer.9.attention.self.query.weight False\n",
      "encoder.layer.9.attention.self.query.bias False\n",
      "encoder.layer.9.attention.self.key.weight False\n",
      "encoder.layer.9.attention.self.key.bias False\n",
      "encoder.layer.9.attention.self.value.weight False\n",
      "encoder.layer.9.attention.self.value.bias False\n",
      "encoder.layer.9.attention.output.dense.weight False\n",
      "encoder.layer.9.attention.output.dense.bias False\n",
      "encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "encoder.layer.9.intermediate.dense.weight False\n",
      "encoder.layer.9.intermediate.dense.bias False\n",
      "encoder.layer.9.output.dense.weight False\n",
      "encoder.layer.9.output.dense.bias False\n",
      "encoder.layer.9.output.LayerNorm.weight False\n",
      "encoder.layer.9.output.LayerNorm.bias False\n",
      "encoder.layer.10.attention.self.query.weight False\n",
      "encoder.layer.10.attention.self.query.bias False\n",
      "encoder.layer.10.attention.self.key.weight False\n",
      "encoder.layer.10.attention.self.key.bias False\n",
      "encoder.layer.10.attention.self.value.weight False\n",
      "encoder.layer.10.attention.self.value.bias False\n",
      "encoder.layer.10.attention.output.dense.weight False\n",
      "encoder.layer.10.attention.output.dense.bias False\n",
      "encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "encoder.layer.10.intermediate.dense.weight False\n",
      "encoder.layer.10.intermediate.dense.bias False\n",
      "encoder.layer.10.output.dense.weight False\n",
      "encoder.layer.10.output.dense.bias False\n",
      "encoder.layer.10.output.LayerNorm.weight False\n",
      "encoder.layer.10.output.LayerNorm.bias False\n",
      "encoder.layer.11.attention.self.query.weight False\n",
      "encoder.layer.11.attention.self.query.bias False\n",
      "encoder.layer.11.attention.self.key.weight False\n",
      "encoder.layer.11.attention.self.key.bias False\n",
      "encoder.layer.11.attention.self.value.weight False\n",
      "encoder.layer.11.attention.self.value.bias False\n",
      "encoder.layer.11.attention.output.dense.weight False\n",
      "encoder.layer.11.attention.output.dense.bias False\n",
      "encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "encoder.layer.11.intermediate.dense.weight False\n",
      "encoder.layer.11.intermediate.dense.bias False\n",
      "encoder.layer.11.output.dense.weight False\n",
      "encoder.layer.11.output.dense.bias False\n",
      "encoder.layer.11.output.LayerNorm.weight False\n",
      "encoder.layer.11.output.LayerNorm.bias False\n",
      "pooler.dense.weight True\n",
      "pooler.dense.bias True\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ActionItemDataSet('/home/venkat/hdd/Venkat/action_item_detection/Shubham/training/new_train.csv',40,tokenizer)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "\n",
    "model.train()\n",
    "for name, param in model.bert.named_parameters():\n",
    "    #print(name,param.requires_grad)\n",
    "    if name.startswith('embeddings'):\n",
    "        #print(param.requires_grad)\n",
    "        param.requires_grad = True\n",
    "    elif(name.startswith('encoder.layer.0.')):\n",
    "        param.requires_grad = True\n",
    "    elif(name.startswith('encoder.layer.1.')):\n",
    "        param.requires_grad = True\n",
    "    elif(name.startswith('encoder.layer.2')):\n",
    "        param.requires_grad = False\n",
    "    elif(name.startswith('encoder.layer.3')):\n",
    "        param.requires_grad = False\n",
    "    elif(name.startswith('encoder.layer.4')):\n",
    "        param.requires_grad = False\n",
    "    elif(name.startswith('encoder.layer.5')):\n",
    "        param.requires_grad = False       \n",
    "    elif(name.startswith('encoder.layer.6')):\n",
    "        param.requires_grad = False\n",
    "    elif(name.startswith('encoder.layer.7')):\n",
    "        param.requires_grad = False\n",
    "    elif(name.startswith('encoder.layer.8')):\n",
    "        param.requires_grad = False\n",
    "    elif(name.startswith('encoder.layer.9')):\n",
    "        param.requires_grad = False\n",
    "    elif(name.startswith('encoder.layer.10')):\n",
    "        param.requires_grad = False\n",
    "    elif(name.startswith('encoder.layer.11')):\n",
    "        #print('-------------',param.requires_grad)\n",
    "        param.requires_grad = False\n",
    "        #print('-------------',param.requires_grad)\n",
    "\n",
    "for name, param in model.bert.named_parameters():\n",
    "    print(name,param.requires_grad)\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "\"\"\"for param in model.parameters():\n",
    "    print(len(model.parameters()))\n",
    "    param.requires_grad = False\"\"\"\n",
    "\n",
    "warmup_steps = 1800\n",
    "learning_rate = 3e-5\n",
    "train_batch_size = 32\n",
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = train_batch_size//gradient_accumulation_steps\n",
    "\n",
    "num_train_epochs = 100\n",
    "num_train_optimization_steps = int(\n",
    "            len(train_dataset) / train_batch_size / gradient_accumulation_steps) * num_train_epochs\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8 )\n",
    "#t_total=num_train_epochs*warmup_steps - for triangulated lr\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=warmup_steps*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Leanirng rate:  0 :: [1.6666666666666667e-08, 1.6666666666666667e-08]\n",
      "current loss:  tensor(0.6801, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda3/envs/newtorch/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Item Metrics: \n",
      "Precision:  0.46632124352331605\n",
      "Recall:  0.967741935483871\n",
      "Missed Predictions:  3\n",
      "Hit rate:  0.967741935483871\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "Missed Predictions:  103\n",
      "Hit rate:  0.0\n",
      "F1 score:  0.6293706293706294\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  200 :: [3.35e-06, 3.35e-06]\n",
      "current loss:  tensor(0.6293, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.5058139534883721\n",
      "Recall:  0.9354838709677419\n",
      "Missed Predictions:  6\n",
      "Hit rate:  0.9354838709677419\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.75\n",
      "Recall:  0.17475728155339806\n",
      "Missed Predictions:  85\n",
      "Hit rate:  0.17475728155339806\n",
      "F1 score:  0.6566037735849056\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  400 :: [6.6833333333333334e-06, 6.6833333333333334e-06]\n",
      "current loss:  tensor(0.4095, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.9506172839506173\n",
      "Recall:  0.8279569892473119\n",
      "Missed Predictions:  16\n",
      "Hit rate:  0.8279569892473119\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.8608695652173913\n",
      "Recall:  0.9611650485436893\n",
      "Missed Predictions:  4\n",
      "Hit rate:  0.9611650485436893\n",
      "F1 score:  0.885057471264368\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  600 :: [1.0016666666666667e-05, 1.0016666666666667e-05]\n",
      "current loss:  tensor(0.2332, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.9411764705882353\n",
      "Recall:  0.8602150537634409\n",
      "Missed Predictions:  13\n",
      "Hit rate:  0.8602150537634409\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.8828828828828829\n",
      "Recall:  0.9514563106796117\n",
      "Missed Predictions:  5\n",
      "Hit rate:  0.9514563106796117\n",
      "F1 score:  0.898876404494382\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  800 :: [1.3350000000000001e-05, 1.3350000000000001e-05]\n",
      "current loss:  tensor(0.0358, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.7909090909090909\n",
      "Recall:  0.9354838709677419\n",
      "Missed Predictions:  6\n",
      "Hit rate:  0.9354838709677419\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9302325581395349\n",
      "Recall:  0.7766990291262136\n",
      "Missed Predictions:  23\n",
      "Hit rate:  0.7766990291262136\n",
      "F1 score:  0.8571428571428572\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  1000 :: [1.6683333333333333e-05, 1.6683333333333333e-05]\n",
      "current loss:  tensor(0.0217, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.8446601941747572\n",
      "Recall:  0.9354838709677419\n",
      "Missed Predictions:  6\n",
      "Hit rate:  0.9354838709677419\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9354838709677419\n",
      "Recall:  0.8446601941747572\n",
      "Missed Predictions:  16\n",
      "Hit rate:  0.8446601941747572\n",
      "F1 score:  0.8877551020408163\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  1200 :: [2.0016666666666668e-05, 2.0016666666666668e-05]\n",
      "current loss:  tensor(0.0117, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.9052631578947369\n",
      "Recall:  0.9247311827956989\n",
      "Missed Predictions:  7\n",
      "Hit rate:  0.9247311827956989\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9306930693069307\n",
      "Recall:  0.912621359223301\n",
      "Missed Predictions:  9\n",
      "Hit rate:  0.912621359223301\n",
      "F1 score:  0.9148936170212766\n",
      "Model saved for the epoch:  2 and step:  1200\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  1400 :: [2.3350000000000002e-05, 2.3350000000000002e-05]\n",
      "current loss:  tensor(0.0055, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.8787878787878788\n",
      "Recall:  0.9354838709677419\n",
      "Missed Predictions:  6\n",
      "Hit rate:  0.9354838709677419\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9381443298969072\n",
      "Recall:  0.883495145631068\n",
      "Missed Predictions:  12\n",
      "Hit rate:  0.883495145631068\n",
      "F1 score:  0.90625\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  1600 :: [2.6683333333333336e-05, 2.6683333333333336e-05]\n",
      "current loss:  tensor(0.0021, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.9032258064516129\n",
      "Recall:  0.9032258064516129\n",
      "Missed Predictions:  9\n",
      "Hit rate:  0.9032258064516129\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.912621359223301\n",
      "Recall:  0.912621359223301\n",
      "Missed Predictions:  9\n",
      "Hit rate:  0.912621359223301\n",
      "F1 score:  0.9032258064516129\n",
      "Model saved for the epoch:  3 and step:  1600\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  1800 :: [2.9983333333333335e-05, 2.9983333333333335e-05]\n",
      "current loss:  tensor(0.0021, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.9139784946236559\n",
      "Recall:  0.9139784946236559\n",
      "Missed Predictions:  8\n",
      "Hit rate:  0.9139784946236559\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9223300970873787\n",
      "Recall:  0.9223300970873787\n",
      "Missed Predictions:  8\n",
      "Hit rate:  0.9223300970873787\n",
      "F1 score:  0.9139784946236559\n",
      "Model saved for the epoch:  3 and step:  1800\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  2000 :: [2.665e-05, 2.665e-05]\n",
      "current loss:  tensor(0.0009, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.8877551020408163\n",
      "Recall:  0.9354838709677419\n",
      "Missed Predictions:  6\n",
      "Hit rate:  0.9354838709677419\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9387755102040817\n",
      "Recall:  0.8932038834951457\n",
      "Missed Predictions:  11\n",
      "Hit rate:  0.8932038834951457\n",
      "F1 score:  0.9109947643979057\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  2200 :: [2.331666666666667e-05, 2.331666666666667e-05]\n",
      "current loss:  tensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.9148936170212766\n",
      "Recall:  0.9247311827956989\n",
      "Missed Predictions:  7\n",
      "Hit rate:  0.9247311827956989\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9313725490196079\n",
      "Recall:  0.9223300970873787\n",
      "Missed Predictions:  8\n",
      "Hit rate:  0.9223300970873787\n",
      "F1 score:  0.9197860962566845\n",
      "Model saved for the epoch:  4 and step:  2200\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  2400 :: [1.9983333333333332e-05, 1.9983333333333332e-05]\n",
      "current loss:  tensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.9157894736842105\n",
      "Recall:  0.9354838709677419\n",
      "Missed Predictions:  6\n",
      "Hit rate:  0.9354838709677419\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9405940594059405\n",
      "Recall:  0.9223300970873787\n",
      "Missed Predictions:  8\n",
      "Hit rate:  0.9223300970873787\n",
      "F1 score:  0.925531914893617\n",
      "Model saved for the epoch:  4 and step:  2400\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  2600 :: [1.665e-05, 1.665e-05]\n",
      "current loss:  tensor(0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.8979591836734694\n",
      "Recall:  0.946236559139785\n",
      "Missed Predictions:  5\n",
      "Hit rate:  0.946236559139785\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9489795918367347\n",
      "Recall:  0.9029126213592233\n",
      "Missed Predictions:  10\n",
      "Hit rate:  0.9029126213592233\n",
      "F1 score:  0.9214659685863874\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  2800 :: [1.3316666666666667e-05, 1.3316666666666667e-05]\n",
      "current loss:  tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Item Metrics: \n",
      "Precision:  0.9072164948453608\n",
      "Recall:  0.946236559139785\n",
      "Missed Predictions:  5\n",
      "Hit rate:  0.946236559139785\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9494949494949495\n",
      "Recall:  0.912621359223301\n",
      "Missed Predictions:  9\n",
      "Hit rate:  0.912621359223301\n",
      "F1 score:  0.9263157894736843\n",
      "Model saved for the epoch:  5 and step:  2800\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  3000 :: [9.983333333333333e-06, 9.983333333333333e-06]\n",
      "current loss:  tensor(0.0006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.9072164948453608\n",
      "Recall:  0.946236559139785\n",
      "Missed Predictions:  5\n",
      "Hit rate:  0.946236559139785\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9494949494949495\n",
      "Recall:  0.912621359223301\n",
      "Missed Predictions:  9\n",
      "Hit rate:  0.912621359223301\n",
      "F1 score:  0.9263157894736843\n",
      "Model saved for the epoch:  5 and step:  3000\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  3200 :: [6.650000000000001e-06, 6.650000000000001e-06]\n",
      "current loss:  tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.9166666666666666\n",
      "Recall:  0.946236559139785\n",
      "Missed Predictions:  5\n",
      "Hit rate:  0.946236559139785\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.95\n",
      "Recall:  0.9223300970873787\n",
      "Missed Predictions:  8\n",
      "Hit rate:  0.9223300970873787\n",
      "F1 score:  0.9312169312169313\n",
      "Model saved for the epoch:  6 and step:  3200\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Step Leanirng rate:  3400 :: [3.316666666666667e-06, 3.316666666666667e-06]\n",
      "current loss:  tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Action Item Metrics: \n",
      "Precision:  0.9361702127659575\n",
      "Recall:  0.946236559139785\n",
      "Missed Predictions:  5\n",
      "Hit rate:  0.946236559139785\n",
      "\n",
      "Non-Action Item Metrics: \n",
      "Precision:  0.9509803921568627\n",
      "Recall:  0.941747572815534\n",
      "Missed Predictions:  6\n",
      "Hit rate:  0.941747572815534\n",
      "F1 score:  0.9411764705882354\n",
      "Model saved for the epoch:  6 and step:  3400\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "infer_steps = 0\n",
    "sfmax = nn.Softmax()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in enumerate((train_dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, labels = batch\n",
    "        loss,scores = model(input_ids, attention_mask = input_mask, labels=labels)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if ((scheduler.get_lr()[0]) ==0):\n",
    "            print(scheduler.get_lr()[0])\n",
    "            break\n",
    "        if global_step%200==0:\n",
    "            print('Step Leanirng rate: ', global_step,'::',scheduler.get_lr())\n",
    "            model.eval()\n",
    "            print('current loss: ', loss)\n",
    "            #validation\n",
    "            ai_precision,ai_recall,f1,ai_hit_rate,nai_hit_rate = getValidationMetrics()\n",
    "            mean_hit_rate = (ai_hit_rate+nai_hit_rate)/2\n",
    "            if f1>0.9 and mean_hit_rate>0.8 and ai_precision>0.9:\n",
    "                output_model_file = 'models/lr_3e-6_ws_1200/bert_ai_classifier_epc_'+str(epoch)+'_step_'+str(global_step)+'_lr_'+str(learning_rate)+'_ws_'+str(warmup_steps)+'_f1'+str(f1)+'.pkl'\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "                torch.save(model_to_save.state_dict(), output_model_file)\n",
    "                print('Model saved for the epoch: ', epoch,'and step: ', global_step)\n",
    "            print('----------------------------------------------------------------')\n",
    "            print('----------------------------------------------------------------')\n",
    "            print()\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newtorch",
   "language": "python",
   "name": "newtorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
