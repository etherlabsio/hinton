{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "from bert_ner_utils import BERT_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing models\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ner_model = BERT_NER('/home/ether/Desktop/BERT_Similarity_experiments/models/bert-ner/',cased=False)\n",
    "ner_model_u = BERT_NER('/home/ether/Desktop/BERT_Similarity_experiments/models/bert-ner-uncased-2/',cased=False,labels=[\"O\",\"E\"])\n",
    "# ner_model_c = BERT_NER('/home/ether/Desktop/BERT_Similarity_experiments/models/bert-ner-cased/',cased=True)\n",
    "ner_model_c = BERT_NER('/home/ether/Desktop/BERT_Similarity_experiments/models/bert-ner-cased-2/',cased=True,labels=[\"O\",\"E\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_segment = \"'gotham city needs a #Wash', said batman in his low, grumbly voice as he waved his wand to cast the spell aguamenti on the city.\"\n",
    "model = ner_model\n",
    "\n",
    "ents,sc,nents,nsc = model.get_entities(test_segment,get_non_entities=True)\n",
    "print(\">\",test_segment,end=\"\\n\\n\")\n",
    "print(\"Entities Detected:\",ner_model.wordize(ents, capitalize=True),end=\"\\n\\n\")\n",
    "print(\"Non Entity tokens, sorted by score:\",end=\"\\n\\n\")\n",
    "\n",
    "for ne,ns in sorted(zip(nents,nsc),key=lambda x: x[1]):\n",
    "    print(ne,ns,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models based on segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handsome. Yeah, this is - I never trying to figure out how to get the either this time about setup. So yes the time account set up with an Enterprise type of organization very particular domain and we've set up for email address and evening and I ran it a command to create a bot with for the time. So that's how it is. None, so we're able to officially read an email address as a bot in able to spend now. What we do is in order for us to be able to connect it to through the UI accelerate away. We had to drive the box where to set up a Lambda function with subscribe to events and expose that endpoint for that Bart and then associate that Lambda to the specific body that they play Through a APA. But once that happens we can invite the board to a chat like a Google like a Channel or corresponding Channel thing in China and okay, once that happens then bar gets notified Lambda gets triggered thing, but we're invited to this Channel and mention Etc. And then it has the next steps are essentially to create another function and add all the quote there to other Bots person respond to the yeah, we sort of\n",
      "\n",
      "UNCASED BERT-NER\n",
      "\n",
      "['UI', 'LAMBDA', 'LAMBDA', 'APA', 'GOOGLE', 'CHINA', 'BAR', 'LAMBDA']\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UNCASED BERT-NER-BINARY\n",
      "\n",
      "['HANDSOME', 'LAMBDA', 'APA', 'GOOGLE', 'CHINA', 'BAR', 'LAMBDA']\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CASED BERT-NER-BINARY\n",
      "\n",
      "['ENTERPRISE', 'UI', 'LAMBDA', 'BART', 'LAMBDA', 'APA', 'GOOGLE', 'CHANNEL', 'CHANNEL', 'CHINA', 'LAMBDA', 'CHANNEL', 'ETC', '.', 'BOTS']\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPACY\n",
      "Google, Channel, APA, Bart, China, evening, Bots, Lambda, UI\n"
     ]
    }
   ],
   "source": [
    "segment = \"Handsome. Yeah, this is - I never trying to figure out how to get the either this time about setup. So yes the time account set up with an Enterprise type of organization very particular domain and we've set up for email address and evening and I ran it a command to create a bot with for the time. So that's how it is. None, so we're able to officially read an email address as a bot in able to spend now. What we do is in order for us to be able to connect it to through the UI accelerate away. We had to drive the box where to set up a Lambda function with subscribe to events and expose that endpoint for that Bart and then associate that Lambda to the specific body that they play Through a APA. But once that happens we can invite the board to a chat like a Google like a Channel or corresponding Channel thing in China and okay, once that happens then bar gets notified Lambda gets triggered thing, but we're invited to this Channel and mention Etc. And then it has the next steps are essentially to create another function and add all the quote there to other Bots person respond to the yeah, we sort of\"\n",
    "print(segment,end=\"\\n\\n\")\n",
    "entities=[]\n",
    "entities_c = []\n",
    "entities_u = []\n",
    "conf = []\n",
    "conf_c = []\n",
    "conf_u=[]\n",
    "for text in sent_tokenize(segment):\n",
    "    ent, con, ne,nc = ner_model.get_entities(text,get_non_entities=True)\n",
    "    ent_c, con_c,ne_c, nc_c = ner_model_c.get_entities(text,get_non_entities=True)\n",
    "    ent_u, con_u,ne_u,nc_u = ner_model_u.get_entities(text,get_non_entities=True)\n",
    "    if len(ent)==0 and len(ent_c)==0 and len(ent_u)==0:\n",
    "        continue\n",
    "    entities.extend(ent)\n",
    "    entities_c.extend(ent_c)\n",
    "    entities_u.extend(ent_u)\n",
    "    conf.extend(con)\n",
    "    conf_c.extend(con_c)\n",
    "    conf_u.extend(con_u)\n",
    "\n",
    "print(\"UNCASED BERT-NER\",end=\"\\n\\n\")\n",
    "# [print(e,c) for e,c in zip(entities,conf)]\n",
    "entities_words = ner_model.wordize(entities)\n",
    "print(entities_words,end=\"\\n\\n\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(\"UNCASED BERT-NER-BINARY\",end=\"\\n\\n\")\n",
    "# [print(e,c) for e,c in zip(entities_u,conf_u)]\n",
    "entities_words_u = ner_model_u.wordize(entities_u)\n",
    "print(entities_words_u,end=\"\\n\\n\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(\"CASED BERT-NER-BINARY\",end=\"\\n\\n\")\n",
    "# [print(e,c) for e,c in zip(entities_c,conf_c)]\n",
    "entities_words_c = ner_model_c.wordize(entities_c)\n",
    "print(entities_words_c,end=\"\\n\\n\")\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(\"SPACY\")\n",
    "doc=nlp(segment)\n",
    "entities_spacy=[]\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ not in [\"CARDINAL\",\"ORDINAL\"]:\n",
    "        entities_spacy.append(ent.text)\n",
    "print(\", \".join(list(set(entities_spacy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on entity-sentences set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/home/venkat/knowledge_graphs/entity_graph_builder/graph_dumps/ppn_sentences.pkl\",\"rb\") as f:\n",
    "    sent_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################################################### \n",
      " Document | #Sentences: 124 \n",
      "\n",
      "BERT NER UNCASED MUL:  0.008064516129032258\n",
      "BERT NER CASED BIN:  0.008064516129032258\n",
      "BERT NER UNCASED BIN:  0.0\n",
      "SPACY NER CASED:  0.008064516129032258\n",
      "SPACY NER UNCASED:  0.0\n",
      "#################################################################################################### \n",
      " Bitbucket | #Sentences: 15 \n",
      "\n",
      "BERT NER UNCASED MUL:  0.8\n",
      "BERT NER CASED BIN:  0.9333333333333333\n",
      "BERT NER UNCASED BIN:  0.9333333333333333\n",
      "SPACY NER CASED:  0.6666666666666666\n",
      "SPACY NER UNCASED:  0.0\n",
      "#################################################################################################### \n",
      " App Version | #Sentences: 6 \n",
      "\n",
      "BERT NER UNCASED MUL:  0.5\n",
      "BERT NER CASED BIN:  0.4166666666666667\n",
      "BERT NER UNCASED BIN:  0.4166666666666667\n",
      "SPACY NER CASED:  0.0\n",
      "SPACY NER UNCASED:  0.0\n",
      "#################################################################################################### \n",
      " Lifecycle | #Sentences: 5 \n",
      "\n",
      "BERT NER UNCASED MUL:  0.2\n",
      "BERT NER CASED BIN:  0.4\n",
      "BERT NER UNCASED BIN:  0.2\n",
      "SPACY NER CASED:  0.6\n",
      "SPACY NER UNCASED:  0.0\n",
      "#################################################################################################### \n",
      " Product Owner | #Sentences: 4 \n",
      "\n",
      "BERT NER UNCASED MUL:  0.0\n",
      "BERT NER CASED BIN:  0.0\n",
      "BERT NER UNCASED BIN:  0.0\n",
      "SPACY NER CASED:  0.0\n",
      "SPACY NER UNCASED:  0.0\n",
      "#################################################################################################### \n",
      " Kahneman | #Sentences: 5 \n",
      "\n",
      "BERT NER UNCASED MUL:  0.8\n",
      "BERT NER CASED BIN:  0.8\n",
      "BERT NER UNCASED BIN:  0.8\n",
      "SPACY NER CASED:  0.8\n",
      "SPACY NER UNCASED:  0.0\n",
      "#################################################################################################### \n",
      " Fix | #Sentences: 9 \n",
      "\n",
      "BERT NER UNCASED MUL:  0.3333333333333333\n",
      "BERT NER CASED BIN:  0.2222222222222222\n",
      "BERT NER UNCASED BIN:  0.0\n",
      "SPACY NER CASED:  0.0\n",
      "SPACY NER UNCASED:  0.0\n",
      "#################################################################################################### \n",
      " GitLab | #Sentences: 51 \n",
      "\n",
      "BERT NER UNCASED MUL:  0.9607843137254902\n",
      "BERT NER CASED BIN:  0.7647058823529411\n",
      "BERT NER UNCASED BIN:  0.9607843137254902\n",
      "SPACY NER CASED:  0.7450980392156863\n",
      "SPACY NER UNCASED:  0.0\n",
      "#################################################################################################### \n",
      " GCM Network Manager | #Sentences: 6 \n",
      "\n",
      "BERT NER UNCASED MUL:  0.611111111111111\n",
      "BERT NER CASED BIN:  1.0\n",
      "BERT NER UNCASED BIN:  0.7222222222222222\n",
      "SPACY NER CASED:  0.0\n",
      "SPACY NER UNCASED:  0.0\n",
      "#################################################################################################### \n",
      " Quora | #Sentences: 5 \n",
      "\n",
      "BERT NER UNCASED MUL:  0.6\n",
      "BERT NER CASED BIN:  1.0\n",
      "BERT NER UNCASED BIN:  0.8\n",
      "SPACY NER CASED:  1.0\n",
      "SPACY NER UNCASED:  0.2\n"
     ]
    }
   ],
   "source": [
    "ctr = 0\n",
    "num_of_entities=10\n",
    "for gold_entity in sent_dict:\n",
    "    entities = []\n",
    "    entities_c = []\n",
    "    entities_u = []\n",
    "    entities_spacy_u = []\n",
    "    entities_spacy_c = []\n",
    "    # Selecting entities based on starting letter\n",
    "    if gold_entity[0]==\"T\":\n",
    "        continue\n",
    "    gold_sentences = sent_dict[gold_entity][:500]\n",
    "    gold_entity = gold_entity.replace(\"the \",\"\")\n",
    "    if len(gold_sentences)<4:\n",
    "        continue\n",
    "    \n",
    "    for text in gold_sentences:\n",
    "        ent, con= ner_model.get_entities(text)\n",
    "        ent_c, con_c= ner_model_c.get_entities(text)\n",
    "        ent_u, con_u= ner_model_u.get_entities(text)\n",
    "        \n",
    "        doc_c = nlp(text)\n",
    "        entities_spacy_c.extend(list(set([x for e in doc_c.ents for x in e.text.upper().split() ])))\n",
    "        doc_u = nlp(text.lower())\n",
    "        entities_spacy_u.extend(list(set([x for e in doc_u.ents for x in e.text.upper().split() ])))\n",
    "        \n",
    "        if len(ent)==0 and len(ent_c)==0 and len(ent_u)==0:\n",
    "            continue\n",
    "        entities.extend(list(set(ner_model.wordize(ent))))\n",
    "        entities_c.extend(list(set(ner_model_c.wordize(ent_c))))\n",
    "        entities_u.extend(list(set(ner_model_u.wordize(ent_u))))\n",
    "    \n",
    "    # Consider n-gram entities. Take mean score of all entity detections.\n",
    "    gold_entity_list = gold_entity.upper().split()\n",
    "    counts_1 = np.mean([entities.count(gold_ent) for gold_ent in gold_entity_list])\n",
    "    counts_2 = np.mean([entities_c.count(gold_ent) for gold_ent in gold_entity_list])\n",
    "    counts_3 = np.mean([entities_u.count(gold_ent) for gold_ent in gold_entity_list])\n",
    "    counts_ner_spacy_cased = entities_spacy_c.count(gold_entity.upper())\n",
    "    counts_ner_spacy_uncased = entities_spacy_u.count(gold_entity.upper())\n",
    "    \n",
    "    print(\"#\"*100,\"\\n\",gold_entity,\"| #Sentences:\",len(gold_sentences),\"\\n\")\n",
    "    print(\"BERT NER UNCASED MUL: \", counts_1/len(gold_sentences))\n",
    "    print(\"BERT NER CASED BIN: \", counts_2/len(gold_sentences))\n",
    "    print(\"BERT NER UNCASED BIN: \", counts_3/len(gold_sentences))\n",
    "    print(\"SPACY NER CASED: \", counts_ner_spacy_cased/len(gold_sentences))\n",
    "    print(\"SPACY NER UNCASED: \", counts_ner_spacy_uncased/len(gold_sentences))\n",
    "    \n",
    "    ctr+=1\n",
    "    if ctr==num_of_entities:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It can be gathered from a couple of answers on Quora that around 25% to even 40% of employees at Google, Apple, Facebook and Amazon are software engineers — people that are technically capable of executing software projects, writing the program code theirselves.',\n",
       " 'You can follow me on LinkedIn, Quora, Twitter, and Instagram where I answer questions related to Mobile Development, especially Android and Flutter.',\n",
       " 'Check the lists of FP disadvantages on Quora and in the article by Alexander Alvin too.',\n",
       " 'Then, moving on, I found the Quora programming community.',\n",
       " 'These are technology companies that might be as young as a two-person startup and also those who have started fully maturing (as an example, Dropbox, Airbnb, and Quora were all at one time or another incubated by Y Combinator).']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heres a link to some documentation the great people over at Atlassian put together to help you understand what a Git workflow is as well as the different types of workflows you can integrate into each repository you work on.\n",
      "['Atlassian', 'Git']\n",
      "\n",
      "Some time ago, I read a piece on how Atlassian determined what to put in their backlog.\n",
      "['Atlassian']\n",
      "\n",
      "The speaker shared that at one of their offices, they had put up persona cards on the bathroom walls so that Atlassians don't risk a minute unfocused on the people they are building their products for.\n",
      "['Atlassians']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ner_model_u\n",
    "for text in sent_dict['Atlassian'][:10]:\n",
    "    print(text)\n",
    "    ents = model.get_entities(text,get_non_entities=True)[0]\n",
    "    print(model.wordize(ents,capitalize=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on Meeting Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/ether/Desktop/BERT_Similarity_experiments/data/entity_validation.json\",\"r\") as f:\n",
    "    entity_val_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We go into that issue because we read the from the Json field column tables. We we have their columns available, but we are not breaking from that column which is creating a problem but then you know, bring a that solution we should start needed it from that columns and only few places we've done ready need to do that the is the case. I think this not of code is the if you want to use the column portion of the values json log then there will be George changes setting. U a D then we start fields and all not so in case of meeting meeting and recording and markers. We have both Json fields and the call proper call call let let can that like that what I telling is will not do anything with that in the Json and value of d there right. We will replace the swing field with the Hyphen iPhone that's. The we are reading the ID from the Json. I'm not from the column. So in the days very while waiting from a database we need to change oriented B a force layer. I I will from the value where it is like it a relationship right but then the column or as become the that valuation can value. So if we start reading from actual columns we should not ask that's what I thinking less less. Let's see. And also just ping up existing links which we have doing the recording link right. That we it work automatically because those will not having open do id. Yeah, maybe if we can translate like internally and finish through that again. From layer where we to transfer it okay let me do five okay. I'll make the new are you working today. Yeah. Actually yesterday was what little bit from this cloud distribution task like basically what you what we to do is that me or we want to point it in and if the API part has a slack in okay part will do it routed to API p gateway but if it doesn't have any other if you are anything with before we have go to load we will go back to the same whatever we have the proxy right? So so the distribution and all to so that that part from started working yesterday I talk to trishant. So there are no blockage from there are point of view so we can like today be able to read that reference script for it. So after this I want to work on a P a service solution connect and is moving back to external load. The API will directed of external. So that with the proper point id. \n",
      "\n",
      "\n",
      " ['Json', 'George'] No. of Sentences: 24 \n",
      "\n",
      "BERT NER UNCASED MUL: \n",
      "{'Trishant', 'Json', 'Iphone', 'Api', 'George'}\n",
      "[0.9447154, 0.8766968, 0.5316695868968964, 0.5936016, 0.51895803, 0.58843654, 0.2873212695121765, 0.42774635553359985, 0.9970293, 0.99557376, 0.6495798, 0.5135834217071533, 0.9620375, 0.9635579, 0.8081783, 0.13638293743133545]\n",
      "\n",
      "BERT NER CASED BIN: \n",
      "{'Json', 'Trishant', 'Iphone', 'Hyphen', 'Api', 'George'}\n",
      "[0.9997675, 0.99957186, 0.99967146, 0.96997046, 0.97862303, 0.98206055, 0.95305043, 0.99725974, 0.97399443, 0.98861736, 0.9952983, 0.9995254, 0.9995004, 0.9993149, 0.12129217386245728, 0.99976283, 0.05452167987823486, 0.91452324, 0.999548, 0.39306920766830444]\n",
      "\n",
      "BERT NER UNCASED BIN: \n",
      "{'Trishant', 'Json', 'P', 'George', 'Gateway'}\n",
      "[0.99943, 0.998691, 0.993737, 0.6033808, 0.24142366647720337, 0.73601514, 0.20057690143585205, 0.99870694, 0.99625957, 0.8795858, 0.10115218162536621, 0.9998323, 0.999196, 0.99945897]\n",
      "\n",
      "SPACY NER CASED {'today', 'Json', 'API', 'Hyphen', 'yesterday', 'the days', 'George', 'five'}\n",
      "SPACY NER UNCASED {'today', 'yesterday', 'five', 'the days'}\n",
      "####################################################################################################\n",
      "Will be logged out by default because we will be checking open in. I'm bidding to it it is like. Okay Okay We will we do integrations like yeah i. To Franklin do a test for it. Yeah, go like call but we can't test this stuff app staging to that is I don't. I can release a build for it. So once I like I'll do like once I get sometime time I'll set up the commands to build file for to. Yeah because like you attach a recorder test say and yeah i. So what kind was thinking that I like instead of posting pushing directly to I'll just make a local then give it to to then after E s test stated then I'll push it. Yeah yeah that you can like two staging to build for then I got it one. I was working on Desktop of sharing screen sharing so the next stop pal like for the next it's working fine now but the changes I had made in the background for the has started the current or that browser meet. So today I'll work on the that of our browser because I had to change the version so electron so I'll work on that to the and then I'll pick the device selection like or for the call if it is not said in the if let's say has not handling if when we have sent it a configuration or options. We have to explicitly set those devices that's pretty small. So first I'll completely complete this task and then I'll pick them. What is it because I thought with that started and what is issue there guys is right. There can be the device Id D we we are sending it to account configuration okay, but sometimes you say rights server options and takes voice okay itself. So once the API call the how we have down display my Java files the same thing we have to make it for the video or not the device selection. \n",
      "\n",
      "\n",
      " ['Franklin', 'Desktop', 'Java'] No. of Sentences: 17 \n",
      "\n",
      "BERT NER UNCASED MUL: \n",
      "{'Franklin', 'Java', 'To', 'Api', 'Electron'}\n",
      "[0.07187098264694214, 0.971778, 0.7965615, 0.2074645757675171, 0.9939275]\n",
      "\n",
      "BERT NER CASED BIN: \n",
      "{'Franklin', 'Java', 'To', 'Desktop'}\n",
      "[0.11654561758041382, 0.99983656, 0.99936944, 0.9978398, 0.9950964, 0.9995697]\n",
      "\n",
      "BERT NER UNCASED BIN: \n",
      "{'Franklin', 'Java', 'Call', 'Electron'}\n",
      "[0.9988023, 0.7876625, 0.99978536, 0.9992409]\n",
      "\n",
      "SPACY NER CASED {'today', 'two', 'Franklin', 'Java', 'API', 'Desktop'}\n",
      "SPACY NER UNCASED {'today', 'two'}\n",
      "####################################################################################################\n",
      "Direction whatever comes to Key Service under theme service to say payment come to this from this action. I think we text themselves that's right so I'll I'll have it ready from my side and then I'll sit with how you the and go what is needed may by tomorrow. Okay.\n",
      "\n",
      "\n",
      " ['Key Service'] No. of Sentences: 3 \n",
      "\n",
      "BERT NER UNCASED MUL: \n",
      "set()\n",
      "[]\n",
      "\n",
      "BERT NER CASED BIN: \n",
      "{'Service', 'Key'}\n",
      "[0.99973077, 0.99982846]\n",
      "\n",
      "BERT NER UNCASED BIN: \n",
      "set()\n",
      "[]\n",
      "\n",
      "SPACY NER CASED {'Key Service', 'tomorrow'}\n",
      "SPACY NER UNCASED {'tomorrow'}\n",
      "####################################################################################################\n",
      "I so once I've finished this small testing. Then I need that team as well. I'm writing of writing the code for that training it on this statement. So you are not both are working together and this one yeah okay yeah more to front end for sure is going. Yeah so I'm pretty I just raised the P have like my and deployed on sitting to basically stop we stop using cookies and using local. So the issue was that ill staging to it was not sending cookies in the request call not for some production was sending cookies like all the store cookies keys and all like like a P a calls I couldn't find a reason for it like by first doing so we are using x C s, but even with victory credentials all circles still sending it. So for now we have this switch to local storage for all like wherever we need to stop completely using the local stories for it now I'm working on like in index I the web website is not like meeting the is not working. So when we like for when we disable in web update it does not like it's not the are not being worried basically so I will add a call back function for that so that from text of for text of we use call once and instead of the to push updates. So that should fix that. Okay Any you have to ice of using images address like those that we shall want right now. M are like it's simpler to use like works like on Browser. It's basically like add event and listening link through that it's pretty forward straightforward. But for some reason like they have deployed even in electron and they want test to use I P C for it but then if we start using that then it will be like a step the implementation for decks of our together. So that function would be much better in case like from the E dot usage and implementation. Okay Okay We have we have done the same thing for recorded plan is still the currently. No no, we got on web on production both the call L are using web sockets. Is that that's the only thing that yesterday column me. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['Browser'] No. of Sentences: 17 \n",
      "\n",
      "BERT NER UNCASED MUL: \n",
      "{'Web', 'Victory', 'Local', 'Dot', 'Electron'}\n",
      "[0.4147031307220459, 0.8855326, 0.7797708, 0.12225759029388428, 0.33489859104156494, 0.21373093128204346]\n",
      "\n",
      "BERT NER CASED BIN: \n",
      "{'M', 'I', 'P', 'C', 'Browser'}\n",
      "[0.9614601, 0.91519994, 0.8662625, 0.94643325, 0.7769169, 0.7585058, 0.79917485]\n",
      "\n",
      "BERT NER UNCASED BIN: \n",
      "{'E', 'Index', 'Electron', 'Web', 'Victory', 'Dot', 'I'}\n",
      "[0.77422136, 0.4337450861930847, 0.7837314, 0.1875535249710083, 0.99972564, 0.6249454, 0.48362767696380615, 0.8940357, 0.05614018440246582]\n",
      "\n",
      "SPACY NER CASED {'Browser', 'yesterday', 'first'}\n",
      "SPACY NER UNCASED {'yesterday', 'first'}\n",
      "####################################################################################################\n",
      "Also as the experiment to test on staging to what time what I've done is calculated in phrase and ent*ties code with respect to segment. I and the overall medium code is also a noted right. So now what I do is I check if any of the key for is lower than this overall quality code and if it is then I just remove it before sending it to platform.\n",
      "\n",
      "\n",
      " [] No. of Sentences: 3 \n",
      "\n",
      "BERT NER UNCASED MUL: \n",
      "set()\n",
      "[]\n",
      "\n",
      "BERT NER CASED BIN: \n",
      "set()\n",
      "[]\n",
      "\n",
      "BERT NER UNCASED BIN: \n",
      "set()\n",
      "[]\n",
      "\n",
      "SPACY NER CASED set()\n",
      "SPACY NER UNCASED set()\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "for seg_chunk in entity_val_set:\n",
    "    gold_entity_list = list(seg_chunk['entities'].keys())\n",
    "    entities = []\n",
    "    entities_c = []\n",
    "    entities_u = []\n",
    "    entities_spacy_u = []\n",
    "    entities_spacy_c = []\n",
    "    conf=[]\n",
    "    conf_c=[]\n",
    "    conf_u=[]\n",
    "    print(seg_chunk['segments'],end=\"\\n\\n\")\n",
    "    gold_sentences = sent_tokenize(seg_chunk['segments'])\n",
    "    for text in gold_sentences:\n",
    "#         text=text.lower() # ABLATION STUDY FOR BERT CASED MODEL\n",
    "        ent, con= ner_model.get_entities(text)\n",
    "        ent_c, con_c= ner_model_c.get_entities(text)\n",
    "        ent_u, con_u= ner_model_u.get_entities(text)\n",
    "        entities.extend(ent)\n",
    "        entities_c.extend(ent_c)\n",
    "        entities_u.extend(ent_u)\n",
    "        conf.extend(con)\n",
    "        conf_c.extend(con_c)\n",
    "        conf_u.extend(con_u)\n",
    "        \n",
    "        doc_c = nlp(text)\n",
    "        entities_spacy_c.extend(list(set([e.text for e in doc_c.ents])))\n",
    "        doc_u = nlp(text.lower())\n",
    "        entities_spacy_u.extend(list(set([e.text for e in doc_u.ents])))\n",
    "    entities = ner_model.wordize(entities, capitalize=True)\n",
    "    entities_c = ner_model_c.wordize(entities_c, capitalize=True)\n",
    "    entities_u = ner_model_u.wordize(entities_u, capitalize=True)\n",
    "    \n",
    "    print(\"\\n\",gold_entity_list,\"No. of Sentences:\",len(gold_sentences),\"\\n\")\n",
    "    print(\"BERT NER UNCASED MUL: \",set(entities),conf,sep=\"\\n\",end=\"\\n\\n\")\n",
    "    print(\"BERT NER CASED BIN: \",set(entities_c),conf_c,sep=\"\\n\",end=\"\\n\\n\")\n",
    "    print(\"BERT NER UNCASED BIN: \",set(entities_u),conf_u,sep=\"\\n\",end=\"\\n\\n\")\n",
    "    print(\"SPACY NER CASED\",set(entities_spacy_c))\n",
    "    print(\"SPACY NER UNCASED\",set(entities_spacy_u))\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Arjun",
   "language": "python",
   "name": "arjun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
