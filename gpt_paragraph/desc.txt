GPT gives good features for sentence embeddings. These embeddings seem to be separated well between in-domain and out-of-domain topics when measured using cosine similarity.

Paragraph embeddings can be constructed using a linear combination of sentence embeddings. When a naive summing of embeddings was performed, the resulting paragraph embeddings were separable by context. On tweaking the algorithm to perform summed aggregation of embeddings on groups of sentences such that their combined length was less than the max permissible length of the model, better results were observed. It was noticed however that the last set of sentences seemed to influence the paragraph the most and would skew the results of the paragraph embedding comparison (using cosine similarity metric). Moreover, there were cases of overlap of scores between two in-domain paragraphs and between one in-domain and one out-of-domain paragraph.

There are a few possible solutions to this problem:
1. Use a different metric.
2. Divide the paragraph equally into chunks and then feed them into the model before aggregating. Improves scores but last sentence bias is not completely negated.
3. Use an additional neural network as an aggregator of these sentence embeddings in order to learn paragraph embeddings in a non-linear space. These networks (possibly LSTM based) could be trained on the objective to learn paragraph features from sentence features based on cosine similarity loss.
4. Train GPT as a language model in order to remove influence of last sentence on the score.

Approach #2 helps to some extent but there is still a need to improve the separation between two in-domain paragraphs and between one in-domain and one out-of-domain paragraph.
The gpt-paragraph-similarity-LSTM file details approach #3.
Using Approach #4, the GPT LM model with an LSTM head is averse to addition of non-domain topics at the end of the in-domain paragraph but does not capture context as well as the GPT with Multi Choice Head model. This results in inconsistent cosine similarity scores.

Conclusion: GPT paragraph embeddings show good topic separation and can be used for separating segments based on context. In order to not rely on aggregation of sentence features, a Bi-LSTM head was used to aggregate the features instead of summing up the sentence-level feature vectors. This resulted in better context-capture across a paragraph.
