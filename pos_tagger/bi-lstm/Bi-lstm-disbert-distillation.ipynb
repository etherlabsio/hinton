{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0305 19:02:47.621206 140580620220224 file_utils.py:35] PyTorch version 1.2.0 available.\n",
      "I0305 19:02:50.027409 140580620220224 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/shubham/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shu', '##bham']\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from transformers import DistilBertTokenizer,AdamW,DistilBertPreTrainedModel,  DistilBertModel, DistilBertConfig\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "torch.manual_seed(1)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "device = 'cuda'\n",
    "print(tokenizer.tokenize('shubham'))\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "def read_examples_from_file(data_dir, mode):\n",
    "    file_path = os.path.join(data_dir, \"{}.txt\".format(mode))\n",
    "    guid_index = 1\n",
    "    examples = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                if words:\n",
    "                    examples.append(InputExample(guid=\"{}-{}\".format(mode, guid_index),\n",
    "                                                 words=words,\n",
    "                                                 labels=labels))\n",
    "                    guid_index += 1\n",
    "                    words = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                splits = line.split(\" \")\n",
    "                words.append(splits[0])\n",
    "                if len(splits) > 1:\n",
    "                    lab = splits[-3].replace(\"\\n\", \"\")\n",
    "                    #print('#####################:',lab,splits[-4].replace(\"\\n\", \"\"))\n",
    "                    if '$' in lab and len(list(lab))>1:\n",
    "                        labels.append(lab[:])\n",
    "                    elif not lab.isalnum():\n",
    "                        labels.append(\"PUNC\")\n",
    "                    else:\n",
    "                        labels.append(lab[:])\n",
    "                else:\n",
    "                    # Examples could have no label for mode = \"test\"\n",
    "                    labels.append(\"PUNC\")\n",
    "            #print(labels)\n",
    "        if words:\n",
    "            examples.append(InputExample(guid=\"%s-%d\".format(mode, guid_index),\n",
    "                                         words=words,\n",
    "                                        labels=labels))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_obj = read_examples_from_file('/home/shubham/Project/pos_tag/data/ner','train')\n",
    "lab_list = ['NNS', 'CD', 'TO', 'VBD', 'WP$', 'LS', 'RP', 'SYM', 'VBN', 'NNPS', 'RBR', 'JJS', 'VBP', 'MD', 'JJ', 'CC', 'VBG', 'IN', 'WP', 'PRP', 'PUNC', 'POS', 'FW', 'JJR', 'EX', 'WRB', 'DT', 'UH', 'VB', 'VBZ', 'RB', 'RBS', 'NN', 'WDT', 'NNP', 'PRP$', 'PDT']\n",
    "\n",
    "label_map = {label:i for i, label in enumerate(lab_list)}\n",
    "ix_to_tag = {i:label for i, label in enumerate(lab_list)}\n",
    "\n",
    "def input_to_features(words,labels,pad_token_label_id=-1):  \n",
    "    tokens = []\n",
    "    label_ids = []\n",
    "    for word, label in zip(words, labels):\n",
    "        #print(word)\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        #print(word_tokens)\n",
    "        tokens.extend(word_tokens)\n",
    "        # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "        label_ids.extend([label_map[label]] + [label_map[label]] * (len(word_tokens) - 1))\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        dbert_input_ids = tokenizer.encode(\"[CLS]\",add_special_tokens=False) + input_ids + tokenizer.encode(\"[SEP]\",add_special_tokens=False) \n",
    "    return torch.tensor(dbert_input_ids, dtype=torch.long,device=device),torch.tensor(input_ids, dtype=torch.long,device=device),torch.tensor(label_ids, dtype=torch.long,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7327, 19164,  2446,  2655,  2000, 17757,  2329, 12559,  1012],\n",
      "       device='cuda:0') tensor([34, 29, 14, 32,  2, 28, 14, 32, 20], device='cuda:0')\n",
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "[100, 19164, 100, 2655, 2000, 17757, 100, 12559, 1012]\n"
     ]
    }
   ],
   "source": [
    "#train_obj = read_examples_from_file('/home/shubham/Project/pos_tag/data/ner','train')\n",
    "#lab_list = ['NNS', 'CD', 'TO', 'VBD', 'WP$', 'LS', 'RP', 'SYM', 'VBN', 'NNPS', 'RBR', 'JJS', 'VBP', 'MD', 'JJ', 'CC', 'VBG', 'IN', 'WP', 'PRP', 'PUNC', 'POS', 'FW', 'JJR', 'EX', 'WRB', 'DT', 'UH', 'VB', 'VBZ', 'RB', 'RBS', 'NN', 'WDT', 'NNP', 'PRP$', 'PDT']\n",
    "\n",
    "training_data =[]\n",
    "for i in train_obj:\n",
    "    training_data.append((i.words, i.labels,))\n",
    "_,a,b = input_to_features(training_data[0][0],training_data[0][1])\n",
    "print(a,b)\n",
    "print(training_data[0][0])\n",
    "print(tokenizer.encode(training_data[0][0],add_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertForTokenClassificationCustom(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(DistilBertForTokenClassificationCustom, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None):\n",
    "\n",
    "        outputs = self.distilbert(input_ids,\n",
    "                            attention_mask=None,\n",
    "                            head_mask=None,\n",
    "                            inputs_embeds=None)\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits)\n",
    "\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForTokenClassificationCustom(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = DistilBertConfig(num_labels=37)\n",
    "teacher_model = DistilBertForTokenClassificationCustom(config)\n",
    "state_dict = torch.load(\"/home/shubham/Project/pos_tag/models/distil/pytorch_model.bin\",map_location=device)\n",
    "teacher_model.load_state_dict(state_dict)\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "learning_rate = 5e-5\n",
    "adam_epsilon = 1e-8\n",
    "weight_decay = 0.0\n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,num_layers=5,bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores,tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "    model.eval()\n",
    "    val_obj = read_examples_from_file('/home/shubham/Project/pos_tag/data/ner','dev')\n",
    "    validation_data =[]\n",
    "    for i in val_obj:\n",
    "        validation_data.append((i.words, i.labels,))\n",
    "    out_list = []\n",
    "    pred_list = []\n",
    "    for sentence, tags in (validation_data[:]): \n",
    "        dbert_input_ids,sentence_in,targets = input_to_features(sentence,tags)\n",
    "        targets = [ix_to_tag[i] for i in targets.tolist()]\n",
    "        out_list.append(targets)\n",
    "        with torch.no_grad():\n",
    "            tag_scores,_ = model(sentence_in)\n",
    "        pred = []\n",
    "        for i in tag_scores:\n",
    "            pred.append(ix_to_tag[int(np.argmax(i.cpu().detach().numpy()))])\n",
    "        pred_list.append(pred)\n",
    "    #print((pred_list))\n",
    "    sc = f1_score(out_list,pred_list)\n",
    "    print(sc)\n",
    "    print(classification_report(out_list,pred_list))\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.5234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.7572721467421224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 1/100 [10:47<17:48:16, 647.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.56      0.62      0.59       939\n",
      "       JJ       0.52      0.53      0.52      2765\n",
      "       IN       0.94      0.96      0.95      4892\n",
      "       DT       0.99      0.98      0.98      3511\n",
      "      POS       0.96      0.98      0.97       423\n",
      "       TO       0.98      0.99      0.99       905\n",
      "       NN       0.66      0.54      0.60      5358\n",
      "      NNP       0.48      0.63      0.54      5817\n",
      "       CD       0.81      0.85      0.83      2935\n",
      "      VBP       0.76      0.64      0.70       365\n",
      "       EX       0.93      0.33      0.48        40\n",
      "      NNS       0.69      0.63      0.66      2486\n",
      "     PUNC       0.96      0.99      0.97      6029\n",
      "      WDT       1.00      0.66      0.80       155\n",
      "      VBG       0.27      0.44      0.33       699\n",
      "       MD       0.95      0.93      0.94       300\n",
      "      PRP       0.99      0.92      0.96       862\n",
      "      VBZ       0.74      0.67      0.70       509\n",
      "       CC       0.97      0.96      0.97       932\n",
      "      VBD       0.94      0.70      0.80      2224\n",
      "     NNPS       0.00      0.00      0.00       165\n",
      "      WRB       0.94      0.88      0.91        93\n",
      "      VBN       0.62      0.43      0.51       928\n",
      "       VB       0.88      0.64      0.74      1112\n",
      "     PRP$       0.95      0.86      0.90       422\n",
      "       RP       0.91      0.07      0.13       148\n",
      "       WP       0.97      0.91      0.94       127\n",
      "      JJR       0.64      0.51      0.57       105\n",
      "      RBS       0.00      0.00      0.00        18\n",
      "      JJS       0.00      0.00      0.00        78\n",
      "      RBR       0.79      0.21      0.33        53\n",
      "      SYM       0.96      0.29      0.45        86\n",
      "      WP$       0.00      0.00      0.00         9\n",
      "       FW       0.00      0.00      0.00        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.00      0.00      0.00         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.76      0.76      0.76     45527\n",
      "macro avg       0.77      0.76      0.76     45527\n",
      "\n",
      "loss tensor(0.7008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8284260662481057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [21:32<17:36:07, 646.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.72      0.71      0.72       939\n",
      "       JJ       0.63      0.65      0.64      2765\n",
      "       IN       0.95      0.98      0.96      4892\n",
      "       DT       0.99      0.98      0.99      3511\n",
      "      POS       0.98      0.99      0.98       423\n",
      "       TO       0.99      1.00      1.00       905\n",
      "       NN       0.74      0.72      0.73      5358\n",
      "      NNP       0.62      0.72      0.67      5817\n",
      "       CD       0.82      0.91      0.86      2935\n",
      "      VBP       0.73      0.78      0.75       365\n",
      "       EX       0.87      1.00      0.93        40\n",
      "      NNS       0.75      0.78      0.77      2486\n",
      "     PUNC       0.97      0.99      0.98      6029\n",
      "      WDT       0.96      0.88      0.92       155\n",
      "      VBG       0.41      0.59      0.48       699\n",
      "       MD       0.97      0.96      0.96       300\n",
      "      PRP       0.98      0.97      0.98       862\n",
      "      VBZ       0.66      0.78      0.71       509\n",
      "       CC       0.99      0.98      0.99       932\n",
      "      VBD       0.87      0.89      0.88      2224\n",
      "     NNPS       0.14      0.02      0.03       165\n",
      "      WRB       0.96      0.94      0.95        93\n",
      "      VBN       0.78      0.66      0.71       928\n",
      "       VB       0.87      0.72      0.79      1112\n",
      "     PRP$       0.98      1.00      0.99       422\n",
      "       RP       0.89      0.34      0.49       148\n",
      "       WP       0.95      0.98      0.96       127\n",
      "      JJR       0.70      0.61      0.65       105\n",
      "      RBS       0.00      0.00      0.00        18\n",
      "      JJS       0.58      0.63      0.60        78\n",
      "      RBR       0.76      0.36      0.49        53\n",
      "      SYM       0.79      0.76      0.77        86\n",
      "      WP$       0.00      0.00      0.00         9\n",
      "       FW       0.00      0.00      0.00        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.67      0.29      0.40         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.82      0.84      0.83     45527\n",
      "macro avg       0.82      0.84      0.83     45527\n",
      "\n",
      "loss tensor(0.3284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8402148152979801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/100 [32:17<17:24:45, 646.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.69      0.76      0.72       939\n",
      "       JJ       0.59      0.69      0.64      2765\n",
      "       IN       0.96      0.98      0.97      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.99      0.99      0.99       423\n",
      "       TO       1.00      1.00      1.00       905\n",
      "       NN       0.77      0.74      0.76      5358\n",
      "      NNP       0.68      0.73      0.70      5817\n",
      "       CD       0.82      0.88      0.85      2935\n",
      "      VBP       0.70      0.78      0.74       365\n",
      "       EX       1.00      0.90      0.95        40\n",
      "      NNS       0.80      0.79      0.79      2486\n",
      "     PUNC       0.98      0.98      0.98      6029\n",
      "      WDT       0.97      0.88      0.92       155\n",
      "      VBG       0.54      0.65      0.59       699\n",
      "       MD       0.97      0.98      0.97       300\n",
      "      PRP       0.99      0.96      0.98       862\n",
      "      VBZ       0.64      0.84      0.73       509\n",
      "       CC       0.99      0.98      0.99       932\n",
      "      VBD       0.88      0.89      0.89      2224\n",
      "     NNPS       0.25      0.11      0.15       165\n",
      "      WRB       0.97      0.96      0.96        93\n",
      "      VBN       0.78      0.66      0.72       928\n",
      "       VB       0.79      0.81      0.80      1112\n",
      "     PRP$       0.97      0.99      0.98       422\n",
      "       RP       0.78      0.47      0.58       148\n",
      "       WP       0.98      0.97      0.98       127\n",
      "      JJR       0.70      0.71      0.71       105\n",
      "      RBS       0.00      0.00      0.00        18\n",
      "      JJS       0.75      0.65      0.70        78\n",
      "      RBR       0.65      0.32      0.43        53\n",
      "      SYM       0.67      0.74      0.71        86\n",
      "      WP$       1.00      0.44      0.62         9\n",
      "       FW       0.00      0.00      0.00        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.44      0.57      0.50         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.83      0.85      0.84     45527\n",
      "macro avg       0.83      0.85      0.84     45527\n",
      "\n",
      "loss tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8484558711668442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/100 [43:02<17:13:31, 645.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.68      0.77      0.72       939\n",
      "       JJ       0.63      0.69      0.66      2765\n",
      "       IN       0.96      0.98      0.97      4892\n",
      "       DT       0.99      0.98      0.99      3511\n",
      "      POS       0.99      0.98      0.98       423\n",
      "       TO       0.99      1.00      1.00       905\n",
      "       NN       0.77      0.75      0.76      5358\n",
      "      NNP       0.69      0.75      0.72      5817\n",
      "       CD       0.84      0.89      0.86      2935\n",
      "      VBP       0.82      0.75      0.78       365\n",
      "       EX       0.88      0.95      0.92        40\n",
      "      NNS       0.81      0.82      0.81      2486\n",
      "     PUNC       0.98      0.99      0.99      6029\n",
      "      WDT       0.95      0.90      0.92       155\n",
      "      VBG       0.54      0.69      0.61       699\n",
      "       MD       0.98      0.96      0.97       300\n",
      "      PRP       0.98      0.98      0.98       862\n",
      "      VBZ       0.77      0.81      0.79       509\n",
      "       CC       0.99      0.98      0.99       932\n",
      "      VBD       0.88      0.90      0.89      2224\n",
      "     NNPS       0.15      0.08      0.10       165\n",
      "      WRB       0.99      0.92      0.96        93\n",
      "      VBN       0.77      0.72      0.74       928\n",
      "       VB       0.79      0.82      0.81      1112\n",
      "     PRP$       0.99      0.99      0.99       422\n",
      "       RP       0.81      0.41      0.55       148\n",
      "       WP       0.98      0.98      0.98       127\n",
      "      JJR       0.74      0.62      0.67       105\n",
      "      RBS       0.00      0.00      0.00        18\n",
      "      JJS       0.67      0.72      0.70        78\n",
      "      RBR       0.82      0.26      0.40        53\n",
      "      SYM       0.91      0.72      0.81        86\n",
      "      WP$       1.00      0.56      0.71         9\n",
      "       FW       0.02      0.04      0.03        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.67      0.86      0.75         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.84      0.86      0.85     45527\n",
      "macro avg       0.84      0.86      0.85     45527\n",
      "\n",
      "loss tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8611672217066114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 5/100 [53:47<17:02:14, 645.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.77      0.76      0.76       939\n",
      "       JJ       0.62      0.72      0.66      2765\n",
      "       IN       0.97      0.98      0.97      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.98      0.98      0.98       423\n",
      "       TO       0.99      1.00      1.00       905\n",
      "       NN       0.77      0.77      0.77      5358\n",
      "      NNP       0.73      0.75      0.74      5817\n",
      "       CD       0.87      0.92      0.89      2935\n",
      "      VBP       0.85      0.77      0.81       365\n",
      "       EX       0.95      0.90      0.92        40\n",
      "      NNS       0.86      0.80      0.83      2486\n",
      "     PUNC       0.98      0.99      0.99      6029\n",
      "      WDT       0.95      0.90      0.92       155\n",
      "      VBG       0.68      0.68      0.68       699\n",
      "       MD       0.98      0.97      0.97       300\n",
      "      PRP       0.99      0.97      0.98       862\n",
      "      VBZ       0.71      0.84      0.77       509\n",
      "       CC       1.00      0.99      0.99       932\n",
      "      VBD       0.90      0.91      0.90      2224\n",
      "     NNPS       0.41      0.25      0.31       165\n",
      "      WRB       1.00      0.95      0.97        93\n",
      "      VBN       0.78      0.73      0.76       928\n",
      "       VB       0.82      0.82      0.82      1112\n",
      "     PRP$       0.97      1.00      0.98       422\n",
      "       RP       0.80      0.59      0.68       148\n",
      "       WP       0.99      0.98      0.98       127\n",
      "      JJR       0.78      0.72      0.75       105\n",
      "      RBS       1.00      0.06      0.11        18\n",
      "      JJS       0.71      0.71      0.71        78\n",
      "      RBR       0.86      0.47      0.61        53\n",
      "      SYM       0.94      0.71      0.81        86\n",
      "      WP$       1.00      0.67      0.80         9\n",
      "       FW       0.11      0.25      0.16        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.75      0.43      0.55         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.86      0.87      0.86     45527\n",
      "macro avg       0.86      0.87      0.86     45527\n",
      "\n",
      "loss tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8642514662676525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/100 [1:04:32<16:51:12, 645.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.76      0.78      0.77       939\n",
      "       JJ       0.63      0.73      0.68      2765\n",
      "       IN       0.97      0.98      0.97      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.99      0.98      0.99       423\n",
      "       TO       0.99      1.00      1.00       905\n",
      "       NN       0.80      0.77      0.79      5358\n",
      "      NNP       0.73      0.75      0.74      5817\n",
      "       CD       0.86      0.93      0.89      2935\n",
      "      VBP       0.83      0.77      0.80       365\n",
      "       EX       1.00      0.68      0.81        40\n",
      "      NNS       0.82      0.82      0.82      2486\n",
      "     PUNC       0.99      0.99      0.99      6029\n",
      "      WDT       0.93      0.92      0.93       155\n",
      "      VBG       0.68      0.72      0.70       699\n",
      "       MD       0.98      0.97      0.97       300\n",
      "      PRP       0.98      0.98      0.98       862\n",
      "      VBZ       0.72      0.85      0.78       509\n",
      "       CC       1.00      0.99      0.99       932\n",
      "      VBD       0.90      0.90      0.90      2224\n",
      "     NNPS       0.28      0.24      0.25       165\n",
      "      WRB       1.00      0.96      0.98        93\n",
      "      VBN       0.82      0.73      0.77       928\n",
      "       VB       0.86      0.82      0.84      1112\n",
      "     PRP$       0.99      1.00      0.99       422\n",
      "       RP       0.75      0.57      0.65       148\n",
      "       WP       0.98      0.98      0.98       127\n",
      "      JJR       0.79      0.65      0.71       105\n",
      "      RBS       0.73      0.44      0.55        18\n",
      "      JJS       0.76      0.73      0.75        78\n",
      "      RBR       0.71      0.42      0.52        53\n",
      "      SYM       0.98      0.74      0.85        86\n",
      "      WP$       1.00      0.56      0.71         9\n",
      "       FW       0.29      0.17      0.21        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.50      0.14      0.22         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.86      0.87      0.86     45527\n",
      "macro avg       0.86      0.87      0.86     45527\n",
      "\n",
      "loss tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8684423274411694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/100 [1:15:17<16:40:06, 645.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.76      0.79      0.77       939\n",
      "       JJ       0.66      0.74      0.70      2765\n",
      "       IN       0.97      0.98      0.97      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.99      0.97      0.98       423\n",
      "       TO       1.00      1.00      1.00       905\n",
      "       NN       0.80      0.79      0.79      5358\n",
      "      NNP       0.74      0.76      0.75      5817\n",
      "       CD       0.88      0.92      0.90      2935\n",
      "      VBP       0.79      0.80      0.80       365\n",
      "       EX       0.97      0.95      0.96        40\n",
      "      NNS       0.81      0.82      0.82      2486\n",
      "     PUNC       0.98      0.99      0.99      6029\n",
      "      WDT       0.96      0.92      0.94       155\n",
      "      VBG       0.70      0.72      0.71       699\n",
      "       MD       0.98      0.97      0.97       300\n",
      "      PRP       0.99      0.97      0.98       862\n",
      "      VBZ       0.75      0.83      0.79       509\n",
      "       CC       0.99      0.99      0.99       932\n",
      "      VBD       0.89      0.91      0.90      2224\n",
      "     NNPS       0.31      0.25      0.28       165\n",
      "      WRB       0.99      0.95      0.97        93\n",
      "      VBN       0.85      0.71      0.78       928\n",
      "       VB       0.84      0.84      0.84      1112\n",
      "     PRP$       0.97      1.00      0.98       422\n",
      "       RP       0.77      0.64      0.70       148\n",
      "       WP       0.97      0.97      0.97       127\n",
      "      JJR       0.78      0.71      0.75       105\n",
      "      RBS       0.80      0.44      0.57        18\n",
      "      JJS       0.75      0.71      0.73        78\n",
      "      RBR       0.73      0.42      0.53        53\n",
      "      SYM       0.92      0.67      0.78        86\n",
      "      WP$       1.00      0.89      0.94         9\n",
      "       FW       0.33      0.17      0.22        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.71      0.71      0.71         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.86      0.87      0.87     45527\n",
      "macro avg       0.87      0.87      0.87     45527\n",
      "\n",
      "loss tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8653720127572196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 8/100 [1:26:02<16:29:12, 645.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.76      0.77      0.77       939\n",
      "       JJ       0.62      0.74      0.67      2765\n",
      "       IN       0.97      0.98      0.97      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.98      0.99      0.98       423\n",
      "       TO       1.00      1.00      1.00       905\n",
      "       NN       0.78      0.78      0.78      5358\n",
      "      NNP       0.75      0.75      0.75      5817\n",
      "       CD       0.88      0.92      0.90      2935\n",
      "      VBP       0.84      0.75      0.80       365\n",
      "       EX       0.97      0.85      0.91        40\n",
      "      NNS       0.83      0.82      0.83      2486\n",
      "     PUNC       0.99      0.99      0.99      6029\n",
      "      WDT       0.95      0.92      0.93       155\n",
      "      VBG       0.69      0.69      0.69       699\n",
      "       MD       0.98      0.97      0.97       300\n",
      "      PRP       0.99      0.98      0.98       862\n",
      "      VBZ       0.78      0.81      0.79       509\n",
      "       CC       0.99      0.98      0.99       932\n",
      "      VBD       0.88      0.90      0.89      2224\n",
      "     NNPS       0.37      0.24      0.29       165\n",
      "      WRB       0.96      0.96      0.96        93\n",
      "      VBN       0.79      0.73      0.76       928\n",
      "       VB       0.82      0.82      0.82      1112\n",
      "     PRP$       0.98      1.00      0.99       422\n",
      "       RP       0.82      0.57      0.67       148\n",
      "       WP       0.98      0.98      0.98       127\n",
      "      JJR       0.77      0.67      0.71       105\n",
      "      RBS       1.00      0.50      0.67        18\n",
      "      JJS       0.80      0.76      0.78        78\n",
      "      RBR       0.68      0.57      0.62        53\n",
      "      SYM       0.96      0.76      0.84        86\n",
      "      WP$       1.00      0.67      0.80         9\n",
      "       FW       0.38      0.25      0.30        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.60      0.43      0.50         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.86      0.87      0.87     45527\n",
      "macro avg       0.86      0.87      0.87     45527\n",
      "\n",
      "loss tensor(0.0402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8692337101971709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 9/100 [1:36:47<16:18:18, 645.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.77      0.78      0.77       939\n",
      "       JJ       0.67      0.71      0.69      2765\n",
      "       IN       0.98      0.98      0.98      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.99      0.98      0.98       423\n",
      "       TO       1.00      1.00      1.00       905\n",
      "       NN       0.81      0.77      0.79      5358\n",
      "      NNP       0.71      0.78      0.74      5817\n",
      "       CD       0.88      0.92      0.90      2935\n",
      "      VBP       0.78      0.79      0.78       365\n",
      "       EX       0.92      0.90      0.91        40\n",
      "      NNS       0.86      0.82      0.84      2486\n",
      "     PUNC       0.98      0.99      0.99      6029\n",
      "      WDT       0.95      0.94      0.94       155\n",
      "      VBG       0.70      0.74      0.72       699\n",
      "       MD       0.98      0.97      0.97       300\n",
      "      PRP       0.99      0.98      0.98       862\n",
      "      VBZ       0.76      0.83      0.80       509\n",
      "       CC       0.99      0.99      0.99       932\n",
      "      VBD       0.88      0.91      0.90      2224\n",
      "     NNPS       0.35      0.26      0.30       165\n",
      "      WRB       0.98      0.95      0.96        93\n",
      "      VBN       0.82      0.74      0.78       928\n",
      "       VB       0.84      0.84      0.84      1112\n",
      "     PRP$       0.98      1.00      0.99       422\n",
      "       RP       0.74      0.64      0.68       148\n",
      "       WP       0.98      0.98      0.98       127\n",
      "      JJR       0.69      0.76      0.72       105\n",
      "      RBS       0.90      0.50      0.64        18\n",
      "      JJS       0.78      0.81      0.79        78\n",
      "      RBR       0.89      0.32      0.47        53\n",
      "      SYM       0.82      0.77      0.80        86\n",
      "      WP$       1.00      0.78      0.88         9\n",
      "       FW       0.33      0.08      0.13        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.83      0.71      0.77         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.86      0.87      0.87     45527\n",
      "macro avg       0.87      0.87      0.87     45527\n",
      "\n",
      "loss tensor(0.4494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8711166399319082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 10/100 [1:47:31<16:07:28, 644.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.78      0.77      0.78       939\n",
      "       JJ       0.66      0.75      0.70      2765\n",
      "       IN       0.97      0.97      0.97      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.97      0.99      0.98       423\n",
      "       TO       0.99      1.00      1.00       905\n",
      "       NN       0.79      0.80      0.79      5358\n",
      "      NNP       0.74      0.77      0.75      5817\n",
      "       CD       0.88      0.93      0.90      2935\n",
      "      VBP       0.84      0.75      0.79       365\n",
      "       EX       0.90      0.95      0.93        40\n",
      "      NNS       0.86      0.81      0.83      2486\n",
      "     PUNC       0.99      0.99      0.99      6029\n",
      "      WDT       0.97      0.89      0.93       155\n",
      "      VBG       0.73      0.73      0.73       699\n",
      "       MD       0.99      0.98      0.98       300\n",
      "      PRP       0.99      0.98      0.98       862\n",
      "      VBZ       0.77      0.84      0.80       509\n",
      "       CC       0.99      0.99      0.99       932\n",
      "      VBD       0.88      0.91      0.90      2224\n",
      "     NNPS       0.34      0.39      0.36       165\n",
      "      WRB       0.97      0.96      0.96        93\n",
      "      VBN       0.80      0.75      0.77       928\n",
      "       VB       0.85      0.82      0.84      1112\n",
      "     PRP$       0.98      1.00      0.99       422\n",
      "       RP       0.69      0.70      0.70       148\n",
      "       WP       0.97      0.98      0.97       127\n",
      "      JJR       0.76      0.67      0.71       105\n",
      "      RBS       0.83      0.56      0.67        18\n",
      "      JJS       0.82      0.74      0.78        78\n",
      "      RBR       0.74      0.58      0.65        53\n",
      "      SYM       0.90      0.81      0.85        86\n",
      "      WP$       1.00      0.67      0.80         9\n",
      "       FW       0.26      0.21      0.23        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.50      0.43      0.46         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.87      0.88      0.87     45527\n",
      "macro avg       0.87      0.88      0.87     45527\n",
      "\n",
      "loss tensor(0.0294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8695111440821269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 11/100 [1:58:16<15:56:27, 644.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.79      0.79      0.79       939\n",
      "       JJ       0.64      0.74      0.69      2765\n",
      "       IN       0.97      0.97      0.97      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.97      1.00      0.98       423\n",
      "       TO       1.00      1.00      1.00       905\n",
      "       NN       0.79      0.79      0.79      5358\n",
      "      NNP       0.74      0.76      0.75      5817\n",
      "       CD       0.89      0.91      0.90      2935\n",
      "      VBP       0.83      0.78      0.80       365\n",
      "       EX       0.86      0.95      0.90        40\n",
      "      NNS       0.82      0.84      0.83      2486\n",
      "     PUNC       0.98      0.99      0.99      6029\n",
      "      WDT       0.96      0.92      0.94       155\n",
      "      VBG       0.80      0.71      0.75       699\n",
      "       MD       0.98      0.98      0.98       300\n",
      "      PRP       0.99      0.98      0.98       862\n",
      "      VBZ       0.77      0.82      0.80       509\n",
      "       CC       0.99      0.98      0.99       932\n",
      "      VBD       0.88      0.92      0.90      2224\n",
      "     NNPS       0.27      0.33      0.30       165\n",
      "      WRB       0.99      0.96      0.97        93\n",
      "      VBN       0.79      0.75      0.77       928\n",
      "       VB       0.85      0.83      0.84      1112\n",
      "     PRP$       0.97      1.00      0.98       422\n",
      "       RP       0.73      0.71      0.72       148\n",
      "       WP       0.98      0.97      0.98       127\n",
      "      JJR       0.69      0.64      0.66       105\n",
      "      RBS       0.90      0.50      0.64        18\n",
      "      JJS       0.77      0.78      0.78        78\n",
      "      RBR       0.67      0.53      0.59        53\n",
      "      SYM       0.79      0.76      0.77        86\n",
      "      WP$       1.00      0.89      0.94         9\n",
      "       FW       0.22      0.25      0.24        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.33      0.14      0.20         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.86      0.88      0.87     45527\n",
      "macro avg       0.87      0.88      0.87     45527\n",
      "\n",
      "loss tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8693783140943112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 12/100 [2:09:06<15:47:55, 646.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.80      0.77      0.78       939\n",
      "       JJ       0.67      0.73      0.70      2765\n",
      "       IN       0.97      0.98      0.97      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.97      1.00      0.98       423\n",
      "       TO       1.00      1.00      1.00       905\n",
      "       NN       0.81      0.78      0.80      5358\n",
      "      NNP       0.73      0.77      0.75      5817\n",
      "       CD       0.88      0.91      0.90      2935\n",
      "      VBP       0.81      0.77      0.79       365\n",
      "       EX       0.90      0.93      0.91        40\n",
      "      NNS       0.81      0.83      0.82      2486\n",
      "     PUNC       0.99      0.99      0.99      6029\n",
      "      WDT       0.95      0.91      0.93       155\n",
      "      VBG       0.69      0.75      0.72       699\n",
      "       MD       0.97      0.97      0.97       300\n",
      "      PRP       0.99      0.98      0.98       862\n",
      "      VBZ       0.77      0.84      0.80       509\n",
      "       CC       1.00      0.99      0.99       932\n",
      "      VBD       0.91      0.89      0.90      2224\n",
      "     NNPS       0.29      0.30      0.30       165\n",
      "      WRB       1.00      0.91      0.96        93\n",
      "      VBN       0.73      0.77      0.75       928\n",
      "       VB       0.82      0.84      0.83      1112\n",
      "     PRP$       0.99      1.00      0.99       422\n",
      "       RP       0.71      0.75      0.73       148\n",
      "       WP       0.98      0.97      0.98       127\n",
      "      JJR       0.71      0.74      0.73       105\n",
      "      RBS       1.00      0.61      0.76        18\n",
      "      JJS       0.81      0.73      0.77        78\n",
      "      RBR       0.79      0.49      0.60        53\n",
      "      SYM       0.92      0.76      0.83        86\n",
      "      WP$       1.00      1.00      1.00         9\n",
      "       FW       0.20      0.17      0.18        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.67      0.57      0.62         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.86      0.88      0.87     45527\n",
      "macro avg       0.87      0.88      0.87     45527\n",
      "\n",
      "loss tensor(0.0319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.874474014973496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 13/100 [2:19:50<15:36:28, 645.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.79      0.80      0.80       939\n",
      "       JJ       0.66      0.74      0.70      2765\n",
      "       IN       0.98      0.97      0.98      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.98      0.99      0.98       423\n",
      "       TO       0.99      1.00      1.00       905\n",
      "       NN       0.81      0.79      0.80      5358\n",
      "      NNP       0.74      0.78      0.76      5817\n",
      "       CD       0.89      0.92      0.90      2935\n",
      "      VBP       0.82      0.78      0.80       365\n",
      "       EX       0.90      0.93      0.91        40\n",
      "      NNS       0.85      0.83      0.84      2486\n",
      "     PUNC       0.99      0.99      0.99      6029\n",
      "      WDT       0.94      0.94      0.94       155\n",
      "      VBG       0.75      0.74      0.74       699\n",
      "       MD       0.98      0.98      0.98       300\n",
      "      PRP       0.98      0.98      0.98       862\n",
      "      VBZ       0.77      0.83      0.80       509\n",
      "       CC       0.99      0.98      0.99       932\n",
      "      VBD       0.92      0.89      0.91      2224\n",
      "     NNPS       0.41      0.32      0.36       165\n",
      "      WRB       0.99      0.96      0.97        93\n",
      "      VBN       0.79      0.75      0.77       928\n",
      "       VB       0.84      0.85      0.85      1112\n",
      "     PRP$       0.97      1.00      0.98       422\n",
      "       RP       0.77      0.67      0.72       148\n",
      "       WP       0.98      0.97      0.97       127\n",
      "      JJR       0.69      0.73      0.71       105\n",
      "      RBS       0.87      0.72      0.79        18\n",
      "      JJS       0.88      0.74      0.81        78\n",
      "      RBR       0.74      0.53      0.62        53\n",
      "      SYM       0.85      0.81      0.83        86\n",
      "      WP$       1.00      1.00      1.00         9\n",
      "       FW       0.27      0.17      0.21        24\n",
      "       UH       0.00      0.00      0.00         5\n",
      "      PDT       0.75      0.43      0.55         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.87      0.88      0.87     45527\n",
      "macro avg       0.87      0.88      0.87     45527\n",
      "\n",
      "loss tensor(0.0358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8718004669736182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 14/100 [2:30:39<15:26:59, 646.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.77      0.80      0.79       939\n",
      "       JJ       0.67      0.75      0.71      2765\n",
      "       IN       0.98      0.97      0.97      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.99      1.00      0.99       423\n",
      "       TO       0.99      1.00      1.00       905\n",
      "       NN       0.79      0.79      0.79      5358\n",
      "      NNP       0.75      0.76      0.75      5817\n",
      "       CD       0.88      0.93      0.91      2935\n",
      "      VBP       0.83      0.76      0.79       365\n",
      "       EX       0.95      0.93      0.94        40\n",
      "      NNS       0.81      0.83      0.82      2486\n",
      "     PUNC       0.99      0.99      0.99      6029\n",
      "      WDT       0.95      0.92      0.94       155\n",
      "      VBG       0.77      0.72      0.75       699\n",
      "       MD       0.98      0.97      0.98       300\n",
      "      PRP       0.98      0.98      0.98       862\n",
      "      VBZ       0.74      0.82      0.78       509\n",
      "       CC       0.99      0.98      0.99       932\n",
      "      VBD       0.89      0.91      0.90      2224\n",
      "     NNPS       0.36      0.29      0.32       165\n",
      "      WRB       0.99      0.95      0.97        93\n",
      "      VBN       0.80      0.76      0.78       928\n",
      "       VB       0.83      0.84      0.84      1112\n",
      "     PRP$       0.99      1.00      0.99       422\n",
      "       RP       0.76      0.64      0.69       148\n",
      "       WP       0.99      0.98      0.99       127\n",
      "      JJR       0.65      0.64      0.64       105\n",
      "      RBS       0.83      0.56      0.67        18\n",
      "      JJS       0.72      0.77      0.75        78\n",
      "      RBR       0.61      0.53      0.57        53\n",
      "      SYM       0.83      0.70      0.76        86\n",
      "      WP$       1.00      0.78      0.88         9\n",
      "       FW       0.16      0.17      0.16        24\n",
      "       UH       0.25      0.20      0.22         5\n",
      "      PDT       0.67      0.57      0.62         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.87      0.88      0.87     45527\n",
      "macro avg       0.87      0.88      0.87     45527\n",
      "\n",
      "loss tensor(0.0322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8716531390305984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 15/100 [2:41:23<15:14:48, 645.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.81      0.81      0.81       939\n",
      "       JJ       0.63      0.75      0.69      2765\n",
      "       IN       0.98      0.97      0.97      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.97      0.99      0.98       423\n",
      "       TO       1.00      1.00      1.00       905\n",
      "       NN       0.81      0.78      0.80      5358\n",
      "      NNP       0.74      0.76      0.75      5817\n",
      "       CD       0.88      0.92      0.90      2935\n",
      "      VBP       0.82      0.77      0.79       365\n",
      "       EX       1.00      0.88      0.93        40\n",
      "      NNS       0.82      0.84      0.83      2486\n",
      "     PUNC       0.99      0.99      0.99      6029\n",
      "      WDT       0.94      0.93      0.93       155\n",
      "      VBG       0.78      0.72      0.75       699\n",
      "       MD       0.99      0.97      0.98       300\n",
      "      PRP       0.99      0.98      0.99       862\n",
      "      VBZ       0.81      0.81      0.81       509\n",
      "       CC       0.99      0.98      0.99       932\n",
      "      VBD       0.91      0.90      0.90      2224\n",
      "     NNPS       0.33      0.32      0.32       165\n",
      "      WRB       1.00      0.96      0.98        93\n",
      "      VBN       0.83      0.76      0.79       928\n",
      "       VB       0.84      0.84      0.84      1112\n",
      "     PRP$       0.98      1.00      0.99       422\n",
      "       RP       0.73      0.64      0.68       148\n",
      "       WP       0.99      0.98      0.98       127\n",
      "      JJR       0.73      0.69      0.71       105\n",
      "      RBS       0.89      0.44      0.59        18\n",
      "      JJS       0.72      0.77      0.75        78\n",
      "      RBR       0.72      0.53      0.61        53\n",
      "      SYM       0.81      0.73      0.77        86\n",
      "      WP$       1.00      0.78      0.88         9\n",
      "       FW       0.12      0.25      0.16        24\n",
      "       UH       0.33      0.20      0.25         5\n",
      "      PDT       0.67      0.57      0.62         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.87      0.88      0.87     45527\n",
      "macro avg       0.87      0.88      0.87     45527\n",
      "\n",
      "loss tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0.8714765063993967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 16/100 [2:52:07<15:03:14, 645.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "       RB       0.76      0.79      0.78       939\n",
      "       JJ       0.67      0.73      0.70      2765\n",
      "       IN       0.97      0.97      0.97      4892\n",
      "       DT       0.99      0.99      0.99      3511\n",
      "      POS       0.98      0.99      0.98       423\n",
      "       TO       0.99      1.00      1.00       905\n",
      "       NN       0.79      0.80      0.80      5358\n",
      "      NNP       0.75      0.76      0.75      5817\n",
      "       CD       0.88      0.92      0.90      2935\n",
      "      VBP       0.83      0.74      0.78       365\n",
      "       EX       0.93      0.95      0.94        40\n",
      "      NNS       0.81      0.84      0.82      2486\n",
      "     PUNC       0.98      0.99      0.99      6029\n",
      "      WDT       0.93      0.94      0.93       155\n",
      "      VBG       0.72      0.74      0.73       699\n",
      "       MD       0.96      0.98      0.97       300\n",
      "      PRP       0.99      0.98      0.98       862\n",
      "      VBZ       0.84      0.81      0.83       509\n",
      "       CC       1.00      0.98      0.99       932\n",
      "      VBD       0.91      0.90      0.90      2224\n",
      "     NNPS       0.48      0.30      0.37       165\n",
      "      WRB       1.00      0.94      0.97        93\n",
      "      VBN       0.80      0.76      0.78       928\n",
      "       VB       0.81      0.83      0.82      1112\n",
      "     PRP$       0.99      1.00      0.99       422\n",
      "       RP       0.74      0.66      0.70       148\n",
      "       WP       0.98      0.98      0.98       127\n",
      "      JJR       0.79      0.69      0.73       105\n",
      "      RBS       0.86      0.67      0.75        18\n",
      "      JJS       0.83      0.73      0.78        78\n",
      "      RBR       0.73      0.51      0.60        53\n",
      "      SYM       0.84      0.73      0.78        86\n",
      "      WP$       1.00      0.78      0.88         9\n",
      "       FW       0.29      0.21      0.24        24\n",
      "       UH       0.50      0.20      0.29         5\n",
      "      PDT       0.75      0.43      0.55         7\n",
      "       LS       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.87      0.88      0.87     45527\n",
      "macro avg       0.87      0.88      0.87     45527\n",
      "\n",
      "loss tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8a30e1963a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.89\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/home/shubham/Project/pos_tag/code/distilation_experiments/lstm_models/model_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-7e68739be3fe>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpred_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdbert_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mix_to_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mout_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-75aa12429d1e>\u001b[0m in \u001b[0;36minput_to_features\u001b[0;34m(words, labels, pad_token_label_id)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print(word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print(word_tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0madded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madded_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    695\u001b[0m             return list(itertools.chain.from_iterable((self._tokenize(token, **kwargs) if token not \\\n\u001b[1;32m    696\u001b[0m                     \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_special_tokens\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                     else [token] for token in tokenized_text)))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0madded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    695\u001b[0m             return list(itertools.chain.from_iterable((self._tokenize(token, **kwargs) if token not \\\n\u001b[1;32m    696\u001b[0m                     \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_special_tokens\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                     else [token] for token in tokenized_text)))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0madded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msub_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                     \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \"\"\"\n\u001b[1;32m    308\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnever_split\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# models. This is also applied to the English models now, but it doesn't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;34m\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xfffd\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, tokenizer.vocab_size, len(label_map))\n",
    "#model.load_state_dict(torch.load(\"/home/shubham/Project/pos_tag/code/distilation_experiments/lstm_models/model_0.8884873404025104.pt\"))\n",
    "model.to(device)\n",
    "#loss_function = nn.NLLLoss()\n",
    "#loss_function = CrossEntropyLoss()\n",
    "\n",
    "def custom_loss(lstm_prob, bert_prob, real_label):\n",
    "    a = 0.8\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    #criterion_ce = nn.NLLLoss()\n",
    "    criterion_ce = CrossEntropyLoss()\n",
    "    return a*criterion_ce(lstm_prob, real_label) + (1-a)*criterion_mse(lstm_prob, bert_prob)\n",
    "\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\"\"\"\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "\"\"\"\n",
    "\n",
    "for epoch in tqdm(range(100)): \n",
    "    model.train()\n",
    "    for sentence, tags in (training_data[:]):\n",
    "        model.zero_grad()\n",
    "        dbert_input_ids,sentence_in,targets = input_to_features(sentence,tags)\n",
    "        #print(targets.shape)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            dbert_logits = teacher_model(dbert_input_ids.unsqueeze(0))\n",
    "        \n",
    "        tag_scores,logits = model(sentence_in)\n",
    "        #print('logits score:', logits)\n",
    "        #print('tag score:', tag_scores)\n",
    "        #print('dbert_logits',dbert_logits[0][1:-1])\n",
    "        #loss = loss_function(tag_scores, targets)\n",
    "        #print(tag_scores.shape)\n",
    "        loss = custom_loss(logits, dbert_logits[0][1:-1],targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('loss',loss)\n",
    "    f1 = evaluation(model)\n",
    "    if f1>0.89:\n",
    "        torch.save(model.state_dict(),'/home/shubham/Project/pos_tag/code/distilation_experiments/lstm_models/model_'+str(f1)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    #inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    inputs = torch.tensor(tokenizer.encode(training_data[0][0],add_special_tokens=False), dtype=torch.long)\n",
    "    print('input ',inputs)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    for i in tag_scores:\n",
    "        print(int(np.argmax(i)))\n",
    "    print(tag_scores.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"out_list = []\n",
    "for i in tag_scores:\n",
    "    out_list.append(ix_to_tag[int(np.argmax(i))])\n",
    "print((training_data[0][0],training_data[0][1]))\n",
    "print(out_list)\n",
    "sc = f1_score(out_list,training_data[0][1])\n",
    "print(sc)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
