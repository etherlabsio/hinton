{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrank import GraphRank, GraphUtils, TextPreprocess\n",
    "import networkx as nx\n",
    "import community\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from ipynb.fs.defs.run_sentence_encoder import get_embedding, EMBED, tf\n",
    "embed = EMBED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = GraphRank()\n",
    "tp = TextPreprocess()\n",
    "utils = GraphUtils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_list(meeting_graph):\n",
    "    segment_list = []\n",
    "    for node, attr in meeting_graph.nodes(data=True):\n",
    "        if attr.get(\"label\") == \"segmentId\":\n",
    "            segment_list.append(attr.get(\"text\"))\n",
    "    return segment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_graph(segment_list):\n",
    "    for i, text in enumerate(segment_list):\n",
    "        original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(text, filter_by_pos=True, stop_words=False)\n",
    "        graph = gr.build_word_graph(input_pos_text=pos_tuple, window=4)\n",
    "        sub_keyphrases = gr.get_keyphrases(graph_obj=graph, input_pos_text=pos_tuple) \n",
    "        if i == 15:\n",
    "            break\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_by_value(dict_var, order='desc', key=None):\n",
    "    \"\"\"\n",
    "    A utility function to sort lists by their value.\n",
    "    Args:\n",
    "        item_list:\n",
    "        order:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    item_list = dict_var.items()\n",
    "    if order == 'desc':\n",
    "        if key is not None:\n",
    "            sorted_list = sorted(item_list, key=lambda x: (x[1][key], x[0]), reverse=True)\n",
    "        else:\n",
    "            sorted_list = sorted(item_list, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    else:\n",
    "        if key is not None:\n",
    "            sorted_list = sorted(item_list, key=lambda x: (x[1][key], x[0]), reverse=False)\n",
    "        else:\n",
    "            sorted_list = sorted(item_list, key=lambda x: (x[1], x[0]), reverse=False)\n",
    "\n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph - Embeddings functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_weight(session, input_placeholder, word1 , word2, embedding_encoder):\n",
    "    word_list = [word1, word2]\n",
    "    cosine_dist = get_embedding(session, input_placeholder, word_list, embedding_encoder)\n",
    "    try:\n",
    "        return 1-cosine_dist\n",
    "    except KeyError:\n",
    "        print(\"word not found: {}--{}\".format(word1, word2))\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_embeddings(word_graph):\n",
    "    in_vocab_count = 0\n",
    "    out_vocab_count = 0\n",
    "\n",
    "    checkpoint_dir = \"checkpoints/\"\n",
    "    file_store_name = checkpoint_dir + \"word_graph_embedding_chkp\" + \"20\" + \".pickle\"\n",
    "\n",
    "    input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "    word_encodings = embed(input_placeholder)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "\n",
    "        with tqdm(total=word_graph.number_of_edges()) as pbar:\n",
    "            for i, (node1, node2, attr) in enumerate(word_graph.edges.data()):\n",
    "                start = timer()\n",
    "                if attr.get(\"edge_emb_wt\") is None:\n",
    "                    emb_edge_weight = get_edge_weight(session, input_placeholder, node1.lower(), node2.lower(), word_encodings)\n",
    "                    if emb_edge_weight == 0:\n",
    "                        out_vocab_count += 1\n",
    "                    else:\n",
    "                        in_vocab_count += 1\n",
    "                    word_graph.add_edge(node1, node2, edge_emb_wt=emb_edge_weight)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                end = timer()\n",
    "                pbar.set_description('processed: {}; time taken: {}'.format((1 + i), (end - start)))\n",
    "                pbar.update(1)\n",
    "                # print(\"Words seen: {}/{}; Time taken: {}\".format(i, len(word_graph.edges.data()), end-start))\n",
    "                if i % 20 == 0 and i != 0:\n",
    "                    # print(\"Computed {} edges\". format(i))\n",
    "                    f = os.path.exists(file_store_name)\n",
    "                    if f:\n",
    "                        os.remove(file_store_name)\n",
    "                    file_store_name = checkpoint_dir + \"word_graph_embedding_chkp\" + str(i) + \".pickle\"\n",
    "                    nx.write_gpickle(word_graph, file_store_name)\n",
    "            print(\"created checkpoint: {}...\".format(file_store_name))\n",
    "\n",
    "#         print(\"###########################\")\n",
    "#         print(\"Total in_vocab_words = {}\".format(in_vocab_count))\n",
    "#         print(\"Total out_of_vocab_words = {}\".format(out_vocab_count))\n",
    "#         print(\"Percentage of out_of_vocab = {}\".format((out_vocab_count/in_vocab_count)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to store computed values in graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build graph and compute edge embeddings\n",
    "\n",
    "The parameters are as follows: \n",
    "1. `meeting_data`: path to pickled graph object of the meeting data\n",
    "2. `segment_list`: list of segments (Using this, a graph for all these segments will be built and then ranked)\n",
    "3. `test_segment`: Single segment to run quick tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# `graph_id` and `meeting_data` are parameters here\n",
    "data_dir = \"data/\"\n",
    "graph_id = \"01DB8DEW0YFYK0ZBP2Q3XR2YT1_5f89df0e-3631-4c64-a7ff-3bf0264c830f\"\n",
    "meeting_data = data_dir + graph_id\n",
    "\n",
    "segment_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_graph = nx.read_gpickle(meeting_data)\n",
    "\n",
    "if len(segment_list) == 0:\n",
    "    segment_list = get_segment_list(meeting_graph)\n",
    "    word_graph = build_word_graph(segment_list)\n",
    "else:\n",
    "    word_graph = build_word_graph(segment_list)\n",
    "\n",
    "# Compute edge embeddings\n",
    "compute_edge_embeddings(word_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_pagerank = nx.pagerank(word_graph, weight=\"edge_emb_wt\")\n",
    "unbiased_pagerank = nx.pagerank(word_graph)\n",
    "\n",
    "sorted_biased_rank = sort_dict_by_value(biased_pagerank)\n",
    "sorted_unbiased_rank = sort_dict_by_value(unbiased_pagerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tup in enumerate(sorted_biased_rank):\n",
    "    word = tup[0]\n",
    "    biased_rank = i\n",
    "    biased_pagerank_score = tup[1]\n",
    "    word_graph.add_node(word, weighted_pagerank_val=biased_pagerank_score, biased_pagerank=biased_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tup in enumerate(sorted_unbiased_rank):\n",
    "    word = tup[0]\n",
    "    unbiased_rank = i\n",
    "    unbiased_pagerank_score = tup[1]\n",
    "    word_graph.add_node(word, original_pagerank_val=unbiased_pagerank_score, original_rank=unbiased_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_wise_ranking(word_graph, segment_list):\n",
    "    for i, sub_text in enumerate(segment_list):\n",
    "        original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(sub_text, filter_by_pos=True, stop_words=False)\n",
    "        sub_keyphrases = gr.get_keyphrases(graph_obj=word_graph, input_pos_text=pos_tuple, post_process=True)\n",
    "        wt_sub_keyphrase = gr.get_keyphrases(word_graph, input_pos_text=pos_tuple, post_process=True, weight=\"edge_emb_wt\")\n",
    "\n",
    "        keyphrase_rank_list = []\n",
    "        for i, phrase_tup in enumerate(wt_sub_keyphrase):\n",
    "            wt_rank = i\n",
    "            wt_word = phrase_tup[0]\n",
    "            for j, un_phrase_tup in enumerate(sub_keyphrases):\n",
    "                word = un_phrase_tup[0]\n",
    "                if word == wt_word:\n",
    "                    orig_rank = j\n",
    "                    tup = (word, orig_rank, wt_rank, sub_text)\n",
    "                    keyphrase_rank_list.append(tup)\n",
    "        \n",
    "        # print(sub_text)\n",
    "        yield keyphrase_rank_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the difference in keyphrase rank based on weighted PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Word \\t\\t ===> Index | Original rank | Weighted rank | Difference\")\n",
    "# for seg in list(get_segment_wise_ranking(word_graph, segment_list[:])):\n",
    "#     print(\"========New Segment=========\")\n",
    "#     print(seg[0][-1])\n",
    "#     for i, (word, orig_rank, wt_rank, text) in enumerate(seg):\n",
    "#         diff = orig_rank - wt_rank\n",
    "#         print(\"{}\".format(word))\n",
    "#         print(\"{} | {} | {} | {}\".format(i, orig_rank, wt_rank, diff))\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for segment relevance scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_word_similarity(session, input_placeholder, sentence, word, embedding_encoder):\n",
    "    word_list = [sentence, word]\n",
    "    cosine_dist = get_embedding(session, input_placeholder, word_list, embedding_encoder)\n",
    "    try:\n",
    "        return 1-cosine_dist\n",
    "    except KeyError:\n",
    "        print(\"word not found: {}--{}\".format(sentence, word))\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_kw = []\n",
    "for i in list(get_segment_wise_ranking(word_graph, segment_list)):\n",
    "    for word, orig_rank, new_rank, text in i:\n",
    "        segment_kw.append((word, orig_rank, new_rank, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "word_encodings = embed(input_placeholder)\n",
    "kw_segment_rank = {}\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    for i in segment_kw:\n",
    "        word = i[0]\n",
    "        orig_rank = i[1]\n",
    "        new_rank = i[2]\n",
    "        segment = i[3]\n",
    "        seg_rank = get_segment_word_similarity(session, input_placeholder, segment, word, word_encodings)\n",
    "        kw_segment_rank[word] = seg_rank\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form a dictionary with all the scores and keyphrases for a segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add unweighted and weighted pagerank scores/rank for each phrase in the example segment\n",
    "\n",
    "`segment_rank_output[\"result\"]` contains list of ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_rank_output = {}\n",
    "segment_rank_output[\"result\"] = {}\n",
    "for i in segment_kw:\n",
    "    word = i[0]\n",
    "    orig_rank = i[1]\n",
    "    new_rank = i[2]\n",
    "    text = i[3]\n",
    "    tup = (orig_rank, new_rank)\n",
    "    segment_rank_output[\"text\"] = text\n",
    "    segment_rank_output[\"result\"][word] = list(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add local relevance rank to results dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_kw_segment_rank = sort_dict_by_value(kw_segment_rank)\n",
    "\n",
    "for i, seg_rank in enumerate(sorted_kw_segment_rank):\n",
    "    word = str(seg_rank[0])\n",
    "    orig_rank_list = segment_rank_output[\"result\"][word]\n",
    "    new_rank_list = list((orig_rank_list[0], orig_rank_list[1], i))\n",
    "    segment_rank_output[\"result\"][word] = new_rank_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The order of keys in ranked list will be as follows:\n",
    "\n",
    "`unweighted_pagerank, weighted_pagerank, local_relevance_rank, pagerank_boosted_rank`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_rank_output[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ranking when boosted with pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will be of the form - `word, cosine similiraity with segment`\n",
    "sorted_kw_segment_rank[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_rank = {}\n",
    "for i in sorted_kw_segment_rank:\n",
    "    pagerank_score = 0\n",
    "    phrase = i[0]\n",
    "    seg_score = i[1]\n",
    "    for sing_word in phrase.split():\n",
    "        try:\n",
    "            pagerank_score += word_graph.node[sing_word][\"weighted_pagerank_val\"]\n",
    "        except KeyError:\n",
    "            singular_word = sing_word[:-1]\n",
    "            try:\n",
    "                pagerank_score += word_graph.node[singular_word][\"weighted_pagerank_val\"]\n",
    "            except:\n",
    "                pagerank_score = 0.0001\n",
    "    # print(pagerank_score)\n",
    "    boosted_rank[phrase] = (pagerank_score + seg_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View pagerank-boosted ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_boosted_rank = sort_dict_by_value(boosted_rank)\n",
    "sorted_boosted_rank[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update result dictionary with new ranks**\n",
    "\n",
    "#### The order of keys in ranked list will be as follows:\n",
    "\n",
    "`unweighted_pagerank, weighted_pagerank, local_relevance_rank, pagerank_boosted_rank`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seg_rank in enumerate(sorted_boosted_rank):\n",
    "    word = str(seg_rank[0])\n",
    "    orig_rank_list = segment_rank_output[\"result\"][word]\n",
    "    new_rank_list = list((orig_rank_list[0], orig_rank_list[1], orig_rank_list[2], i))\n",
    "    segment_rank_output[\"result\"][word] = new_rank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segment_rank_output[\"text\"])\n",
    "sort_dict_by_value(segment_rank_output[\"result\"], order=\"asc\", key=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
