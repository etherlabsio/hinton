{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODULE_URL = \"https://tfhub.dev/google/universal-sentence-encoder/2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED = hub.Module(MODULE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(session, input_placeholder, word, embedding_encoder):\n",
    "    message_embeddings = session.run(embedding_encoder, feed_dict={input_placeholder: word})\n",
    "    dist = cosine(np.array(message_embeddings[0]), np.array(message_embeddings[1]))\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vector(session, word_placeholder, word, embedding_encoder):\n",
    "    message_embeddings = session.run(embedding_encoder, feed_dict={word_placeholder: word})\n",
    "    # dist = cosine(np.array(message_embeddings[0]), np.array(message_embeddings[1]))\n",
    "\n",
    "    return message_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute a representation for each message, showing various lengths supported.\n",
    "# word = \"knowledge graph\"\n",
    "# word2 = \"ontology\"\n",
    "# word3 = \"searchengine\"\n",
    "# word4 = \"donald trump\"\n",
    "# word5 = \"test staging today\"\n",
    "# sentence = \"Entities and relations need to be defined well to build good graph representations.\"\n",
    "# sentence2 = \"General knowledge is a subject that I learnt in school and I think it is cool.\"\n",
    "# paragraph = (\n",
    "#     \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "#     \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "#     \"the more 'diluted' the embedding will be.\")\n",
    "messages = [\"test\", \"netflix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.3972410730057163\n",
      "Message: knowledge graph\n",
      "Embedding size: 512\n",
      "Embedding: [0.007142378017306328, 0.012228659354150295, -0.0032539102248847485, ...]\n",
      "Cosine similarity: 1.000000036336652\n",
      "\n",
      "Message: Entities and relations need to be defined well to build good graph representations.\n",
      "Embedding size: 512\n",
      "Embedding: [0.04317520558834076, -0.03976033255457878, -0.03193525969982147, ...]\n",
      "Cosine similarity: 0.43334046563711204\n",
      "\n",
      "Message: General knowledge is a subject that I learnt in school and I think it is cool.\n",
      "Embedding size: 512\n",
      "Embedding: [-0.03692379966378212, 0.037080660462379456, 0.03148544952273369, ...]\n",
      "Cosine similarity: 0.4512204099164938\n",
      "\n",
      "Message: Universal Sentence Encoder embeddings also support short paragraphs. There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.\n",
      "Embedding size: 512\n",
      "Embedding: [0.018790962174534798, 0.045365139842033386, -0.020010892301797867, ...]\n",
      "Cosine similarity: 0.21234553781395893\n",
      "\n",
      "Message: ontology\n",
      "Embedding size: 512\n",
      "Embedding: [-0.01575186289846897, 0.010849528014659882, -0.031328387558460236, ...]\n",
      "Cosine similarity: 0.531373370250308\n",
      "\n",
      "Message: searchengine\n",
      "Embedding size: 512\n",
      "Embedding: [0.053117889910936356, 0.05548816919326782, -0.05840738117694855, ...]\n",
      "Cosine similarity: 0.2671173952958965\n",
      "\n",
      "Message: donald trump\n",
      "Embedding size: 512\n",
      "Embedding: [0.016245897859334946, 0.03911378234624863, -0.0373338907957077, ...]\n",
      "Cosine similarity: 0.08428626108960524\n",
      "\n",
      "Message: test staging today\n",
      "Embedding size: 512\n",
      "Embedding: [-0.06633664667606354, -0.06972751766443253, -0.012996497564017773, ...]\n",
      "Cosine similarity: 0.2943828558980224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "word_encodings = embed(input_placeholder)\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    start = timer()\n",
    "    message_embeddings = get_embedding_vector(session, input_placeholder, messages, word_encodings)\n",
    "    end = timer()\n",
    "    print(\"Time taken: {}\".format(end-start))\n",
    "    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "        print(\"Message: {}\".format(messages[i]))\n",
    "        print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "        message_embedding_snippet = \", \".join(\n",
    "            (str(x) for x in message_embedding[:3]))\n",
    "        print(\"Embedding: [{}, ...]\".format(message_embedding_snippet))\n",
    "        dist = cosine(np.array(message_embeddings[0]), message_embedding)\n",
    "        print(\"Cosine similarity: {}\\n\".format(1-dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
