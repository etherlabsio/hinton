{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_value(item_list, order='desc'):\n",
    "    \"\"\"\n",
    "    A utility function to sort lists by their value.\n",
    "    Args:\n",
    "        item_list:\n",
    "        order:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if order == 'desc':\n",
    "        sorted_list = sorted(item_list, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    else:\n",
    "        sorted_list = sorted(item_list, key=lambda x: (x[1], x[0]), reverse=False)\n",
    "\n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Custom graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ray/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from graphrank.graphrank import GraphRank\n",
    "from graphrank.utils import GraphUtils, TextPreprocess\n",
    "from graphrank import dgraph as dg\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = GraphRank()\n",
    "tp = TextPreprocess()\n",
    "utils = GraphUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"Initially my thought was that you ordered have an issue with in we see the memory leak Indians and all the period of time it kind of accumulates and crashes. So but this is not the the memory leak issue. It's a genuine kind of segmentation fault happened when the websocket connection got disclosed with the client, but somehow was not able to get the code for that. Probably let me negotiate more on that and see what could like find any clues or basically if you can't reproduce that issue.That's one thing called. Yeah, probably like if you're going to launch like just wanted to understand from the prior to from a perspective. Like if you want to launch with the support for Janus like shall we do like do you need to like probably we need to do some through testing and probably built on issues and probably kind of get money on things done to get the get the thing out or like won't support only Zoom as and no no will be primary.\"\n",
    "new_text = \"So last week whatever 16 years with respect to the playlist and DRM key, right? \" \\\n",
    "\"So I was able to test on Safari and chrome both wearing it was able to forward the cookies. \" \\\n",
    "\"I was just like trying to trace out the cookies whether it's cool being sent in the DRM ta PA all those things. \" \\\n",
    "\"So one thing is I had tested it, but I wanted the Deep also to test from IOS app also whether we can pass the cookies. \" \\\n",
    "\"So once that is done, it is like tested it but I just want him to also confirm that part that it can send a cookies from it was have also know but what I am right now stuck is the Eco meat to a double AC p-- a Gateway. \" \\\n",
    "\"It's not able to proxying it actually. \"\\\n",
    "\"So as you spend like I was trying to do with the goatee also, there also is not able to do it the same problem is that this something which is going.\" \\\n",
    "\"Hang on, okay.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(new_text, filter_by_pos=True, pos_filter=['NOUN', 'PROPN', 'ADJ', 'FW'], stop_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Initially', 'ADV'),\n",
       "  ('my', 'ADJ'),\n",
       "  ('thought', 'NOUN'),\n",
       "  ('was', 'VERB'),\n",
       "  ('that', 'ADP'),\n",
       "  ('you', 'PRON'),\n",
       "  ('ordered', 'VERB'),\n",
       "  ('have', 'VERB'),\n",
       "  ('an', 'DET'),\n",
       "  ('issue', 'NOUN'),\n",
       "  ('with', 'ADP'),\n",
       "  ('in', 'ADP'),\n",
       "  ('we', 'PRON'),\n",
       "  ('see', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('memory', 'NOUN'),\n",
       "  ('leak', 'NOUN'),\n",
       "  ('Indians', 'PROPN'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('all', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('period', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('time', 'NOUN'),\n",
       "  ('it', 'PRON'),\n",
       "  ('kind', 'ADV'),\n",
       "  ('of', 'ADP'),\n",
       "  ('accumulates', 'NOUN'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('crashes', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('So', 'ADV'),\n",
       "  ('but', 'CCONJ'),\n",
       "  ('this', 'DET'),\n",
       "  ('is', 'VERB'),\n",
       "  ('not', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('memory', 'NOUN'),\n",
       "  ('leak', 'NOUN'),\n",
       "  ('issue', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Its', 'ADJ'),\n",
       "  ('a', 'DET'),\n",
       "  ('genuine', 'ADJ'),\n",
       "  ('kind', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('segmentation', 'NOUN'),\n",
       "  ('fault', 'NOUN'),\n",
       "  ('happened', 'VERB'),\n",
       "  ('when', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('websocket', 'NOUN'),\n",
       "  ('connection', 'NOUN'),\n",
       "  ('got', 'VERB'),\n",
       "  ('disclosed', 'VERB'),\n",
       "  ('with', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('client', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('but', 'CCONJ'),\n",
       "  ('somehow', 'ADV'),\n",
       "  ('was', 'VERB'),\n",
       "  ('not', 'ADV'),\n",
       "  ('able', 'ADJ'),\n",
       "  ('to', 'PART'),\n",
       "  ('get', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('code', 'NOUN'),\n",
       "  ('for', 'ADP'),\n",
       "  ('that', 'DET'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Probably', 'ADV'),\n",
       "  ('let', 'VERB'),\n",
       "  ('me', 'PRON'),\n",
       "  ('negotiate', 'VERB'),\n",
       "  ('more', 'ADV'),\n",
       "  ('on', 'ADP'),\n",
       "  ('that', 'DET'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('see', 'VERB'),\n",
       "  ('what', 'NOUN'),\n",
       "  ('could', 'VERB'),\n",
       "  ('like', 'VERB'),\n",
       "  ('find', 'VERB'),\n",
       "  ('any', 'DET'),\n",
       "  ('clues', 'NOUN'),\n",
       "  ('or', 'CCONJ'),\n",
       "  ('basically', 'ADV'),\n",
       "  ('if', 'ADP'),\n",
       "  ('you', 'PRON'),\n",
       "  ('can', 'VERB'),\n",
       "  ('not', 'ADV'),\n",
       "  ('reproduce', 'VERB'),\n",
       "  ('that', 'DET'),\n",
       "  ('issue', 'NOUN'),\n",
       "  ('.', 'PUNCT'),\n",
       "  ('Thats', 'VERB'),\n",
       "  ('one', 'NUM'),\n",
       "  ('thing', 'NOUN'),\n",
       "  ('called', 'VERB'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Yeah', 'INTJ'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('probably', 'ADV'),\n",
       "  ('like', 'INTJ'),\n",
       "  ('if', 'ADP'),\n",
       "  ('you', 'PRON'),\n",
       "  ('are', 'VERB'),\n",
       "  ('going', 'VERB'),\n",
       "  ('to', 'PART'),\n",
       "  ('launch', 'VERB'),\n",
       "  ('like', 'ADP'),\n",
       "  ('just', 'ADV'),\n",
       "  ('wanted', 'VERB'),\n",
       "  ('to', 'PART'),\n",
       "  ('understand', 'VERB'),\n",
       "  ('from', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('prior', 'ADJ'),\n",
       "  ('to', 'ADP'),\n",
       "  ('from', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('perspective', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Like', 'ADP'),\n",
       "  ('if', 'ADP'),\n",
       "  ('you', 'PRON'),\n",
       "  ('want', 'VERB'),\n",
       "  ('to', 'PART'),\n",
       "  ('launch', 'VERB'),\n",
       "  ('with', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('support', 'NOUN'),\n",
       "  ('for', 'ADP'),\n",
       "  ('Janus', 'PROPN'),\n",
       "  ('like', 'INTJ'),\n",
       "  ('shall', 'VERB'),\n",
       "  ('we', 'PRON'),\n",
       "  ('do', 'VERB'),\n",
       "  ('like', 'INTJ'),\n",
       "  ('do', 'VERB'),\n",
       "  ('you', 'PRON'),\n",
       "  ('need', 'VERB'),\n",
       "  ('to', 'PART'),\n",
       "  ('like', 'VERB'),\n",
       "  ('probably', 'ADV'),\n",
       "  ('we', 'PRON'),\n",
       "  ('need', 'VERB'),\n",
       "  ('to', 'PART'),\n",
       "  ('do', 'VERB'),\n",
       "  ('some', 'DET'),\n",
       "  ('through', 'ADP'),\n",
       "  ('testing', 'NOUN'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('probably', 'ADV'),\n",
       "  ('built', 'VERB'),\n",
       "  ('on', 'ADP'),\n",
       "  ('issues', 'NOUN'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('probably', 'ADV'),\n",
       "  ('kind', 'ADV'),\n",
       "  ('of', 'ADV'),\n",
       "  ('get', 'VERB'),\n",
       "  ('money', 'NOUN'),\n",
       "  ('on', 'ADP'),\n",
       "  ('things', 'NOUN'),\n",
       "  ('done', 'VERB'),\n",
       "  ('to', 'PART'),\n",
       "  ('get', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('get', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('thing', 'NOUN'),\n",
       "  ('out', 'ADV'),\n",
       "  ('or', 'CCONJ'),\n",
       "  ('like', 'INTJ'),\n",
       "  ('will', 'VERB'),\n",
       "  ('not', 'ADV'),\n",
       "  ('support', 'VERB'),\n",
       "  ('only', 'ADV'),\n",
       "  ('Zoom', 'PROPN'),\n",
       "  ('as', 'ADP'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('no', 'INTJ'),\n",
       "  ('no', 'INTJ'),\n",
       "  ('will', 'VERB'),\n",
       "  ('be', 'VERB'),\n",
       "  ('primary', 'ADJ'),\n",
       "  ('.', 'PUNCT')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_graph = gr.build_word_graph(filtered_pos_tuple, original_tokens=original_tokens, window=4, reset_graph_context=True, preserve_common_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Initially',\n",
       "  'my',\n",
       "  'thought',\n",
       "  'was',\n",
       "  'that',\n",
       "  'you',\n",
       "  'ordered',\n",
       "  'have',\n",
       "  'an',\n",
       "  'issue',\n",
       "  'with',\n",
       "  'in',\n",
       "  'we',\n",
       "  'see',\n",
       "  'the',\n",
       "  'memory',\n",
       "  'leak',\n",
       "  'Indians',\n",
       "  'and',\n",
       "  'all',\n",
       "  'the',\n",
       "  'period',\n",
       "  'of',\n",
       "  'time',\n",
       "  'it',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'accumulates',\n",
       "  'and',\n",
       "  'crashes',\n",
       "  '.'],\n",
       " ['So',\n",
       "  'but',\n",
       "  'this',\n",
       "  'is',\n",
       "  'not',\n",
       "  'the',\n",
       "  'the',\n",
       "  'memory',\n",
       "  'leak',\n",
       "  'issue',\n",
       "  '.'],\n",
       " ['Its',\n",
       "  'a',\n",
       "  'genuine',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'segmentation',\n",
       "  'fault',\n",
       "  'happened',\n",
       "  'when',\n",
       "  'the',\n",
       "  'websocket',\n",
       "  'connection',\n",
       "  'got',\n",
       "  'disclosed',\n",
       "  'with',\n",
       "  'the',\n",
       "  'client',\n",
       "  ',',\n",
       "  'but',\n",
       "  'somehow',\n",
       "  'was',\n",
       "  'not',\n",
       "  'able',\n",
       "  'to',\n",
       "  'get',\n",
       "  'the',\n",
       "  'code',\n",
       "  'for',\n",
       "  'that',\n",
       "  '.'],\n",
       " ['Probably',\n",
       "  'let',\n",
       "  'me',\n",
       "  'negotiate',\n",
       "  'more',\n",
       "  'on',\n",
       "  'that',\n",
       "  'and',\n",
       "  'see',\n",
       "  'what',\n",
       "  'could',\n",
       "  'like',\n",
       "  'find',\n",
       "  'any',\n",
       "  'clues',\n",
       "  'or',\n",
       "  'basically',\n",
       "  'if',\n",
       "  'you',\n",
       "  'can',\n",
       "  'not',\n",
       "  'reproduce',\n",
       "  'that',\n",
       "  'issue.Thats',\n",
       "  'one',\n",
       "  'thing',\n",
       "  'called',\n",
       "  '.'],\n",
       " ['Yeah',\n",
       "  ',',\n",
       "  'probably',\n",
       "  'like',\n",
       "  'if',\n",
       "  'you',\n",
       "  'are',\n",
       "  'going',\n",
       "  'to',\n",
       "  'launch',\n",
       "  'like',\n",
       "  'just',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  'understand',\n",
       "  'from',\n",
       "  'the',\n",
       "  'prior',\n",
       "  'to',\n",
       "  'from',\n",
       "  'a',\n",
       "  'perspective',\n",
       "  '.'],\n",
       " ['Like',\n",
       "  'if',\n",
       "  'you',\n",
       "  'want',\n",
       "  'to',\n",
       "  'launch',\n",
       "  'with',\n",
       "  'the',\n",
       "  'support',\n",
       "  'for',\n",
       "  'Janus',\n",
       "  'like',\n",
       "  'shall',\n",
       "  'we',\n",
       "  'do',\n",
       "  'like',\n",
       "  'do',\n",
       "  'you',\n",
       "  'need',\n",
       "  'to',\n",
       "  'like',\n",
       "  'probably',\n",
       "  'we',\n",
       "  'need',\n",
       "  'to',\n",
       "  'do',\n",
       "  'some',\n",
       "  'through',\n",
       "  'testing',\n",
       "  'and',\n",
       "  'probably',\n",
       "  'built',\n",
       "  'on',\n",
       "  'issues',\n",
       "  'and',\n",
       "  'probably',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'get',\n",
       "  'money',\n",
       "  'on',\n",
       "  'things',\n",
       "  'done',\n",
       "  'to',\n",
       "  'get',\n",
       "  'the',\n",
       "  'get',\n",
       "  'the',\n",
       "  'thing',\n",
       "  'out',\n",
       "  'or',\n",
       "  'like',\n",
       "  'will',\n",
       "  'not',\n",
       "  'support',\n",
       "  'only',\n",
       "  'Zoom',\n",
       "  'as',\n",
       "  'and',\n",
       "  'no',\n",
       "  'no',\n",
       "  'will',\n",
       "  'be',\n",
       "  'primary',\n",
       "  '.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_weights, top_words = gr.node_weighting(word_graph, normalize_nodes='degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'keyphrases'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_graph.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thought': 0.007351317736216116,\n",
       " 'issue': 0.006514905907044505,\n",
       " 'memory': 0.006461907715020191,\n",
       " 'leak': 0.006395049647394857,\n",
       " 'indians': 0.006689927646751073,\n",
       " 'period': 0.006696020106917546,\n",
       " 'time': 0.00670373370352077,\n",
       " 'accumulates': 0.006677866295956615,\n",
       " 'crashes': 0.006515492138870556,\n",
       " 'genuine': 0.006958471693640845,\n",
       " 'kind': 0.007090730060803407,\n",
       " 'segmentation': 0.007214104701968952,\n",
       " 'fault': 0.007114195274491018,\n",
       " 'websocket': 0.0073253276387255884,\n",
       " 'connection': 0.007416915861054011,\n",
       " 'client': 0.007677323214128335,\n",
       " 'code': 0.007682135165436623,\n",
       " 'clues': 0.007994929206707583,\n",
       " 'prior': 0.007427691183993973,\n",
       " 'perspective': 0.007409816884576187,\n",
       " 'support': 0.007635200996811918,\n",
       " 'janus': 0.0075768754946275475,\n",
       " 'testing': 0.007789675075036461,\n",
       " 'issues': 0.008102669372506817,\n",
       " 'money': 0.008521390410118805,\n",
       " 'zoom': 0.037037037037037035,\n",
       " 'primary': 0.037037037037037035}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(node_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zoom', 0.037037037037037035),\n",
       " ('primary', 0.037037037037037035),\n",
       " ('money', 0.008521390410118805),\n",
       " ('issues', 0.008102669372506817),\n",
       " ('clues', 0.007994929206707583),\n",
       " ('testing', 0.007789675075036461),\n",
       " ('code', 0.007682135165436623),\n",
       " ('client', 0.007677323214128335),\n",
       " ('support', 0.007635200996811918),\n",
       " ('janus', 0.0075768754946275475),\n",
       " ('prior', 0.007427691183993973),\n",
       " ('connection', 0.007416915861054011),\n",
       " ('perspective', 0.007409816884576187),\n",
       " ('thought', 0.007351317736216116),\n",
       " ('websocket', 0.0073253276387255884),\n",
       " ('segmentation', 0.007214104701968952),\n",
       " ('fault', 0.007114195274491018),\n",
       " ('kind', 0.007090730060803407),\n",
       " ('genuine', 0.006958471693640845),\n",
       " ('time', 0.00670373370352077),\n",
       " ('period', 0.006696020106917546),\n",
       " ('indians', 0.006689927646751073),\n",
       " ('accumulates', 0.006677866295956615),\n",
       " ('crashes', 0.006515492138870556),\n",
       " ('issue', 0.006514905907044505),\n",
       " ('memory', 0.006461907715020191),\n",
       " ('leak', 0.006395049647394857)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Multi-keywords terms based on their co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_terms = gr.retrieve_multi_keyterms(word_graph, normalize_nodes='degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['thought'], [0.007351317736216116]),\n",
       " (['issue'], [0.006514905907044505]),\n",
       " (['memory', 'leak', 'indians'],\n",
       "  [0.006461907715020191, 0.006395049647394857, 0.006689927646751073]),\n",
       " (['period'], [0.006696020106917546]),\n",
       " (['time'], [0.00670373370352077]),\n",
       " (['kind'], [0.007090730060803407]),\n",
       " (['accumulates'], [0.006677866295956615]),\n",
       " (['crashes'], [0.006515492138870556]),\n",
       " (['memory', 'leak', 'issue'],\n",
       "  [0.006461907715020191, 0.006395049647394857, 0.006514905907044505]),\n",
       " (['genuine', 'kind'], [0.006958471693640845, 0.007090730060803407]),\n",
       " (['segmentation', 'fault'], [0.007214104701968952, 0.007114195274491018]),\n",
       " (['websocket', 'connection'], [0.0073253276387255884, 0.007416915861054011]),\n",
       " (['client'], [0.007677323214128335]),\n",
       " (['code'], [0.007682135165436623]),\n",
       " (['clues'], [0.007994929206707583]),\n",
       " (['prior'], [0.007427691183993973]),\n",
       " (['perspective'], [0.007409816884576187]),\n",
       " (['support'], [0.007635200996811918]),\n",
       " (['janus'], [0.0075768754946275475]),\n",
       " (['testing'], [0.007789675075036461]),\n",
       " (['issues'], [0.008102669372506817]),\n",
       " (['money'], [0.008521390410118805]),\n",
       " (['zoom'], [0.037037037037037035]),\n",
       " (['primary'], [0.037037037037037035])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute aggregated scores for multi-keyword terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_words, multi_word_scores = gr.compute_multiterm_score(word_graph, normalize_nodes='degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['ios', 'app'], 0.025693100915893403),\n",
       " (['double', 'ac'], 0.024840489410996154),\n",
       " (['problem'], 0.02477826617356075),\n",
       " (['eco', 'meat'], 0.023681704973582587),\n",
       " (['drm', 'key'], 0.023075865117243112),\n",
       " (['goatee'], 0.02046870608967078),\n",
       " (['gateway'], 0.015316573833184105),\n",
       " (['week'], 0.01365641827332166),\n",
       " (['safari'], 0.013445110654064615),\n",
       " (['deep'], 0.012846550457946701),\n",
       " (['pa'], 0.012155604703065113),\n",
       " (['cool'], 0.012155604703065113),\n",
       " (['years'], 0.01202401969615893),\n",
       " (['stuck'], 0.011801430472019604),\n",
       " (['playlist'], 0.011730872171876851),\n",
       " (['respect'], 0.011709260678902326),\n",
       " (['cookies'], 0.011255125648218471),\n",
       " (['drm'], 0.011097811982827683)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_by_value(list(zip(multi_words, multi_word_scores)), order='desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get final list of Keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = gr.get_keyphrases(word_graph, normalize_nodes='degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ios app', 0.025693100915893403),\n",
       " ('double ac', 0.024840489410996154),\n",
       " ('problem', 0.02477826617356075),\n",
       " ('eco meat', 0.023681704973582587),\n",
       " ('drm key', 0.023075865117243112),\n",
       " ('goatee', 0.02046870608967078),\n",
       " ('gateway', 0.015316573833184105),\n",
       " ('week', 0.01365641827332166),\n",
       " ('safari', 0.013445110654064615),\n",
       " ('deep', 0.012846550457946701),\n",
       " ('pa', 0.012155604703065113),\n",
       " ('cool', 0.012155604703065113),\n",
       " ('years', 0.01202401969615893),\n",
       " ('stuck', 0.011801430472019604),\n",
       " ('playlist', 0.011730872171876851),\n",
       " ('respect', 0.011709260678902326),\n",
       " ('cookies', 0.011255125648218471),\n",
       " ('drm', 0.011097811982827683)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Keyphrases gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from ios app\n",
      "a double ac\n",
      "the same problem\n",
      "the eco meat to a\n",
      "drm key\n",
      "with the goatee\n",
      "a gateway\n",
      "last week\n",
      "on safari\n",
      "the deep\n",
      "in the drm ta pa\n",
      "cool\n",
      "xnumberx years with respect to the playlist\n",
      "stuck\n",
      "the playlist\n",
      "with respect to the playlist\n",
      "the cookies\n",
      "drm key\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "keywords = [i[0] for i in keyphrases]\n",
    "def reverse_search(keywords, text):\n",
    "    index_list = []\n",
    "    for position, word in enumerate(keywords):\n",
    "        if len(word_tokenize(word))>1:\n",
    "            word = word_tokenize(word)\n",
    "            index = text.index(word[0])\n",
    "            flag = True\n",
    "            inc_index = index\n",
    "            for w in word:\n",
    "                if not (w == text[inc_index]):\n",
    "                    #print (w, text[inc_index])\n",
    "                    flag = False\n",
    "                    break\n",
    "                inc_index+=1\n",
    "                #print (inc_index)\n",
    "            if not flag:\n",
    "                continue\n",
    "            else:\n",
    "                index_list.append(index)\n",
    "        else:\n",
    "            index = text.index(word)\n",
    "            index_list.append(index)\n",
    "    return index_list\n",
    "\n",
    "indexes = reverse_search(keywords,text)\n",
    "pos_text = nltk.pos_tag(text)\n",
    "new_sentence = []\n",
    "    \n",
    "for index in indexes:\n",
    "    sentence = [keywords[indexes.index(index)]]\n",
    "    for word, pos in pos_text[index-1::-1]:\n",
    "        if pos in ['DT','JJ','NN','IN']:\n",
    "            sentence.insert(0, word)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    sub_index = index + len(word_tokenize(keywords[indexes.index(index)]))\n",
    "    for word, pos in pos_text[sub_index:]:\n",
    "        if word in [',','.']:\n",
    "            break\n",
    "        else:\n",
    "            if pos in ['VBN','VBD', 'IN', 'DT', 'NN', 'TO']:\n",
    "                sentence.append(word)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    new_sentence.append(sentence)\n",
    "for key_phrase in new_sentence:\n",
    "    print (*key_phrase, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ios app',\n",
       " 'double ac',\n",
       " 'problem',\n",
       " 'eco meat',\n",
       " 'drm key',\n",
       " 'goatee',\n",
       " 'gateway',\n",
       " 'week',\n",
       " 'safari',\n",
       " 'deep',\n",
       " 'pa',\n",
       " 'cool',\n",
       " 'years',\n",
       " 'stuck',\n",
       " 'playlist',\n",
       " 'respect',\n",
       " 'cookies',\n",
       " 'drm']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Normalization on phrases > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = gr.get_keyphrases(word_graph, normalize=True, preserve_common_words=False, normalize_nodes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = gr.post_process(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(word_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare against JGTextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jgtextrank as jg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases_jg = jg.keywords_extraction(new_text, weight_comb='sum')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases_jg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for number of degrees for each node\n",
    "\n",
    "The idea, here, is to check if there is a correlation between the above `top_words` (which is based on `pagerank` scores) and degree scores for the same nodes. If yes, then possibly use the `degree centrality` metric to penalize the \"common words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = dict(word_graph.degree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by_value(degree.items(), order='desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nx.degree(word_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet = dict(nx.betweenness_centrality(word_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by_value(bet.items(), order='desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality = dict(nx.degree_centrality(word_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by_value(degree_centrality.items(), order='desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test graph extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"So last week whatever 16 years with respect to the playlist and DRM key, right? \" \\\n",
    "\"So I was able to test on Safari and chrome both wearing it was able to forward the cookies. \" \\\n",
    "\"I was just like trying to trace out the cookies whether it's cool being sent in the DRM ta PA all those things. \" \\\n",
    "\"So one thing is I had tested it, but I wanted the Deep also to test from IOS app also whether we can pass the cookies. \" \\\n",
    "\"So once that is done, it is like tested it but I just want him to also confirm that part that it can send a cookies from it was have also know but what I am right now stuck is the Eco meat to a double AC p-- a Gateway. \" \\\n",
    "\"It's not able to proxying it actually. \"\\\n",
    "\"So as you spend like I was trying to do with the goatee also, there also is not able to do it the same problem is that this something which is going.\" \\\n",
    "\"Hang on, okay.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_proc = tr.preprocessing(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens, new_pos_tuple = get_pos_tuple(new_text_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_graph = gr.build_word_graph(new_pos_tuple, original_tokens=new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_utils.draw_graph(new_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_weights, top_words = gr.node_weighting(graph_obj=new_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(node_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_words, multi_word_scores = gr.compute_multiterm_score(graph_obj=new_graph, original_tokens=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keyphrases = gr.get_keyphrases(graph_obj=new_graph, original_tokens=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_graph.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gr.graph.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntactic_filter = ['ADJ', 'NOUN', 'PROPN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pos_list = [(word, pos) for sent in pos_tuple for word, pos in sent if pos in syntactic_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pos_list == filtered_pos_tuple[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pos_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'seamless' in word_graph.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokens, pos_tuple, filtered_pos_tuple = tp.preprocess_text(new_text, filter_by_pos=True, pos_filter=['NOUN', 'PROPN', 'ADJ', 'FW', 'VERB', 'ADV', 'PRON', 'DET'], stop_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntactic_filter = ['NOUN', 'PROPN', 'ADJ', 'FW', 'VERB', 'ADV', 'PRON', 'DET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_graph = gr.build_word_graph(filtered_pos_tuple, original_tokens=original_tokens, window=4, reset_graph_context=False, syntactic_filter=syntactic_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.draw_graph(topic_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_terms = gr.retrieve_multi_keyterms(topic_graph, preserve_common_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_phrases = gr.get_keyphrases(topic_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['slack', 'slack', 'channels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict.fromkeys(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [\"redirect URL redirect\",\"kind\",\"UI nature\",\"track particles\",\"slack slack Marketplace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in b:\n",
    "    j = i.split()\n",
    "    print(list(dict.fromkeys(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textrank",
   "language": "python",
   "name": "textrank"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
