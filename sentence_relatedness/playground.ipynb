{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json \n",
    "from IPython.display import clear_output\n",
    "import requests\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertPreTrainingHeads\n",
    "from bert_utils import *\n",
    "import time\n",
    "from scipy import spatial\n",
    "\n",
    "import nltk\n",
    "import string,itertools\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import unicodedata\n",
    "from more_itertools import locate\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = json.load(open('contractions.txt','rb'))\n",
    "contractions = contractions['contractions']\n",
    "    \n",
    "def replaceContractions(text):\n",
    "    #text = text.lower()\n",
    "    c_filt_text = ''\n",
    "    for word in text.split(' '):\n",
    "        if word in contractions:\n",
    "            c_filt_text = c_filt_text+' '+contractions[word]\n",
    "        else:\n",
    "            c_filt_text = c_filt_text+' '+word\n",
    "    return c_filt_text.strip()\n",
    "\n",
    "def stripText(text):\n",
    "    text = replaceContractions(text.lower())\n",
    "    text = re.sub('(\\d+[A-z]+)|(([A-z]+\\d+))',' ',text) #remove alphanumeric words\n",
    "    text = re.sub('-',' ', text)\n",
    "    text = re.sub('\\s+',' ', text)\n",
    "    text = re.sub(\"'\",' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data\n",
    "import json\n",
    "import os\n",
    "test_file_list = os.listdir('./test_data/')\n",
    "master_text = ''\n",
    "\n",
    "for file in test_file_list:\n",
    "    curr_file = json.load(open('test_data/'+file,'rb'))\n",
    "    master_text = master_text+' '+curr_file\n",
    "    \n",
    "sent_bucket = master_text.split('.')\n",
    "sent_bucket = [ele.strip() for ele in sent_bucket if len(ele)>5]\n",
    "sent_bucket = [stripText(ele) for ele in sent_bucket]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getregexChunks(text, grammar):\n",
    "\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
    "                                                    for tagged_sent in tagged_sents))\n",
    "    return [(ele[0], ele[1], ele[2], ctr) for ele,ctr in zip(all_chunks,range(len(all_chunks)))]\n",
    "\n",
    "def getCandidatePhrases(text, pos_search_pattern_list=[r\"\"\"base: {(<JJ.*>*<NN.*>+<IN>)?<JJ>*<NN.*>+}\"\"\",\n",
    "                                           r\"\"\"nounverb:{<NN.*>+<VB.*>+}\"\"\",\n",
    "                                           r\"\"\"verbnoun:{<VB.*>+<NN.*>+}\"\"\"]):\n",
    "                                       #r\"\"\" nounnoun:{<NN.+>+<.+>{1,2}<NN.+>+}\"\"\"]):\n",
    "                                       #r\"\"\"baseverb: {(<JJ.+>+<IN>)?<JJ>*<VB.*>+}\"\"\"]):\n",
    "    text = stripText(text)\n",
    "    punct = set(string.punctuation)\n",
    "    all_chunks = []\n",
    "\n",
    "    for pattern in pos_search_pattern_list:\n",
    "        all_chunks+=getregexChunks(text, pattern)\n",
    "    \n",
    "    candidate_phrases = [' '.join(word for word, pos, \n",
    "                           chunk,ctr in group).lower() \n",
    "                  for key, group in itertools.groupby(all_chunks, \n",
    "                  lambda_unpack(lambda word, pos, chunk, ctr: chunk != 'O')) if key]\n",
    "    \n",
    "#     candidate_locs = [' '.join(str(ctr) for word, pos, \n",
    "#                            chunk,ctr in group).lower() \n",
    "#                   for key, group in itertools.groupby(all_chunks, \n",
    "#                   lambda_unpack(lambda word, pos, chunk, ctr: chunk != 'O')) if key]\n",
    "    \n",
    "    filtered_candidates = []\n",
    "    for key_phrase in candidate_phrases:\n",
    "        curr_filtr_phrase = stripStopWordsFromText(key_phrase,stop_words)\n",
    "        if len(curr_filtr_phrase)>0:\n",
    "            filtered_candidates.append(curr_filtr_phrase)\n",
    "        \n",
    "    #remove the key-phrases starting with stop_words assuming that the stop_word is a verb and \n",
    "    #the noun would be covered in the next pattern\n",
    "#     filtered_candidates = []\n",
    "#     for key_phrase in candidate_phrases:\n",
    "#         if key_phrase.split(' ')[0] not in stop_words and key_phrase.split(' ')[-1] not in stop_words:\n",
    "#             filtered_candidates.append(key_phrase)\n",
    "    candidate_phrases = filterCandidatePhrases(text,filtered_candidates)\n",
    "    candidate_phrases,candidate_locs = getPhraseListLocations(text, candidate_phrases)\n",
    "    return candidate_phrases,candidate_locs\n",
    "    \n",
    "def lambda_unpack(f):\n",
    "    return lambda args: f(*args)\n",
    "\n",
    "def getWordLevelFeats(sent,token_feat_dict,tokenizer):\n",
    "    word_feat_list = []\n",
    "    for word in sent.split(' '):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        if len(tokenized_word)==1:\n",
    "            word_feat_list.append(np.array(token_feat_dict[tokenized_word]))\n",
    "        else:\n",
    "            tok_feats = []\n",
    "            for tok in tokenized_word:\n",
    "                tok_feats+=np.array(token_feat_dict[tok])\n",
    "            word_feat_list.append(tok_feats)\n",
    "            \n",
    "    return word_feat_list\n",
    "\n",
    "def getWordFeatsFromBertTokenFeats(sent_tokens,bert_tokens,bert_token_feats):\n",
    "    #steps for merging the bert tokens to get the BERT features for actual words\n",
    "    #1. iterate over the BERT base tokenizer\n",
    "    #2. lookup for the actual word in the current BERT lookup postions\n",
    "    #3. If found:\n",
    "        #3a. the word is not tokenized further - use the current BERT features as word embedding\n",
    "    #else:\n",
    "        #3b. the word is tokenized in BERT - find the sequence of tokens and sum up the features to get the word vector\n",
    "    base_ctr = 0\n",
    "    bert_ctr = 0\n",
    "    word_feat_list = []\n",
    "\n",
    "    for word in sent_tokens:\n",
    "        if bert_tokens[bert_ctr] == word:#word not further tokenized, use the same feature vector\n",
    "            word_feat_list.append(np.array(bert_token_feats[bert_ctr].detach().numpy()))\n",
    "            base_ctr+=1\n",
    "            bert_ctr+=1\n",
    "        else:\n",
    "            aggr_feats = np.array(bert_token_feats[bert_ctr].detach().numpy())\n",
    "            aggr_word = bert_tokens[bert_ctr]\n",
    "            merge_next = True\n",
    "            while merge_next and bert_ctr<len(bert_tokens)-1:\n",
    "                if '#' in bert_tokens[bert_ctr+1]:\n",
    "                    aggr_word = aggr_word+bert_tokens[bert_ctr+1]\n",
    "                    bert_ctr+=1\n",
    "                    aggr_feats+=np.array(bert_token_feats[bert_ctr].detach().numpy())\n",
    "                else:\n",
    "                    merge_next = False\n",
    "                    bert_ctr+=1\n",
    "            word_feat_list.append(aggr_feats)\n",
    "    assert len(sent_tokens)==len(word_feat_list)\n",
    "    return word_feat_list\n",
    "\n",
    "def getPOSPhrases(sent_tokens,candidate_pos_tags = ['NN','NNPS','NNS','NNP','VBG','VBN','VBP','VBZ','JJ','JJR','JJS'],\n",
    "                  tag_pairs = ['NN','VB','JJ']):\n",
    "\n",
    "    candidate_pos_tags = ['NN','NNPS','NNS','NNP','VBG','VBN','VBP','VBZ','JJ','JJR','JJS']\n",
    "    tag_pairs = ['NN','VB','JJ']\n",
    "\n",
    "    sent_tokens_pos= nltk.pos_tag(sent_tokens)\n",
    "    sent_tokens = [(ele[0],ele[1],ctr) for ctr,ele in zip(range(len(sent_tokens_pos)),sent_tokens_pos) if ele[1] in candidate_pos_tags and len(ele[0])>2]\n",
    "\n",
    "    noun_tags = [(tok,ctr) for tok,pos,ctr in sent_tokens if 'NN' in pos]\n",
    "    verb_tags = [(tok,ctr) for tok,pos,ctr in sent_tokens if 'VB' in pos]\n",
    "    adj_tags = [(tok,ctr) for tok,pos,ctr in sent_tokens if 'JJ' in pos]\n",
    "\n",
    "    #return dict(zip(['noun_tags','verb_tags','adj_tags'],[[noun_tags],[verb_tags],[adj_tags]]))\n",
    "    return dict(zip(['noun_tags','verb_tags','adj_tags'],[noun_tags,verb_tags,adj_tags]))\n",
    "\n",
    "def getPOSSetsForSent(text_sent_tokens):\n",
    "    pos_sent = getPOSPhrases(text_sent_tokens)\n",
    "    merged_pos_list = []\n",
    "    \n",
    "    for key in pos_sent.keys():\n",
    "        merge_list = []\n",
    "        phrase_list = []\n",
    "\n",
    "        curr_candidate_tokens = [ele[0] for ele in pos_sent[key]]\n",
    "        pos_tok_seq = [ele[1] for ele in pos_sent[key]]\n",
    "        \n",
    "        assert len(curr_candidate_tokens) == len(pos_tok_seq)\n",
    "\n",
    "        diff_list = [pos_tok_seq[i+1]-pos_tok_seq[i] for i in range(len(pos_tok_seq)-1)]\n",
    "        diff_list.append(0)\n",
    "\n",
    "        for ctr in range(len(pos_tok_seq)):\n",
    "            if diff_list[ctr]==1:\n",
    "                merge_list.append(pos_tok_seq[ctr])\n",
    "                merge_list.append(pos_tok_seq[ctr+1])\n",
    "            else:\n",
    "                phrase_list.append(merge_list)\n",
    "                merge_list = []\n",
    "                if pos_tok_seq[ctr] not in reduce(lambda x,y: x+y,phrase_list):\n",
    "                    phrase_list.append([pos_tok_seq[ctr]])\n",
    "\n",
    "        merged_pos_list.append([list(set(ele)) for ele in phrase_list if len(ele)>0])\n",
    "    return dict(zip(pos_sent.keys(),merged_pos_list))\n",
    "\n",
    "def posSetFeats(text_sent_tokens,token_bert_feats):\n",
    "    \n",
    "    assert len(text_sent_tokens)==len(token_bert_feats)\n",
    "    \n",
    "    sent_pos_sets = getPOSSetsForSent(text_sent_tokens)\n",
    "    pos_wise_feats = []\n",
    "    pos_words = []\n",
    "    for pos_key in sent_pos_sets.keys():\n",
    "        curr_pos_key_feats = []\n",
    "        curr_pos_entity = []\n",
    "        \n",
    "        for pos_set in sent_pos_sets[pos_key]:\n",
    "            if len(pos_set)>1:\n",
    "                feat_list = []\n",
    "                sent = ''\n",
    "                #merge all the tags in current list\n",
    "                for token in pos_set:\n",
    "                    feat_list.append(token_bert_feats[token])\n",
    "                    sent = sent+' '+text_sent_tokens[token]\n",
    "                curr_pos_key_feats.append(sum(feat_list))\n",
    "                curr_pos_entity.append(sent.strip())\n",
    "            else:\n",
    "                curr_pos_key_feats.append(token_bert_feats[pos_set[0]])\n",
    "                curr_pos_entity.append(text_sent_tokens[pos_set[0]])\n",
    "        pos_wise_feats.append(curr_pos_key_feats)\n",
    "        pos_words.append(curr_pos_entity)\n",
    "        \n",
    "    return dict(zip(sent_pos_sets.keys(),pos_wise_feats)),pos_words \n",
    "\n",
    "###################### Key-phrase based cosine similarity ######################\n",
    "def getKeyPhraseFeatures(kp_list, kp_loc_idx,text_feats, text_tokens):\n",
    "    \n",
    "    key_phrase_feats = []\n",
    "    for ele,loc_list in zip(kp_list,kp_loc_idx):\n",
    "        if len(ele.split(' '))==1:\n",
    "            idx_val = int(loc_list[0])\n",
    "            key_phrase_feats.append(getTokenFeature(ele,idx_val,text_feats,text_tokens))\n",
    "        else:\n",
    "            curr_feature_vec = []\n",
    "            for tok,tok_idx in zip(ele.split(' '),loc_list.split(' ')):\n",
    "                curr_feature_vec.append(getTokenFeature(tok,int(tok_idx),text_feats,text_tokens))\n",
    "            key_phrase_feats.append(sum(curr_feature_vec))\n",
    "    return key_phrase_feats\n",
    "            \n",
    "def getTokenFeature(token, token_idx, text_feats, text_tokens):    \n",
    "    if text_tokens[token_idx]==token:\n",
    "        feat_vec = text_feats[token_idx]\n",
    "    else:\n",
    "        #print('Token not found in the location, searching entire text.: ', token)\n",
    "        if token in text_tokens:\n",
    "            idx_val = text_tokens.index(token)\n",
    "            feat_vec = text_feats[idx_val]\n",
    "        else:\n",
    "            #print('Token not found.. returning default feature vector: ', token)\n",
    "            feat_vec = np.full(len(text_feats[0]),0.01)\n",
    "    return feat_vec\n",
    "\n",
    "## ----------------- Methods borrowed from BERT tokenizer -----------------\n",
    "\n",
    "def tokenize(text, never_split = [], do_lower_case = True):\n",
    "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "    text = _clean_text(text)\n",
    "    # This was added on November 1st, 2018 for the multilingual and Chinese\n",
    "    # models. This is also applied to the English models now, but it doesn't\n",
    "    # matter since the English models were not trained on any Chinese data\n",
    "    # and generally don't have any Chinese data in them (there are Chinese\n",
    "    # characters in the vocabulary because Wikipedia does have some Chinese\n",
    "    # words in the English Wikipedia.).\n",
    "    orig_tokens = whitespace_tokenize(text)\n",
    "    split_tokens = []\n",
    "    for token in orig_tokens:\n",
    "        if do_lower_case and token not in never_split:\n",
    "            token = token.lower()\n",
    "            token = _run_strip_accents(token)\n",
    "        split_tokens.extend(_run_split_on_punc(token))\n",
    "\n",
    "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "    return output_tokens\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _run_split_on_punc(text):\n",
    "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "    chars = list(text)\n",
    "    i = 0\n",
    "    start_new_word = True\n",
    "    output = []\n",
    "    while i < len(chars):\n",
    "        char = chars[i]\n",
    "        if _is_punctuation(char):\n",
    "            output.append([char])\n",
    "            start_new_word = True\n",
    "        else:\n",
    "            if start_new_word:\n",
    "                output.append([])\n",
    "            start_new_word = False\n",
    "            output[-1].append(char)\n",
    "        i += 1\n",
    "\n",
    "    return [\"\".join(x) for x in output]\n",
    "\n",
    "def _clean_text(text):\n",
    "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "    output = []\n",
    "    for char in text:\n",
    "        cp = ord(char)\n",
    "        if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "            continue\n",
    "        if _is_whitespace(char):\n",
    "            output.append(\" \")\n",
    "        else:\n",
    "            output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "def _is_control(char):\n",
    "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _is_whitespace(char):\n",
    "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def _run_strip_accents(text):\n",
    "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    output = []\n",
    "    for char in text:\n",
    "        cat = unicodedata.category(char)\n",
    "        if cat == \"Mn\":\n",
    "            continue\n",
    "        output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "def getKPBasedSimilarity(text1, text2,  bert_layer = -1):\n",
    "    \n",
    "    \n",
    "    token_feats_1,final_feats1,text1_bert_tokenized = getBERTFeatures(model, text1, attn_head_idx=bert_layer)\n",
    "    token_feats_2,final_feats2,text2_bert_tokenized = getBERTFeatures(model, text2, attn_head_idx=bert_layer)\n",
    "\n",
    "    text1_sent_tokens = tokenize(text1)\n",
    "    text2_sent_tokens = tokenize(text2)\n",
    "\n",
    "    merged_feats_text1 = getWordFeatsFromBertTokenFeats(text1_sent_tokens,text1_bert_tokenized,token_feats_1)\n",
    "    merged_feats_text2 = getWordFeatsFromBertTokenFeats(text2_sent_tokens,text2_bert_tokenized,token_feats_2)\n",
    "\n",
    "    #get candidate key-phrases for both sentences\n",
    "    kps_sent1,kps_loc_sent1 = getCandidatePhrases(text1)\n",
    "    kps_sent2,kps_loc_sent2 = getCandidatePhrases(text2)\n",
    "\n",
    "#     print(kps_sent1)\n",
    "#     print()\n",
    "#     print(kps_sent2)\n",
    "#     print()\n",
    "    \n",
    "    sent1_kp_feats = getKeyPhraseFeatures(kps_sent1,kps_loc_sent1,merged_feats_text1,text1_sent_tokens)\n",
    "    sent2_kp_feats = getKeyPhraseFeatures(kps_sent2,kps_loc_sent2,merged_feats_text2,text2_sent_tokens)\n",
    "\n",
    "    curr_max = 0\n",
    "    for sent1_kp, feats1 in zip(kps_sent1,sent1_kp_feats):\n",
    "        for sent2_kp, feats2 in zip(kps_sent2,sent2_kp_feats):\n",
    "            if len(sent1_kp)<3 or len(sent2_kp)<3:\n",
    "                curr_sim = 0.1\n",
    "            else:\n",
    "                curr_sim = 1-spatial.distance.cosine(feats1,feats2)\n",
    "            if len(sent1_kp.split(' '))==1 and len(sent2_kp.split(' '))==1:\n",
    "                #penalize by 5 points?\n",
    "                curr_sim = curr_sim-0.05\n",
    "            #print(sent1_kp,'<>',sent2_kp,': ',curr_sim)\n",
    "            if curr_sim>curr_max:\n",
    "                curr_max = curr_sim\n",
    "\n",
    "#     print()      \n",
    "#     print('-----------------------------------------------------------')\n",
    "#     print('final similarity:', curr_max)\n",
    "#     print('-----------------------------------------------------------')\n",
    "#     print(\"Sentence level score: \", 1-spatial.distance.cosine(final_feats1,final_feats2))\n",
    "#     print('-----------------------------------------------------------')\n",
    "    \n",
    "    return curr_max\n",
    "\n",
    "def getPOSBasedSimilarity(text1, text2,  bert_layer = -1):\n",
    "\n",
    "    token_feats_1,final_feats1,text1_bert_tokenized = getBERTFeatures(model, text1, attn_head_idx=layer)\n",
    "    token_feats_2,final_feats2,text2_bert_tokenized = getBERTFeatures(model, text2, attn_head_idx=layer)\n",
    "\n",
    "    text1_sent_tokens = tokenize(text1)\n",
    "    text2_sent_tokens = tokenize(text2)\n",
    "\n",
    "    merged_feats_text1 = getWordFeatsFromBertTokenFeats(text1_sent_tokens,text1_bert_tokenized,token_feats_1)\n",
    "    merged_feats_text2 = getWordFeatsFromBertTokenFeats(text2_sent_tokens,text2_bert_tokenized,token_feats_2)\n",
    "\n",
    "    sent1_pos_feats,sent1_pos = posSetFeats(text1_sent_tokens,merged_feats_text1)\n",
    "    sent2_pos_feats,sent2_pos = posSetFeats(text2_sent_tokens,merged_feats_text2)\n",
    "\n",
    "    #Do pos-tag wise feature similarity and take max() similarity in each pos as metric\n",
    "\n",
    "    #get pos_tag wise cosine similarity\n",
    "    pos_idx = 0 #0-noun, 1-verb, 2 - adj\n",
    "    curr_pos = list(sent1_pos_feats.keys())[pos_idx]\n",
    "    sent1_tags = sent1_pos[pos_idx]\n",
    "    sent2_tags = sent2_pos[pos_idx]\n",
    "\n",
    "    #print('Current POS: ',curr_pos)\n",
    "    #print()\n",
    "    sent1_curr_pos_feats = sent1_pos_feats[curr_pos]\n",
    "    sent2_curr_pos_feats = sent2_pos_feats[curr_pos]\n",
    "\n",
    "    curr_max = 0\n",
    "    if len(sent1_curr_pos_feats)>0 and len(sent2_curr_pos_feats)>0:\n",
    "        for sent1_ctr in range(len(sent1_curr_pos_feats)):\n",
    "            for sent2_ctr in range(len(sent2_curr_pos_feats)):\n",
    "                curr_sim = 1-spatial.distance.cosine(sent1_curr_pos_feats[sent1_ctr],sent2_curr_pos_feats[sent2_ctr])\n",
    "                if curr_sim>curr_max:\n",
    "                    curr_max = curr_sim\n",
    "    return curr_max\n",
    "\n",
    "def getCosineSimilarity(text1, text2,  bert_layer = -1):\n",
    "\n",
    "    token_feats_1,final_feats1,text1_bert_tokenized = getBERTFeatures(model, text1, attn_head_idx=layer)\n",
    "    token_feats_2,final_feats2,text2_bert_tokenized = getBERTFeatures(model, text2, attn_head_idx=layer)\n",
    "\n",
    "    return 1-spatial.distance.cosine(final_feats1,final_feats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKPBasedSimilarity_loop(tup1,tup2, text1, text2, bert_layer = -1):\n",
    "    \n",
    "    token_feats_1,final_feats1,text1_bert_tokenized = tup1\n",
    "    token_feats_2,final_feats2,text2_bert_tokenized = tup2\n",
    "\n",
    "    text1_sent_tokens = tokenize(text1)\n",
    "    text2_sent_tokens = tokenize(text2)\n",
    "\n",
    "    merged_feats_text1 = getWordFeatsFromBertTokenFeats(text1_sent_tokens,text1_bert_tokenized,token_feats_1)\n",
    "    merged_feats_text2 = getWordFeatsFromBertTokenFeats(text2_sent_tokens,text2_bert_tokenized,token_feats_2)\n",
    "\n",
    "    #get candidate key-phrases for both sentences\n",
    "    kps_sent1,kps_loc_sent1 = getCandidatePhrases(text1)\n",
    "    kps_sent2,kps_loc_sent2 = getCandidatePhrases(text2)\n",
    "\n",
    "    sent1_kp_feats = getKeyPhraseFeatures(kps_sent1,kps_loc_sent1,merged_feats_text1,text1_sent_tokens)\n",
    "    sent2_kp_feats = getKeyPhraseFeatures(kps_sent2,kps_loc_sent2,merged_feats_text2,text2_sent_tokens)\n",
    "\n",
    "    curr_max = 0\n",
    "    for sent1_kp, feats1 in zip(kps_sent1,sent1_kp_feats):\n",
    "        for sent2_kp, feats2 in zip(kps_sent2,sent2_kp_feats):\n",
    "            if len(sent1_kp)<3 or len(sent2_kp)<3:\n",
    "                curr_sim = 0.1\n",
    "            else:\n",
    "                curr_sim = 1-spatial.distance.cosine(feats1,feats2)\n",
    "            if len(sent1_kp.split(' '))==1 and len(sent2_kp.split(' '))==1:\n",
    "                #penalize by 5 points?\n",
    "                curr_sim = curr_sim-0.05\n",
    "            #print(sent1_kp,'<>',sent2_kp,': ',curr_sim)\n",
    "            if curr_sim>curr_max:\n",
    "                curr_max = curr_sim\n",
    "\n",
    "\n",
    "#     print()      \n",
    "#     print('-----------------------------------------------------------')\n",
    "#     print('final similarity:', curr_max)\n",
    "#     print('-----------------------------------------------------------')\n",
    "#     print(\"Sentence level score: \", 1-spatial.distance.cosine(final_feats1,final_feats2))\n",
    "#     print('-----------------------------------------------------------')\n",
    "    \n",
    "    return curr_max\n",
    "\n",
    "def getPOSBasedSimilarity_loop(tup1, tup2,  text1, text2, bert_layer = -1):\n",
    "\n",
    "    token_feats_1,final_feats1,text1_bert_tokenized = tup1\n",
    "    token_feats_2,final_feats2,text2_bert_tokenized = tup2\n",
    "\n",
    "    text1_sent_tokens = tokenize(text1)\n",
    "    text2_sent_tokens = tokenize(text2)\n",
    "\n",
    "    merged_feats_text1 = getWordFeatsFromBertTokenFeats(text1_sent_tokens,text1_bert_tokenized,token_feats_1)\n",
    "    merged_feats_text2 = getWordFeatsFromBertTokenFeats(text2_sent_tokens,text2_bert_tokenized,token_feats_2)\n",
    "\n",
    "    sent1_pos_feats,sent1_pos = posSetFeats(text1_sent_tokens,merged_feats_text1)\n",
    "    sent2_pos_feats,sent2_pos = posSetFeats(text2_sent_tokens,merged_feats_text2)\n",
    "\n",
    "    #Do pos-tag wise feature similarity and take max() similarity in each pos as metric\n",
    "\n",
    "    #get pos_tag wise cosine similarity\n",
    "    pos_idx = 0 #0-noun, 1-verb, 2 - adj\n",
    "    curr_pos = list(sent1_pos_feats.keys())[pos_idx]\n",
    "    sent1_tags = sent1_pos[pos_idx]\n",
    "    sent2_tags = sent2_pos[pos_idx]\n",
    "\n",
    "    #print('Current POS: ',curr_pos)\n",
    "    #print()\n",
    "    sent1_curr_pos_feats = sent1_pos_feats[curr_pos]\n",
    "    sent2_curr_pos_feats = sent2_pos_feats[curr_pos]\n",
    "\n",
    "    curr_max = 0\n",
    "    if len(sent1_curr_pos_feats)>0 and len(sent2_curr_pos_feats)>0:\n",
    "        for sent1_ctr in range(len(sent1_curr_pos_feats)):\n",
    "            for sent2_ctr in range(len(sent2_curr_pos_feats)):\n",
    "                dist = 1-spatial.distance.cosine(sent1_curr_pos_feats[sent1_ctr],sent2_curr_pos_feats[sent2_ctr])\n",
    "                if dist>curr_max:\n",
    "                    curr_max = dist\n",
    "                #print(sent1_tags[sent1_ctr],'<>',sent2_tags[sent2_ctr],\":\",dist)\n",
    "            #print()\n",
    "#     else:\n",
    "#         print(\"No %s tokens in one of the texts, skipping\" % (curr_pos))\n",
    "#     print('-----------------------------------------------------------')\n",
    "#     print('final similarity:', curr_max)\n",
    "#     print('-----------------------------------------------------------')\n",
    "#     print(\"Sentence level score: \", 1-spatial.distance.cosine(final_feats1,final_feats2))\n",
    "#     print('-----------------------------------------------------------')\n",
    "\n",
    "    return curr_max\n",
    "\n",
    "def getCosineSimilarity_loop(tup1, tup2,  bert_layer = -1):\n",
    "\n",
    "    token_feats_1,final_feats1,text1_bert_tokenized = tup1\n",
    "    token_feats_2,final_feats2,text2_bert_tokenized = tup2\n",
    "\n",
    "    return 1-spatial.distance.cosine(final_feats1,final_feats2)\n",
    "\n",
    "###-------------------------------------- New functions --------------------------------------###\n",
    "\n",
    "def removeStopwords(text):\n",
    "    sent = ' '.join([tok for tok in text.split(' ') if tok not in stop_words])\n",
    "    return sent\n",
    "\n",
    "def getStartEndPOSList(text,candidate_phrases_list):\n",
    "    start_pos_list = []\n",
    "    end_pos_list = []\n",
    "    processed_list = []\n",
    "    for candidate in candidate_phrases_list:\n",
    "        start_pos = [match.start() for match in re.finditer(candidate, text)]\n",
    "        if len(start_pos)==1:\n",
    "            processed_list.append(candidate)\n",
    "            start_pos_list.append(start_pos[0])\n",
    "            end_pos_list.append(start_pos[0]+len(candidate))\n",
    "        else:\n",
    "            tok_ctr = processed_list.count(candidate)\n",
    "            start_pos_list.append(start_pos[tok_ctr])\n",
    "            end_pos_list.append(start_pos[tok_ctr]+len(candidate))\n",
    "            processed_list.append(candidate)\n",
    "    return start_pos_list, end_pos_list\n",
    "\n",
    "def filterCandidatePhrases(text, candidate_phrases_list):\n",
    "    drop_list = []\n",
    "    merge_list = []\n",
    "    merge_list_start = []\n",
    "    merge_list_end = []\n",
    "\n",
    "    filtered_sent = removeStopwords(text)\n",
    "    filtered_phrase_list = [removeStopwords(phrase) for phrase in candidate_phrases_list]\n",
    "\n",
    "    start_pos_list, end_pos_list = getStartEndPOSList(text,candidate_phrases_list)\n",
    "    filtered_start_pos_list, filtered_end_pos_list = getStartEndPOSList(filtered_sent,filtered_phrase_list)\n",
    "    assert len(filtered_start_pos_list)==len(filtered_phrase_list)\n",
    "\n",
    "    for i in range(len(start_pos_list)):\n",
    "        curr_start,curr_end,ctr = start_pos_list[i],end_pos_list[i],i\n",
    "\n",
    "        for j in range(i+1, len(start_pos_list)):\n",
    "            lookup_start, lookup_end, lookup_ctr = start_pos_list[j], end_pos_list[j], j\n",
    "            if curr_start==lookup_start and curr_end==lookup_end:\n",
    "                continue\n",
    "            if (curr_start<=lookup_start and curr_end>=lookup_end) or (lookup_start<=curr_start and lookup_end>=curr_end):\n",
    "                if len(candidate_phrases_list[i])<len(candidate_phrases_list[j]):\n",
    "                    drop_list.append(candidate_phrases_list[i])\n",
    "                else:\n",
    "                    drop_list.append(candidate_phrases_list[j])\n",
    "\n",
    "        for k in range(len(start_pos_list)):\n",
    "            if filtered_start_pos_list[i]-filtered_end_pos_list[k]==1:\n",
    "                merge_list.append([candidate_phrases_list[i],candidate_phrases_list[k]])\n",
    "                drop_list.append(candidate_phrases_list[i])\n",
    "                drop_list.append(candidate_phrases_list[k])\n",
    "                merge_list_start.append(min(start_pos_list[i],start_pos_list[k]))\n",
    "                merge_list_end.append(max(end_pos_list[i],end_pos_list[k]))\n",
    "\n",
    "    for ctr in range(len(merge_list)):\n",
    "        candidate_phrases_list.append(text[merge_list_start[ctr]:merge_list_end[ctr]])\n",
    "        \n",
    "    #do not do set operation\n",
    "    for ele in drop_list:\n",
    "        if ele in candidate_phrases_list:\n",
    "            candidate_phrases_list.remove(ele)\n",
    "    return candidate_phrases_list\n",
    "\n",
    "def stripStopWordsFromText(sent, stop_words):\n",
    "    fw_ctr = 0\n",
    "    bw_ctr = 0\n",
    "    for tok in sent.split(' '):\n",
    "        if tok in stop_words:\n",
    "            fw_ctr+=1\n",
    "        else:\n",
    "            break\n",
    "    for tok in reversed(sent.split(' ')):\n",
    "        if tok in stop_words:\n",
    "            bw_ctr-=1\n",
    "        else:\n",
    "            break\n",
    "    if bw_ctr!=0:\n",
    "        stripped_kp = ' '.join(sent.split(' ')[fw_ctr:bw_ctr])\n",
    "    else:\n",
    "        stripped_kp = ' '.join(sent.split(' ')[fw_ctr:])\n",
    "            \n",
    "    return stripped_kp.strip()\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    results=[]\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            results.append((ind,ind+sll-1))        \n",
    "    range_list = [list(range(ele[0],ele[1]+1)) for ele in results]\n",
    "    \n",
    "    return range_list\n",
    "\n",
    "def getPhraseListLocations(text, candidate_phrases):\n",
    "    #assuming that the \n",
    "    phrase_idx_list = []\n",
    "    token_sent_list = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text)]\n",
    "    token_list = list(chain(*token_sent_list))\n",
    "    for phrase in candidate_phrases:\n",
    "        phrase_tokens = nltk.word_tokenize(phrase)\n",
    "        phrase_idx = find_sub_list(phrase_tokens,token_list)\n",
    "        phrase_idx_list.append(phrase_idx)\n",
    "     \n",
    "    processed_phrase_list = []\n",
    "    processed_idx_list = []\n",
    "    for phrase, loc_idx in zip(candidate_phrases,phrase_idx_list):\n",
    "        if len(loc_idx)==1:\n",
    "            processed_phrase_list.append(phrase)\n",
    "            processed_idx_list.append(loc_idx[0])\n",
    "        else:\n",
    "            #count number of times the phrase has occurred in the list\n",
    "            if phrase not in processed_phrase_list:\n",
    "                kp_occ_ctr = candidate_phrases.count(phrase)\n",
    "                if kp_occ_ctr == len(loc_idx):\n",
    "                    #append current key-phrase `kp_occ_ctr` times into the lists\n",
    "                    processed_phrase_list+=[phrase]*kp_occ_ctr\n",
    "                    processed_idx_list+=loc_idx\n",
    "                else: \n",
    "                    idx_drop_list = []\n",
    "                    #the phrase index is calculated as part of another key-phrase index\n",
    "                    #check other sublists that are \n",
    "                    #find other locations \n",
    "                    for lookup_loc in phrase_idx_list:\n",
    "                        if lookup_loc!=loc_idx and len(lookup_loc[0])!=len(loc_idx[0]):\n",
    "                            for i in range(len(curr)):\n",
    "                                if((set(loc_idx[i]) & set(lookup_loc[0]))== set(loc_idx[i])):\n",
    "                                    idx_drop_list.append(loc_idx[i])\n",
    "                    for to_insert_loc in loc_idx:\n",
    "                        if to_insert_loc not in idx_drop_list:\n",
    "                            processed_phrase_list.append(phrase)\n",
    "                            processed_idx_list.append(to_insert_loc)\n",
    "                            \n",
    "    str_loc_list = []\n",
    "    for ele in processed_idx_list:\n",
    "        str_loc = ''\n",
    "        for tok in ele:\n",
    "            str_loc = str_loc+' '+str(tok)\n",
    "        str_loc_list.append(str_loc.strip())\n",
    "            \n",
    "    return processed_phrase_list,str_loc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_id, experiment_name\n",
    "# 01daapwr6w051q9wwqy99jsgfy - Generic\n",
    "# 01daaqy88qzb19jqz5prjfr76y - Engineering\n",
    "# 01daaqyn9gbebc92aywnxedp0c - HR\n",
    "# 01daatanxnrqa35e6004hb7mbn - Marketing\n",
    "# 01daatbc3ak1qwc5nyc5ahv2xz - Product\n",
    "# 01dadp74wfv607knpcb6vvxgtg - AI\n",
    "# 01daayheky5f4e02qvrjptftxv - Ether Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config_path = '/Users/venkat/Documents/mlflow/mlflow_bert/bert_mlflow_pyfunc/artifacts/bert_config.json'\n",
    "mind_path = '/Users/venkat/Documents/mlflow/mlflow_bert/bert_mlflow_pyfunc/artifacts/mind.pkl'\n",
    "model_path = '/Users/venkat/Documents/mlflow/mlflow_bert_deploy/mind-01daayheky5f4e02qvrjptftxv/artifacts/model.bin'\n",
    "\n",
    "config = BertConfig.from_json_file(config_path)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "mind_dict = pickle.load(open(mind_path,'rb'))\n",
    "\n",
    "model = BertForPreTrainingCustom(config)\n",
    "state_dict = torch.load(model_path,map_location='cpu')\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate pair-wise cosine similarity\n",
    "#extract features from all layers\n",
    "feats_all_layers = [] #list of lists with each element being features from all layers\n",
    "\n",
    "for layer in range(12):\n",
    "    text1_list = []\n",
    "    text2_list = []\n",
    "    sent_feat_list = []\n",
    "    for text in sent_bucket:\n",
    "        sent_feat_list.append(getBERTFeatures(model, text, attn_head_idx=layer))\n",
    "    feats_all_layers.append(sent_feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kp_dist_all_layers = []\n",
    "cosine_dist_all_layers = []\n",
    "\n",
    "for sent_feat_list in feats_all_layers:\n",
    "\n",
    "    kp_dist_list = []\n",
    "    cs_dist_list = []\n",
    "    text1_list = []\n",
    "    text2_list = []\n",
    "\n",
    "    for i in range(len(sent_bucket)):\n",
    "        sent1 = sent_bucket[i]\n",
    "        for j in range(i+1,len(sent_bucket)):\n",
    "            sent2 = sent_bucket[j]\n",
    "            text1_list.append(sent1)\n",
    "            text2_list.append(sent2)\n",
    "            tup1 = sent_feat_list[i]\n",
    "            tup2 = sent_feat_list[j]\n",
    "            #pos_dist_list.append(getPOSBasedSimilarity_loop(tup1, tup2, sent1, sent2))\n",
    "            kp_dist_list.append(getKPBasedSimilarity_loop(tup1, tup2,sent1, sent2))\n",
    "            cs_dist_list.append(getCosineSimilarity_loop(tup1, tup2))\n",
    "    print()\n",
    "    kp_dist_all_layers.append(kp_dist_list)\n",
    "    cosine_dist_all_layers.append(cs_dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#put all metrics into single dataframe\n",
    "score_df = pd.DataFrame({'Sent1': text1_list,\n",
    "                        'Sent2': text2_list,\n",
    "                        'KP_Similarity_layer0': kp_dist_all_layers[0],\n",
    "                         'KP_Similarity_layer1':kp_dist_all_layers[1],\n",
    "                         'KP_Similarity_layer2':kp_dist_all_layers[2],\n",
    "                         'KP_Similarity_layer3':kp_dist_all_layers[3],\n",
    "                         'KP_Similarity_layer4':kp_dist_all_layers[4],\n",
    "                         'KP_Similarity_layer5':kp_dist_all_layers[5],\n",
    "                         'KP_Similarity_layer6':kp_dist_all_layers[6],\n",
    "                         'KP_Similarity_layer7':kp_dist_all_layers[7],\n",
    "                         'KP_Similarity_layer8':kp_dist_all_layers[8],\n",
    "                         'KP_Similarity_layer9':kp_dist_all_layers[9],\n",
    "                         'KP_Similarity_layer10':kp_dist_all_layers[10],\n",
    "                         'KP_Similarity_layer11':kp_dist_all_layers[11],\n",
    "                        'Cos_Similarity_layer0': cosine_dist_all_layers[0],\n",
    "                        'Cos_Similarity_layer1': cosine_dist_all_layers[1],\n",
    "                        'Cos_Similarity_layer2': cosine_dist_all_layers[2],\n",
    "                        'Cos_Similarity_layer3': cosine_dist_all_layers[3],\n",
    "                        'Cos_Similarity_layer4': cosine_dist_all_layers[4],\n",
    "                        'Cos_Similarity_layer5': cosine_dist_all_layers[5],\n",
    "                        'Cos_Similarity_layer6': cosine_dist_all_layers[6],\n",
    "                        'Cos_Similarity_layer7': cosine_dist_all_layers[7],\n",
    "                        'Cos_Similarity_layer8': cosine_dist_all_layers[8],\n",
    "                        'Cos_Similarity_layer9': cosine_dist_all_layers[9],\n",
    "                        'Cos_Similarity_layer10': cosine_dist_all_layers[10],\n",
    "                        'Cos_Similarity_layer11': cosine_dist_all_layers[11]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    feat_name = 'kp_cs_diff_layer'+str(i)\n",
    "    score_df[feat_name] = score_df['Cos_Similarity_layer'+str(i)]-score_df['KP_Similarity_layer'+str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss_df = score_df[['Sent1','Sent2','KP_Similarity_layer11','Cos_Similarity_layer11','kp_cs_diff_layer11']].reset_index()\n",
    "# ss_df = ss_df.sort_values(by='kp_cs_diff_layer11', ascending=False)\n",
    "\n",
    "# plt.plot(t, ss_df['kp_cs_diff_layer11'], 'g', label='delta') # plotting t, a separately \n",
    "# plt.plot(t, ss_df['KP_Similarity_layer11'], 'r', label='keyphrase') # plotting t, b separately \n",
    "# plt.plot(t, ss_df['Cos_Similarity_layer11'], 'b', label='cosine') # plotting t, c separately \n",
    "# plt.legend(loc='upper left')\n",
    "# plt.savefig('/Users/venkat/Downloads/Viz1.png',dpi = 300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = np.linspace(0, 2*math.pi, 630)\n",
    "\n",
    "for i in range(1,13):\n",
    "    print(i-1)\n",
    "    kp_layer = 'KP_Similarity_layer'+str(i-1)\n",
    "    cs_layer = 'Cos_Similarity_layer'+str(i-1)\n",
    "    diff_layer = 'kp_cs_diff_layer'+str(i-1)\n",
    "    \n",
    "    curr_df = score_df[['Sent1','Sent2',kp_layer,cs_layer,diff_layer]]\n",
    "    curr_df = curr_df.sort_values(by=diff_layer, ascending=False)\n",
    "    curr_df = curr_df[curr_df[kp_layer]>0]\n",
    "    t = np.linspace(0, 2*math.pi, len(curr_df))\n",
    "    \n",
    "    plt.plot(t, curr_df[diff_layer], 'g', label='delta') # plotting t, a separately \n",
    "    plt.plot(t, curr_df[kp_layer], 'r', label='keyphrase') # plotting t, b separately \n",
    "    plt.plot(t, curr_df[cs_layer], 'b', label='cosine') # plotting t, c separately \n",
    "    layer = 'Layer-'+str(i)\n",
    "    plt.title(layer)\n",
    "    save_name = '/Users/venkat/Downloads/Viz_KP>0_'+str(i)+'.png'\n",
    "    plt.savefig(save_name,dpi = 300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer = 12\n",
    "kp_layer = 'KP_Similarity_layer'+str(layer-1)\n",
    "cs_layer = 'Cos_Similarity_layer'+str(layer-1)\n",
    "diff_layer = 'kp_cs_diff_layer'+str(layer-1)\n",
    "\n",
    "curr_df = score_df[['Sent1','Sent2',kp_layer,cs_layer,diff_layer]]\n",
    "curr_df = curr_df.sort_values(by=diff_layer, ascending=False)\n",
    "curr_df = curr_df[curr_df[kp_layer]>0]\n",
    "\n",
    "curr_df.sort_values(by=diff_layer, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = 575\n",
    "print(curr_df['Sent1'][key])\n",
    "print()\n",
    "print(curr_df['Sent2'][key])\n",
    "print()\n",
    "print(getKPBasedSimilarity(curr_df['Sent1'][key],curr_df['Sent2'][key],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localtest",
   "language": "python",
   "name": "localtest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
