{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:32:41.652698Z",
     "start_time": "2019-07-30T09:32:32.538206Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertPreTrainingHeads\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "device = 'cpu'\n",
    "import sys\n",
    "import os\n",
    "import iso8601\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:32:45.481211Z",
     "start_time": "2019-07-30T09:32:42.510145Z"
    }
   },
   "outputs": [],
   "source": [
    "from model_loader import ModelLoader\n",
    "from text_utils import *\n",
    "import pickle\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:33:16.534821Z",
     "start_time": "2019-07-30T09:33:07.512588Z"
    }
   },
   "outputs": [],
   "source": [
    "config_path = '../community_detection/data/bert_config.json'\n",
    "mind_path = '../community_detection/data/se_minds_new.pkl'\n",
    "model_path = '../community_detection/data/bert_10epc_se_1e-6_sl40.bin'\n",
    "\n",
    "model_loader = ModelLoader(config_path, model_path)\n",
    "model = model_loader.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T09:33:16.585547Z",
     "start_time": "2019-07-30T09:33:16.558050Z"
    }
   },
   "outputs": [],
   "source": [
    "def getNSPScore(sample_text):\n",
    "    \n",
    "    m = torch.nn.Softmax()\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(sample_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [0]*tokenized_text.index('[SEP]')+[1]*(len(tokenized_text)-tokenized_text.index('[SEP]'))\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    pred_score, seq_rel, seq_out, pool_out = model1(tokens_tensor, segments_tensors)\n",
    "    return m(seq_rel).detach().numpy()[0][0] #returns probability of being next sentence\n",
    "\n",
    "def getSentMatchScore(sent1, sent2, nsp_dampening_factor = 0.7):\n",
    "    \n",
    "    sent1_feats = getBERTFeatures(model1, sent1, attn_head_idx)\n",
    "    sent2_feats = getBERTFeatures(model1, sent2, attn_head_idx)\n",
    "    \n",
    "    cosine_distance = 1- cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    nsp_score = np.mean([nsp_score_1,nsp_score_2])*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    #print (\"nsp score -> \" + str(nsp_score))\n",
    "    #print (\"cosine score -> \" + str(cosine_distance))\n",
    "    return score\n",
    "    \n",
    "\n",
    "def getSentMatchScore_wfeature(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    nsp_input1 = sent1+' [SEP] '+sent2\n",
    "    #nsp_input2 = sent2+' [SEP] '+sent1\n",
    "    \n",
    "    nsp_score_1 = getNSPScore(nsp_input1)\n",
    "    #nsp_score_2 = getNSPScore(nsp_input2)\n",
    "    \n",
    "    nsp_score = nsp_score_1 * nsp_dampening_factor\n",
    "    #nsp_score = nsp_score_1*nsp_dampening_factor\n",
    "    \n",
    "    len_diff = abs(len(sent1.split(' '))-len(sent2.split(' ')))\n",
    "    if len_diff>2*(min(len(sent1.split(' ')),len(sent2.split(' ')))):\n",
    "        #give more weight to nsp if the sentences of largely varying lengths\n",
    "        score = 0.4*cosine_distance+0.6*nsp_score\n",
    "    else:\n",
    "        score = np.mean([cosine_distance,nsp_score])\n",
    "    \n",
    "    return score\n",
    "\n",
    "def getSentMatchScore_wfeature_cosine(sent1, sent2, sent1_feats, sent2_feats, nsp_dampening_factor = 0.7):\n",
    "    cosine_distance = 1-cosine(sent1_feats, sent2_feats)\n",
    "    \n",
    "    return cosine_distance\n",
    "    \n",
    "\n",
    "def getBERTFeatures(model, text, attn_head_idx = -1): #attn_head_idx - index o[]\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    if len(tokenized_text)>200:\n",
    "        tokenized_text = tokenized_text[0:200]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    _, _, seq_out, pool_out = model(tokens_tensor)\n",
    "    seq_out = list(getPooledFeatures(seq_out[attn_head_idx]).T)\n",
    "    #pool_out = list(pool_out.detach().numpy().T)\n",
    "    \n",
    "    return seq_out\n",
    "\n",
    "def getPooledFeatures(np_array):\n",
    "    np_array = np_array.reshape(np_array.shape[1],np_array.shape[2]).detach().numpy()\n",
    "    np_array_mp = np.mean(np_array, axis=0).reshape(1, -1)\n",
    "    return np_array_mp\n",
    "\n",
    "def replaceContractions(text):\n",
    "    #text = text.lower()\n",
    "    c_filt_text = ''\n",
    "    for word in text.split(' '):\n",
    "        if word in contractions:\n",
    "            c_filt_text = c_filt_text+' '+contractions[word]\n",
    "        else:\n",
    "            c_filt_text = c_filt_text+' '+word\n",
    "    return c_filt_text\n",
    "\n",
    "def cleanText(text):\n",
    "        \n",
    "    text = text.replace('\\\\n','')\n",
    "    text = text.replace('\\\\','')\n",
    "    #text = text.replace('\\t', '')\n",
    "    #text = re.sub('\\[(.*?)\\]','',text) #removes [this one]\n",
    "    text = re.sub('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?\\s',\n",
    "                ' __url__ ',text) #remove urls\n",
    "    #text = re.sub('\\'','',text)\n",
    "    #text = re.sub(r'\\d+', ' __number__ ', text) #replaces numbers\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def formatTime(tz_time, datetime_object=False):\n",
    "    isoTime = iso8601.parse_date(tz_time)\n",
    "    ts = isoTime.timestamp()\n",
    "    ts = datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "\n",
    "    if datetime_object:\n",
    "        ts = datetime.fromisoformat(ts)\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load it as csv.\n",
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('../data/ethercast_1.csv', index_col=False, header=0);\n",
    "df = df.sort_values(by='created_at')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
